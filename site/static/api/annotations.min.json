{"version": "1.0", "generated": "2025-11-30T14:23:14.071911", "project": "coin-or-knowledge-base", "description": "Annotated optimization solver library documentation", "layers": {"layer-0": {"name": "layer-0", "library_count": 2, "libraries": {"CoinUtils": {"name": "CoinUtils", "file_count": 68, "pass2_count": 33, "files": {"src/CoinPresolveZeros.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveZeros.hpp", "filename": "CoinPresolveZeros.hpp", "file": "CoinPresolveZeros.hpp", "brief": "Drop and reintroduce explicit zero coefficients", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinBronKerbosch.hpp": {"path": "layer-0/CoinUtils/src/CoinBronKerbosch.hpp", "filename": "CoinBronKerbosch.hpp", "file": "CoinBronKerbosch.hpp", "brief": "Bron-Kerbosch Algorithm for maximal clique enumeration", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Bron-Kerbosch with pivoting for weighted cliques:\n  BK(R, P, X):  R = clique being built, P = candidates, X = excluded\n  1. If P ∪ X = ∅: report R as maximal clique\n  2. Choose pivot u ∈ P ∪ X maximizing |N(u) ∩ P|\n  3. For each v ∈ P \\ N(u):\n     - BK(R ∪ {v}, P ∩ N(v), X ∩ N(v))\n     - P := P \\ {v}; X := X ∪ {v}\n  Pivoting prunes branches that cannot produce new maximal cliques.", "math": "Clique: complete subgraph where all vertices are pairwise adjacent.\n  Weight threshold: only enumerate cliques with Σw(v) > minWeight.\n  Pivoting strategies: degree, weight, or modified degree×weight.", "complexity": "Time: O(3^(n/3)) worst case for all maximal cliques\n  With pivoting: much faster in practice, especially for sparse graphs.\n  Space: O(n) for recursion stack", "ref": ["Bron, Kerbosch (1973). \"Algorithm 457: Finding all cliques of an\n     undirected graph\". CACM 16(9):575-577.", "Tomita, Tanaka, Takahashi (2006). \"The worst-case time complexity\n     for generating all maximal cliques\". Theoretical Computer Science.\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["cgraph conflict graph", "weights array containing the weights for each vertex", "pivotingStrategy pivoting strategy used in BK algorithm.", "array containing the weights for each vertex"], "has_pass2": true}, "src/CoinCliqueList.hpp": {"path": "layer-0/CoinUtils/src/CoinCliqueList.hpp", "filename": "CoinCliqueList.hpp", "file": "CoinCliqueList.hpp", "brief": "Sequential storage for cliques found in conflict graphs", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Clique Storage:\n  Stores cliques as variable-length arrays with shared element pool\n  Operations:\n  - addClique(size, elements[]): append new clique\n  - cliqueElements(idx): retrieve clique by index\n  - computeNodeOccurrences: build inverse index (node → cliques)\n\n  Inverse index enables efficient lookup of which cliques contain a node", "math": "Clique C ⊆ V is complete subgraph: ∀u,v ∈ C, (u,v) ∈ E\n  Used in conflict graphs where edges represent incompatibility\n  Clique inequality: Σ_{i∈C} x_i ≤ 1 (at most one var in clique can be 1)", "complexity": "addClique: O(clique_size) for copying elements\n  cliqueElements/cliqueSize: O(1) random access\n  computeNodeOccurrences: O(total_elements) one-time setup\n  nNodeOccurrences: O(1) after setup", "see": ["CoinBronKerbosch for clique enumeration algorithm", "CoinConflictGraph for the underlying conflict structure\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["_iniClqCap initial capacity to store cliques", "_iniClqElCap initial capacity of the elements\nof the cliques", "idxClq index of a clique.", "idxClq index of a clique.", "idxNode index of the node", "idxNode index of the node", "cgraph conflict graph", "idxs indexes of the clique", "size size of the clique"], "has_pass2": true}, "src/CoinPresolveSubst.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveSubst.hpp", "filename": "CoinPresolveSubst.hpp", "file": "CoinPresolveSubst.hpp", "brief": "Variable substitution from equality constraints", "algorithm": "Variable Substitution (Non-Singleton):\n  For implied free variable x_j in equality constraint i:\n  1. Solve constraint i for x_j: x_j = (b_i - Σ_{k≠j} a_ik x_k) / a_ij\n  2. Substitute x_j expression into all other rows containing x_j\n  3. After substitution, x_j becomes column singleton\n  4. Apply implied_free_action to remove x_j and row i\n\n  Fill-in control: only substitute if total nnz doesn't increase too much", "math": "Substitution creates fill-in: originally empty positions become nonzero\n  Fill estimate: (nnz(col_j) - 1) × (nnz(row_i) - 1) new elements\n  Controlled by fill_level parameter to prevent explosion", "complexity": "Time: O(nnz(col_j) × nnz(row_i)) per substitution\n  Can significantly reduce problem when fill-in is controlled\n  Postsolve must undo all substitutions in reverse order", "see": ["CoinPresolveImpliedFree for singleton case", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinDynamicConflictGraph.hpp": {"path": "layer-0/CoinUtils/src/CoinDynamicConflictGraph.hpp", "filename": "CoinDynamicConflictGraph.hpp", "file": "CoinDynamicConflictGraph.hpp", "brief": "CoinConflictGraph implementation which supports modifications.", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["_size number of vertices of the\nconflict graph", "numCols number of variables", "colType column types", "colLB column lower bounds", "colUB column upper bounds", "matrixByRow row-wise constraint matrix", "sense row sense", "rowRHS row right hand side", "rowRange row ranges", "node index", "nodeConflicts conflicts to be added", "nConflicts number of conflicts to be added", "idxNode index of the node", "deg degree of the node", "idxNode index of the node", "deg degree of the node"], "return": "a vector of updated bounds with the format (idx, (lb, ub))", "has_pass2": false}, "src/CoinDistance.hpp": {"path": "layer-0/CoinUtils/src/CoinDistance.hpp", "filename": "CoinDistance.hpp", "file": "CoinDistance.hpp", "brief": "Platform-independent iterator distance utilities\n\nProvides wrapper functions around std::distance to ensure consistent\nbehavior across different compilers and platforms. These utilities are\nused throughout COIN-OR for iterator-based calculations.", "tparam": ["ForwardIterator Iterator type (must satisfy ForwardIterator requirements)", "Distance Integral type to store the result", "ForwardIterator Iterator type (must satisfy ForwardIterator requirements)"], "param": ["first Iterator to the beginning of the range", "last Iterator to the end of the range", "first Iterator to the beginning of the range", "last Iterator to the end of the range"], "see": ["coinDistance(ForwardIterator, ForwardIterator)", "coinDistance(ForwardIterator, ForwardIterator, Distance&)"], "return": "Number of elements between first and last", "has_pass2": false}, "src/CoinUtility.hpp": {"path": "layer-0/CoinUtils/src/CoinUtility.hpp", "filename": "CoinUtility.hpp", "file": "CoinUtility.hpp", "brief": "Factory functions for CoinPair and CoinTriple\n\nProvides convenience functions for creating CoinPair and CoinTriple objects\nwith automatic template argument deduction, similar to std::make_pair.", "see": ["CoinSort.hpp for CoinPair and CoinTriple definitions", "CoinPair, CoinMakeTriple()", "CoinTriple, CoinMakePair()"], "tparam": ["S Type of the first element", "T Type of the second element", "S Type of the first element", "T Type of the second element", "U Type of the third element"], "param": ["s First element value", "t Second element value", "s First element value", "t Second element value", "u Third element value"], "return": "A CoinPair containing copies of s and t", "has_pass2": false}, "src/CoinCliqueSet.hpp": {"path": "layer-0/CoinUtils/src/CoinCliqueSet.hpp", "filename": "CoinCliqueSet.hpp", "file": "CoinCliqueSet.hpp", "brief": "Set of cliques", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["_iniClqCap initial capacity to store cliques", "_iniClqElCap initial capacity of the elements\nof the cliques", "size size of the clique to be added", "els indexes of the clique to be added", "size size of the clique", "els indexes of the clique", "size size of the clique", "els indexes of the clique", "hashCode hash value of\nthe clique (call method vectorHashCode)"], "has_pass2": false}, "src/CoinLpIO.hpp": {"path": "layer-0/CoinUtils/src/CoinLpIO.hpp", "filename": "CoinLpIO.hpp", "file": "CoinLpIO.hpp", "brief": "LP file format reader/writer for linear programming problems\n\nReads/writes CPLEX LP format with Min/Max objective, constraints,\nbounds, and integer/binary variable declarations.", "algorithm": "LP File Parsing:\n  Sections: Minimize/Maximize, Subject To, Bounds, Integers/Generals/Binaries, End\n  Algebraic format: constraint_name: coeff1*var1 + coeff2*var2 <= rhs\n\n  Constraint types: <= (L), >= (G), = (E)\n  Bound keywords: Free, Inf, -Inf\n  Variable types: Integers, Generals (same), Binaries, Semi-Continuous", "math": "Reads: min/max c'x s.t. Ax ≤ b, Ax ≥ b, Ax = b\n  Bounds: lb ≤ x ≤ ub or x free\n  Integrality: x ∈ Z or x ∈ {0,1}", "complexity": "Parsing: O(file_size), token-based lexer\n  More human-readable than MPS but less standardized", "see": ["CoinMpsIO for MPS file format support", "CoinFileIO for underlying file I/O abstraction"], "has_pass2": true}, "src/CoinPragma.hpp": {"path": "layer-0/CoinUtils/src/CoinPragma.hpp", "filename": "CoinPragma.hpp", "file": "CoinPragma.hpp", "brief": "Compiler-specific pragma settings (mainly MSVC warnings)", "has_pass2": false}, "src/CoinNodeHeap.hpp": {"path": "layer-0/CoinUtils/src/CoinNodeHeap.hpp", "filename": "CoinNodeHeap.hpp", "file": "CoinNodeHeap.hpp", "brief": "Monotone min-heap for Dijkstra's algorithm", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Monotone Min-Heap:\n  Binary heap with decreasing-key only (no arbitrary updates).\n  Heap property: parent.cost ≤ children.cost\n  Operations:\n  - update(node, newCost): decrease key, bubble up (newCost < oldCost)\n  - removeFirst(): extract minimum, bubble down replacement\n  - isEmpty(): check if all nodes at infinity", "math": "Used in Dijkstra where distances only decrease.\n  Monotonicity allows simpler implementation than general heap.\n  Position array enables O(1) node lookup for decrease-key.", "complexity": "update: O(log n) for bubble-up\n  removeFirst: O(log n) for bubble-down\n  isEmpty: O(1)\n  Space: O(n) for heap + position array", "see": ["CoinShortestPath for Dijkstra implementation using this heap\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["node used to store the element that was removed"], "has_pass2": true}, "src/CoinPresolveFixed.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveFixed.hpp", "filename": "CoinPresolveFixed.hpp", "file": "CoinPresolveFixed.hpp", "brief": "Remove fixed variables and make variables fixed", "algorithm": "Fixed Variable Removal:\n  For variable x_j with l_j = u_j = v (fixed):\n  1. Substitute x_j = v into all constraints: b_i := b_i - a_ij·v\n  2. Remove column j from constraint matrix\n  3. Adjust objective constant: z += c_j·v\n  4. Store column for postsolve restoration\n\n  make_fixed_action: Force bounds equal to fix variable\n  remove_fixed_action: Remove already-fixed variable from matrix", "math": "Substitution: Ax = b with x_j = v becomes A'x' = b - a_j·v\n  where A' is A without column j, x' is x without x_j\n  Postsolve: restore x_j = v in solution", "complexity": "Time: O(nnz(column_j)) per fixed variable\n  Reduces problem dimension, may create new singletons or empty rows\n  Very common in practical models (constants, derived bounds)", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinMessageHandler.hpp": {"path": "layer-0/CoinUtils/src/CoinMessageHandler.hpp", "filename": "CoinMessageHandler.hpp", "file": "CoinMessageHandler.hpp", "brief": "Flexible message handling with severity levels and i18n support\n\nProvides CoinMessageHandler for formatted output with:\n- Severity-based filtering (warnings, errors, info)\n- Detail/logging levels\n- Multi-language support (messages default to US English)\n- Output to stdout or FILE pointer", "algorithm": "Message Handling System:\n  Message construction: printf-style formatting with type safety\n  Severity levels: 0=nothing, 1=fatal, 2=error, 3=warning, 4+=info\n  Detail filtering: only print if message detail ≤ handler logLevel\n\n  Output chain: CoinMessageHandler << value1 << value2 << CoinMessageEol\n  Supports: int, double, char, string, and custom precision", "complexity": "Message lookup: O(1) by message number\n  Formatting: O(message_length)\n  Filtering: O(1) severity/detail check before formatting", "see": ["CoinMessage for standard COIN-OR message definitions", "CoinOneMessage for individual message container"], "has_pass2": true}, "src/CoinAlloc.hpp": {"path": "layer-0/CoinUtils/src/CoinAlloc.hpp", "filename": "CoinAlloc.hpp", "file": "CoinAlloc.hpp", "brief": "Memory pool allocator for small fixed-size blocks\n\nOptional memory pool that reduces malloc overhead for small allocations.\nConfigure with COINUTILS_MEMPOOL_MAXPOOLED and COINUTILS_MEMPOOL_ALIGNMENT.", "has_pass2": false}, "src/CoinWarmStart.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStart.hpp", "filename": "CoinWarmStart.hpp", "file": "CoinWarmStart.hpp", "brief": "Abstract interfaces for warm start information in optimization solvers\n\nDefines abstract base classes for storing and manipulating warm start\ninformation. Warm starts allow solvers to resume from a previous solution\nstate, potentially reducing solution time significantly. The diff mechanism\nallows efficient incremental updates to warm start information.\n\nDerived classes implement solver-specific warm start data (e.g., basis\nstatus for simplex, barrier iterates for interior point methods).", "see": ["CoinWarmStartBasis for a common implementation (simplex basis)", "CoinWarmStartDiff, CoinWarmStartBasis", "applyDiff()", "generateDiff()", "CoinWarmStart::generateDiff(), CoinWarmStart::applyDiff()"], "return": "Pointer to newly allocated copy (caller owns memory)", "param": ["oldWarmStart The baseline warm start to compare against", "diff The diff to apply"], "has_pass2": false}, "src/CoinTime.hpp": {"path": "layer-0/CoinUtils/src/CoinTime.hpp", "filename": "CoinTime.hpp", "file": "CoinTime.hpp", "brief": "Cross-platform timing utilities (CPU and wall clock)\n\nProvides CoinCpuTime() and CoinWallclockTime() that work on\nWindows, Mac, and Unix/Linux platforms.", "has_pass2": false}, "src/CoinSimpFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinSimpFactorization.hpp", "filename": "CoinSimpFactorization.hpp", "file": "CoinSimpFactorization.hpp", "brief": "Simple LU factorization for LP basis matrices\n\nStraightforward LU factorization implementation. Less optimized than\nCoinFactorization but simpler and useful as reference implementation.", "algorithm": "Sparse LU with Markowitz Pivoting:\n  Computes PA = LU minimizing fill-in during elimination.\n  1. Find pivot (r,s) minimizing Markowitz count: (nnz_row - 1)(nnz_col - 1)\n  2. Swap row r to current position, column s to current position\n  3. Perform Gaussian elimination on remaining submatrix\n  4. Store L factors by row and column, U factors by row and column\n  Updates via eta vectors (product form of inverse):\n  B_{k+1}^{-1} = E_k * B_k^{-1} where E_k is eta matrix", "math": "LU factorization: PA = LU, L unit lower triangular, U upper triangular\n  FTRAN: solve Bx = b via Ly = Pb, Ux = y\n  BTRAN: solve B'x = b via U'y = b, L'z = y, x = P'z\n  Update: B_new = B_old + (a_q - B*e_p)*e_p' handled by eta vectors", "complexity": "Factorization: O(n³) worst, typically O(n·nnz) with good pivoting\n  FTRAN/BTRAN: O(nnz(L) + nnz(U)) per solve\n  Update: O(n) per eta vector, refactorize after ~100 pivots", "see": ["CoinOtherFactorization for the base class", "CoinFactorization for the optimized sparse implementation"], "has_pass2": true}, "src/CoinFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinFactorization.hpp", "filename": "CoinFactorization.hpp", "file": "CoinFactorization.hpp", "brief": "LU factorization of sparse basis matrix for simplex\n\nImplements LU factorization with hyper-sparse handling for efficient\nFTRAN/BTRAN operations. Supports rank-one updates during pivoting.", "algorithm": "LU factorization with Markowitz pivot selection", "math": "Decomposes basis matrix B = L·U where:\n      - L is lower triangular (stored by columns, implicit unit diagonal)\n      - U is upper triangular (stored by columns)\n      FTRAN solves Bx = b via: Ly = b, then Ux = y\n      BTRAN solves B'y = c via: U'z = c, then L'y = z", "complexity": "Factorization: O(nnz * fill) where fill depends on pivot order\n            FTRAN/BTRAN: O(nnz(L) + nnz(U)) per solve\n            Rank-one update: O(nnz(column)) via Forrest-Tomlin", "ref": ["Forrest, Tomlin (1972). \"Updated Triangular Factors of the Basis\n     to Maintain Sparsity in the Product Form Simplex Method\".\n     Mathematical Programming 2:263-278.", "Suhl, Suhl (1990). \"Computing Sparse LU Factorizations for\n     Large-Scale Linear Programming Bases\". ORSA J. Computing 2:325-335.", "Forrest, Tomlin (1972). Mathematical Programming 2:263-278."], "author": "John Forrest", "see": ["CoinDenseFactorization for dense matrices", "CoinOslFactorization for OSL-derived implementation", "forrestTomlin() to check which update method is active", "updateColumnTranspose for BTRAN (B'y = c)", "updateColumnFT for FTRAN (Bx = b)"], "param": ["matrix Full constraint matrix A", "rowIsBasic Array marking basic rows (>=0 means basic)", "columnIsBasic Array marking basic columns (>=0 means basic)", "areaFactor Multiplier for memory allocation (0 = auto)", "regionSparse The column entering the basis (after FTRAN)", "pivotRow Row index of the leaving variable", "pivotCheck Expected pivot value for accuracy verification", "checkBeforeModifying If true, validate before modifying factors", "acceptablePivot Minimum acceptable |pivot| for stability", "regionSparse Work vector (must start as zero, returns zero)", "regionSparse2 Right-hand side b on input, solution x on output", "regionSparse Work vector (must start as zero, returns zero)", "regionSparse2 Right-hand side c on input, solution y on output"], "return": "0 = success, -1 = singular, -2 = too many basic, -99 = memory", "has_pass2": true}, "src/CoinPresolveMonitor.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveMonitor.hpp", "filename": "CoinPresolveMonitor.hpp", "file": "CoinPresolveMonitor.hpp", "brief": "Monitor rows/columns for debugging presolve transforms", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinMpsIO.hpp": {"path": "layer-0/CoinUtils/src/CoinMpsIO.hpp", "filename": "CoinMpsIO.hpp", "file": "CoinMpsIO.hpp", "brief": "MPS file format reader/writer for LP and MIP problems\n\nReads/writes standard MPS format including extensions for quadratic,\nconic, and SOS constraints. Supports free format and compression.", "algorithm": "MPS File Parsing:\n  Sections: NAME, ROWS, COLUMNS, RHS, RANGES, BOUNDS, ENDATA\n  Fixed format: columns 1-2 (type), 5-12 (name1), 15-22 (name2), etc.\n  Free format: whitespace-separated fields\n\n  Row types: N (free/objective), E (equality), L (≤), G (≥)\n  Bound types: LO, UP, FX, FR, MI, PL, BV (binary), UI/LI (integer)\n  Extensions: QUADOBJ (Hessian), SOS (special ordered sets)", "math": "Reads: min c'x s.t. l ≤ Ax ≤ u, lb ≤ x ≤ ub\n  RANGES section: row_i has both lower and upper when range specified\n  Integer markers: 'MARKER' 'INTORG'/'INTEND' for integer variables", "complexity": "Parsing: O(file_size), single pass with hash tables for names\n  Supports gzip/bzip2 compression via CoinFileIO abstraction", "see": ["CoinLpIO for LP file format support", "CoinFileIO for underlying file I/O abstraction"], "has_pass2": true}, "src/CoinFinite.hpp": {"path": "layer-0/CoinUtils/src/CoinFinite.hpp", "filename": "CoinFinite.hpp", "file": "CoinFinite.hpp", "brief": "Numeric limit constants and floating-point validation functions\n\nProvides platform-independent definitions for numeric limits (COIN_DBL_MAX,\nCOIN_INT_MAX, etc.) and functions to check for special floating-point values\n(infinity, NaN). These are fundamental utilities used throughout COIN-OR\nfor bounds checking and numerical validation.", "see": ["CoinIsnan(), CoinFinite()", "CoinIsnan()", "CoinFinite()"], "param": ["val The value to test", "val The value to test"], "return": "true if val is a finite number, false if infinity or NaN", "has_pass2": false}, "src/CoinModelUseful.hpp": {"path": "layer-0/CoinUtils/src/CoinModelUseful.hpp", "filename": "CoinModelUseful.hpp", "file": "CoinModelUseful.hpp", "brief": "Helper classes for CoinModel (Link, LinkedList, Hash)", "see": ["CoinModel for the main model building interface"], "has_pass2": false}, "src/CoinBuild.hpp": {"path": "layer-0/CoinUtils/src/CoinBuild.hpp", "filename": "CoinBuild.hpp", "file": "CoinBuild.hpp", "brief": "Efficient row-by-row or column-by-column model construction\n\nCoinBuild accumulates rows or columns for batch addition to a model.\nMore efficient than adding one row/column at a time to CoinPackedMatrix.", "see": ["CoinModel for flexible model building with names and expressions", "CoinPackedMatrix for the final sparse matrix format"], "has_pass2": false}, "src/CoinPackedMatrix.hpp": {"path": "layer-0/CoinUtils/src/CoinPackedMatrix.hpp", "filename": "CoinPackedMatrix.hpp", "file": "CoinPackedMatrix.hpp", "brief": "Sparse matrix stored in compressed row or column format\n\nCoinPackedMatrix represents a sparse matrix using compressed storage.\nCan be stored row-major or column-major. Efficient for major-dimension\noperations (accessing rows in row-major, columns in column-major).", "algorithm": "Compressed Sparse Column/Row (CSC/CSR) format", "math": "Storage arrays for m×n matrix with nnz nonzeros:\n      - elements[nnz]: coefficient values\n      - indices[nnz]: minor indices (row indices for CSC, column for CSR)\n      - starts[major+1]: start position of each major vector\n      Memory: O(nnz + major) vs O(m·n) for dense", "complexity": "Major vector access: O(1) to get start, O(nnz_j) to iterate\n            Minor vector access: O(nnz) full scan required\n            Matrix-vector multiply: O(nnz)\n\n@note Column-major (CSC) is standard for LP constraint matrices since\n      simplex primarily accesses columns (entering variables).", "see": ["CoinPackedVector for sparse vector storage", "CoinShallowPackedVector for row/column views"], "has_pass2": true}, "src/CoinPresolveMatrix.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveMatrix.hpp", "filename": "CoinPresolveMatrix.hpp", "file": "CoinPresolveMatrix.hpp", "brief": "Core presolve data structures and action base class\n\nDefines CoinPrePostsolveMatrix (common base), CoinPresolveMatrix (for\npresolve), CoinPostsolveMatrix (for postsolve), and CoinPresolveAction\n(base class for all presolve transformations).", "algorithm": "LP Presolve - simplifies LP before solving via reversible transforms", "math": "Presolve applies a sequence of transformations T_k to original LP:\n      min c'x s.t. Ax = b, l <= x <= u  -->  min c'x' s.t. A'x' = b', l' <= x' <= u'\n      Each T_k is recorded to enable postsolve recovery of original solution.\n\n@note Presolve typically reduces problem size 30-90%, dramatically speeding solve", "ref": ["Andersen, E., Andersen, K. (1995). \"Presolving in Linear Programming\".\n     Mathematical Programming 71:221-245."], "see": ["CoinPresolveDoubleton, CoinPresolveForcing, etc. for specific transforms"], "has_pass2": true}, "src/CoinWarmStartVector.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartVector.hpp", "filename": "CoinWarmStartVector.hpp", "file": "CoinWarmStartVector.hpp", "brief": "Template warm start storing a single typed vector", "tparam": ["T Element type (typically double)\n\nGeneric warm start class that stores a vector of values.\nUsed as building block for CoinWarmStartDual and CoinWarmStartPrimalDual."], "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartDual which uses this for dual vector storage"], "has_pass2": false}, "src/CoinShallowPackedVector.hpp": {"path": "layer-0/CoinUtils/src/CoinShallowPackedVector.hpp", "filename": "CoinShallowPackedVector.hpp", "file": "CoinShallowPackedVector.hpp", "brief": "Non-owning sparse vector reference (view into external storage)\n\nCoinShallowPackedVector provides read-only access to sparse vector data\nstored elsewhere. It maintains only pointers, not copies. Use when you\nneed a lightweight view into a row/column of a CoinPackedMatrix.", "see": ["CoinPackedVectorBase for the interface", "CoinPackedVector for an owning sparse vector"], "has_pass2": false}, "src/CoinSmartPtr.hpp": {"path": "layer-0/CoinUtils/src/CoinSmartPtr.hpp", "filename": "CoinSmartPtr.hpp", "file": "CoinSmartPtr.hpp", "brief": "Intrusive reference-counted smart pointer", "author": "Carl Laird, Andreas Waechter (IBM)\n\nProvides Coin::SmartPtr<T> for automatic memory management.\nObjects must inherit from Coin::ReferencedObject.", "has_pass2": false}, "src/CoinRational.hpp": {"path": "layer-0/CoinUtils/src/CoinRational.hpp", "filename": "CoinRational.hpp", "file": "CoinRational.hpp", "brief": "Rational number representation with double-to-rational conversion\n\nProvides a simple rational number class that can convert floating-point\nvalues to rational approximations. Useful in optimization for converting\nfloating-point coefficients to exact rational form when needed.", "return": "The denominator value", "param": ["n Numerator", "d Denominator (must not be zero)", "val The floating-point value to approximate", "maxdelta Maximum allowed absolute error", "maxdnom Maximum allowed denominator", "val Value to approximate", "maxdelta Maximum error tolerance", "maxdnom Maximum denominator allowed"], "algorithm": "Stern-Brocot tree / mediant search for best rational approximation", "math": "Given lower bound a/b and upper bound c/d, the mediant (a+c)/(b+d)\n      is the simplest rational between them. Binary search narrows to\n      the best approximation within tolerance.", "complexity": "O(log(maxdnom)) iterations", "ref": ["Stern (1858), Brocot (1861). See also: Graham, Knuth, Patashnik\n     \"Concrete Mathematics\" Ch. 4.5 for Stern-Brocot tree theory."], "has_pass2": true}, "src/CoinSignal.hpp": {"path": "layer-0/CoinUtils/src/CoinSignal.hpp", "filename": "CoinSignal.hpp", "file": "CoinSignal.hpp", "brief": "Cross-platform signal handler typedef (CoinSighandler_t)", "has_pass2": false}, "src/CoinPresolveTripleton.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveTripleton.hpp", "filename": "CoinPresolveTripleton.hpp", "file": "CoinPresolveTripleton.hpp", "brief": "Tripleton row presolve: three-variable equation substitution", "algorithm": "Tripleton Elimination:\n  Given equality constraint ax + by + cz = d with three variables:\n  1. Solve for y: y = (d - ax - cz) / b\n  2. Substitute y into objective and all constraints containing y\n  3. Eliminate the constraint row and variable y\n  Only applied if substitution doesn't increase total nonzeros", "math": "Original: ax + by + cz = d\n  Substitution: y = (d - ax - cz) / b\n  Objective: c_y·y = c_y·d/b - (c_y·a/b)x - (c_y·c/b)z\n  Postsolve recovers y from optimal (x*, z*)", "complexity": "Time: O(nnz(col_y) × avg_row_length) per tripleton\n  Fill-in check prevents growth: only apply if beneficial\n  Less common than doubleton but can still reduce problem size", "see": ["CoinPresolveDoubleton for two-variable case", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinPackedVectorBase.hpp": {"path": "layer-0/CoinUtils/src/CoinPackedVectorBase.hpp", "filename": "CoinPackedVectorBase.hpp", "file": "CoinPackedVectorBase.hpp", "brief": "Abstract base class for read-only sparse vector access\n\nProvides the read-only interface for sparse vectors stored as parallel\nindex/value arrays. This is the base class for CoinPackedVector (owning)\nand CoinShallowPackedVector (non-owning reference).", "see": ["CoinPackedVector for a modifiable sparse vector", "CoinShallowPackedVector for a lightweight view"], "return": "Count of index/value pairs", "param": ["denseSize Length of the dense vector to create", "i Index in the conceptual dense vector", "methodName Name of calling method (for error message)", "className Name of calling class (for error message)", "i Index to search for", "i Index to search for", "dense Pointer to dense vector (must have length >= max index + 1)"], "has_pass2": false}, "src/CoinDenseVector.hpp": {"path": "layer-0/CoinUtils/src/CoinDenseVector.hpp", "filename": "CoinDenseVector.hpp", "file": "CoinDenseVector.hpp", "brief": "Dense vector template with element-wise arithmetic operations\n\nCoinDenseVector<T> stores all elements in a contiguous array.\nUnlike sparse vectors, every position has storage. Supports\narithmetic operators (+, -, *, /) with other dense vectors.", "tparam": ["T Element type (typically double, float, or int)"], "algorithm": "Dense Vector Storage:\n  Contiguous array of size n: elements_[0..n-1]\n  All positions allocated, O(1) random access\n  Element-wise operations: v3 = v1 + v2 applies to all indices\n\n  Supported operations: +, -, *, / (element-wise)\n  Norms: oneNorm (L1), twoNorm (L2/Euclidean), infNorm (L∞/max)", "math": "Dense storage: x[i] stored for all i ∈ {0,...,n-1}\n  oneNorm = Σ|x_i|, twoNorm = √(Σx_i²), infNorm = max|x_i|", "complexity": "Element access: O(1)\n  Vector operations: O(n)\n  Space: O(n) always, regardless of sparsity", "see": ["CoinPackedVector for sparse storage", "CoinIndexedVector for sparse with dense backing"], "has_pass2": true}, "src/CoinPresolveIsolated.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveIsolated.hpp", "filename": "CoinPresolveIsolated.hpp", "file": "CoinPresolveIsolated.hpp", "brief": "Handle isolated constraints (connected only by free variables)", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinConflictGraph.hpp": {"path": "layer-0/CoinUtils/src/CoinConflictGraph.hpp", "filename": "CoinConflictGraph.hpp", "file": "CoinConflictGraph.hpp", "brief": "Conflict graph for binary variable incompatibilities in MIP", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Conflict Graph Construction:\n  For binary variables x_i ∈ {0,1}, build graph G where:\n  - Nodes: original variables x_i and complements x̄_i = 1 - x_i\n  - Edges: (u,v) if u=1 and v=1 simultaneously infeasible\n  Sources of conflicts:\n  1. Set packing constraints: x_i + x_j ≤ 1 → edge (x_i, x_j)\n  2. Variable bounds: x_i ≤ x_j → edge (x_i, x̄_j)\n  3. Clique constraints: Σx_i ≤ 1 → complete subgraph", "math": "Independent set in conflict graph ↔ feasible partial assignment.\n  Clique C in conflict graph → valid inequality Σ_{i∈C} x_i ≤ 1.\n  Maximum clique gives strongest clique cut.", "complexity": "Space: O(n²) worst case, but typically sparse O(n + m)\n  Conflict check: O(1) with adjacency matrix, O(degree) with lists\n  Clique storage: large cliques stored explicitly to save space", "see": ["CoinBronKerbosch for clique enumeration", "CoinCliqueList for clique storage\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["_cols number of columns in the mixed-integer\nlinear program. The number of elements in the conflict\ngraph will be _cols*2 (it consider complementary variables)", "other conflict graph to be copied", "n1 node index", "n2 node index", "node node index", "temp temporary storage area for storing conflicts,\nshould have space for all elements in the graph (size())", "iv auxiliary incidence array used to eliminate\nduplicates. It should have the size of the graph (size())\nand all elements shoud be initialized as false.", "idxNode index of the node", "deg degree of the node", "idxNode index of the node", "mdegree modified degree of the node", "other conflict graph to be copied"], "return": "true if there is an edge between\nn1 and n2 in the conflict graph, 0 otherwise.", "has_pass2": true}, "src/CoinSnapshot.hpp": {"path": "layer-0/CoinUtils/src/CoinSnapshot.hpp", "filename": "CoinSnapshot.hpp", "file": "CoinSnapshot.hpp", "brief": "Lightweight read-only snapshot of problem state at a B&B node\n\nCoinSnapshot captures problem data for cut generators and branching code.\nMay own or reference external arrays (see owned_ flag). Designed for\npassing problem state without copying full solver structures.", "see": ["CoinPackedMatrix for the constraint matrix format"], "has_pass2": false}, "src/CoinPresolveDoubleton.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveDoubleton.hpp", "filename": "CoinPresolveDoubleton.hpp", "file": "CoinPresolveDoubleton.hpp", "brief": "Doubleton row presolve: substitute y from ax+by=c", "algorithm": "Doubleton Elimination:\n  Given equality constraint ax + by = c with two variables:\n  1. Solve for y: y = (c - ax) / b\n  2. Substitute y in objective: c_x x + c_y y → c_x x + c_y(c-ax)/b\n  3. Substitute y in all constraints containing y\n  4. Transfer bounds: l_y ≤ y ≤ u_y becomes bounds on x\n  5. Remove row and column y from problem", "math": "Original: min c_x·x + c_y·y s.t. ax + by = c, bounds\n  After: min (c_x - c_y·a/b)x + c_y·c/b, modified bounds on x\n  Postsolve recovers y = (c - ax)/b from optimal x*", "complexity": "Time: O(nnz(col_y)) per doubleton - updating all rows with y\n  Typically reduces problem size significantly when equality rows exist\n  Cascading effect: may create new singletons or doubletons", "see": ["CoinPresolveMatrix for the presolve framework", "CoinPresolveTripleton for three-variable equations", "CoinPresolveSingleton for single-variable equations"], "has_pass2": true}, "src/CoinOslFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinOslFactorization.hpp", "filename": "CoinOslFactorization.hpp", "file": "CoinOslFactorization.hpp", "brief": "LU factorization derived from IBM OSL (Optimization Subroutine Library)\n\nPort of OSL's factorization code. Provides alternative to CoinFactorization\nwith different numerical characteristics.", "author": "John Forrest (original OSL code from IBM)", "see": ["CoinOtherFactorization for the base class", "CoinFactorization for the primary sparse implementation"], "has_pass2": false}, "src/CoinPresolveDual.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveDual.hpp", "filename": "CoinPresolveDual.hpp", "file": "CoinPresolveDual.hpp", "brief": "Fix variables using dual bounds and reduced cost analysis", "algorithm": "Dual Bound Propagation:\n  Use dual feasibility to fix primal variables:\n  1. For slacks s_i: d_{n+i} = -y_i, bound y_i from slack bounds\n  2. For singletons x_j: d_j = c_j - y_i·a_ij, bound y_i from d_j sign\n  3. Propagate bounds through reduced cost equation d_j = c_j - y'a_j\n  4. If d_j > 0 at optimality: x_j at lower bound (minimization)\n  5. If d_j < 0 at optimality: x_j at upper bound", "math": "Reduced cost: d_j = c_j - Σ_i y_i a_ij\n  Dual feasibility (min): d_j ≥ 0 if x_j at lower, d_j ≤ 0 if at upper\n  Bound propagation: y_i^L ≤ y_i ≤ y_i^U → d_j^L ≤ d_j ≤ d_j^U", "complexity": "Time: O(iterations × nnz) for bound propagation\n  Very powerful for detecting fixed variables without solving LP\n  Can prove unboundedness when required bound is missing", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinIndexedVector.hpp": {"path": "layer-0/CoinUtils/src/CoinIndexedVector.hpp", "filename": "CoinIndexedVector.hpp", "file": "CoinIndexedVector.hpp", "brief": "Sparse vector with dense backing array for O(1) element access\n\nCoinIndexedVector combines sparse index storage with a dense values array,\nenabling O(1) random access while tracking which positions are non-zero.\nDesigned for simplex operations where sparse updates need fast access.\nHas optional \"packed\" mode that behaves more like CoinPackedVector.", "algorithm": "Indexed sparse vector - hybrid sparse/dense storage", "math": "Storage for n-dimensional vector with k nonzeros:\n      - elements[n]: dense array storing values (zeros at unused positions)\n      - indices[k]: list of positions with nonzero values\n      Random access: elements[i] gives value directly\n      Sparse iteration: for j in indices, access elements[indices[j]]", "complexity": "Element access: O(1) via dense array\n            Sparse iteration: O(k) where k = number of nonzeros\n            Insert: O(1) amortized\n            Clear: O(k) - must zero out nonzero positions in dense array\n\n@note Does NOT support negative indices or duplicate checking", "see": ["CoinPackedVector for pure sparse storage", "CoinDenseVector for pure dense storage"], "has_pass2": true}, "src/CoinHelperFunctions.hpp": {"path": "layer-0/CoinUtils/src/CoinHelperFunctions.hpp", "filename": "CoinHelperFunctions.hpp", "file": "CoinHelperFunctions.hpp", "brief": "Low-level utility functions for array manipulation, copying, and comparison\n\nProvides optimized template functions for common array operations used throughout\nCOIN-OR: copying (with Duff's device optimization), filling, zeroing, sorting checks,\niota generation, and element deletion. Also includes random number generation and\nfile I/O utilities.\n\nMany functions offer ~2x speedup over naive loops via loop unrolling and\nDuff's device techniques.", "algorithm": "Duff's device - Tom Duff's loop unrolling technique that uses switch\n           statement fall-through to handle remainder elements, reducing loop\n           overhead from O(n) branches to O(n/8) branches.", "complexity": "Summary of major functions:\n            - CoinCopyN, CoinDisjointCopyN, CoinFillN, CoinZeroN: O(n)\n            - CoinIsSorted: O(n)\n            - CoinIotaN: O(n)\n            - CoinDeleteEntriesFromArray: O(n + k log k) where k = deletions\n            - CoinDrand48: O(1) per call", "ref": ["Duff, T. (1983). \"Duff's Device\" - Usenet comp.lang.c posting.\n     Originally for copying to memory-mapped display registers."], "see": ["CoinSort.hpp for sorting functions", "CoinError.hpp for exception handling", "CoinDisjointCopyN for faster non-overlapping copy", "CoinCopyN for count-based interface", "CoinCopyN for overlap-safe copy", "CoinDrand48 for global (non-thread-safe) random numbers"], "tparam": ["T Element type (must be copy-assignable)", "T Element type (must be copy-assignable)", "T Element type (must be copy-assignable)", "T Element type", "T Element type", "T Type supporting operator<", "T Type supporting operator<", "T Numeric type supporting negation and comparison with 0"], "param": ["from Pointer to source array", "size Number of elements to copy", "to Pointer to destination array", "first Pointer to first source element", "last Pointer past last source element", "to Pointer to destination array\n\n@note Size computed as (last - first), cast to CoinBigIndex", "from Pointer to source array", "size Number of elements to copy", "to Pointer to destination array\n\n@note Arrays must not overlap; behavior undefined if they do\n@note For POD types, CoinMemcpyN may be faster", "array Source array (may be null)", "size Number of elements to copy", "array Source array (may be null)", "size Size of array to allocate", "copySize Number of elements to copy from source (must be <= size)", "name Source string (may be null)", "x1 First value", "x2 Second value", "x1 First value", "x2 Second value", "value Input value"], "return": "Newly allocated copy, or NULL if array was NULL\n\n@note Caller owns returned memory (delete[])", "has_pass2": true}, "src/CoinPresolveSingleton.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveSingleton.hpp", "filename": "CoinPresolveSingleton.hpp", "file": "CoinPresolveSingleton.hpp", "brief": "Singleton row/column presolve transforms", "algorithm": "Singleton Column/Row Elimination:\n  Singleton column (one nonzero): variable x_j appears in one constraint\n  - Compute x_j bounds from constraint bounds and coefficient sign\n  - If bounds tighter than variable bounds, update variable bounds\n  - If bounds prove infeasibility, detect and report\n\n  Slack doubleton: row ax ≤ b with single variable, explicit bound\n  - Convert to column bound: x ≤ b/a (adjust for sign)\n  - Remove the constraint, tighten column bound", "math": "For singleton row ax_j = b: x_j = b/a (fixed value)\n  For singleton column in ax_j ≤ b with a > 0: x_j ≤ b/a\n  For singleton column in ax_j ≥ b with a > 0: x_j ≥ b/a", "complexity": "Time: O(1) per singleton processed\n  Very effective: singletons common in practical LP/MIP models\n  Cascading: removing singletons may create new singletons", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinPresolveUseless.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveUseless.hpp", "filename": "CoinPresolveUseless.hpp", "file": "CoinPresolveUseless.hpp", "brief": "Remove useless (redundant) constraints", "see": ["CoinPresolveForcing for related forcing constraint detection", "CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinFileIO.hpp": {"path": "layer-0/CoinUtils/src/CoinFileIO.hpp", "filename": "CoinFileIO.hpp", "file": "CoinFileIO.hpp", "brief": "Abstract file I/O with automatic compression detection\n\nProvides CoinFileInput and CoinFileOutput base classes with factory\nmethods that auto-detect gzip/bzip2 compression. Used by CoinMpsIO\nand CoinLpIO for reading/writing problem files.", "see": ["CoinMpsIO for MPS file format", "CoinLpIO for LP file format"], "has_pass2": false}, "src/CoinDenseFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinDenseFactorization.hpp", "filename": "CoinDenseFactorization.hpp", "file": "CoinDenseFactorization.hpp", "brief": "Dense matrix factorization and CoinOtherFactorization base class\n\nProvides CoinOtherFactorization abstract base class for alternative\nfactorization methods, plus CoinDenseFactorization for small dense\nproblems using LAPACK-style LU.", "algorithm": "Dense LU Factorization (LAPACK-style):\n  Computes PA = LU using partial pivoting (row interchanges).\n  1. For k = 1 to n: find pivot = max|A(i,k)| for i >= k\n  2. Swap rows k and pivot row\n  3. Compute L(i,k) = A(i,k) / A(k,k) for i > k\n  4. Update A(i,j) -= L(i,k) * A(k,j) for i,j > k", "math": "Dense LU: PA = LU where P is permutation, L unit lower triangular,\n  U upper triangular. FTRAN solves Lx = b then Ux = y.\n  BTRAN solves U'y = b then L'x = y.", "complexity": "Time: O(n³/3) for factorization, O(n²) per solve\n  Space: O(n²) for dense storage\n  Use only for small basis matrices (n < 100-200)", "author": "John Forrest", "see": ["CoinFactorization for sparse factorization", "CoinSimpFactorization for simple implementation"], "has_pass2": true}, "src/CoinParam.hpp": {"path": "layer-0/CoinUtils/src/CoinParam.hpp", "filename": "CoinParam.hpp", "file": "CoinParam.hpp", "brief": "Command line parameter parsing with keyword matching\n\nProvides base class for 'keyword value' parameters with support for\ninteger, double, string, and keyword-set values. Includes partial\nkeyword matching, range validation, and help text.", "see": ["CoinParamUtils namespace for parsing utilities"], "has_pass2": false}, "src/CoinPresolveForcing.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveForcing.hpp", "filename": "CoinPresolveForcing.hpp", "file": "CoinPresolveForcing.hpp", "brief": "Forcing and useless constraint detection", "algorithm": "Forcing Constraint Detection:\n  Compute implied bounds on row activity: LB_i ≤ Σ a_ij x_j ≤ UB_i\n  using variable bounds l_j ≤ x_j ≤ u_j\n\n  Useless constraint: LB_i ≥ row_lb and UB_i ≤ row_ub\n  - Constraint can never be violated → remove it\n\n  Forcing constraint: LB_i = row_ub or UB_i = row_lb\n  - All variables forced to bounds that achieve the activity limit\n  - Fix variables, remove constraint", "math": "Activity bounds: LB_i = Σ_{a_ij>0} a_ij l_j + Σ_{a_ij<0} a_ij u_j\n  UB_i = Σ_{a_ij>0} a_ij u_j + Σ_{a_ij<0} a_ij l_j\n  Forcing: if UB_i = row_lb, then each term must achieve its min", "complexity": "Time: O(nnz) to compute activity bounds for all rows\n  Very effective for tightly bounded problems\n  Can fix many variables simultaneously", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinSort.hpp": {"path": "layer-0/CoinUtils/src/CoinSort.hpp", "filename": "CoinSort.hpp", "file": "CoinSort.hpp", "brief": "Sorting utilities for pairs, triples, and parallel arrays\n\nProvides CoinPair, CoinTriple, and sort functions for sorting\nmultiple related arrays together (e.g., indices and values).", "algorithm": "Parallel Array Sorting:\n  Problem: sort array A while permuting arrays B, C, ... identically\n  Approach 1 (standard): Create array of tuples, sort, scatter back\n  Approach 2 (EKK sort): In-place quicksort carrying parallel arrays\n\n  EKK Quicksort variant (COIN_USE_EKK_SORT):\n  1. Check if already sorted (linear scan)\n  2. Median-of-three pivot selection\n  3. Non-recursive stack-based partitioning\n  4. Insertion sort for small subarrays (n ≤ 10)\n  Falls back to std::sort for n > 10000", "complexity": "CoinSort_2/3: O(n log n) average, O(n²) worst\n  CoinShortSort_2: O(n log n) but optimized for small n\n  Space: O(n) for tuple approach, O(log n) stack for EKK", "see": ["CoinPackedVector for typical use case (sorting indices with values)"], "has_pass2": true}, "src/CoinPresolvePsdebug.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolvePsdebug.hpp", "filename": "CoinPresolvePsdebug.hpp", "file": "CoinPresolvePsdebug.hpp", "brief": "Debug and consistency checking for presolve\n\nEnable with PRESOLVE_DEBUG and PRESOLVE_CONSISTENCY defines.", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinPresolveImpliedFree.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveImpliedFree.hpp", "filename": "CoinPresolveImpliedFree.hpp", "file": "CoinPresolveImpliedFree.hpp", "brief": "Detect and process implied free variables", "algorithm": "Implied Free Variable Detection:\n  For singleton variable x_j (one nonzero a_ij in row i):\n  1. Compute implied bounds on x_j from constraint i and other var bounds\n  2. If implied bounds strictly within [l_j, u_j]: x_j is implied free\n  3. Remove constraint i and variable x_j (neither can be binding)\n\n  Non-singleton case: use substitution to reduce to singleton first", "math": "For row: Σ_k a_ik x_k = b_i, with x_j singleton\n  x_j = (b_i - Σ_{k≠j} a_ik x_k) / a_ij\n  Implied bounds from other variables' bounds → [l_j^impl, u_j^impl]\n  If l_j < l_j^impl and u_j > u_j^impl: x_j implied free", "complexity": "Time: O(nnz(row)) per candidate variable\n  Very effective for network and assignment problems\n  Reduces problem by one row and one column per implied free", "see": ["CoinPresolveSubst for non-singleton case", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinPresolveTighten.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveTighten.hpp", "filename": "CoinPresolveTighten.hpp", "file": "CoinPresolveTighten.hpp", "brief": "Tighten variable bounds using constraint propagation", "algorithm": "Bound Tightening:\n  For each constraint Σ a_ij x_j ≤ b_i:\n  1. Compute max activity without x_k: M_k = Σ_{j≠k} max(a_ij l_j, a_ij u_j)\n  2. New upper bound: x_k ≤ (b_i - M_k) / a_ik (if a_ik > 0)\n  3. New lower bound: x_k ≥ (b_i - M_k) / a_ik (if a_ik < 0)\n  4. Iterate until no bound changes or fixed point reached\n\n  tighten_zero_cost: fix variables with zero objective at bounds", "math": "Implied bound: given Σa_j x_j ≤ b and x_j ∈ [l_j, u_j]\n  x_k ≤ (b - Σ_{j≠k} a_j l_j) / a_k when a_k > 0\n  Dual bound tightening uses similar logic from reduced costs", "complexity": "Time: O(iterations × nnz) for propagation\n  Very effective for tightly constrained problems\n  May prove infeasibility or fix variables", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinWarmStartPrimalDual.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartPrimalDual.hpp", "filename": "CoinWarmStartPrimalDual.hpp", "file": "CoinWarmStartPrimalDual.hpp", "brief": "Warm start using both primal and dual variable values\n\nStores both primal (column) and dual (row) solution vectors.\nUseful for interior point methods. Includes diff capability.", "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartDual for dual-only warm start"], "has_pass2": false}, "src/CoinFloatEqual.hpp": {"path": "layer-0/CoinUtils/src/CoinFloatEqual.hpp", "filename": "CoinFloatEqual.hpp", "file": "CoinFloatEqual.hpp", "brief": "Function objects for testing equality of real numbers\n\nTwo objects are provided; one tests for equality to an absolute tolerance,\none to a scaled tolerance. The tests will handle IEEE floating point, but\nnote that infinity == infinity. Mathematicians are rolling in their graves,\nbut this matches the behaviour for the common practice of using\n<code>DBL_MAX</code> (<code>numeric_limits<double>::max()</code>, or similar\nlarge finite number) as infinity.", "algorithm": "Floating-Point Comparison:\n  CoinAbsFltEq: |f1 - f2| < ε (absolute tolerance)\n  CoinRelFltEq: |f1 - f2| ≤ ε(1 + max(|f1|, |f2|)) (relative tolerance)\n\n  Special cases:\n  - NaN: never equal to anything (including itself)\n  - Infinity: inf == inf returns true (practical choice)\n  - Exact equality: checked first to handle ±0", "math": "Absolute: suitable when values are O(1) magnitude\n  Relative: suitable when values span orders of magnitude\n  Default ε = 1e-10 (double), 1e-6 (float)", "complexity": "O(1) per comparison\n\n<p>\nExample usage:\n@verbatim\n  double d1 = 3.14159 ;\n  double d2 = d1 ;\n  double d3 = d1+.0001 ;\n\n  CoinAbsFltEq eq1 ;\n  CoinAbsFltEq eq2(.001) ;\n\n  assert(  eq1(d1,d2) ) ;\n  assert( !eq1(d1,d3) ) ;\n  assert(  eq2(d1,d3) ) ;\n@endverbatim\nCoinRelFltEq follows the same pattern.", "see": ["CoinFinite.hpp for infinity/NaN handling utilities"], "has_pass2": true}, "src/CoinModel.hpp": {"path": "layer-0/CoinUtils/src/CoinModel.hpp", "filename": "CoinModel.hpp", "file": "CoinModel.hpp", "brief": "High-level model building with string names and expressions\n\nCoinModel provides a flexible interface for constructing LP/MIP models.\nSupports row/column names, string-based expressions, and incremental\nbuilding. Convert to CoinPackedMatrix for solver use.", "algorithm": "Model Construction Interface:\n  Three building modes:\n  1. Row-by-row: addRow(indices, values, lb, ub)\n  2. Column-by-column: addCol(indices, values, lb, ub, obj)\n  3. Element-by-element: setElement(row, col, value)\n\n  Name handling: hash tables for row/column name lookup\n  Expression parsing: \"2 x1 + 3 x2 <= 5\" string syntax", "complexity": "Insertion: O(1) amortized with dynamic arrays\n  Name lookup: O(1) expected with hash tables\n  Conversion to CoinPackedMatrix: O(nnz)", "see": ["CoinBuild for simpler row-by-row construction", "CoinPackedMatrix for the final sparse matrix format"], "has_pass2": true}, "src/CoinCutPool.hpp": {"path": "layer-0/CoinUtils/src/CoinCutPool.hpp", "filename": "CoinCutPool.hpp", "file": "CoinCutPool.hpp", "brief": "Class for storing a pool of cuts", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["idxs indexes of variables of the cut", "coefs coefficients of the variables the cut", "nz size of the cut", "rhs right-hand side of the cut", "other cut to be checked.", "x current solution of the LP relaxation of the MILP.", "numCols number of variables of the MILP.", "idxs indexes of the variables of the\ncut to be added.", "coefs coefficients of the variables of\nthe cut to be added.", "nz size of the cut to be added", "rhs right-hand side of the cut to be added"], "has_pass2": false}, "src/CoinSearchTree.hpp": {"path": "layer-0/CoinUtils/src/CoinSearchTree.hpp", "filename": "CoinSearchTree.hpp", "file": "CoinSearchTree.hpp", "brief": "Search tree data structures for branch-and-bound\n\nProvides tree node management with various comparison strategies\n(best-first, depth-first, breadth-first).", "algorithm": "Branch-and-Bound Tree Search:\n  Maintains a priority queue of open nodes, each representing a subproblem.\n  Node selection strategies:\n  - Best-first: select node with best LP bound (minimize gap)\n  - Depth-first: select deepest node (find solutions quickly)\n  - Breadth-first: select shallowest node (explore broadly)\n  - Best-estimate: use estimated integer solution quality", "math": "Node attributes:\n  - depth: distance from root (root = 0)\n  - quality: LP relaxation value (lower bound for minimization)\n  - fractionality: number of integer-infeasible variables\n  - true_lower_bound: valid dual bound (may differ with column generation)", "complexity": "Node operations: O(log n) insertion, O(log n) extraction\n  where n = number of open nodes. Uses heap-based priority queue.\n  Sibling nodes stored together for efficient processing.", "see": ["CoinTreeNode for node data interface", "CoinTreeSiblings for sibling node grouping"], "has_pass2": true}, "src/CoinMessage.hpp": {"path": "layer-0/CoinUtils/src/CoinMessage.hpp", "filename": "CoinMessage.hpp", "file": "CoinMessage.hpp", "brief": "Standard COIN-OR message definitions (enum and preloaded set)\n\nDefines COIN_Message enum for standard messages (MPS errors, presolve\ninfo, etc.) and CoinMessage class that preloads these into a handler.", "see": ["CoinMessageHandler for the message handling facilities", "CoinOneMessage for individual message storage"], "has_pass2": false}, "src/CoinStaticConflictGraph.hpp": {"path": "layer-0/CoinUtils/src/CoinStaticConflictGraph.hpp", "filename": "CoinStaticConflictGraph.hpp", "file": "CoinStaticConflictGraph.hpp", "brief": "static CoinConflictGraph implementation with fast queries", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["numCols number of variables", "colType column types", "colLB column lower bounds", "colUB column upper bounds", "matrixByRow row-wise constraint matrix", "sense row sense", "rowRHS row right hand side", "rowRange row ranges", "cgraph conflict graph", "n number of elements in the induced subgraph", "elements indexes of nodes in the induced subgraph", "idxNode index of the node", "deg degree of the node", "idxNode index of the node", "deg degree of the node"], "return": "a vector of updated bounds with the format (idx, (lb, ub))", "has_pass2": false}, "src/CoinWarmStartBasis.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartBasis.hpp", "filename": "CoinWarmStartBasis.hpp", "file": "CoinWarmStartBasis.hpp", "brief": "Simplex basis warm start with variable status (basic/nonbasic)\n\nStores status of each variable (structural and artificial) using\n2 bits per variable. Includes diff capability for branch-and-bound.", "algorithm": "Simplex Basis Warm Start:\n  For LP with m constraints and n variables, a basis B identifies\n  m basic variables. Restarting from a known basis avoids Phase I.\n  Status encoding (2 bits per variable):\n  - 00 (isFree): nonbasic at zero\n  - 01 (basic): in the basis (value determined by constraints)\n  - 10 (atUpperBound): nonbasic at upper bound\n  - 11 (atLowerBound): nonbasic at lower bound", "math": "Basis matrix B is m×m submatrix of A with basic columns.\n  Basic solution: x_B = B⁻¹b, x_N = 0 (or at bounds)\n  Warm start restores this basis without recomputing from scratch.", "complexity": "Space: O((m+n)/4) bytes using 2-bit packing (4 vars per byte)\n  Diff operations for B&B: O(changed variables) instead of O(m+n)\n  generateDiff/applyDiff enable efficient tree-based warm starting", "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartDual for dual-only warm start"], "has_pass2": true}, "src/CoinCliqueExtender.hpp": {"path": "layer-0/CoinUtils/src/CoinCliqueExtender.hpp", "filename": "CoinCliqueExtender.hpp", "file": "CoinCliqueExtender.hpp", "brief": "Clique extender", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["cgraph conflict graph", "extMethod method used to extend cliques (optional).\nValues: 0 = no extension; 1 = random; 2 = max degree;\n3 = max modified degree; 4 = reduced cost (inversely proportional);\n5 = reduced cost (inversely proportional) + modified degree.", "rc reduced cost (optional)", "clqIdxs indexes of the clique to be extended", "clqSize size of the clique to be extended", "clqIdxs indexes of the clique to be extended", "clqSize size of the clique to be extended", "clqIdxs indexes of the clique to be extended", "clqIdxs size of the clique to be extended", "clqIdxs indexes of the clique to be extended", "clqIdxs size of the clique to be extended"], "has_pass2": false}, "src/CoinPackedVector.hpp": {"path": "layer-0/CoinUtils/src/CoinPackedVector.hpp", "filename": "CoinPackedVector.hpp", "file": "CoinPackedVector.hpp", "brief": "Sparse vector that owns its index/value storage\n\nCoinPackedVector stores a sparse vector as parallel arrays of indices\nand values. Unlike CoinShallowPackedVector, this class owns its storage\nand supports modification operations.", "see": ["CoinPackedVectorBase for the read-only interface", "CoinShallowPackedVector for non-owning sparse vector reference", "CoinIndexedVector for sparse vector with dense backing array"], "algorithm": "Merge-style sparse dot product", "math": "Computes x·y = Σ x_i * y_i where x and y share index i\n      Uses sorted merge to find common indices in linear time.", "complexity": "O(k1 + k2) where k1, k2 are the number of nonzeros\n@pre Both vectors must be sorted by increasing index", "has_pass2": true}, "src/CoinPresolveEmpty.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveEmpty.hpp", "filename": "CoinPresolveEmpty.hpp", "file": "CoinPresolveEmpty.hpp", "brief": "Drop and reinsert empty rows/columns\n\nShould be last presolve step. Physical removal of empty entities.", "algorithm": "Empty Row/Column Removal:\n  drop_empty_cols_action:\n  - Compress column arrays, renumber remaining columns\n  - Store original bounds, cost, solution for postsolve\n  - Adjust objective constant for fixed-at-bound variables\n\n  drop_empty_rows_action:\n  - Renumber rows, compress row bounds\n  - Track which row filled each position for postsolve", "math": "Empty column j: a_*j = 0 (all zeros in column)\n  For minimization: set x_j = l_j if c_j > 0, x_j = u_j if c_j < 0\n  Empty row i: a_i* = 0 → constraint always satisfied, remove", "complexity": "Time: O(m + n) for scanning and renumbering\n  Must be done last: other transforms may create empty rows/cols\n  Postsolve reinserts in reverse order, first step after solve", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinAdjacencyVector.hpp": {"path": "layer-0/CoinUtils/src/CoinAdjacencyVector.hpp", "filename": "CoinAdjacencyVector.hpp", "file": "CoinAdjacencyVector.hpp", "brief": "Vector of growable vectors", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["idxRow row index", "idxRow row index", "idxNode graph node", "idxNeigh neighbor that will be searched", "idxNode graph node", "idxNeigh neighbor that will be added to idxNode", "idxNode graph node", "idxNeigh neighbor that will be added to idxNode", "idxNode graph node", "n number of neighbors that will be added to idxNode", "elements neighbors that will be added to idxNode", "el sorted vector", "newEl element to be added to the sorted vector"], "has_pass2": false}, "src/CoinError.hpp": {"path": "layer-0/CoinUtils/src/CoinError.hpp", "filename": "CoinError.hpp", "file": "CoinError.hpp", "brief": "Exception class and assertion macros for COIN-OR error handling\n\nProvides CoinError, a rich exception class that captures context about\nwhere and why an error occurred, plus assertion macros that can optionally\nthrow CoinError instead of calling abort().", "see": ["CoinMessage.hpp for message handling without exceptions", "CoinAssert macros for assertion-based error reporting", "CoinError"], "return": "Description of what went wrong", "param": ["doPrint If false, do nothing (default true)", "message__ Description of the error condition", "methodName__ Name of the method detecting the error", "className__ Name of the class (or hint string for assertions)", "fileName_ Source file name (optional, for assertions)", "line Source line number (optional, -1 if not an assertion)", "source CoinError to copy from", "rhs CoinError to copy from"], "has_pass2": false}, "src/CoinWarmStartDual.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartDual.hpp", "filename": "CoinWarmStartDual.hpp", "file": "CoinWarmStartDual.hpp", "brief": "Warm start using dual variable values only\n\nStores dual solution vector for warm starting interior point methods\nor dual simplex. Includes diff capability for branch-and-bound.", "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartPrimalDual for combined primal/dual warm start"], "has_pass2": false}, "src/CoinOddWheelSeparator.hpp": {"path": "layer-0/CoinUtils/src/CoinOddWheelSeparator.hpp", "filename": "CoinOddWheelSeparator.hpp", "file": "CoinOddWheelSeparator.hpp", "brief": "Odd-wheel cut separator", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["cgraph conflict graph", "x current solution of the LP relaxation of the MILP", "rc reduced cost of the variables in the LP relaxation\nof the MILP", "extMethod strategy that will be used to lift odd cycles,\ntransforming them into odd wheels: 0 = no lifting, 1 = only one\nvariable as wheel center, 2 = a clique as wheel center."], "has_pass2": false}, "src/CoinStructuredModel.hpp": {"path": "layer-0/CoinUtils/src/CoinStructuredModel.hpp", "filename": "CoinStructuredModel.hpp", "file": "CoinStructuredModel.hpp", "brief": "Block-structured model composed of CoinModel blocks", "see": ["CoinModel for individual block representation"], "has_pass2": false}, "src/CoinPresolveDupcol.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveDupcol.hpp", "filename": "CoinPresolveDupcol.hpp", "file": "CoinPresolveDupcol.hpp", "brief": "Detect and remove duplicate columns and rows", "algorithm": "Duplicate Detection via Random Hashing:\n  To find duplicate columns efficiently:\n  1. Assign random weight r_i to each row\n  2. Compute column hash: h_j = Σ_i r_i · a_ij\n  3. Columns with same hash are candidates (verify exactly)\n\n  Duplicate columns (same cost): combine bounds\n  - x_j and x_k identical → replace with x_new, l_new = l_j + l_k\n\n  Duplicate rows: keep tighter constraint, remove redundant\n  - If row_i ⊆ row_k (interval containment), remove row_k", "math": "Column equivalence: a_*j = a_*k (element-wise)\n  Combined variable: x_new = x_j + x_k\n  Postsolve: split x_new* back to feasible (x_j*, x_k*)", "complexity": "Time: O(nnz) for hashing, O(candidates × nnz) for verification\n  Random hashing minimizes false positives\n  Common in models with symmetry or copy-paste construction", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}}}, "SuiteSparse": {"name": "SuiteSparse", "file_count": 17, "pass2_count": 9, "files": {"Mongoose/Include/Mongoose.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose.hpp", "filename": "Mongoose.hpp", "param": ["filename the filename or path to the Matrix Market File.", "filename the filename or path to the Matrix Market File."], "has_pass2": false}, "Mongoose/Include/Mongoose_Logger.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Logger.hpp", "filename": "Mongoose_Logger.hpp", "param": ["timingType The portion of the library being timed (MatchingTiming,\n  CoarseningTiming, RefinementTiming, FMTiming, QPTiming, or IOTiming).", "timingType The portion of the library being timed (MatchingTiming,\n  CoarseningTiming, RefinementTiming, FMTiming, QPTiming, or IOTiming).", "timingType The portion of the library being timed (MatchingTiming,\n  CoarseningTiming, RefinementTiming, FMTiming, QPTiming, or IOTiming)."], "has_pass2": false}, "Mongoose/Include/Mongoose_IO.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_IO.hpp", "filename": "Mongoose_IO.hpp", "param": ["filename the filename or path to the Matrix Market File.", "filename the filename or path to the Matrix Market File.", "matcode the four character Matrix Market type code.", "filename the filename or path to the Matrix Market File.", "filename the filename or path to the Matrix Market File.", "matcode the four character Matrix Market type code."], "has_pass2": false}, "SuiteSparse_config/SuiteSparse_config.h": {"path": "layer-0/SuiteSparse/SuiteSparse_config/SuiteSparse_config.h", "filename": "SuiteSparse_config.h", "file": "SuiteSparse_config.h", "brief": "Central configuration and utility header for all SuiteSparse libraries\n\nThis file provides the shared foundation for the entire SuiteSparse\ncollection of sparse matrix algorithms. Key features include:\n\n- Compiler and platform detection (GCC, Clang, MSVC, ICC, NVCC)\n- Configurable memory management with replaceable malloc/calloc/realloc/free\n- Portable BLAS/LAPACK interface handling Fortran name mangling and integer sizes\n- OpenMP detection and thread-safe timing utilities\n- Complex number type definitions for C/C++ interoperability\n\n@note Applications can customize memory allocation by calling the\n      SuiteSparse_config_*_set functions before using any SuiteSparse library.", "see": ["amd.h, colamd.h, cholmod.h, umfpack.h, klu.h for library-specific headers", "SuiteSparse_finish()", "SuiteSparse_start()", "SuiteSparse_calloc(), SuiteSparse_free()", "SuiteSparse_malloc(), SuiteSparse_free()", "SuiteSparse_malloc(), SuiteSparse_free()", "SuiteSparse_malloc(), SuiteSparse_calloc()", "SuiteSparse_toc(), SuiteSparse_time()", "SuiteSparse_tic()"], "param": ["nitems Number of items to allocate (enforced >= 1)", "size_of_item Size in bytes of each item (typically sizeof(type))", "nitems Number of items to allocate (enforced >= 1)", "size_of_item Size in bytes of each item (typically sizeof(type))", "nitems_new New number of items desired", "nitems_old Current number of items (for size calculation)", "size_of_item Size in bytes of each item", "p Pointer to existing allocation (may be NULL for initial alloc)", "ok Output: set to 1 on success, 0 on failure", "p Pointer to memory block to free (NULL is safe)", "tic Output array of 2 doubles to store timer state", "tic Timer state from previous SuiteSparse_tic call", "x First value", "y Second value", "ar Real part of numerator a", "ai Imaginary part of numerator a", "br Real part of denominator b", "bi Imaginary part of denominator b", "cr Output: real part of result c", "ci Output: imaginary part of result c", "version Optional output array for version components:\n               version[0] = major, version[1] = minor, version[2] = patch.\n               May be NULL if only the combined version is needed."], "return": "Pointer to allocated memory, or NULL on failure", "math": "Implements the hypot function: sqrt(x^2 + y^2) using a numerically\n      stable algorithm that avoids intermediate overflow/underflow.", "has_pass2": true}, "AMD/Include/amd.h": {"path": "layer-0/SuiteSparse/AMD/Include/amd.h", "filename": "amd.h", "file": "amd.h", "brief": "Approximate Minimum Degree ordering for sparse matrix factorization\n\nAMD computes a fill-reducing permutation P for sparse Cholesky or LU\nfactorization. Given a symmetric matrix A (or A+A' if A is unsymmetric),\nAMD finds P such that P*A*P' has fewer nonzeros in its Cholesky factor\nthan A would.", "algorithm": "Approximate Minimum Degree (AMD)\n  Repeatedly eliminates the node with minimum approximate external degree:\n  1. Select pivot i with minimum approximate degree d̃(i)\n  2. Form element e from eliminated node and merge with adjacent elements\n  3. Update approximate degrees for remaining nodes\n  4. Apply aggressive absorption: absorb element e into element f if\n     adj(e) ⊆ adj(f), even when f is not adjacent to current pivot", "math": "Approximate external degree:\n  d̃(i) ≈ |adj(i) ∪ (∪ adj(e) : e ∈ elements adjacent to i)| - |absorbed|\n  The true minimum degree would require d(i) = |L_{*i}| - 1 (column count\n  in Cholesky factor), but this is expensive to compute exactly.", "complexity": "Time: O(nnz(A)·α(n)) average case with aggressive absorption,\n  where α is the inverse Ackermann function. Worst case O(n·nnz).\n  Space: O(nnz(A) + n) for the quotient graph representation.", "ref": ["Amestoy, Davis, Duff (1996). \"An Approximate Minimum Degree Ordering\n     Algorithm\". SIAM J. Matrix Analysis and Applications 17(4):886-905.", "George, Liu (1989). \"The Evolution of the Minimum Degree Ordering\n     Algorithm\". SIAM Review 31(1):1-19.\n\nKey features:\n- Aggressive absorption for better approximate degrees\n- Dense row/column detection and deferral\n- Post-ordering of the elimination tree\n- Both int32_t and int64_t versions available"], "see": ["colamd.h for column ordering of unsymmetric matrices", "cholmod.h for sparse Cholesky factorization using AMD", "amd_l_order() for 64-bit integer version", "amd_defaults() to initialize Control array", "amd_order() for full parameter documentation", "amd_order()", "amd_valid() for full documentation", "amd_defaults()", "amd_control()", "amd_info()"], "param": ["n Matrix dimension (A is n-by-n). Must be >= 0.", "Ap Column pointers array of size n+1. Ap[j] gives the start of column j.", "Ai Row indices array of size Ap[n]. Contains row indices for each column.", "P Output permutation array of size n. P[k]=i means row i is the k-th pivot.", "Control Optional control parameters of size AMD_CONTROL. NULL uses defaults.", "Info Optional output statistics of size AMD_INFO. May be NULL.", "n_row Number of rows in the matrix", "n_col Number of columns in the matrix", "Ap Column pointers array of size n_col+1", "Ai Row indices array of size Ap[n_col]", "Control Output array of size AMD_CONTROL to initialize", "Control Control array of size AMD_CONTROL (NULL prints defaults)", "Info Statistics array of size AMD_INFO from amd_order", "version Output array filled with [major, minor, patch] version"], "return": "AMD_OK on success, AMD_OK_BUT_JUMBLED if input had unsorted/duplicate\n        entries, AMD_INVALID for bad input, AMD_OUT_OF_MEMORY on allocation failure.", "has_pass2": true}, "CSparse/Include/cs.h": {"path": "layer-0/SuiteSparse/CSparse/Include/cs.h", "filename": "cs.h", "file": "cs.h", "brief": "Concise Sparse matrix library - teaching implementation of sparse algorithms\n\nCSparse provides a minimal, readable implementation of core sparse matrix\noperations. It serves as both a standalone library and educational reference\nfor sparse linear algebra algorithms.\n\nKey features:\n- Sparse matrix in triplet or compressed-column (CSC) format\n- Sparse Cholesky (cs_chol), LU (cs_lu), and QR (cs_qr) factorization\n- Fill-reducing orderings via AMD\n- Direct solvers: cs_cholsol, cs_lusol, cs_qrsol\n- Dulmage-Mendelsohn decomposition (cs_dmperm)", "algorithm": "Sparse Matrix Operations:\n  - Compressed-column (CSC): column pointers + row indices + values\n  - cs_compress: O(nnz) triplet to CSC conversion\n  - cs_multiply: sparse matrix-matrix product using symbolic + numeric phases\n  - cs_chol/cs_lu/cs_qr: left-looking factorization algorithms", "math": "Sparse formats:\n  CSC: A stored as (p, i, x) where p[j]..p[j+1]-1 index column j entries\n  Triplet (COO): (row[k], col[k], val[k]) for k = 0..nz-1", "complexity": "Space: O(nnz + n) for CSC format\n  cs_compress: O(nnz) time for triplet → CSC\n  cs_multiply: O(flops) where flops depends on sparsity pattern\n  cs_chol/cs_lu: O(nnz(L)²/n) typical for sparse factors", "ref": ["Davis (2006). \"Direct Methods for Sparse Linear Systems\".\n     SIAM. ISBN: 978-0-898716-13-9"], "see": ["CXSparse for extended version with complex numbers", "CHOLMOD, UMFPACK for production-quality implementations", "cs_compress() to convert triplet to CSC", "CS_CSC(), CS_TRIPLET() macros to check format", "cs_schol() for symbolic Cholesky analysis", "cs_sqr() for symbolic QR analysis", "cs_chol() for numeric Cholesky", "cs_lu() for numeric LU", "cs_qr() for numeric QR", "cs_dmperm() for computing this decomposition", "cs_scc() for strongly connected components"], "param": ["order 0=natural, 1=AMD", "order ordering,", "tol pivot tolerance", "order 0=natural, 1-3=AMD variants", "values 1=copy values, 0=pattern only"], "has_pass2": true}, "LAGraph/include/LAGraphX.h": {"path": "layer-0/SuiteSparse/LAGraph/include/LAGraphX.h", "filename": "LAGraphX.h", "return": "Any GraphBLAS errors that may have been encountered", "has_pass2": false}, "UMFPACK/Include/umfpack.h": {"path": "layer-0/SuiteSparse/UMFPACK/Include/umfpack.h", "filename": "umfpack.h", "file": "umfpack.h", "brief": "Multifrontal sparse LU factorization for unsymmetric matrices\n\nUMFPACK computes a sparse LU factorization of a general (unsymmetric)\nsquare matrix A:\n  P*R*A*Q = L*U\nwhere P and Q are permutation matrices, R is diagonal scaling, L is\nunit lower triangular, and U is upper triangular.\n\nKey features:\n- Multifrontal algorithm with BLAS-3 dense kernels\n- Automatic strategy selection (symmetric vs unsymmetric)\n- Fill-reducing orderings: AMD (symmetric), COLAMD (unsymmetric)\n- Real and complex matrices (double precision)\n- Row scaling for numerical stability\n\nTypical workflow:\n1. umfpack_di_symbolic: Symbolic analysis (ordering, memory estimates)\n2. umfpack_di_numeric: Numerical LU factorization\n3. umfpack_di_solve: Solve Ax = b, A'x = b, etc.\n4. umfpack_di_free_symbolic, umfpack_di_free_numeric: Free memory", "algorithm": "Multifrontal LU with threshold partial pivoting:\n  1. Symbolic analysis: compute column elimination tree and frontal matrices\n     - COLAMD ordering for unsymmetric, AMD for symmetric patterns\n     - Identify supernodes: columns with identical nonzero patterns\n  2. Numerical factorization (multifrontal):\n     - Process columns in elimination tree postorder\n     - For each frontal matrix: dense partial LU with threshold pivoting\n     - Assemble child contributions (extend-add operation)\n     - Extract L and U columns, pass remainder to parent front\n  3. Solve phase: forward/back substitution through frontal matrices", "math": "Factorization: P·R·A·Q = L·U where:\n  - P, Q are permutation matrices (row/column reordering)\n  - R is diagonal scaling matrix (row equilibration)\n  - L is unit lower triangular, U is upper triangular\n  Threshold pivoting: select pivot if |a_kk| ≥ τ·max_i|a_ik| (τ ≈ 0.1)", "complexity": "Time: O(nnz(L+U)·f̄) where f̄ is average front size\n  Typically O(n^1.5) to O(n^2) for 2D problems, O(n^2) for 3D\n  Space: O(nnz(L+U) + front_stack) where front_stack depends on ordering", "ref": ["Davis (2004). \"Algorithm 832: UMFPACK V4.3 - An unsymmetric-pattern\n     multifrontal method\". ACM Trans. Math. Software 30(2):196-199.", "Davis (2004). \"A column pre-ordering strategy for the unsymmetric-pattern\n     multifrontal method\". ACM Trans. Math. Software 30(2):165-195.", "Duff, Reid (1983). \"The Multifrontal Solution of Indefinite Sparse\n     Symmetric Linear Equations\". ACM Trans. Math. Software 9(3):302-325."], "see": ["klu.h for circuit simulation matrices (often faster for this case)", "cholmod.h for symmetric positive definite matrices", "umfpack_di_numeric() for numerical factorization", "umfpack_di_free_symbolic() to free returned object", "umfpack_di_symbolic() for symbolic analysis", "umfpack_di_solve() to solve using the factorization", "umfpack_di_numeric() for computing the factorization", "UMFPACK_CONTROL_* defines for parameter indices", "cs_compress in CSparse for similar functionality"], "param": ["n_row Number of rows in A", "n_col Number of columns in A", "Ap Column pointers (size n_col+1)", "Ai Row indices (size Ap[n_col])", "Ax Numerical values (may be NULL for pattern-only analysis)", "Symbolic Output: opaque symbolic object", "Control Control parameters (NULL for defaults)", "Info Output: statistics (NULL to ignore)", "Ap Column pointers (size n_col+1)", "Ai Row indices (size Ap[n_col])", "Ax Numerical values (size Ap[n_col])", "Symbolic Symbolic analysis from umfpack_di_symbolic", "Numeric Output: opaque numeric factorization object", "Control Control parameters (NULL for defaults)", "Info Output: statistics (NULL to ignore)", "sys System to solve: UMFPACK_A, UMFPACK_At, UMFPACK_Aat, etc.", "Ap Column pointers (used for iterative refinement, or NULL)", "Ai Row indices (used for iterative refinement, or NULL)", "Ax Numerical values (used for iterative refinement, or NULL)", "X Output: solution vector (size n)", "B Input: right-hand side vector (size n)", "Numeric LU factorization from umfpack_di_numeric", "Control Control parameters (NULL for defaults)", "Info Output: statistics (NULL to ignore)", "Symbolic Pointer to Symbolic handle (set to NULL on output)", "Numeric Pointer to Numeric handle (set to NULL on output)", "Control Output: array of size UMFPACK_CONTROL with defaults", "n_row Number of rows", "n_col Number of columns", "nz Number of triplet entries", "Ti,Tj,Tx Row indices, column indices, values (triplet input)", "Ap,Ai,Ax Column pointers, row indices, values (CSC output)", "Map Optional: Map[k] gives destination index for triplet k", "lnz Output: # nonzeros in L", "unz Output: # nonzeros in U", "n_row,n_col Output: matrix dimensions", "nz_udiag Output: # nonzeros on diagonal of U", "Numeric Input: Numeric object from umfpack_*_numeric"], "return": "UMFPACK_OK on success, error code otherwise", "has_pass2": true}, "BTF/Include/btf.h": {"path": "layer-0/SuiteSparse/BTF/Include/btf.h", "filename": "btf.h", "file": "btf.h", "brief": "Block Triangular Form permutation for sparse matrices\n\nBTF computes permutations to transform a sparse matrix into block upper\ntriangular form (BTF). This decomposes the matrix into independent blocks\nthat can be processed separately, improving efficiency for factorization.\n\nThree main routines:\n- btf_maxtrans: Maximum transversal (zero-free diagonal matching)\n- btf_strongcomp: Strongly connected components (block decomposition)\n- btf_order: Combined BTF ordering (calls both above)", "algorithm": "Tarjan's Strongly Connected Components:\n  Find permutation P such that P*A*P' is block upper triangular.\n  1. Build directed graph: edge i→j if A(i,j) ≠ 0 and i ≠ j\n  2. Find SCCs using depth-first search with low-link values\n  3. Order SCCs in reverse topological order", "math": "Structural rank: max # nonzeros achievable on diagonal = sprank(A)\n  If sprank(A) < n, matrix is structurally singular.\n  BTF form: P*A*Q = | B_11  B_12 ... | with square diagonal blocks B_ii\n                    |  0   B_22 ... |", "complexity": "Maximum transversal: O(nnz + n) time and O(n) space\n  SCC decomposition: O(nnz + n) time (single DFS pass)\n  Total BTF ordering: O(nnz + n) time and space", "ref": ["Duff (1981). \"On Algorithms for Obtaining a Maximum Transversal\".\n     ACM Trans. Math. Software 7(3):315-330.", "Tarjan (1972). \"Depth-first search and linear graph algorithms\".\n     SIAM J. Computing 1(2):146-160."], "see": ["klu.h for sparse LU factorization using BTF", "btf_order() for combined BTF ordering", "btf_maxtrans()", "btf_order() for combined BTF ordering", "btf_strongcomp()", "btf_maxtrans(), btf_strongcomp()", "btf_order()"], "param": ["nrow Number of rows in A", "ncol Number of columns in A", "Ap Column pointers array of size ncol+1", "Ai Row indices array of size Ap[ncol]", "maxwork Max work limit as multiple of nnz(A); <= 0 for no limit", "work Output: actual work done, or -1 if limit reached", "Match Output: Match[i]=j if row i matched with column j, -1 if unmatched", "Work Workspace array of size 5*ncol", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "Q Optional input column permutation (may be NULL). Modified on output.", "P Output: row/col permutation. P[k]=j means j is the k-th row/col.", "R Output: block boundaries. Block b is rows/cols R[b] to R[b+1]-1.", "Work Workspace array of size 4*n", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "maxwork Max work limit for maxtrans; <= 0 for no limit", "work Output: work done in maxtrans, -1 if limit reached", "P Output: row permutation of size n", "Q Output: column permutation of size n (may be flagged negative)", "R Output: block boundaries of size n+1", "nmatch Output: number of nonzeros on diagonal of P*A*Q", "Work Workspace array of size 5*n", "version Output array filled with [major, minor, patch]"], "return": "Number of columns matched (structural rank)", "has_pass2": true}, "KLU/Include/klu.h": {"path": "layer-0/SuiteSparse/KLU/Include/klu.h", "filename": "klu.h", "file": "klu.h", "brief": "Sparse LU factorization optimized for circuit simulation matrices\n\nKLU computes a sparse LU factorization of a square matrix A:\n  P*A*Q = L*U\nwhere P and Q are permutation matrices, L is unit lower triangular,\nand U is upper triangular.\n\nKLU is specifically designed for matrices arising from circuit simulation,\nwhich tend to be sparse and nearly block-triangular. The factorization\nproceeds in three phases:\n1. klu_analyze: BTF pre-ordering + fill-reducing ordering (AMD/COLAMD)\n2. klu_factor: Numerical LU factorization (left-looking, column-by-column)\n3. klu_solve: Forward/back substitution to solve Ax = b", "algorithm": "Left-looking LU with BTF preprocessing and partial pivoting:\n  1. BTF pre-ordering (Tarjan's algorithm):\n     - Find strongly connected components of digraph(A)\n     - Permute to block upper triangular form (BTF)\n     - Each diagonal block factored independently\n  2. Fill-reducing ordering within blocks:\n     - AMD for symmetric pattern blocks, COLAMD for unsymmetric\n  3. Left-looking column-by-column LU:\n     - For column k: solve L_{1:k-1, 1:k-1} · x = A_{1:k-1, k}\n     - Apply partial pivoting within column\n     - Extract L_{k+1:n, k} and U_{1:k, k}", "math": "Block Triangular Form: P·A·Q has structure\n  | B_11  B_12  ...  B_1m |\n  |   0   B_22  ...  B_2m |\n  |   ⋮     ⋱    ⋱    ⋮  |\n  |   0    0   ...  B_mm |\n  Each B_ii is factored as P_i·B_ii = L_i·U_i independently.\n  Off-diagonal blocks solved via triangular solves.", "complexity": "Time: O(Σ nnz(L_i)·nnz(U_i)/n_i + off-diagonal work)\n  For circuit matrices: typically O(n) to O(n log n) due to BTF structure\n  Much faster than UMFPACK for matrices with many small diagonal blocks.\n  Space: O(nnz(L+U)) for factors, O(maxblock) workspace per block.", "ref": ["Davis, Palamadai (2010). \"KLU: A Direct Sparse Solver for Circuit\n     Simulation Problems\". ACM Trans. Math. Software.", "Duff, Reid (1978). \"An Implementation of Tarjan's Algorithm for the\n     Block Triangularization of a Matrix\". ACM Trans. Math. Software 4(2)."], "see": ["btf.h for BTF decomposition", "amd.h, colamd.h for fill-reducing orderings", "umfpack.h for general-purpose multifrontal LU", "klu_analyze() to create this object", "klu_factor() to use this for numerical factorization", "klu_factor() to create this object", "klu_solve() to use for solving linear systems", "klu_refactor() to update with new numerical values", "klu_defaults() to initialize", "klu_defaults()", "klu_factor() to perform numerical factorization", "klu_free_symbolic() to free the returned object", "klu_analyze()", "klu_analyze() for automatic ordering", "klu_analyze_given()", "klu_solve() to solve linear systems using the factorization", "klu_refactor() to update factorization with new values", "klu_factor()", "klu_factor()", "klu_factor()", "klu_tsolve() for transpose solve A'x = b", "klu_solve()", "klu_solve()", "klu_solve()", "klu_solve() for standard solve Ax = b", "klu_free_symbolic()", "klu_condest() for a more accurate (but slower) condition estimate"], "param": ["Common Control/statistics structure to initialize", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "Common Control parameters and output statistics", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "P User's row permutation (size n, NULL for identity)", "Q User's column permutation (size n, NULL for identity)", "Common Control parameters and output statistics", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "Ax Numerical values array of size Ap[n]", "Symbolic Symbolic analysis from klu_analyze", "Common Control parameters and output statistics", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "ldim Leading dimension of B (>= n for column-major storage)", "nrhs Number of right-hand sides", "B Input: right-hand side(s). Output: solution(s). Size ldim*nrhs", "Common Control parameters", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "ldim Leading dimension of B", "nrhs Number of right-hand sides", "B Input: right-hand side(s). Output: solution(s)", "Common Control parameters", "Ap Column pointers (must match original pattern)", "Ai Row indices (must match original pattern)", "Ax New numerical values", "Symbolic Symbolic analysis from klu_analyze", "Numeric Existing numeric object (modified in place)", "Common Control parameters", "Symbolic Pointer to Symbolic object pointer (set to NULL on return)", "Common Control parameters", "Numeric Pointer to Numeric object pointer (set to NULL on return)", "Common Control parameters", "Ap Column pointers array", "Ax Numerical values array", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "Common Result stored in Common->condest", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "Common Result stored in Common->rcond", "version Output array filled with [major, minor, patch]"], "return": "TRUE on success, FALSE if Common is NULL", "has_pass2": true}, "CHOLMOD/Include/cholmod.h": {"path": "layer-0/SuiteSparse/CHOLMOD/Include/cholmod.h", "filename": "cholmod.h", "file": "cholmod.h", "brief": "Comprehensive sparse Cholesky factorization library\n\nCHOLMOD provides high-performance sparse Cholesky factorization for\nsymmetric positive definite (SPD) and symmetric positive semi-definite\nmatrices. It supports:\n\nKey features:\n- Supernodal and simplicial Cholesky (LL' and LDL')\n- Fill-reducing orderings: AMD, COLAMD, METIS, CAMD\n- Real, complex, and pattern-only matrices\n- Single and double precision (float/double)\n- Row/column updates and downdates\n- GPU acceleration (NVIDIA CUDA)\n\nTypical workflow:\n1. cholmod_start: Initialize Common workspace\n2. cholmod_analyze: Symbolic analysis (fill-reducing ordering)\n3. cholmod_factorize: Numerical Cholesky factorization\n4. cholmod_solve: Solve Ax = b using the factors\n5. cholmod_finish: Free workspace", "algorithm": "Update/Downdate (rank-k modification):\n  Given L·L' = A, compute L̃ such that L̃·L̃' = A ± C·C':\n  - Uses Givens rotations for numerical stability\n  - O(k × nnz(L)) complexity for rank-k modification", "complexity": "Factorization: O(nnz(L)²/m) with supernodal method\n  Solve: O(nnz(L)) per right-hand side\n  Fill-in (nnz(L)) depends on ordering quality", "math": "For A = L·L': L(j,j) = √(A(j,j) - Σ L(j,k)²)\n  L(i,j) = (A(i,j) - Σ L(i,k)·L(j,k)) / L(j,j) for i > j", "ref": ["Davis (2006). \"Direct Methods for Sparse Linear Systems\".\n     SIAM. ISBN: 978-0-898716-13-9. Chapter 4-8.", "Chen, Davis, Hager, Rajamanickam (2008). \"Algorithm 887: CHOLMOD,\n     Supernodal Sparse Cholesky Factorization and Update/Downdate\".\n     ACM Trans. Math. Software 35(3)."], "see": ["ldl.h for simple LDL' factorization", "amd.h, colamd.h for fill-reducing orderings", "cholmod_finish() to free all workspace at end", "cholmod_start()", "cholmod_finish()", "cholmod_allocate_sparse() to create", "cholmod_triplet for triplet (COO) format", "cholmod_analyze() to create symbolic factor", "cholmod_factorize() to compute numeric factor", "cholmod_solve() to solve using the factor", "cholmod_allocate_dense() to create", "cholmod_solve() to solve with dense right-hand sides", "cholmod_factorize() to compute numerical factorization", "cholmod_analyze_p() to provide a user permutation", "cholmod_analyze()", "cholmod_analyze() for symbolic analysis", "cholmod_solve() to solve using the factorization", "cholmod_factorize()", "cholmod_factorize() to compute the factorization", "cholmod_solve2() for reusable workspace version", "cholmod_solve()"], "param": ["Common Workspace object to initialize", "Common Workspace to free", "A Sparse matrix to analyze (symmetric or rectangular)", "Common Control parameters and workspace", "A Sparse matrix to factorize (must match pattern from analyze)", "L Factor object from cholmod_analyze (updated with numeric values)", "Common Control parameters and workspace", "sys System to solve (see CHOLMOD_A, CHOLMOD_L, etc.)", "L Cholesky factorization from cholmod_factorize", "B Dense right-hand side matrix", "Common Control parameters and workspace"], "return": "TRUE on success, FALSE on failure", "has_pass2": true}, "LDL/Include/ldl.h": {"path": "layer-0/SuiteSparse/LDL/Include/ldl.h", "filename": "ldl.h", "file": "ldl.h", "brief": "Simple sparse LDL' factorization for symmetric matrices\n\nLDL computes a sparse LDL' factorization of a symmetric matrix A:\n  A = L * D * L'\nwhere L is unit lower triangular and D is diagonal. This factorization\nworks for symmetric indefinite matrices (D may have negative entries).\n\nThe factorization is performed in two phases:\n1. ldl_symbolic: Compute elimination tree and allocate storage\n2. ldl_numeric: Compute numerical values of L and D\n\nTriangular solves (ldl_lsolve, ldl_dsolve, ldl_ltsolve) complete the\nsolution of Ax = b.", "algorithm": "Sparse LDL' factorization (up-looking):\n  1. Symbolic: compute elimination tree and nonzero pattern of L\n     - Parent[j] = min{i > j : L(i,j) ≠ 0} defines elimination tree\n     - Lnz[j] = # nonzeros in column j of L (excluding diagonal)\n  2. Numeric: compute L and D column by column\n     - For each column j: gather contributions from columns k < j\n     - where k has nonzero in row j (i.e., L(j,k) ≠ 0)\n     - D(j,j) = A(j,j) - Σ L(j,k)² D(k,k)\n     - L(i,j) = (A(i,j) - Σ L(i,k) L(j,k) D(k,k)) / D(j,j)", "math": "A = L * D * L' where L is unit lower triangular, D is diagonal.\n  Unlike Cholesky (A = L*L'), LDL' handles indefinite matrices.\n  If A is positive definite, D has all positive entries.\n  D may have negative entries for indefinite matrices.", "complexity": "Time: O(nnz(L)²/n) average, O(nnz(L)·nnz(A)) worst case\n  Space: O(nnz(L) + n) for factors\n  Simple implementation - for production use CHOLMOD instead.", "ref": ["Davis (2005). \"Algorithm 849: A Concise Sparse Cholesky Factorization\n     Package\". ACM Trans. Math. Software 31(4):587-591."], "see": ["cholmod.h for more sophisticated Cholesky implementation", "amd.h for fill-reducing orderings to use with LDL"], "param": ["n Matrix dimension", "Ap Column pointers for A (size n+1)", "Ai Row indices for A", "Lp Output: column pointers for L (size n+1)", "Parent Output: elimination tree (size n)", "Lnz Output: # nonzeros in each column of L (size n)", "Flag Workspace (size n)", "P Fill-reducing permutation (size n, or NULL for natural order)", "Pinv Inverse permutation (size n, or NULL)", "n Matrix dimension", "Ap,Ai,Ax Sparse matrix A in CSC format", "Lp,Parent,Lnz From ldl_symbolic", "Li,Lx Output: row indices and values of L", "D Output: diagonal matrix D (size n)", "Y,Pattern,Flag Workspace arrays (size n each)", "P,Pinv Permutation and inverse (or NULL)"], "return": "n if successful, k if D[k] is zero (matrix singular at column k)", "has_pass2": true}, "COLAMD/Include/colamd.h": {"path": "layer-0/SuiteSparse/COLAMD/Include/colamd.h", "filename": "colamd.h", "file": "colamd.h", "brief": "Column Approximate Minimum Degree ordering for sparse LU factorization\n\nCOLAMD computes a column permutation Q that reduces fill-in during LU\nfactorization of an unsymmetric matrix A. The ordering minimizes the\nfill-in of A*Q when factored as LU.\n\nSYMAMD computes a symmetric ordering for a symmetric matrix, using COLAMD\non the matrix's structure. Both are related to the minimum degree family\nof algorithms.", "algorithm": "SYMAMD (Symmetric AMD using COLAMD):\n  For symmetric A, apply COLAMD to A+A' structure to find fill-reducing order.", "math": "Approximate column degree for LU:\n  deg(j) ≈ # rows in current column's nonzero pattern after elimination\n  Aggressive absorption reduces degree overestimates from element merging.", "complexity": "Time: O(nnz(A)·α(n)) average with aggressive absorption\n  Space: O(nnz(A) + n) for quotient graph representation\n  Slightly slower than AMD but better for unsymmetric patterns.", "ref": ["Davis, Gilbert, Larimore, Ng (2004). \"A Column Approximate Minimum\n     Degree Ordering Algorithm\". ACM Trans. Math. Software 30(3):353-376."], "see": ["amd.h for symmetric matrix ordering", "umfpack.h for sparse LU factorization using COLAMD", "colamd()", "colamd_recommended()", "colamd_set_defaults()", "colamd_recommended(), colamd_set_defaults()", "colamd()", "amd_order() for alternative symmetric ordering", "symamd()", "colamd_report()", "symamd_report()"], "param": ["nnz Number of nonzeros in the matrix", "n_row Number of rows in the matrix", "n_col Number of columns in the matrix", "knobs Output array of size COLAMD_KNOBS to initialize", "n_row Number of rows in A", "n_col Number of columns in A", "Alen Size of array A (use colamd_recommended to compute)", "A Input: row indices in CSC format. Output: destroyed, replaced with permutation", "p Input: column pointers. Output: column permutation Q", "knobs Control parameters (NULL uses defaults)", "stats Output statistics and error codes", "n Matrix dimension (A is n-by-n symmetric)", "A Row indices array (CSC format, lower triangle only)", "p Column pointers array of size n+1", "perm Output permutation array of size n+1", "knobs Control parameters (NULL uses defaults)", "stats Output statistics and error codes", "allocate Memory allocator function (calloc signature)", "release Memory release function (free signature)", "stats Statistics array from colamd() call", "stats Statistics array from symamd() call", "version Output array filled with [major, minor, patch]"], "return": "Recommended array size, or 0 if inputs are invalid", "has_pass2": true}, "GraphBLAS/zstd/zstd_subset/compress/zstd_ldm.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/compress/zstd_ldm.h", "filename": "zstd_ldm.h", "return": "The length of the last literals.\n\nNOTE: The source must be at most the maximum block size, but the predefined\nsequences can be any size, and may be longer than the block. In the case that\nthey are longer than the block, the last sequences may need to be split into\ntwo. We handle that case correctly, and update `rawSeqStore` appropriately.\nNOTE: This function does not return any errors.", "has_pass2": false}, "GraphBLAS/zstd/zstd_subset/compress/zstd_compress_internal.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/compress/zstd_compress_internal.h", "filename": "zstd_compress_internal.h", "return": ": 0 on success or error code", "has_pass2": false}, "GraphBLAS/zstd/zstd_subset/common/huf.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/common/huf.h", "filename": "huf.h", "return": ": 0==HUF_decompress4X1, 1==HUF_decompress4X2 .\n Assumption : 0 < dstSize <= 128 KB", "has_pass2": false}, "GraphBLAS/zstd/zstd_subset/common/zstd_trace.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/common/zstd_trace.h", "filename": "zstd_trace.h", "param": ["cctx The dctx pointer for the compression.\n            It can be used as a key to map begin() to end().", "ctx The return value of ZSTD_trace_compress_begin().", "trace The zstd tracing info.", "dctx The dctx pointer for the decompression.\n            It can be used as a key to map begin() to end().", "ctx The return value of ZSTD_trace_decompress_begin().", "trace The zstd tracing info."], "has_pass2": false}}}}}, "layer-1": {"name": "layer-1", "library_count": 4, "libraries": {"Clp": {"name": "Clp", "file_count": 86, "pass2_count": 6, "files": {"src/AbcDualRowSteepest.hpp": {"path": "layer-1/Clp/src/AbcDualRowSteepest.hpp", "filename": "AbcDualRowSteepest.hpp", "file": "AbcDualRowSteepest.hpp", "brief": "Steepest edge pivot selection for ABC dual simplex", "author": "John Forrest (FasterCoin, 2012)\n\nImplements steepest edge algorithm (Forrest-Goldfarb) for choosing the\nleaving variable in dual simplex. Normalizes infeasibilities by the\nsquared norm of the tableau row to select the direction of steepest\nimprovement.\n\nModes (controlled by constructor parameter):\n- 0: Uninitialized weights\n- 1: Full steepest edge with maintained weights\n- 2: Partial uninitialized (only scan some infeasible rows)\n- 3: Starts partial, may switch to full (default, recommended)\n\nThe ABC version optimizes weight storage and updates for cache efficiency.", "see": ["\"Implementing the Dantzig-Wolfe decomposition\" by Forrest & Goldfarb", "AbcDualRowPivot for the base interface", "AbcDualRowDantzig for simpler but often slower alternative", "ClpDualRowSteepest for the standard (non-ABC) implementation"], "has_pass2": false}, "src/ClpModelParameters.hpp": {"path": "layer-1/Clp/src/ClpModelParameters.hpp", "filename": "ClpModelParameters.hpp", "file": "ClpModelParameters.hpp", "brief": "Enumeration types for ClpModel parameter access\n\nDefines typed parameter indices for ClpModel's get/set methods:\n\nClpIntParam - Integer parameters:\n- ClpMaxNumIteration: Maximum simplex iterations\n- ClpMaxNumIterationHotStart: Hot start iteration limit\n- ClpNameDiscipline: Row/column naming mode (0=auto, 1=lazy, 2=full)\n\nClpDblParam - Double parameters:\n- ClpDualObjectiveLimit: Stop when dual objective exceeds\n- ClpPrimalObjectiveLimit: Stop when primal objective exceeds\n- ClpDualTolerance: Dual feasibility tolerance\n- ClpPrimalTolerance: Primal feasibility tolerance\n- ClpObjOffset: Constant term in objective\n- ClpMaxSeconds/ClpMaxWallSeconds: Time limits\n- ClpPresolveTolerance: Presolve zero tolerance\n\nClpStrParam - String parameters:\n- ClpProbName: Problem name from MPS file\n\nUtility templates for array operations:\n- ClpDisjointCopyN(), ClpFillN(), ClpCopyOfArray(): Non-COIN versions\n\nClpTrustedData: Opaque structure for passing user data to trusted code.", "see": ["ClpModel for parameter getter/setter methods"], "has_pass2": false}, "src/ClpGubMatrix.hpp": {"path": "layer-1/Clp/src/ClpGubMatrix.hpp", "filename": "ClpGubMatrix.hpp", "file": "ClpGubMatrix.hpp", "brief": "Generalized Upper Bound (GUB) matrix for special LP structure\n\nImplements GUB constraints - sets of variables where exactly one (or at\nmost one) must be in the basis. This structure appears in problems like\nassignment, crew scheduling, and set partitioning.\n\nGUB structure: For each \"set\" of columns, sum(x_j for j in set) = 1 (or ≤1).\nInstead of adding these as explicit rows, GUB exploits the structure to:\n- Reduce basis size (fewer rows to factorize)\n- Speed up pricing (only one variable per set can improve)\n- Handle larger problems efficiently\n\nKey concepts:\n- Each set has a \"key\" variable that's basic if any in set is basic\n- Hidden rows track GUB constraints implicitly\n- Special pivot logic handles set membership constraints", "see": ["ClpPackedMatrix which this extends", "ClpDynamicMatrix for column generation with GUB structure", "ClpSimplex for the solver that uses this matrix"], "has_pass2": false}, "src/ClpLsqr.hpp": {"path": "layer-1/Clp/src/ClpLsqr.hpp", "filename": "ClpLsqr.hpp", "file": "ClpLsqr.hpp", "brief": "LSQR iterative solver for sparse least-squares problems\n\nImplements the LSQR algorithm of Paige and Saunders (1982) for solving:\n- Ax = b (exact solve)\n- min ||b - Ax||_2 (least squares)\n- min ||(b) - (A    )x||_2 (damped/regularized)\n      ||(0)   (damp*I)  ||\n\nLSQR is a conjugate-gradient-like method based on Lanczos bidiagonalization.\nOnly requires matrix-vector products, not explicit matrix storage.\n\nReferences:\n- Paige & Saunders, \"LSQR: An algorithm for sparse linear equations\n  and sparse least squares\", ACM TOMS 8(1), 1982\n- Saunders, \"Solution of sparse rectangular systems using LSQR and CRAIG\",\n  BIT 35, 1995\n\nUsed by PDCO for iterative solution of normal equations.", "see": ["ClpPdco which uses LSQR for linear solves", "ClpPdcoBase for the problem interface"], "has_pass2": false}, "src/ClpSimplexDual.hpp": {"path": "layer-1/Clp/src/ClpSimplexDual.hpp", "filename": "ClpSimplexDual.hpp", "file": "ClpSimplexDual.hpp", "brief": "Dual simplex algorithm implementation", "author": "John Forrest\n\nImplements the dual simplex method for LP. This is a \"mix-in\" class that\ninherits from ClpSimplex but adds no data - ClpSimplex objects are cast\nto this type when running dual simplex.\n\nThe dual simplex maintains dual feasibility (reduced costs have correct signs)\nwhile iterating toward primal feasibility. It's typically faster than primal\nsimplex for most problems, especially after adding cuts in branch-and-bound.\n\nKey algorithmic features:\n- Single-phase approach with weighted objective for dual feasibility\n- Fake bounds (updatedDualBound_) to handle dual infeasibility\n- Sparse data structures to exploit problem sparsity\n- Steepest edge or Dantzig pivot selection for choosing leaving variable\n- Anti-degeneracy via cost perturbation (from OSL heritage)", "see": ["ClpSimplex for the base simplex class", "ClpSimplexPrimal for primal simplex variant", "ClpDualRowPivot for pivot row selection strategies", "ClpDualRowSteepest, ClpDualRowDantzig for specific strategies"], "has_pass2": false}, "src/ClpNonLinearCost.hpp": {"path": "layer-1/Clp/src/ClpNonLinearCost.hpp", "filename": "ClpNonLinearCost.hpp", "file": "ClpNonLinearCost.hpp", "brief": "Piecewise linear cost handling and bound infeasibility tracking\n\nManages piecewise linear objective functions and tracks bound violations\nduring primal simplex. When variables move outside their bounds, this class\ncomputes the appropriate infeasibility penalty costs.\n\nThe class supports two methods:\n- Method 1: Full piecewise linear with explicit breakpoints\n- Method 2: Simple big-M penalty for bound violations (faster)\n\nKey functionality:\n- Tracks which variables are below lower, feasible, or above upper bound\n- Computes infeasibility-weighted costs during Phase I\n- Updates costs efficiently as variables move between regions\n\nThis enables the two-phase simplex method where Phase I minimizes\ninfeasibility before Phase II minimizes the true objective.", "see": ["ClpSimplexPrimal for how this is used in primal simplex", "ClpSimplex::infeasibilityCost() for the penalty weight"], "has_pass2": false}, "src/ClpCholeskyPardiso.hpp": {"path": "layer-1/Clp/src/ClpCholeskyPardiso.hpp", "filename": "ClpCholeskyPardiso.hpp", "file": "ClpCholeskyPardiso.hpp", "brief": "Intel MKL Pardiso sparse direct solver for Cholesky factorization\n\nWraps Intel's Pardiso solver from the Math Kernel Library (MKL) for\nCholesky factorization of normal equations in interior point methods.\n\nPardiso provides:\n- Highly optimized sparse direct solving on Intel CPUs\n- Parallel factorization using OpenMP\n- Out-of-core capability for large problems\n\nRequires Intel MKL library and PARDISO_BARRIER compile flag.", "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization"], "has_pass2": false}, "src/AbcSimplex.hpp": {"path": "layer-1/Clp/src/AbcSimplex.hpp", "filename": "AbcSimplex.hpp", "file": "AbcSimplex.hpp", "brief": "AVX/SIMD-optimized simplex solver (\"A Better Clp\")", "author": "John Forrest (FasterCoin, 2012)\n\nAbcSimplex is an optimized reimplementation of ClpSimplex designed for\nmodern CPU architectures. Key optimizations include:\n\n- Cache-friendly data layout: arrays organized as [rows|columns] contiguously\n- SIMD/AVX vectorization support for matrix operations\n- Optional parallelization via pthreads or Intel Cilk Plus\n- Partitioned vectors for parallel-safe sparse operations\n\nBuild modes (CLP_HAS_ABC):\n- 1: Serial Abc, no inheritance\n- 2: Serial Abc with inheritance (ClpSimplex can use Abc internally)\n- 3: Cilk parallel, no inheritance\n- 4: Cilk parallel with inheritance\n\nThe class inherits from ClpSimplex for API compatibility but maintains\nits own internal data structures (abcSolution_, abcDj_, abcLower_, etc.)\nthat are better suited for vectorized operations.", "see": ["ClpSimplex for the standard (non-AVX) implementation", "AbcSimplexDual for AVX-optimized dual simplex", "AbcSimplexPrimal for AVX-optimized primal simplex", "AbcMatrix for the optimized matrix class"], "algorithm": "Data Layout Optimization:\n      All arrays use [structural|slack] layout instead of separate storage.\n      This improves cache locality during basis updates. Key arrays:\n      - abcSolution_[0..n+m]: primal solution values\n      - abcDj_[0..n+m]: reduced costs (dual solution)\n      - abcLower_/abcUpper_[0..n+m]: variable bounds", "complexity": "Same asymptotic complexity as ClpSimplex:\n      Per-iteration: O(m²) average for basis update\n      Practical speedup: 2-4x from SIMD, additional from parallelism", "ref": ["Forrest, J.J. and Goldfarb, D. (1992). \"Steepest-edge simplex\n      algorithms for linear programming\". Math. Programming 57:341-374."], "param": ["indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints"], "has_pass2": true}, "src/ClpLinearObjective.hpp": {"path": "layer-1/Clp/src/ClpLinearObjective.hpp", "filename": "ClpLinearObjective.hpp", "file": "ClpLinearObjective.hpp", "brief": "Standard linear objective function (c'x)\n\nImplements the standard LP objective: minimize c'x. This is a thin wrapper\naround a coefficient vector that implements the ClpObjective interface.\n\nFor quadratic objectives (x'Qx/2 + c'x), see ClpQuadraticObjective.", "see": ["ClpObjective for the abstract objective interface", "ClpQuadraticObjective for convex QP objectives", "ClpModel for how objectives are stored in the model"], "has_pass2": false}, "src/ClpNode.hpp": {"path": "layer-1/Clp/src/ClpNode.hpp", "filename": "ClpNode.hpp", "file": "ClpNode.hpp", "brief": "Node representation for branch-and-bound fathoming\n\nSupport classes for Clp's fathom capability - solving subproblems\nin a branch-and-bound tree. Used when Clp is embedded in CBC.\n\nClasses:\n- ClpNode: State of a B&B node (bounds, basis, factorization, pseudocosts)\n- ClpNodeStuff: Shared data across nodes (pseudocosts, solver options)\n- ClpHashValue: Hash table for deduplicating values\n\nKey methods:\n- applyNode(): Restore node state to model\n- chooseVariable(): Select branching variable\n- fixOnReducedCosts(): Tighten bounds using reduced cost fixing", "see": ["Cbc for the full branch-and-bound solver", "ClpSimplex which solves individual nodes"], "has_pass2": false}, "src/AbcMatrix.hpp": {"path": "layer-1/Clp/src/AbcMatrix.hpp", "filename": "AbcMatrix.hpp", "file": "AbcMatrix.hpp", "brief": "Cache-optimized matrix for ABC simplex with multiple copies", "author": "John Forrest (FasterCoin, 2012)\n\nImplements a scaled matrix wrapper optimized for ABC's data layout.\nMay maintain up to THREE copies for different access patterns:\n1. Scaled CoinPackedMatrix without gaps (column-major)\n2. Row copy partitioned by: non-basic, basic, fixed\n3. Vector copy for specific operations\n\nKey optimizations:\n- Block partitioning: NUMBER_ROW_BLOCKS/NUMBER_COLUMN_BLOCKS for parallelism\n- Row copy organized for efficient dual pricing (dualColumn1)\n- Primal column operations with CoinPartitionedVector\n\nAlso defines helper classes:\n- AbcMatrix2: Row-oriented copy for fast transpose operations\n- AbcMatrix3: Column-oriented with block structure for parallel pricing", "see": ["AbcSimplex which uses this matrix", "ClpPackedMatrix for the standard (non-ABC) matrix", "CoinPackedMatrix in CoinUtils for underlying storage"], "has_pass2": false}, "src/ClpCholeskyUfl.hpp": {"path": "layer-1/Clp/src/ClpCholeskyUfl.hpp", "filename": "ClpCholeskyUfl.hpp", "file": "ClpCholeskyUfl.hpp", "brief": "SuiteSparse CHOLMOD interface for Cholesky factorization\n\nWraps the CHOLMOD library from SuiteSparse (University of Florida) for\nCholesky factorization of normal equations in interior point methods.\n\nCHOLMOD provides:\n- Supernodal and simplicial Cholesky modes\n- Automatic fill-reducing ordering (AMD, METIS)\n- Efficient memory management for sparse factors\n\nThis is the most portable high-quality option across platforms.\nRequires SuiteSparse/CHOLMOD library.", "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization", "http://www.cise.ufl.edu/research/sparse/cholmod"], "has_pass2": false}, "src/ClpDualRowSteepest.hpp": {"path": "layer-1/Clp/src/ClpDualRowSteepest.hpp", "filename": "ClpDualRowSteepest.hpp", "file": "ClpDualRowSteepest.hpp", "brief": "Steepest edge pivot selection for dual simplex\n\nImplements the steepest edge algorithm for choosing the leaving variable\nin dual simplex. Instead of choosing the most infeasible row (Dantzig),\nsteepest edge normalizes by the squared norm of the tableau row, selecting\nthe direction of steepest improvement in the dual objective.\n\nThis is the recommended pivot strategy - it typically requires fewer\niterations than Dantzig, especially on degenerate problems.", "see": ["\"Implementing the Dantzig-Wolfe decomposition\" by Forrest & Goldfarb\n     for the steepest edge algorithm", "ClpDualRowPivot for the base interface", "ClpDualRowDantzig for simpler but often slower alternative", "ClpSimplexDual for the dual simplex algorithm"], "has_pass2": false}, "src/AbcSimplexPrimal.hpp": {"path": "layer-1/Clp/src/AbcSimplexPrimal.hpp", "filename": "AbcSimplexPrimal.hpp", "file": "AbcSimplexPrimal.hpp", "brief": "AVX-optimized primal simplex algorithm", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the primal simplex method using ABC's optimized data structures.\nLike ClpSimplexPrimal, this is a \"mix-in\" class - AbcSimplex objects are\ncast to this type at runtime when running primal simplex.\n\nKey optimizations over ClpSimplexPrimal:\n- Vectorized column selection in primalColumn()\n- Optimized ratio test in primalRow()\n- Better cache utilization through partitioned vectors\n- Support for minor iterations (multiple pivots per factorization check)\n\nThe algorithm structure mirrors ClpSimplexPrimal:\n- Choose incoming column (pivot column)\n- Compute pivot column in tableau\n- Choose outgoing row via ratio test\n- Update solution and basis\n\nSpecial features:\n- alwaysOptimal(): Don't change infeasibility cost, always report optimal\n- exactOutgoing(): Prevent variables from going slightly negative\n- lexSolve(): Lexicographic resolution for degeneracy", "see": ["AbcSimplex for the base optimized simplex class", "AbcSimplexDual for optimized dual simplex", "ClpSimplexPrimal for the standard (non-AVX) implementation"], "has_pass2": false}, "src/ClpNetworkMatrix.hpp": {"path": "layer-1/Clp/src/ClpNetworkMatrix.hpp", "filename": "ClpNetworkMatrix.hpp", "file": "ClpNetworkMatrix.hpp", "brief": "Specialized matrix for pure network LP problems\n\nImplements efficient storage for network flow problems where each column\n(arc) has exactly two nonzeros: +1 at the head node and -1 at the tail node.\nThis representation requires only O(n) storage for row indices vs O(2n) for\na general sparse matrix.\n\nNetwork problems have special structure that allows very fast simplex pivots\nand specialized algorithms. This class exploits that structure while\npresenting the standard ClpMatrixBase interface.\n\nStorage: indices_ stores pairs of row indices (tail, head) for each column.\nA row index of -1 indicates a slack arc with only one endpoint.", "see": ["ClpMatrixBase for the abstract matrix interface", "ClpPackedMatrix for general sparse matrix storage", "CoinPackedMatrix in CoinUtils for underlying sparse format"], "has_pass2": false}, "src/ClpConstraintLinear.hpp": {"path": "layer-1/Clp/src/ClpConstraintLinear.hpp", "filename": "ClpConstraintLinear.hpp", "file": "ClpConstraintLinear.hpp", "brief": "Linear constraint implementation for nonlinear extensions\n\nImplements ClpConstraint for a linear constraint: sum(a_j * x_j) = b.\nUsed with ClpSimplexNonlinear when linear constraints appear alongside\nnonlinear objective or other nonlinear constraints.\n\nStorage:\n- Sparse representation: parallel column_ and coefficient_ arrays\n- numberCoefficients_: number of nonzeros in the constraint\n\nKey methods:\n- gradient(): Returns constant gradient (the a_j coefficients)\n- markNonlinear(): Returns 0 (no nonlinear terms)\n\nThis is the simplest constraint type - the gradient is constant\nand independent of the solution point.", "see": ["ClpConstraint for the abstract interface", "ClpConstraintQuadratic for quadratic constraints", "ClpSimplexNonlinear for the nonlinear solver"], "has_pass2": false}, "src/ClpFactorization.hpp": {"path": "layer-1/Clp/src/ClpFactorization.hpp", "filename": "ClpFactorization.hpp", "file": "ClpFactorization.hpp", "brief": "Wrapper around CoinFactorization for use within Clp simplex\n\nClpFactorization adapts CoinFactorization to the Clp solver context,\nhandling the integration between the simplex algorithm and the LU\nfactorization of the basis matrix. It also supports special cases\nlike network bases (ClpNetworkBasis) and dense factorizations.\n\nKey responsibilities:\n- Factorize the basis matrix given ClpSimplex model and basic variable info\n- Provide FTRAN (forward solve) and BTRAN (backward solve) operations\n- Handle basis updates via replaceColumn (Forrest-Tomlin update)\n- Switch between factorization types (sparse LU, dense, network)", "see": ["CoinFactorization in CoinUtils for the underlying LU implementation", "ClpNetworkBasis for special network structure handling", "ClpSimplex for how factorization integrates with simplex iterations"], "has_pass2": false}, "src/ClpPEPrimalColumnDantzig.hpp": {"path": "layer-1/Clp/src/ClpPEPrimalColumnDantzig.hpp", "filename": "ClpPEPrimalColumnDantzig.hpp", "file": "ClpPEPrimalColumnDantzig.hpp", "brief": "Positive Edge enhanced Dantzig pricing for primal simplex", "author": "Jeremy Omer, Mehdi Towhidi\n\nCombines classic Dantzig pricing (most negative reduced cost) with\nPositive Edge compatibility checking. Prioritizes compatible columns\nthat can make real progress on degenerate problems.\n\nUses bi-dimensional pricing: candidates are scored by both reduced cost\nand compatibility, weighted by psi parameter (default 0.5).", "see": ["ClpPESimplex for the compatibility framework", "ClpPrimalColumnDantzig for the base pricing rule", "ClpPEPrimalColumnSteepest for steepest edge variant"], "has_pass2": false}, "src/MyEventHandler.hpp": {"path": "layer-1/Clp/src/MyEventHandler.hpp", "filename": "MyEventHandler.hpp", "file": "MyEventHandler.hpp", "brief": "Example event handler demonstrating callback customization\n\nSample implementation of ClpEventHandler showing how users can\nintercept simplex events for custom processing. Used in unit tests.\n\nTo create your own handler:\n1. Inherit from ClpEventHandler\n2. Override event() method\n3. Return 0 to continue, non-zero to stop\n4. Attach via ClpSimplex::passInEventHandler()\n\nAvailable events (see ClpEventHandler::Event):\n- endOfIteration: After each pivot\n- endOfFactorization: After basis refactorization\n- endOfValuesPass: After crash/values pass\n- node: During B&B (if used with Cbc)\n\nThe model_ pointer gives access to full solver state including\ncurrent solution, basis, and objective value.", "see": ["ClpEventHandler for the event types and interface", "ClpSimplex::passInEventHandler() to install"], "has_pass2": false}, "src/ClpParamUtils.hpp": {"path": "layer-1/Clp/src/ClpParamUtils.hpp", "filename": "ClpParamUtils.hpp", "file": "ClpParamUtils.hpp", "brief": "Parameter handler callback functions for ClpParam", "author": "Lou Hafer\n\nImplements the push/pull functions that transfer values between\nClpParam objects and ClpSimplex solver state. These callbacks are\ninvoked when parameters are modified via the command-line interface.\n\nAction handlers (doXxxParam):\n- doDebugParam(): Enable debug output\n- doExitParam(): Terminate solver\n- doHelpParam(): Display help text\n- doImportParam(): Read MPS/LP file\n- doPrintMaskParam(): Set print filter pattern\n- doSolutionParam(): Write solution file\n- doVersionParam(): Print version info\n\nPush handlers (pushClpXxxParam):\nTransfer parameter values from ClpParam to ClpSimplex:\n- pushClpDblParam(): Double parameters\n- pushClpIntParam(): Integer parameters\n- pushClpKwdParam(): Keyword (enum) parameters\n- pushClpStrParam(): String parameters\n- pushClpBoolParam(): Boolean parameters\n\nSolution I/O:\n- saveSolution()/restoreSolution(): Serialize solution state", "see": ["ClpParam for the parameter definition class", "ClpParameters for the parameter container"], "has_pass2": false}, "src/ClpPrimalColumnSteepest.hpp": {"path": "layer-1/Clp/src/ClpPrimalColumnSteepest.hpp", "filename": "ClpPrimalColumnSteepest.hpp", "file": "ClpPrimalColumnSteepest.hpp", "brief": "Steepest edge and Devex pivot selection for primal simplex\n\nImplements advanced pivot column selection strategies for primal simplex:\n- Steepest Edge: normalizes reduced costs by column norms for faster convergence\n- Devex: approximate steepest edge with cheaper weight updates\n\nThe mode parameter controls behavior:\n- 0: Exact Devex\n- 1: Full steepest edge\n- 2: Partial exact Devex (only scans some nonbasics)\n- 3: Switches between 0 and 2 based on factorization (default)\n- 4: Starts partial Dantzig/Devex, may switch to 0 or 2", "see": ["\"Implementing the Dantzig-Wolfe decomposition\" by Forrest & Goldfarb", "ClpPrimalColumnPivot for the base interface", "ClpPrimalColumnDantzig for simpler Dantzig rule", "ClpSimplexPrimal for the primal simplex algorithm"], "has_pass2": false}, "src/ClpDummyMatrix.hpp": {"path": "layer-1/Clp/src/ClpDummyMatrix.hpp", "filename": "ClpDummyMatrix.hpp", "file": "ClpDummyMatrix.hpp", "brief": "Placeholder matrix with dimensions but no data\n\nImplements ClpMatrixBase with only dimensions (rows, columns, elements)\nbut no actual matrix data. Used primarily with ClpPdco where the user\nprovides custom matrix-vector products via callbacks.\n\nAll matrix operations are valid but return empty/zero results:\n- times()/transposeTimes(): No-op (multiply by empty matrix)\n- getElements()/getIndices(): Return NULL\n- unpack(): Produces empty sparse vector\n\nUse case: PDCO interior point with user-defined operators\nWhen the constraint matrix is too large to store explicitly or has\nspecial structure (e.g., arising from discretization), the user\nimplements matrix-vector products directly instead of storing A.", "see": ["ClpPdco for the interior point method using this", "ClpPdcoBase for user-defined objective/matrix interface", "ClpMatrixBase for the full matrix interface"], "has_pass2": false}, "src/ClpSimplex.hpp": {"path": "layer-1/Clp/src/ClpSimplex.hpp", "filename": "ClpSimplex.hpp", "file": "ClpSimplex.hpp", "brief": "Main simplex solver class - orchestrates primal and dual simplex algorithms", "author": "John Forrest\n\nClpSimplex is the core solver class that implements the simplex method for\nlinear programming. It inherits problem data from ClpModel and adds all the\nalgorithm machinery: basis handling, factorization management, pivot selection,\nand the main iteration loops.\n\nThe actual algorithm implementations are split into derived mix-in classes:\n- ClpSimplexDual: dual simplex algorithm (typically faster, default choice)\n- ClpSimplexPrimal: primal simplex algorithm\n- ClpSimplexOther: auxiliary methods (crossover, parametric, etc.)\n\nKey concepts:\n- Status array: tracks whether each variable is basic, at bound, or free\n- Factorization: LU factors of the basis matrix (via ClpFactorization)\n- Pivot selection: choosing entering/leaving variables (pluggable strategies)", "algorithm": "Simplex Method for Linear Programming:\n  Maintains a basic feasible solution (BFS) at a vertex of the polytope.\n  Each iteration moves along an edge to an adjacent vertex with better objective.\n  - Primal simplex: maintains primal feasibility, achieves dual feasibility\n  - Dual simplex: maintains dual feasibility, achieves primal feasibility\n  Terminates when both primal and dual feasibility achieved (optimality).", "complexity": "O(2^n) worst-case (Klee-Minty), but typically polynomial in practice.\n  Per-iteration cost: O(m²) for basis update + O(mn) for pricing.\n  Iteration count: typically O(m) to O(3m) for practical problems.", "ref": ["Dantzig, G.B. (1963). \"Linear Programming and Extensions\".\n  Princeton University Press. [Original simplex method]", "Forrest, J.J. and Goldfarb, D. (1992). \"Steepest-edge simplex algorithms\n  for linear programming\". Math. Programming 57:341-374. [Steepest edge pricing]\n\n@note The algorithm_ member indicates variant: positive=primal, negative=dual", "Wright, S.J. (1997). \"Primal-Dual Interior-Point Methods\". SIAM."], "see": ["ClpModel for problem data storage", "ClpSimplexDual for dual simplex implementation", "ClpSimplexPrimal for primal simplex implementation", "ClpFactorization for basis factorization", "ClpDualRowPivot, ClpPrimalColumnPivot for pivot selection strategies", "CoinFactorization in CoinUtils for the underlying LU factorization"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the constraints whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints"], "has_pass2": true}, "src/ClpPEPrimalColumnSteepest.hpp": {"path": "layer-1/Clp/src/ClpPEPrimalColumnSteepest.hpp", "filename": "ClpPEPrimalColumnSteepest.hpp", "file": "ClpPEPrimalColumnSteepest.hpp", "brief": "Positive Edge enhanced steepest edge for primal simplex", "author": "Jeremy Omer, Mehdi Towhidi\n\nCombines steepest edge pricing (Forrest-Goldfarb) with Positive Edge\ncompatibility checking. The most effective anti-degeneracy variant\nfor primal simplex.\n\nModes: 0=exact devex, 1=full steepest, 2=partial exact devex,\n       3=adaptive (switches based on factorization), 4=partial Dantzig start\n\nUses bi-dimensional pricing: candidates scored by steepest edge weight\nand compatibility, weighted by psi parameter.", "see": ["ClpPESimplex for the compatibility framework", "ClpPrimalColumnSteepest for the base pricing rule", "ClpPEPrimalColumnDantzig for Dantzig variant"], "has_pass2": false}, "src/ClpSolve.hpp": {"path": "layer-1/Clp/src/ClpSolve.hpp", "filename": "ClpSolve.hpp", "file": "ClpSolve.hpp", "brief": "Algorithm selection and configuration for ClpSimplex::initialSolve()", "author": "John Forrest\n\nClpSolve is a parameter object that controls how ClpSimplex solves an LP.\nIt specifies the algorithm (dual/primal simplex, barrier), presolve settings,\ncrash procedures, and various solver options.\n\nSolve types available:\n- useDual: Dual simplex (default, usually fastest)\n- usePrimal: Primal simplex\n- usePrimalorSprint: Primal with sprint heuristic for large problems\n- useBarrier: Interior point method (requires Cholesky factorization)\n- automatic: Let solver choose based on problem characteristics\n\nCrash options for initial basis:\n- Idiot: Heuristic crash using pseudo-interior point ideas\n- Sprint: Column generation-like approach for large sparse problems\n- All slack: Simple but safe starting point", "see": ["ClpSimplex::initialSolve() which uses this class", "ClpInterior for barrier/interior point implementation", "Idiot.hpp for the Idiot crash procedure"], "has_pass2": false}, "src/AbcWarmStart.hpp": {"path": "layer-1/Clp/src/AbcWarmStart.hpp", "filename": "AbcWarmStart.hpp", "file": "AbcWarmStart.hpp", "brief": "Extended warm start with factorization caching for ABC", "author": "John Forrest (FasterCoin, 2012)\n\nExtends CoinWarmStartBasis to store additional information for efficient\nrestart of ABC simplex. Beyond basic/nonbasic status, can store:\n\nType encoding:\n- 0: Standard basis (as CoinWarmStartBasis)\n- 1,2: Plus factor order as shorts/ints (high bit = column)\n- 3,4: Plus compact saved factorization\n- +8: Steepest edge weights stored (as floats)\n\nAlso defines AbcWarmStartOrganizer for managing multiple warm starts\nin tree search (branch and bound) scenarios.\n\nKey features:\n- deleteRows()/deleteColumns(): Maintain valid basis after problem changes\n- createBasis0/12/34(): Create warm starts of different types\n- Support for incremental factorization updates", "see": ["CoinWarmStartBasis in CoinUtils for the base class", "AbcSimplex which uses warm starts", "ClpSimplex::getWarmStart() for standard warm start creation"], "has_pass2": false}, "src/Idiot.hpp": {"path": "layer-1/Clp/src/Idiot.hpp", "filename": "Idiot.hpp", "file": "Idiot.hpp", "brief": "Heuristic crash procedure for finding initial LP solutions\n\nThe \"Idiot\" algorithm is a simple heuristic that finds approximate primal\nsolutions quickly. Despite its self-deprecating name (which is copylefted!),\nit serves as an effective \"crash\" procedure to warm-start the simplex method.\n\nThe algorithm works well on problems where the volume algorithm succeeds,\noften finding better primal solutions, though it produces no dual solution.\nIt's particularly useful for:\n- Getting a starting basis before simplex (crash mode)\n- Homogeneous problems with similar constraint structure\n\nKey methods:\n- crash(): Lightweight initialization (2-5 passes typical)\n- solve(): Full approximate solution (50-100 iterations)\n- crossOver(): Transition to simplex for exact solution", "see": ["ClpSimplex::initialSolve() which may call Idiot automatically", "ClpSimplexPrimal for the simplex algorithm that uses the crash solution"], "has_pass2": false}, "src/AbcNonLinearCost.hpp": {"path": "layer-1/Clp/src/AbcNonLinearCost.hpp", "filename": "AbcNonLinearCost.hpp", "file": "AbcNonLinearCost.hpp", "brief": "Piecewise linear cost and infeasibility tracking for ABC", "author": "John Forrest (FasterCoin, 2012)\n\nManages bound violations and infeasibility costs during ABC primal simplex.\nTracks whether each variable is below lower, feasible, or above upper bound,\nand computes appropriate penalty costs.\n\nStatus encoding (4 bits original, 4 bits current):\n- CLP_BELOW_LOWER (0): Below lower bound, stored bound is upper\n- CLP_FEASIBLE (1): Within bounds\n- CLP_ABOVE_UPPER (2): Above upper bound, stored bound is lower\n- CLP_SAME (4): Current status same as original\n\nKey methods:\n- checkInfeasibilities(): Update costs based on current solution\n- setOne()/setOneBasic(): Update bounds and cost for a variable\n- changeInCost(): Cost change when moving through a bound\n\nThis enables two-phase simplex where Phase I minimizes infeasibility.", "see": ["AbcSimplexPrimal which uses this for infeasibility tracking", "ClpNonLinearCost for the standard (non-ABC) implementation"], "has_pass2": false}, "src/ClpInterior.hpp": {"path": "layer-1/Clp/src/ClpInterior.hpp", "filename": "ClpInterior.hpp", "file": "ClpInterior.hpp", "brief": "Interior point (barrier) method for LP", "author": "John Tomlin (PDCO), John Forrest (predictor-corrector)\n\nImplements interior point methods as an alternative to simplex for LP.\nInterior point follows a central path through the interior of the feasible\nregion, converging to optimality. Often faster than simplex for very large\nor dense problems.\n\nTwo main variants supported:\n- Predictor-Corrector: Standard Mehrotra-style algorithm\n- PDCO: Primal-Dual Barrier method with regularization\n\nRequires Cholesky factorization for the normal equations A*D*A'.\nMultiple Cholesky backends available (dense, MUMPS, TAUCS, etc.)", "see": ["ClpPredictorCorrector for the predictor-corrector implementation", "ClpPdco for PDCO variant", "ClpCholeskyBase for Cholesky factorization interface", "ClpSimplex for the alternative simplex method"], "has_pass2": false}, "src/ClpCholeskyDense.hpp": {"path": "layer-1/Clp/src/ClpCholeskyDense.hpp", "filename": "ClpCholeskyDense.hpp", "file": "ClpCholeskyDense.hpp", "brief": "Dense Cholesky factorization for interior point methods\n\nImplements Cholesky factorization when A*D*A' becomes effectively dense.\nUses blocked recursive algorithms for cache efficiency and supports\nparallel execution via ClpCholeskySpawn.\n\nThis is used as a fallback from sparse Cholesky when fill-in is excessive,\nor for problems with inherently dense normal equations.\n\nKey methods:\n- factorize(): Dense LL' factorization with row dropping for singularity\n- solve(): Forward/backward triangular solves (solveF1/B1 variants)\n\nAlso exports C functions for recursive blocked factorization that can be\nparallelized (ClpCholeskyCfactor, etc.).", "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses Cholesky for normal equations"], "has_pass2": false}, "src/CoinAbcCommon.hpp": {"path": "layer-1/Clp/src/CoinAbcCommon.hpp", "filename": "CoinAbcCommon.hpp", "file": "CoinAbcCommon.hpp", "brief": "Common definitions for ABC (A Better Coin) optimized simplex\n\nCore type definitions and macros for the FasterCoin 2012 SIMD-optimized\nsimplex implementation. Provides:\n- Type aliases: CoinSimplexDouble, CoinSimplexInt\n- Compiler-specific forced inlining (ABC_INLINE)\n- Parallelism control: ABC_PARALLEL (0=none, 1=pthreads, 2=Cilk)\n- LAPACK integration options (ABC_USE_LAPACK, ABC_DENSE_CODE)\n- Low-level zero testing macros using IEEE 754 bit patterns\n- AbcTolerancesEtc class for algorithm tolerance parameters", "see": ["AbcSimplex for the main ABC simplex implementation", "CoinAbcFactorization for the optimized factorization"], "has_pass2": false}, "src/ClpCholeskyTaucs.hpp": {"path": "layer-1/Clp/src/ClpCholeskyTaucs.hpp", "filename": "ClpCholeskyTaucs.hpp", "file": "ClpCholeskyTaucs.hpp", "brief": "TAUCS sparse solver interface for Cholesky factorization\n\nWraps Sivan Toledo's TAUCS library for Cholesky factorization of normal\nequations in interior point methods.\n\nTAUCS provides:\n- Left-looking supernodal factorization\n- Multiple ordering methods (AMD, METIS, GENMMD)\n- Research-quality implementation with educational focus\n\nNote: Requires modifications to taucs.h for C++ linkage and LAPACK\ndpotf2.f for singularity handling (see class documentation).", "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization", "http://www.tau.ac.il/~stoledo/taucs/"], "has_pass2": false}, "src/ClpDualRowDantzig.hpp": {"path": "layer-1/Clp/src/ClpDualRowDantzig.hpp", "filename": "ClpDualRowDantzig.hpp", "file": "ClpDualRowDantzig.hpp", "brief": "Dantzig's rule for dual simplex pivot selection\n\nImplements the simplest pivot row selection: choose the basic variable\nwith the largest primal infeasibility. Simple and fast per iteration,\nbut may require many more iterations than steepest edge on degenerate\nor difficult problems.\n\nThis is Dantzig's original 1947 rule applied to dual simplex.\nUse ClpDualRowSteepest for better performance on most problems.", "see": ["ClpDualRowPivot for the base interface", "ClpDualRowSteepest for recommended steepest edge variant", "ClpSimplexDual for the dual simplex algorithm"], "has_pass2": false}, "src/ClpMatrixBase.hpp": {"path": "layer-1/Clp/src/ClpMatrixBase.hpp", "filename": "ClpMatrixBase.hpp", "file": "ClpMatrixBase.hpp", "brief": "Abstract base class for constraint matrices in Clp\n\nDefines the interface that all matrix types must implement for use with\nClp algorithms. The abstraction allows specialized matrix formats that\nexploit structure (network, GUB, ±1 matrices) while providing a uniform\ninterface to the simplex solver.\n\nKey virtual methods:\n- times(), transposeTimes(): Matrix-vector multiplication (y = A*x, y = A'*x)\n- subsetTransposeTimes(): Partial BTRAN for steepest edge pricing\n- getPackedMatrix(): Access underlying CoinPackedMatrix\n- scale(): Apply row/column scaling", "see": ["ClpPackedMatrix for standard sparse matrix (wraps CoinPackedMatrix)", "ClpNetworkMatrix for pure network structure", "ClpPlusMinusOneMatrix for ±1 coefficient matrices", "ClpGubMatrix for generalized upper bound structure", "CoinPackedMatrix in CoinUtils for sparse storage format"], "has_pass2": false}, "src/ClpCholeskyWssmpKKT.hpp": {"path": "layer-1/Clp/src/ClpCholeskyWssmpKKT.hpp", "filename": "ClpCholeskyWssmpKKT.hpp", "file": "ClpCholeskyWssmpKKT.hpp", "brief": "WSSMP solver for KKT system (augmented system) formulation\n\nVariant of ClpCholeskyWssmp that solves the KKT/augmented system directly\ninstead of forming and factoring the normal equations A*D*A'.\n\nThe KKT system has the form:\n  [ -D   A' ] [ dx ]   [ r1 ]\n  [  A   0  ] [ dy ] = [ r2 ]\n\nSolving this 2x2 block system avoids forming A*D*A' explicitly, which\ncan be beneficial when A*D*A' would be very dense but the augmented\nsystem remains sparse.\n\nAdds solveKKT() method for direct augmented system solves.", "see": ["ClpCholeskyWssmp for the normal equations variant", "ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization"], "has_pass2": false}, "src/ClpCholeskyMumps.hpp": {"path": "layer-1/Clp/src/ClpCholeskyMumps.hpp", "filename": "ClpCholeskyMumps.hpp", "file": "ClpCholeskyMumps.hpp", "brief": "MUMPS sparse direct solver interface for Cholesky factorization\n\nWraps the MUMPS (MUltifrontal Massively Parallel sparse direct Solver)\nlibrary for Cholesky factorization of normal equations in interior point.\n\nMUMPS provides:\n- Multifrontal factorization with supernodes\n- Distributed memory parallelism (MPI)\n- Dense column handling for improved performance\n\nRequires MUMPS library to be installed and linked. The DMUMPS_STRUC_C\nstructure is forward-declared as void for compilation without MUMPS headers.", "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization", "http://mumps.enseeiht.fr/ for MUMPS documentation"], "has_pass2": false}, "src/AbcCommon.hpp": {"path": "layer-1/Clp/src/AbcCommon.hpp", "filename": "AbcCommon.hpp", "file": "AbcCommon.hpp", "brief": "Configuration macros for ABC (A Better Clp) build modes", "author": "John Forrest (FasterCoin, 2012)\n\nControls how the ABC optimized simplex code is built and integrated.\n\nCLP_HAS_ABC values:\n- 0: ABC disabled (use standard ClpSimplex only)\n- 1: Serial ABC, standalone (no inheritance into ClpSimplex)\n- 2: Serial ABC with inheritance (ClpSimplex can delegate to ABC)\n- 3: Cilk parallel ABC, standalone\n- 4: Cilk parallel ABC with inheritance\n\nABC_PARALLEL values:\n- 0: No parallelization\n- 1: Use pthreads for parallelization\n- 2: Use Intel Cilk Plus for parallelization\n\nABC_INHERIT: When defined, ClpSimplex::initialSolve() can automatically\nuse AbcSimplex when beneficial.", "see": ["AbcSimplex for the optimized simplex implementation", "ClpSimplex for the standard implementation"], "has_pass2": false}, "src/ClpConstraint.hpp": {"path": "layer-1/Clp/src/ClpConstraint.hpp", "filename": "ClpConstraint.hpp", "file": "ClpConstraint.hpp", "brief": "Abstract base class for nonlinear constraints\n\nDefines the interface for general (potentially nonlinear) constraints\nused in nonlinear programming extensions. The standard LP constraints\n(Ax ≤ b) are handled directly by ClpModel; this class is for more\ngeneral constraint forms g(x) ≤ 0.\n\nKey methods that derived classes must implement:\n- gradient(): Compute constraint gradient and function value\n- markNonlinear(): Identify which variables appear nonlinearly\n- markNonzero(): Identify sparsity pattern of gradient\n\nThe type_ member indicates constraint type: 0=linear, 1=nonlinear.\nRow number -1 indicates this is an objective function, not a constraint.", "see": ["ClpObjective for objective function handling (similar interface)", "ClpModel for standard linear constraint handling"], "has_pass2": false}, "src/ClpPackedMatrix.hpp": {"path": "layer-1/Clp/src/ClpPackedMatrix.hpp", "filename": "ClpPackedMatrix.hpp", "file": "ClpPackedMatrix.hpp", "brief": "Standard sparse matrix implementation for Clp (wraps CoinPackedMatrix)\n\nThe default matrix type for Clp, implementing ClpMatrixBase using\nCoinPackedMatrix for sparse storage. This is appropriate for general\nsparse LP matrices without special structure.\n\nAdds Clp-specific functionality on top of CoinPackedMatrix:\n- Scaling integration with the solver\n- Optimized matrix-vector products for simplex\n- Row copy management for efficient BTRAN", "see": ["ClpMatrixBase for the abstract interface", "CoinPackedMatrix in CoinUtils for underlying storage", "ClpNetworkMatrix for network-structured problems", "ClpPlusMinusOneMatrix for ±1 matrices"], "has_pass2": false}, "src/ClpPdcoBase.hpp": {"path": "layer-1/Clp/src/ClpPdcoBase.hpp", "filename": "ClpPdcoBase.hpp", "file": "ClpPdcoBase.hpp", "brief": "Abstract base class for PDCO problem customization\n\nStrategy pattern interface for defining custom convex objectives in PDCO.\nDerived classes provide:\n- matVecMult(): Matrix-vector products with A and A'\n- getGrad(): Gradient of the objective function\n- getHessian(): Diagonal Hessian approximation\n- getObj(): Objective function value\n- matPrecon(): Preconditioner application\n\nThe d1, d2 scalars are regularization parameters that make the\naugmented system nonsingular (d1 for primal, d2 for dual regularization).", "see": ["ClpPdco which uses this interface", "ClpLsqr for the iterative solver"], "has_pass2": false}, "src/ClpPresolve.hpp": {"path": "layer-1/Clp/src/ClpPresolve.hpp", "filename": "ClpPresolve.hpp", "file": "ClpPresolve.hpp", "brief": "Clp interface to CoinPresolve for LP preprocessing\n\nWraps the CoinPresolve framework for use with Clp models. Presolve\nsimplifies an LP before solving by applying reversible transformations:\nremoving fixed variables, empty rows/columns, redundant constraints, etc.\n\nThe typical workflow is:\n1. presolvedModel() - create simplified model\n2. Solve the presolved model\n3. postsolve() - recover solution to original model", "see": ["CoinPresolveMatrix in CoinUtils for the transformation framework", "ClpSimplex::initialSolve() which can invoke presolve automatically"], "has_pass2": false}, "src/ClpObjective.hpp": {"path": "layer-1/Clp/src/ClpObjective.hpp", "filename": "ClpObjective.hpp", "file": "ClpObjective.hpp", "brief": "Abstract base class for objective functions\n\nDefines the interface for objective functions in Clp. Derived classes\nimplement specific objective types (linear, quadratic, etc.).\n\nKey methods that derived classes must implement:\n- gradient(): Returns objective gradient at current solution\n- reducedGradient(): Gradient accounting for basic/nonbasic structure\n- stepLength(): Optimal step size in a given direction\n- objectiveValue(): Evaluate objective at a point\n\nThe type_ member indicates the objective type: 1=linear, 2=quadratic.", "see": ["ClpLinearObjective for standard LP objective (c'x)", "ClpQuadraticObjective for QP objective (x'Qx/2 + c'x)", "ClpModel for how objectives are used in models"], "has_pass2": false}, "src/ClpPlusMinusOneMatrix.hpp": {"path": "layer-1/Clp/src/ClpPlusMinusOneMatrix.hpp", "filename": "ClpPlusMinusOneMatrix.hpp", "file": "ClpPlusMinusOneMatrix.hpp", "brief": "Specialized matrix where all nonzeros are +1 or -1\n\nEfficient storage for constraint matrices where every coefficient is either\n+1 or -1. Common in set partitioning, covering, and assignment problems.\nNo element values are stored - only row indices, with separate start arrays\nfor positive and negative entries in each column.\n\nStorage: For each column, startPositive_[j] to startNegative_[j]-1 are +1s,\nand startNegative_[j] to startPositive_[j+1]-1 are -1s.\n\nAlso defines ClpPoolMatrix (when CLP_POOL_MATRIX is set) for matrices with\nfew distinct coefficient values, using a pool of unique elements.", "see": ["ClpMatrixBase for the abstract matrix interface", "ClpNetworkMatrix for pure network structure (+1/-1 with 2 per column)", "ClpPackedMatrix for general sparse storage"], "has_pass2": false}, "src/ClpHelperFunctions.hpp": {"path": "layer-1/Clp/src/ClpHelperFunctions.hpp", "filename": "ClpHelperFunctions.hpp", "file": "ClpHelperFunctions.hpp", "brief": "BLAS-1 style dense vector operations for Clp\n\nNon-templated dense array operations optimized for LP use. These exist\nseparately from CoinDenseVector to enable architecture-specific optimization\nwithout template overhead (per JJF's design note).\n\nBasic operations (double and CoinWorkDouble variants):\n- maximumAbsElement(): L-infinity norm (max |x_i|)\n- setElements(): Fill array with constant\n- multiplyAdd(): y = a1*x + a2*y (AXPY variant)\n- innerProduct(): x'y dot product\n- getNorms(): Simultaneous L1 and L2 norm computation\n\nExtended precision support (COIN_LONG_WORK):\nWhen enabled, provides long double versions for extra precision in\nnumerically sensitive computations (e.g., basis factorization).\n\nPDCO helper functions (when ClpPdco_H defined):\n- pdxxxmerit(): Merit function for Newton's method\n- pdxxxresid1(): Primal-dual residuals\n- pdxxxresid2(): Complementarity residuals\n- pdxxxstep(): Line search step length\n\nClpTraceDebug: Debug assertion macro (active when NDEBUG not defined).", "see": ["CoinHelperFunctions for general utility functions", "ClpPdco for the PDCO interior point method"], "has_pass2": false}, "src/ClpQuadraticObjective.hpp": {"path": "layer-1/Clp/src/ClpQuadraticObjective.hpp", "filename": "ClpQuadraticObjective.hpp", "file": "ClpQuadraticObjective.hpp", "brief": "Quadratic objective function for convex QP (x'Qx/2 + c'x)\n\nImplements convex quadratic objectives for quadratic programming.\nThe quadratic term is stored as a CoinPackedMatrix Q, supporting\nboth full symmetric and half (lower triangular) storage.\n\nThe objective is: minimize (1/2)x'Qx + c'x\nwhere Q must be positive semi-definite for convexity.\n\nFor pure linear objectives, use ClpLinearObjective instead.", "see": ["ClpObjective for the abstract objective interface", "ClpLinearObjective for linear-only objectives", "ClpSimplex::loadQuadraticObjective() for loading QP problems"], "has_pass2": false}, "src/ClpEventHandler.hpp": {"path": "layer-1/Clp/src/ClpEventHandler.hpp", "filename": "ClpEventHandler.hpp", "file": "ClpEventHandler.hpp", "brief": "Callback interface for handling solver events during optimization\n\nProvides a mechanism for user code to receive callbacks during the solve\nprocess. Users derive from ClpEventHandler and override event() to handle\nevents like end of iteration, factorization, or presolve stages.\n\nCommon uses:\n- Handling Ctrl-C to gracefully stop optimization\n- Logging progress or updating a GUI\n- Implementing custom termination criteria\n- Monitoring solution quality during solve\n\nAlso defines ClpDisasterHandler for recovery from numerical difficulties.", "see": ["ClpSimplex::setEventHandler() to install a handler", "ClpModel::secondaryStatus() for event-based status codes"], "has_pass2": false}, "src/ClpPredictorCorrector.hpp": {"path": "layer-1/Clp/src/ClpPredictorCorrector.hpp", "filename": "ClpPredictorCorrector.hpp", "file": "ClpPredictorCorrector.hpp", "brief": "Mehrotra's predictor-corrector interior point algorithm", "author": "John Forrest\n\nImplements the primal-dual interior point method for LP/QP using\nMehrotra's predictor-corrector with Gondzio's multiple centrality corrections.\n\nReferences:\n- Mehrotra, \"On the implementation of a primal-dual interior point method\",\n  SIAM Journal on Optimization, 2 (1992)\n- Gondzio, \"Multiple centrality corrections in a primal-dual method\",\n  Computational Optimization and Applications, 6 (1996)\n\nThis is a mix-in class - ClpInterior objects are cast to this type at\nalgorithm time. No additional data is stored.\n\nKey methods:\n- solve(): Main predictor-corrector iteration loop\n- findStepLength(): Compute max step maintaining positivity\n- complementarityGap(): Measure of optimality (should → 0)", "see": ["ClpInterior for the base interior point class", "ClpCholeskyBase for the linear system solver", "ClpPdco for PDCO alternative algorithm"], "has_pass2": false}, "src/ClpSimplexPrimal.hpp": {"path": "layer-1/Clp/src/ClpSimplexPrimal.hpp", "filename": "ClpSimplexPrimal.hpp", "file": "ClpSimplexPrimal.hpp", "brief": "Primal simplex algorithm implementation", "author": "John Forrest\n\nImplements the primal simplex method for LP. This is a \"mix-in\" class that\ninherits from ClpSimplex but adds no data - ClpSimplex objects are cast\nto this type when running primal simplex.\n\nThe primal simplex maintains primal feasibility (variables within bounds)\nwhile iterating toward dual feasibility (optimality). Useful when starting\nfrom a feasible solution or when dual simplex struggles.\n\nKey algorithmic features:\n- Single-phase approach with infeasibilityCost_ weighting\n- Explicit bounds on reduced costs for feasibility handling\n- Sparse data structures exploiting problem sparsity\n- Steepest edge or Dantzig pivot selection for entering variable\n- Anti-degeneracy via cost perturbation\n- Supports nonlinear costs (though not heavily tested)", "see": ["ClpSimplex for the base simplex class", "ClpSimplexDual for dual simplex variant (often faster)", "ClpPrimalColumnPivot for pivot column selection strategies", "ClpPrimalColumnSteepest, ClpPrimalColumnDantzig for specific strategies"], "has_pass2": false}, "src/ClpPdco.hpp": {"path": "layer-1/Clp/src/ClpPdco.hpp", "filename": "ClpPdco.hpp", "file": "ClpPdco.hpp", "brief": "PDCO (Primal-Dual interior point for Convex Objectives) algorithm", "author": "John Tomlin\n\nImplements the PDCO algorithm for convex optimization problems, an\nalternative to Mehrotra's predictor-corrector method.\n\nPDCO characteristics:\n- Uses iterative LSQR for linear systems (vs. direct Cholesky)\n- Better for poorly conditioned normal equations\n- Supports user-defined objective via ClpPdcoBase\n- Regularization parameters d1, d2 for numerical stability\n\nThis is a mix-in class - ClpInterior objects are cast to this type at\nalgorithm time. No additional data is stored.", "see": ["ClpPdcoBase for user-customizable objective/Hessian", "ClpLsqr for the iterative linear solver", "ClpPredictorCorrector for the alternative direct method"], "has_pass2": false}, "src/AbcSimplexDual.hpp": {"path": "layer-1/Clp/src/AbcSimplexDual.hpp", "filename": "AbcSimplexDual.hpp", "file": "AbcSimplexDual.hpp", "brief": "AVX-optimized dual simplex algorithm", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the dual simplex method using ABC's optimized data structures.\nLike ClpSimplexDual, this is a \"mix-in\" class - AbcSimplex objects are\ncast to this type at runtime when running dual simplex.\n\nKey optimizations over ClpSimplexDual:\n- Vectorized ratio tests in dualColumn1/dualColumn2\n- Parallel pricing with pthreads or Cilk (whileIteratingThread/Cilk)\n- Better cache utilization through partitioned vectors\n- Optimized bound flipping with flipBounds()\n\nThe algorithm follows the same structure as ClpSimplexDual:\nouter loop (factorize/clean) -> inner loop (pivot iterations)\n\nParallelization modes:\n- Serial: whileIteratingSerial()\n- pthreads: whileIteratingThread()\n- Cilk: whileIteratingCilk()", "see": ["AbcSimplex for the base optimized simplex class", "AbcSimplexPrimal for optimized primal simplex", "ClpSimplexDual for the standard (non-AVX) implementation"], "has_pass2": false}, "src/ClpSolver.hpp": {"path": "layer-1/Clp/src/ClpSolver.hpp", "filename": "ClpSolver.hpp", "file": "ClpSolver.hpp", "brief": "Standalone Clp solver driver and command-line interface\n\nProvides the main entry points for running Clp as a standalone solver:\n- ClpMain0(): Initialize model with default settings\n- ClpMain1(): Process command queue or command-line arguments\n\nFeatures:\n- Signal handler: Ctrl+C sets iteration limit to 0 for graceful stop\n- BLAS threading control: Defaults to single-thread (openblas_set_num_threads)\n- AMPL interface: clpReadAmpl() reads .nl files\n- FOREIGN_BARRIER: Detects availability of external Cholesky solvers\n\nABC_INHERIT mode: When enabled, uses AbcSimplex instead of ClpSimplex\nfor SIMD-optimized solving.\n\nUser-driven extensions (CLP_USER_DRIVEN1):\n- userChoiceValid1(): Validate leaving variable choice\n- userChoiceValid2(): Validate in/out pair\n- userChoiceWasGood(): Post-pivot callback", "see": ["ClpParameters for parameter handling", "AbcSimplex for the ABC optimized variant", "Clp_C_Interface.h for C language bindings"], "has_pass2": false}, "src/AbcPrimalColumnDantzig.hpp": {"path": "layer-1/Clp/src/AbcPrimalColumnDantzig.hpp", "filename": "AbcPrimalColumnDantzig.hpp", "file": "AbcPrimalColumnDantzig.hpp", "brief": "Dantzig's rule for ABC primal simplex pivot selection", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the simplest pivot selection: choose the nonbasic variable\nwith most negative reduced cost. This is Dantzig's original 1947 rule\nadapted for the ABC optimized simplex framework.\n\nAs the comments note, this \"lumbers over all columns - slow\" but is\nuseful for debugging or when steepest edge overhead isn't worthwhile.\n\nUse AbcPrimalColumnSteepest for better performance on most problems.", "see": ["AbcPrimalColumnPivot for the base interface", "AbcPrimalColumnSteepest for recommended steepest edge variant", "ClpPrimalColumnDantzig for the standard (non-ABC) implementation"], "has_pass2": false}, "src/ClpSimplexOther.hpp": {"path": "layer-1/Clp/src/ClpSimplexOther.hpp", "filename": "ClpSimplexOther.hpp", "file": "ClpSimplexOther.hpp", "brief": "Auxiliary simplex operations: ranging, parametrics, and utilities", "author": "John Forrest\n\nMix-in class providing operations beyond basic primal/dual simplex:\n\nSensitivity Analysis (Ranging):\n- dualRanging(): How much can objective coefficients change?\n- primalRanging(): How much can variable bounds change?\n\nParametric Programming:\n- parametrics(): Solve family of LPs as parameters vary continuously\n- Trace optimal solution as bounds/costs change from start to end theta\n\nModel Manipulation:\n- dualOfModel(): Create dual formulation of the LP\n- crunch(): Quick presolve for subproblem extraction\n- gubVersion(): Convert to GUB-structured model\n\nI/O:\n- writeBasis()/readBasis(): MPS format basis files\n\nLike ClpSimplexDual/Primal, this is a mix-in - objects are cast at runtime.", "see": ["ClpSimplex which this extends", "ClpSimplexDual for dual simplex algorithm", "ClpSimplexPrimal for primal simplex algorithm"], "has_pass2": false}, "src/AbcPrimalColumnSteepest.hpp": {"path": "layer-1/Clp/src/AbcPrimalColumnSteepest.hpp", "filename": "AbcPrimalColumnSteepest.hpp", "file": "AbcPrimalColumnSteepest.hpp", "brief": "Steepest edge and Devex for ABC primal simplex", "author": "John Forrest (FasterCoin, 2012)\n\nImplements advanced pivot column selection for ABC primal simplex:\n- Steepest Edge: Normalizes reduced costs by column norms\n- Devex: Approximate steepest edge with cheaper updates\n\nModes (controlled by constructor parameter):\n- 0: Exact Devex\n- 1: Full steepest edge\n- 2: Partial exact Devex (scan subset of nonbasics)\n- 3: Switches between 0/2 based on factorization (default)\n- 4: Starts as partial Dantzig/Devex, may switch\n- 5: Always partial Dantzig\n- >=10: Mini-sprint mode\n\nKey optimizations over ClpPrimalColumnSteepest:\n- Uses CoinPartitionedVector for parallel column blocks\n- Vectorized weight updates in djsAndDevex()\n- Efficient partial pricing with partialPricing()", "see": ["AbcPrimalColumnPivot for the base interface", "AbcPrimalColumnDantzig for simpler alternative", "ClpPrimalColumnSteepest for the standard (non-ABC) implementation"], "has_pass2": false}, "src/ClpParam.hpp": {"path": "layer-1/Clp/src/ClpParam.hpp", "filename": "ClpParam.hpp", "file": "ClpParam.hpp", "brief": "Individual parameter definitions for Clp control", "author": "Lou Hafer\n\nExtends CoinParam with Clp-specific parameter codes and types.\nClpParamCode enumeration defines all configurable options:\n\n- Action params: DUALSIMPLEX, PRIMALSIMPLEX, BARRIER, etc.\n- Boolean params: AUTOSCALE, PERTURBATION, SPARSEFACTOR\n- Keyword params: CHOLESKY, CRASH, PRESOLVE, SCALING\n- Integer params: MAXITERATION, IDIOT, SPRINT, THREADS\n- Double params: DUALTOLERANCE, PRIMALTOLERANCE, TIMELIMIT\n\nEach parameter has push/pull functions for value validation\nand transfer between parameter objects and solver state.", "see": ["ClpParameters for the parameter container", "CoinParam for the base parameter class"], "has_pass2": false}, "src/CbcOrClpParam.hpp": {"path": "layer-1/Clp/src/CbcOrClpParam.hpp", "filename": "CbcOrClpParam.hpp", "file": "CbcOrClpParam.hpp", "brief": "Shared parameter codes for Cbc and Clp solvers\n\nLegacy parameter handling shared between Cbc (branch-and-cut) and Clp\n(linear programming). This file is intentionally duplicated between\nCbc/Test and Clp/Test for simplicity.\n\nParameter code ranges (CbcOrClpParameterType):\n- 1-100: Double parameters (CLP_PARAM_DBL_*, CBC_PARAM_DBL_*)\n- 101-200: Integer parameters (CLP_PARAM_INT_*, CBC_PARAM_INT_*)\n- 201-300: Clp string parameters\n- 301-400: Cbc string parameters\n- 401-500: Clp actions (mostly)\n- 501-600: Cbc actions (mostly)\n\nConditional compilation:\n- COIN_HAS_CBC: Include Cbc-specific parameters\n- COIN_HAS_CLP: Include Clp-specific parameters\n\nNote: Being superseded by ClpParam/CbcParam for cleaner separation.", "see": ["ClpParam for modern Clp parameter handling", "ClpParameters for parameter container\n@deprecated Use ClpParam for new code"], "has_pass2": false}, "src/ClpSimplexNonlinear.hpp": {"path": "layer-1/Clp/src/ClpSimplexNonlinear.hpp", "filename": "ClpSimplexNonlinear.hpp", "file": "ClpSimplexNonlinear.hpp", "brief": "Nonlinear LP solver using reduced gradient and SLP methods", "author": "John Forrest\n\nExtends ClpSimplexPrimal to handle nonlinear objectives and constraints.\nTwo main algorithms available:\n\n1. Reduced Gradient (primal()):\n   - For nonlinear objectives with linear constraints\n   - Computes gradient at current point\n   - Uses simplex-style basis changes\n\n2. Sequential Linear Programming (primalSLP()):\n   - Linearizes nonlinear constraints at current point\n   - Solves LP approximation with trust region\n   - Iterates until convergence\n   - deltaTolerance controls trust region size\n\nImplementation note: This class has no data members - it's a \"behavior\"\nclass that is cast from ClpSimplexPrimal at algorithm time. This avoids\nobject slicing but requires careful use.\n\nKey methods:\n- directionVector(): Computes search direction from reduced costs\n- whileIterating(): Main iteration loop with pivot mode control\n- pivotColumn(): Selects entering variable considering nonlinearity", "see": ["ClpSimplexPrimal for the base primal simplex", "ClpConstraint for nonlinear constraint interface", "ClpQuadraticObjective for quadratic objectives"], "has_pass2": false}, "src/AbcDualRowDantzig.hpp": {"path": "layer-1/Clp/src/AbcDualRowDantzig.hpp", "filename": "AbcDualRowDantzig.hpp", "file": "AbcDualRowDantzig.hpp", "brief": "Dantzig's rule for ABC dual simplex pivot selection", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the simplest pivot selection: choose the basic variable with\nlargest primal infeasibility. This is Dantzig's original 1947 rule\nadapted for the ABC optimized simplex framework.\n\nFast per iteration but may require more iterations than steepest edge.\nUse AbcDualRowSteepest for better performance on most problems.\n\nThe implementation maintains an infeasible_ vector for efficient\ntracking of which rows are currently infeasible.", "see": ["AbcDualRowPivot for the base interface", "AbcDualRowSteepest for recommended steepest edge variant", "ClpDualRowDantzig for the standard (non-ABC) implementation"], "has_pass2": false}, "src/ClpParameters.hpp": {"path": "layer-1/Clp/src/ClpParameters.hpp", "filename": "ClpParameters.hpp", "file": "ClpParameters.hpp", "brief": "Command-line and runtime parameter management", "author": "Lou Hafer\n\nContainer for Clp's configurable parameters, supporting:\n- Command-line parsing (standalone clp executable)\n- Runtime parameter access (embedded use)\n- Integration with CBC mode\n\nParameter types: string, int, double, boolean, keyword, file paths.\nOrganized into categories: strategy, display, algorithm control.", "see": ["ClpParam for individual parameter definitions", "CoinParam for the underlying parameter framework"], "has_pass2": false}, "src/ClpNetworkBasis.hpp": {"path": "layer-1/Clp/src/ClpNetworkBasis.hpp", "filename": "ClpNetworkBasis.hpp", "file": "ClpNetworkBasis.hpp", "brief": "Specialized factorization for pure network problems", "author": "John Forrest\n\nExploits network structure for O(n) factorization instead of O(n^2-3).\nNetwork LPs have constraint matrices that are node-arc incidence matrices\nof directed graphs - each column has exactly one +1 and one -1.\n\nUses spanning tree representation:\n- parent/descendant/sibling arrays encode tree structure\n- FTRAN/BTRAN reduce to tree traversal operations\n- Updates are simple tree modifications\n\nRequires ClpNetworkMatrix to detect network structure.", "see": ["ClpNetworkMatrix for the network matrix representation", "ClpFactorization for the general factorization interface"], "has_pass2": false}, "src/ClpModel.hpp": {"path": "layer-1/Clp/src/ClpModel.hpp", "filename": "ClpModel.hpp", "file": "ClpModel.hpp", "brief": "Base class for LP/QP models - stores problem data without algorithm logic\n\nClpModel holds the complete representation of a linear or quadratic program:\nconstraint matrix, variable bounds, objective coefficients, and solution vectors.\nThis is the base class inherited by ClpSimplex and ClpInterior - it knows about\nproblem data but nothing about solution algorithms.\n\nKey responsibilities:\n- Problem loading from various formats (MPS, GMPL, CoinModel, raw arrays)\n- Row/column manipulation (add, delete, modify bounds)\n- Matrix scaling for numerical stability\n- Solution and status storage\n- Name management for rows and columns", "see": ["ClpSimplex for simplex algorithm implementation", "ClpInterior for interior point algorithm", "ClpMatrixBase for the constraint matrix interface", "ClpObjective for objective function handling", "CoinPackedMatrix in CoinUtils for sparse matrix storage"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the constraints whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints"], "has_pass2": false}, "src/ClpPrimalQuadraticDantzig.hpp": {"path": "layer-1/Clp/src/ClpPrimalQuadraticDantzig.hpp", "filename": "ClpPrimalQuadraticDantzig.hpp", "file": "ClpPrimalQuadraticDantzig.hpp", "brief": "Dantzig-style pricing for quadratic programming\n\nExtends ClpPrimalColumnPivot for QP problems where the reduced cost\ndepends on the current solution (due to the quadratic objective).\n\nDantzig's rule: Choose the variable with largest reduced cost violation.\nFor QP, the reduced cost c̄_j = c_j + (Qx)_j - π'A_j changes as x changes.\n\nThis is the simplest QP pricing rule - it examines all non-basic variables\nto find the most attractive one. Slow but robust.\n\nquadraticInfo_: Holds current quadratic gradient Qx and related state.", "see": ["ClpPrimalColumnPivot for the abstract pivot interface", "ClpQuadraticObjective for the QP objective representation", "ClpSimplexPrimalQuadratic for the QP simplex solver"], "has_pass2": false}, "src/AbcSimplexFactorization.hpp": {"path": "layer-1/Clp/src/AbcSimplexFactorization.hpp", "filename": "AbcSimplexFactorization.hpp", "file": "AbcSimplexFactorization.hpp", "brief": "LU factorization wrapper for ABC simplex", "author": "John Forrest (FasterCoin, 2012)\n\nWraps CoinAbcAnyFactorization (or CoinFactorization) for use with\nAbcSimplex. Handles basis factorization and FTRAN/BTRAN operations\nwith ABC's optimized data structures.\n\nKey operations:\n- factorize(): Factor the basis matrix from AbcSimplex\n- updateColumnFT(): Forward transformation with FT update\n- updateColumnTranspose(): Backward transformation (BTRAN)\n- checkReplacePart1/2, replaceColumnPart3(): Basis updates\n\nFactorization type selection:\n- goDenseThreshold_: Use dense factorization below this size\n- goSmallThreshold_: Use small/optimized factorization\n- goLongThreshold_: Use long/ordered factorization above this\n\nThe implementation can delegate to either CoinAbcAnyFactorization\n(ABC-optimized) or standard CoinFactorization based on build config.", "see": ["AbcSimplex which uses this factorization", "ClpFactorization for the standard (non-ABC) wrapper", "CoinAbcFactorization in CoinUtils for ABC factorization"], "has_pass2": false}, "src/AbcDualRowPivot.hpp": {"path": "layer-1/Clp/src/AbcDualRowPivot.hpp", "filename": "AbcDualRowPivot.hpp", "file": "AbcDualRowPivot.hpp", "brief": "Abstract base class for dual pivot row selection in ABC", "author": "John Forrest (FasterCoin, 2012)\n\nDefines the interface for choosing which row (basic variable) should\nleave the basis in dual simplex iterations within the ABC framework.\n\nKey virtual methods:\n- pivotRow(): Select row with largest infeasibility (returns -1 if optimal)\n- updateWeights(): Update edge norms after pivot and do FT update\n- updatePrimalSolution(): Update solution after pivot\n- saveWeights(): Persist weights across factorizations\n\nThe ABC version differs from ClpDualRowPivot in using CoinIndexedVector\nreferences (not pointers) for better cache behavior.", "see": ["AbcDualRowDantzig for simple largest-infeasibility selection", "AbcDualRowSteepest for steepest edge variant", "ClpDualRowPivot for the standard (non-ABC) interface"], "has_pass2": false}, "src/ClpMessage.hpp": {"path": "layer-1/Clp/src/ClpMessage.hpp", "filename": "ClpMessage.hpp", "file": "ClpMessage.hpp", "brief": "Message codes and localization for Clp solver output\n\nDefines the CLP_Message enum with all status and diagnostic message codes\nused by Clp during optimization. Messages are organized by solver phase:\n- CLP_SIMPLEX_*: General simplex status (finished, infeasible, etc.)\n- CLP_DUAL_*: Dual simplex specific messages\n- CLP_PRIMAL_*: Primal simplex specific messages\n- CLP_BARRIER_*: Interior point method messages\n\nClpMessage inherits from CoinMessages to provide localized message text.\nControl output verbosity via ClpSimplex::setLogLevel().", "see": ["CoinMessageHandler in CoinUtils for the messaging infrastructure", "ClpSimplex::setLogLevel() to control output verbosity"], "has_pass2": false}, "src/ClpConstraintQuadratic.hpp": {"path": "layer-1/Clp/src/ClpConstraintQuadratic.hpp", "filename": "ClpConstraintQuadratic.hpp", "file": "ClpConstraintQuadratic.hpp", "brief": "Quadratic constraint implementation: x'Qx + c'x ≤ b\n\nImplements ClpConstraint for quadratic constraints. The constraint\nfunction is: 0.5 * sum_{ij}(Q_ij * x_i * x_j) + sum_j(c_j * x_j) ≤ b\n\nStorage (sparse symmetric Q):\n- start_[i]: Column i's entries start at this index\n- column_[k]: Column index j for Q_ij (or -1 for linear term c_i)\n- coefficient_[k]: Value Q_ij or c_i\n\nKey methods:\n- gradient(): Returns Qx + c (linear in x, unlike the quadratic function)\n- markNonlinear(): Returns columns appearing in quadratic terms\n\nUsed in quadratically constrained quadratic programs (QCQP) with\nClpSimplexNonlinear's SLP (sequential linear programming) approach.", "see": ["ClpConstraint for the abstract interface", "ClpConstraintLinear for purely linear constraints", "ClpSimplexNonlinear for the SLP solver"], "has_pass2": false}, "src/ClpPrimalColumnPivot.hpp": {"path": "layer-1/Clp/src/ClpPrimalColumnPivot.hpp", "filename": "ClpPrimalColumnPivot.hpp", "file": "ClpPrimalColumnPivot.hpp", "brief": "Abstract base class for primal simplex pivot column selection\n\nIn primal simplex, the pivot column (entering variable) is chosen based on\nreduced cost. This class defines the interface for different selection\nstrategies. Derived classes implement specific rules.\n\nKey methods:\n- pivotColumn(): Select which column (variable) enters the basis\n- updateWeights(): Maintain pricing information after pivots\n- saveWeights(): Preserve weights across refactorizations", "see": ["ClpPrimalColumnDantzig for simple most-negative reduced cost", "ClpPrimalColumnSteepest for steepest edge pricing (recommended)", "ClpSimplexPrimal for the primal simplex algorithm"], "has_pass2": false}, "src/CoinAbcHelperFunctions.hpp": {"path": "layer-1/Clp/src/CoinAbcHelperFunctions.hpp", "filename": "CoinAbcHelperFunctions.hpp", "file": "CoinAbcHelperFunctions.hpp", "brief": "SIMD-optimized scatter/gather operations for ABC factorization\n\nHigh-performance kernels for sparse matrix operations using:\n- Manual loop unrolling (UNROLL_SCATTER, UNROLL_GATHER)\n- AVX/AVX2 intrinsics when available\n- Cilk parallel loops with configurable grainsize\n- Prefetching hints (coin_prefetch macros)\n\nKey operations:\n- CoinAbcScatterUpdate: region[index[j]] -= value[j] * pivot\n- CoinAbcGatherUpdate: dot product over sparse indices\n- Memory utilities: CoinAbcMemcpyLong, CoinAbcMemset0Long\n\nIncludes specialized ScatterUpdateN functions for N=1..8 and 4N variants\nusing function pointer dispatch (scatterStruct) for optimal performance.", "algorithm": "Function Pointer Dispatch (scatterStruct):\n  Precomputed table of specialized ScatterUpdate{1..8,4N} functions\n  avoids branch misprediction in small-column cases.", "complexity": "O(nnz) per scatter/gather operation\n  With SIMD: ~4x speedup on vectorizable sections\n  With Cilk: ~Kx speedup on K cores for large columns", "see": ["CoinAbcFactorization which uses these kernels", "AbcSimplex which uses these for FTRAN/BTRAN"], "has_pass2": true}, "src/MyMessageHandler.hpp": {"path": "layer-1/Clp/src/MyMessageHandler.hpp", "filename": "MyMessageHandler.hpp", "file": "MyMessageHandler.hpp", "brief": "Example message handler demonstrating custom logging\n\nSample implementation of CoinMessageHandler showing how users can\ncustomize message output. Used in unit tests to demonstrate\ncapturing solver progress and collecting feasible extreme points.\n\nFeatures demonstrated:\n- Override print() to intercept all solver messages\n- Access to model_ for querying current solution\n- Collection of feasible extreme points during solve\n- Custom file output via FILE* pointer\n\nUse case - Feasible point enumeration:\nDuring optimization, each time the solver finds a new feasible\nextreme point (basic feasible solution), the handler captures it.\nUseful for problems where alternative optima matter.\n\nTo create your own handler:\n1. Inherit from CoinMessageHandler\n2. Override print() method\n3. Attach via ClpSimplex::passInMessageHandler()", "see": ["CoinMessageHandler for the base interface", "ClpMessage for Clp message definitions", "ClpSimplex::passInMessageHandler() to install"], "has_pass2": false}, "src/ClpDualRowPivot.hpp": {"path": "layer-1/Clp/src/ClpDualRowPivot.hpp", "filename": "ClpDualRowPivot.hpp", "file": "ClpDualRowPivot.hpp", "brief": "Abstract base class for dual simplex pivot row selection\n\nIn dual simplex, the pivot row (leaving variable) is chosen based on\nprimal infeasibility. This class defines the interface for different\nselection strategies. Derived classes implement specific rules.\n\nKey methods:\n- pivotRow(): Select which row (basic variable) leaves the basis\n- updateWeights(): Maintain pricing information after pivots\n- updatePrimalSolution(): Update solution after basis change", "see": ["ClpDualRowDantzig for simple most-infeasible selection", "ClpDualRowSteepest for steepest edge pricing (recommended)", "ClpSimplexDual for the dual simplex algorithm"], "has_pass2": false}, "src/ClpPrimalColumnDantzig.hpp": {"path": "layer-1/Clp/src/ClpPrimalColumnDantzig.hpp", "filename": "ClpPrimalColumnDantzig.hpp", "file": "ClpPrimalColumnDantzig.hpp", "brief": "Dantzig's rule for primal simplex pivot selection\n\nImplements the simplest pivot column selection: choose the nonbasic variable\nwith the most negative reduced cost (for minimization). This is Dantzig's\noriginal 1947 rule.\n\nSimple and fast per iteration, but typically requires more iterations than\nsteepest edge methods on degenerate or difficult problems. Use\nClpPrimalColumnSteepest for better performance on most problems.", "see": ["ClpPrimalColumnPivot for the base interface", "ClpPrimalColumnSteepest for recommended steepest edge variant", "ClpSimplexPrimal for the primal simplex algorithm"], "has_pass2": false}, "src/CoinAbcDenseFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcDenseFactorization.hpp", "filename": "CoinAbcDenseFactorization.hpp", "file": "CoinAbcDenseFactorization.hpp", "brief": "Abstract base class for ABC factorization and dense submatrix handling", "author": "John Forrest\n\nDefines CoinAbcAnyFactorization, the abstract interface for all ABC\nfactorization variants. Key responsibilities:\n\n- Status management (factorization state, pivot count)\n- Numeric parameters (pivot tolerance, zero tolerance)\n- Dense column handling for Schur complement\n- FTRAN/BTRAN interface (forward/backward triangular solves)\n- Update interface (Forrest-Tomlin rank-one updates)\n\nDesign pattern:\n- Abstract base providing common interface and scalar state\n- CoinAbcBaseFactorization adds full sparse LU implementation\n- Specialized variants (Small, Long, Ordered) instantiate with different settings\n\nThe dense factorization handles the Schur complement that forms when\nmany columns become dense during sparse LU factorization.", "see": ["CoinAbcBaseFactorization for sparse LU implementation", "CoinAbcFactorization for template-instantiated variants"], "has_pass2": false}, "src/ClpCholeskyBase.hpp": {"path": "layer-1/Clp/src/ClpCholeskyBase.hpp", "filename": "ClpCholeskyBase.hpp", "file": "ClpCholeskyBase.hpp", "brief": "Cholesky factorization base class for interior point methods\n\nProvides Cholesky factorization of the normal equations matrix A*D*A'\nused in predictor-corrector interior point methods. The factorization\ncan use AMD ordering to reduce fill-in.\n\nThe base class provides a simple sparse Cholesky implementation with\nsupernodal dense blocks. Derived classes can interface to more\nsophisticated factorizations (MUMPS, Pardiso, etc.).\n\nKey methods:\n- order(): Compute fill-reducing ordering (AMD by default)\n- symbolic(): Set up sparsity structure of factor\n- factorize(): Numeric factorization of A*D*A'\n- solve(): Solve system using computed factors", "see": ["ClpInterior for the interior point algorithm", "ClpCholeskyDense for dense Cholesky when A*D*A' becomes dense", "CoinFactorization in CoinUtils for simplex LU factorization"], "has_pass2": false}, "src/ClpPEDualRowDantzig.hpp": {"path": "layer-1/Clp/src/ClpPEDualRowDantzig.hpp", "filename": "ClpPEDualRowDantzig.hpp", "file": "ClpPEDualRowDantzig.hpp", "brief": "Positive Edge enhanced Dantzig pricing for dual simplex", "author": "Jeremy Omer\n\nCombines classic dual Dantzig pricing (most infeasible row) with\nPositive Edge compatibility checking. Prioritizes compatible rows\nthat can make real progress on degenerate problems.\n\nUses bi-dimensional pricing: candidates scored by infeasibility\nand compatibility, weighted by psi parameter (default 0.5).", "see": ["ClpPESimplex for the compatibility framework", "ClpDualRowDantzig for the base pricing rule", "ClpPEDualRowSteepest for steepest edge variant"], "has_pass2": false}, "src/ClpGubDynamicMatrix.hpp": {"path": "layer-1/Clp/src/ClpGubDynamicMatrix.hpp", "filename": "ClpGubDynamicMatrix.hpp", "file": "ClpGubDynamicMatrix.hpp", "brief": "Dynamic column generation with Generalized Upper Bound structure\n\nCombines GUB constraints with dynamic column generation. Columns are\npartitioned into GUB sets where at most one column per set can be basic.\n\nGUB structure: sum_{j in S_k} x_j <= u_k for each set S_k\n\nKey features:\n- Dynamic: Columns added/removed during optimization\n- GUB-aware: Pricing respects set membership\n- Efficient RHS: Avoids explicit GUB row representation\n\nSynchronization modes:\n- 0: Status good, update internal state\n- 1: Variable flagged, prevent from entering\n- 2: Unflag all variables\n- 3: Reset costs (primal)\n- 4: Correct dual infeasibility count\n- 5: Check if re-factorization needed\n- 8: Clean up set\n- 9: Adjust bounds for incoming column\n\nStatus tracking:\n- DynamicStatus: inSmall (active), atUpperBound, atLowerBound\n- flagged(): Prevented from entering basis", "see": ["ClpGubMatrix for static GUB handling", "ClpDynamicMatrix for non-GUB dynamic columns", "ClpDynamicExampleMatrix for column generation example"], "has_pass2": false}, "src/ClpCholeskyWssmp.hpp": {"path": "layer-1/Clp/src/ClpCholeskyWssmp.hpp", "filename": "ClpCholeskyWssmp.hpp", "file": "ClpCholeskyWssmp.hpp", "brief": "WSSMP sparse direct solver interface for Cholesky factorization\n\nWraps IBM's Watson Sparse Matrix Package (WSSMP) for Cholesky factorization\nof normal equations in interior point methods.\n\nWSSMP provides high-performance sparse direct solving with:\n- Multifrontal factorization\n- Shared memory parallelism\n- Dense column threshold for hybrid sparse/dense\n\nRequires WSSMP library to be installed and linked.", "see": ["ClpCholeskyBase for the abstract interface", "ClpCholeskyWssmpKKT for KKT system variant", "ClpInterior which uses this factorization"], "has_pass2": false}, "src/ClpPEDualRowSteepest.hpp": {"path": "layer-1/Clp/src/ClpPEDualRowSteepest.hpp", "filename": "ClpPEDualRowSteepest.hpp", "file": "ClpPEDualRowSteepest.hpp", "brief": "Positive Edge enhanced steepest edge for dual simplex", "author": "Jeremy Omer\n\nCombines dual steepest edge pricing with Positive Edge compatibility\nchecking. The most effective anti-degeneracy variant for dual simplex.\n\nModes: 0=uninitialized, 1=full, 2=partial uninitialized,\n       3=adaptive (starts partial, may switch to full)\n\nUses bi-dimensional pricing: candidates scored by steepest edge weight\nand compatibility, weighted by psi parameter.", "see": ["ClpPESimplex for the compatibility framework", "ClpDualRowSteepest for the base pricing rule", "ClpPEDualRowDantzig for Dantzig variant"], "has_pass2": false}, "src/CoinAbcFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcFactorization.hpp", "filename": "CoinAbcFactorization.hpp", "file": "CoinAbcFactorization.hpp", "brief": "ABC optimized LU factorization variants", "author": "John Forrest\n\nTemplate-instantiation header that creates multiple factorization variants\nfrom CoinAbcBaseFactorization using preprocessor macros:\n\n- CoinAbcFactorization: Standard double precision\n- CoinAbcLongFactorization: Long double for extra precision (COIN_BIG_DOUBLE=1)\n- CoinAbcSmallFactorization: Optimized for small matrices (ABC_SMALL=4)\n- CoinAbcOrderedFactorization: Ordered variant\n\nAll inherit the same SIMD-optimized scatter/gather operations and\nparallelized factorization from CoinAbcBaseFactorization.", "see": ["CoinAbcBaseFactorization for the actual implementation", "CoinAbcHelperFunctions for SIMD kernels"], "has_pass2": false}, "src/ClpPESimplex.hpp": {"path": "layer-1/Clp/src/ClpPESimplex.hpp", "filename": "ClpPESimplex.hpp", "file": "ClpPESimplex.hpp", "brief": "Positive Edge anti-degeneracy framework for simplex", "author": "Jeremy Omer\n\nCore infrastructure for the Positive Edge method that reduces degenerate\npivots in simplex. Identifies \"compatible\" variables that can make real\nprogress toward optimality.\n\nKey concepts:\n- Primal degenerates: basic variables at bound (zero pivot impact)\n- Dual degenerates: non-basic variables with zero reduced cost\n- Compatible columns/rows: can make positive improvement\n- Bi-dimensional pricing: balances reduced cost vs. compatibility\n\nThe psi parameter (default 0.5) controls priority given to compatibles.\nSmaller psi = more priority to compatible variables.\n\nReference: Towhidi & Orban, \"Customizing the solution process of COIN-OR's\nlinear solvers with Python\", Math Programming Computation (2014)", "see": ["ClpPEPrimalColumnSteepest, ClpPEDualRowSteepest for pivot implementations"], "has_pass2": false}, "src/AbcPrimalColumnPivot.hpp": {"path": "layer-1/Clp/src/AbcPrimalColumnPivot.hpp", "filename": "AbcPrimalColumnPivot.hpp", "file": "AbcPrimalColumnPivot.hpp", "brief": "Abstract base class for primal pivot column selection in ABC", "author": "John Forrest (FasterCoin, 2012)\n\nDefines the interface for choosing which column (nonbasic variable)\nshould enter the basis in primal simplex iterations within ABC.\n\nKey virtual methods:\n- pivotColumn(): Select column with best (most negative) reduced cost\n- updateWeights(): Update edge norms after pivot\n- saveWeights(): Persist weights across factorizations\n\nUses CoinPartitionedVector for parallel-safe operations across\nmultiple column blocks.", "see": ["AbcPrimalColumnDantzig for simple most-negative reduced cost", "AbcPrimalColumnSteepest for steepest edge/Devex variants", "ClpPrimalColumnPivot for the standard (non-ABC) interface"], "has_pass2": false}, "src/CoinAbcBaseFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcBaseFactorization.hpp", "filename": "CoinAbcBaseFactorization.hpp", "file": "CoinAbcBaseFactorization.hpp", "brief": "Core ABC SIMD-optimized LU factorization implementation", "author": "John Forrest (FasterCoin 2012)\n\nImplements CoinAbcTypeFactorization (type varies by #define), the main\nsparse LU factorization with SIMD optimizations. This file is included\nmultiple times with different macro settings to generate variants:\n\n- CoinAbcFactorization: Standard double precision\n- CoinAbcLongFactorization: Long double (COIN_BIG_DOUBLE=1)\n- CoinAbcSmallFactorization: Optimized for small matrices (ABC_SMALL=4)\n- CoinAbcOrderedFactorization: With ordered elimination\n\nKey optimizations over standard CoinFactorization:\n- SIMD scatter/gather operations (AVX when available)\n- Cache-aligned memory layout (BLOCKING8)\n- Cilk parallel factorization (ABC_PARALLEL=2)\n- Optimized pivot search (Markowitz with tie-breaking)\n\nStorage format:\n- L stored by rows (startRowL_) and columns (startColumnL_)\n- U stored in column-major with explicit row indices\n- Pivot region separate for fast diagonal access", "algorithm": "ABC (Alternative Basis Code) LU Factorization:\n  Enhanced sparse LU with modern CPU optimizations:\n  1. Markowitz pivot selection with singleton detection\n  2. SIMD-vectorized scatter/gather for column updates\n  3. Cache-blocking (BLOCKING8) for L2/L3 efficiency\n  4. Parallel elimination using Cilk (when ABC_PARALLEL=2)\n  5. Hyper-sparse mode for very sparse intermediate results", "complexity": "Factorization: O(nnz × fill-in), highly structure-dependent\n  FTRAN/BTRAN: O(nnz(L) + nnz(U)) with SIMD acceleration\n  SIMD speedup: 2-4x for dense column operations", "math": "B = L·U with row/column permutations P, Q:\n  P·B·Q = L·U, where L unit lower triangular, U upper triangular\n  Markowitz criterion: select pivot minimizing (r_i - 1)(c_j - 1)\n\nNote: 32 bits assumed sufficient for row/column counts,\nbut CoinBigIndex can be 64-bit for element indices.", "see": ["CoinAbcFactorization.hpp for instantiation", "CoinAbcHelperFunctions.hpp for SIMD kernels"], "has_pass2": true}, "src/ClpDynamicMatrix.hpp": {"path": "layer-1/Clp/src/ClpDynamicMatrix.hpp", "filename": "ClpDynamicMatrix.hpp", "file": "ClpDynamicMatrix.hpp", "brief": "Dynamic column generation matrix with GUB structure\n\nSupports column generation by maintaining a pool of potential columns\nand dynamically adding promising ones to the active working matrix.\nBuilt on GUB structure where each \"set\" can generate multiple columns.\n\nColumn generation workflow:\n1. Solve restricted master problem with current columns\n2. Use dual prices to identify promising new columns (pricing subproblem)\n3. Add columns with negative reduced cost via createVariable()\n4. Re-optimize and repeat until no improving columns exist\n\nKey features:\n- Maintains both \"small\" working matrix and full column pool\n- Columns move between pool and working matrix as needed\n- Automatic compression when working matrix gets too large\n- Integrates with GUB structure for efficient handling", "see": ["ClpGubMatrix for the underlying GUB structure", "ClpPackedMatrix which this extends", "ClpSimplexPrimal which drives the column generation"], "has_pass2": false}, "src/CoinAbcCommonFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcCommonFactorization.hpp", "filename": "CoinAbcCommonFactorization.hpp", "file": "CoinAbcCommonFactorization.hpp", "brief": "Common infrastructure for ABC SIMD-optimized factorization\n\nShared definitions, statistics tracking, and LAPACK interfaces for\nABC factorization variants. Provides:\n\n- ABC_SMALL modes: Controls memory copies for factorization\n  - -1: Force copies (no tests)\n  - 0: Force copy of U\n  - 2: Force no copies (no tests)\n\n- CoinAbcStatistics: Tracks element counts through L, R, U phases\n- CoinAbcStack: Stack structure for depth-first traversal\n- CoinAbcDgetrf/Dgetrs: LAPACK-compatible LU routines (homegrown or external)\n\nBlocking parameters for cache-friendly factorization:\n- BLOCKING8: 8-element blocks for SIMD alignment\n- SWAP_FACTOR: Controls swap threshold", "see": ["CoinAbcBaseFactorization for the main implementation", "CoinAbcDenseFactorization for dense submatrix handling"], "has_pass2": false}, "src/OsiClp/OsiClpSolverInterface.hpp": {"path": "layer-1/Clp/src/OsiClp/OsiClpSolverInterface.hpp", "filename": "OsiClpSolverInterface.hpp", "algorithm": "Hot-Start Optimization for Strong Branching:\n  1. markHotStart(): Save current basis, factorization, solution\n  2. Modify bounds (single variable typically)\n  3. solveFromHotStart(): Resolve with warm start (few iterations)\n  4. Repeat steps 2-3 for each branching candidate\n  5. unmarkHotStart(): Release saved state\n  This avoids full refactorization for each strong branching LP.", "complexity": "Most operations O(1) delegations to ClpSimplex\n  initialSolve/resolve: O(m·n·iterations) for underlying simplex\n  Hot-start resolve: O(k·m) where k = iterations to re-optimize (typically small)\n  getBInvARow/getBInvACol: O(nnz(B^{-1})) per call\n\n@invariant basis_ synchronized with modelPtr_->status arrays after each solve\n@invariant fakeMinInSimplex_ == true implies linearObjective_ is negated copy\n@invariant smallModel_ != NULL implies hot-start state is active", "ref": ["Lougee-Heimer, R., et al. (2003). \"The Common Optimization INterface for\n  Operations Research: Promoting open-source software in the operations\n  research community\". IBM J. Res. Dev. 47(1):57-66."], "param": ["indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": true}, "src/Clp_C_Interface.h": {"path": "layer-1/Clp/src/Clp_C_Interface.h", "filename": "Clp_C_Interface.h", "file": "Clp_C_Interface.h", "brief": "C language interface to Clp solver\n\nPure C API for embedding Clp in C programs or creating language bindings.\nDesign follows OSL V3 conventions for familiarity.\n\nOpaque handles:\n- Clp_Simplex: Pointer to internal ClpSimplex object\n- Clp_Solve: Pointer to ClpSolve options object\n\nNaming convention: C++ method foo() becomes Clp_foo(model, ...)\nwhere model is the first parameter.\n\nKey function groups:\n- Construction: Clp_newModel(), Clp_deleteModel()\n- Problem setup: Clp_loadProblem(), Clp_readMps()\n- Solving: Clp_dual(), Clp_primal(), Clp_initialSolve()\n- Solution access: Clp_getColSolution(), Clp_getRowActivity()\n- Parameters: Clp_setLogLevel(), Clp_setMaximumIterations()\n\nCallback support: clp_callback typedef for user message handling.\n\nThread safety: Each Clp_Simplex is independent; do not share across threads.", "algorithm": "Warm-Starting Protocol:\n  1. Solve initial LP: Clp_initialSolve()\n  2. Modify problem (bounds, RHS, etc.)\n  3. Re-solve from basis: Clp_dual() or Clp_primal()\n  Warm start typically requires O(k) iterations where k << m+n.", "complexity": "Same as underlying ClpSimplex methods:\n  Per-iteration: O(m²) average, O(m³) worst-case for factorization\n  Total: O(m·n·iterations) for complete solve", "see": ["ClpSimplex for the C++ class being wrapped", "ClpSolve for solve options"], "return": "infeasibility ray, or NULL returned if none/wrong.", "has_pass2": true}, "src/ClpConfig.h": {"path": "layer-1/Clp/src/ClpConfig.h", "filename": "ClpConfig.h", "file": "ClpConfig.h", "brief": "Clp configuration and platform detection\n\nHandles build configuration for cross-platform compatibility:\n- HAVE_CONFIG_H: Uses autoconf-generated config.h\n- CLPLIB_BUILD: Building the library vs. using it\n- DLL_EXPORT: Windows DLL export declarations\n- GCC visibility: Hidden visibility with explicit exports\n\nDefines CLPLIB_EXPORT macro for public API symbols.\n\nConfiguration precedence:\n1. config.h (autoconf) when HAVE_CONFIG_H defined\n2. config_clp.h for configured installations\n3. config_clp_default.h for manual builds", "has_pass2": false}}}, "CppAD": {"name": "CppAD", "file_count": 3, "pass2_count": 2, "files": {"include/cppad/cppad.hpp": {"path": "layer-1/CppAD/include/cppad/cppad.hpp", "filename": "cppad.hpp", "file": "cppad.hpp", "brief": "Contains all variables and functions defined by CppAD package", "algorithm": "Operator overloading records a \"tape\" of operations.\n           Forward mode: propagates derivatives from inputs to outputs.\n           Reverse mode: propagates derivatives from outputs to inputs.\n           Reverse mode is more efficient for many inputs, few outputs.", "ref": ["Griewank, Walther (2008). \"Evaluating Derivatives: Principles and\n     Techniques of Algorithmic Differentiation\", 2nd ed. SIAM."], "see": ["AD for the automatic differentiation scalar type", "ADFun for the recorded function object", "Ipopt for nonlinear optimization using CppAD derivatives\n\n@namespace CppAD"], "has_pass2": true}, "include/cppad/core/ad_fun.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_fun.hpp", "filename": "ad_fun.hpp", "file": "ad_fun.hpp", "brief": "ADFun class - function object storing recorded AD operations\n\nADFun<Base> stores a recorded sequence of operations and provides\nmethods to evaluate the function and its derivatives.\n\nKey methods:\n- **Forward(p, x)**: p-th order forward mode Taylor coefficient\n- **Reverse(q, w)**: q-th order reverse mode derivative\n- **Jacobian(x)**: Full Jacobian matrix\n- **Hessian(x, w)**: Weighted Hessian matrix\n- **SparseJacobian/SparseHessian**: Sparse derivative computation", "algorithm": "Forward mode computes derivatives in the direction of inputs.\n           Reverse mode computes derivatives from outputs backward.\n           Forward: O(n) per direction, Reverse: O(m) per output.\n           Use forward when n < m, reverse when n > m.", "see": ["AD<Base> for the scalar type that records operations"], "tparam": ["Base Underlying numeric type (typically double)", "RecBase Type used during recording (usually same as Base)\n\nADFun stores a \"tape\" of operations recorded using AD<Base> scalars.\nThis tape can be used to efficiently compute:\n- Function values at new points\n- Jacobians (first derivatives)\n- Hessians (second derivatives)\n- Higher-order Taylor coefficients\n- Sparsity patterns\n\nThe recorded operations can be optimized to remove redundancies.\n\n@invariant Once constructed, the function signature (n inputs, m outputs) is fixed"], "has_pass2": true}, "include/cppad/core/ad.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad.hpp", "filename": "ad.hpp", "file": "ad.hpp", "brief": "Core AD<Base> automatic differentiation scalar type\n\nAD<Base> is the fundamental type for automatic differentiation.\nOperations on AD<Base> values are recorded on a \"tape\" which can\nlater be used to compute derivatives.\n\nAD types:\n- **Constant**: Value known, not recorded on tape\n- **Parameter**: Value known, may be recorded for dynamic parameters\n- **Variable**: Value depends on independent variables, recorded on tape", "see": ["Independent() to declare independent variables", "ADFun to create a function from recorded operations"], "tparam": ["Base Underlying numeric type (typically double)\n\nAD<Base> overloads arithmetic operators to record operations\non a tape for later derivative computation.\n\nUsage:\n@code\nvector< AD<double> > x(n);\nIndependent(x);           // Mark as independent variables\nAD<double> y = f(x);      // Operations are recorded\nADFun<double> fun(x, y);  // Create function object\n@endcode\n\nMember data:\n- value_: Current numerical value (Base type)\n- tape_id_: ID of tape recording this variable\n- taddr_: Address in the tape\n- ad_type_: Whether constant/parameter/variable"], "has_pass2": false}}}, "Osi": {"name": "Osi", "file_count": 15, "pass2_count": 2, "files": {"src/OsiSpx/OsiSpxSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiSpx/OsiSpxSolverInterface.hpp", "filename": "OsiSpxSolverInterface.hpp", "param": ["<code>[indexfirst,indexLast)</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "indexList the indices of those variables", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast)</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast)</code> contains the indices of\n    \t         the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiGrb/OsiGrbSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiGrb/OsiGrbSolverInterface.hpp", "filename": "OsiGrbSolverInterface.hpp", "param": ["<code>[indexfirst,indexLast]</code> contains the indices of\n\t the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast]</code> contains the indices of\n\t the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast]</code> contains the indices of\n\t the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiXpr/OsiXprSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiXpr/OsiXprSolverInterface.hpp", "filename": "OsiXprSolverInterface.hpp", "param": ["indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the variables whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiGlpk/OsiGlpkSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiGlpk/OsiGlpkSolverInterface.hpp", "filename": "OsiGlpkSolverInterface.hpp", "param": ["indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the variables whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiCpx/OsiCpxSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiCpx/OsiCpxSolverInterface.hpp", "filename": "OsiCpxSolverInterface.hpp", "param": ["<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/Osi/OsiRowCut.hpp": {"path": "layer-1/Osi/src/Osi/OsiRowCut.hpp", "filename": "OsiRowCut.hpp", "file": "OsiRowCut.hpp", "brief": "Row-based cutting plane (linear inequality)\n\nRow cuts are the most common form of cutting planes, representing\na linear inequality constraint: lb <= a'x <= ub\n\nCommon cut families that produce row cuts:\n- Gomory cuts (from LP tableau)\n- Mixed-integer rounding (MIR)\n- Knapsack covers\n- Clique cuts\n- Flow covers", "math": "lb <= sum(a[i] * x[i]) <= ub", "see": ["OsiCut for base class", "OsiCuts for cut pool management", "Cgl (Cut Generation Library) for cut generators", "CoinPackedVector for sparse row representation"], "has_pass2": true}, "src/Osi/OsiBranchingObject.hpp": {"path": "layer-1/Osi/src/Osi/OsiBranchingObject.hpp", "filename": "OsiBranchingObject.hpp", "file": "OsiBranchingObject.hpp", "brief": "Branch-and-bound objects and branching decisions\n\nThis file defines the object-oriented framework for branching in MIP:\n\n- **OsiObject**: Abstract base for anything that can be branched on\n  (integer variables, SOS constraints, etc.)\n- **OsiBranchingObject**: Describes how to perform a specific branch\n- **OsiBranchingInformation**: Solver state passed to branching decisions\n\nThe branching model:\n1. OsiObject::infeasibility() measures how far from feasible\n2. OsiObject::createBranch() creates an OsiBranchingObject\n3. OsiBranchingObject::branch() modifies solver bounds", "see": ["OsiSolverInterface::branchAndBound for MIP solving", "Cbc for full branch-and-cut implementation"], "has_pass2": false}, "src/Osi/OsiFeatures.hpp": {"path": "layer-1/Osi/src/Osi/OsiFeatures.hpp", "filename": "OsiFeatures.hpp", "brief": "number of features", "has_pass2": false}, "src/Osi/OsiChooseVariable.hpp": {"path": "layer-1/Osi/src/Osi/OsiChooseVariable.hpp", "filename": "OsiChooseVariable.hpp", "file": "OsiChooseVariable.hpp", "brief": "Variable selection strategies for branch-and-bound\n\nIn MIP solving, choosing which variable to branch on significantly\naffects tree size and solve time. This file provides:\n\n- OsiChooseVariable: Base class for branching variable selection\n- OsiChooseStrong: Strong branching (evaluates candidates by solving LPs)\n\nCommon selection strategies:\n- Most infeasible (furthest from integer)\n- Pseudo-cost (estimated objective change)\n- Strong branching (actual objective change from LP solves)\n- Reliability branching (strong branching with pseudo-cost fallback)", "see": ["OsiBranchingObject for branch execution", "Cbc for more sophisticated variable selection"], "has_pass2": false}, "src/Osi/OsiPresolve.hpp": {"path": "layer-1/Osi/src/Osi/OsiPresolve.hpp", "filename": "OsiPresolve.hpp", "file": "OsiPresolve.hpp", "brief": "OSI interface to problem simplification (presolve)\n\nPresolve reduces problem size before solving by applying\ntransformations that preserve optimal solutions:\n- Singleton row/column removal\n- Bound tightening\n- Coefficient reduction\n- Duplicate row/column detection\n\nWorkflow:\n1. presolvedModel() creates simplified problem\n2. Solve the simplified problem\n3. postsolve() transforms solution back to original space", "see": ["CoinPresolveMatrix for implementation details", "ClpPresolve for Clp-specific presolve"], "has_pass2": false}, "src/Osi/OsiCuts.hpp": {"path": "layer-1/Osi/src/Osi/OsiCuts.hpp", "filename": "OsiCuts.hpp", "file": "OsiCuts.hpp", "brief": "Container for collections of row cuts and column cuts\n\nOsiCuts serves as a cut pool for branch-and-cut algorithms:\n- Stores both row cuts (linear inequalities) and column cuts (bound changes)\n- Provides iteration over all cuts\n- Supports insertion, removal, and transfer of cuts\n\nTypical usage in cut generation:\n1. Cut generator creates OsiCuts object\n2. Generator adds cuts via insert()\n3. Solver applies cuts via OsiSolverInterface::applyCuts()", "see": ["OsiRowCut for linear inequality cuts", "OsiColCut for variable bound cuts", "OsiSolverInterface::applyCuts for adding cuts to solver", "OsiSolverInterface::applyCuts for applying cuts to a model"], "has_pass2": false}, "src/Osi/OsiColCut.hpp": {"path": "layer-1/Osi/src/Osi/OsiColCut.hpp", "filename": "OsiColCut.hpp", "file": "OsiColCut.hpp", "brief": "Column-based cuts for variable bound tightening\n\nColumn cuts represent bound changes on variables rather than\nadding new constraints. They are used for:\n- Bound tightening from probing\n- Implication-based bound strengthening\n- Reduced cost fixing", "see": ["OsiRowCut for linear inequality cuts", "OsiCut for base class"], "has_pass2": false}, "src/Osi/OsiSolverInterface.hpp": {"path": "layer-1/Osi/src/Osi/OsiSolverInterface.hpp", "filename": "OsiSolverInterface.hpp", "file": "OsiSolverInterface.hpp", "brief": "Abstract base class defining the Open Solver Interface (OSI)\n\nOSI provides a uniform API for accessing different LP/MIP solvers\n(Clp, CPLEX, Gurobi, GLPK, etc.) through a common interface. This\nallows solver-independent application code.\n\nKey capabilities:\n- LP relaxation solving (initialSolve, resolve)\n- Model query (getColLower, getRowUpper, getObjCoefficients)\n- Solution query (getColSolution, getRowPrice, getReducedCost)\n- Problem modification (setColBounds, addRow, addCol)\n- Warm starting (getWarmStart, setWarmStart)\n- Cut management (applyCuts, applyRowCuts)\n- MIP support (setInteger, branchAndBound)\n\nTypical workflow:\n1. Create solver-specific instance (e.g., OsiClpSolverInterface)\n2. Load problem via loadProblem() or readMps()\n3. Call initialSolve() for first LP solution\n4. Call resolve() after modifications\n5. Query solution via getColSolution(), getObjValue()", "algorithm": "Design Pattern: Abstract Factory + Strategy\n  - Factory: concrete implementations (OsiClpSolverInterface, etc.)\n    instantiate solver-specific backends\n  - Strategy: algorithm choice (simplex vs barrier) delegated to\n    underlying solver; OSI provides uniform control interface\n  - Bridge: decouples client code from solver implementation details\n\n@invariant Solution pointers (getColSolution, getRowPrice, etc.) are\n  invalidated by any problem modification or subsequent solve call.\n  Client code must copy data if needed across modifications.\n\n@invariant Warm start objects must be compatible with the solver that\n  created them. Cross-solver warm starting requires basis translation.", "complexity": "All query methods (getColSolution, etc.): O(1) pointer return\n  initialSolve/resolve: delegated to solver, typically O(m·n·iterations)\n  applyCuts: O(cuts × row_length) for constraint matrix update", "ref": ["Lougee-Heimer et al. (2003). \"The Common Optimization INterface\n  for Operations Research\". IBM J. Research & Development 47(1):57-66."], "see": ["OsiClpSolverInterface for Clp implementation", "OsiCuts for cut pool management", "CoinPackedMatrix for constraint matrix format"], "return": "Array of size getNumCols() with solution values", "has_pass2": true}, "src/Osi/OsiCut.hpp": {"path": "layer-1/Osi/src/Osi/OsiCut.hpp", "filename": "OsiCut.hpp", "file": "OsiCut.hpp", "brief": "Abstract base class for cutting planes in branch-and-cut\n\nCutting planes are linear inequalities that can be added to an LP\nrelaxation to tighten the formulation without cutting off any\ninteger-feasible solutions.\n\nThe OsiCut hierarchy:\n- OsiCut: Abstract base with effectiveness measure\n- OsiRowCut: Row-based cut (a'x >= b or a'x <= b)\n- OsiColCut: Column-based cut (bound tightening)", "see": ["OsiRowCut for linear inequality cuts", "OsiColCut for variable bound cuts", "OsiCuts for managing collections of cuts", "Cgl (Cut Generation Library) for cut generators"], "has_pass2": false}, "src/OsiMsk/OsiMskSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiMsk/OsiMskSolverInterface.hpp", "filename": "OsiMskSolverInterface.hpp", "param": ["<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}}}, "qpOASES": {"name": "qpOASES", "file_count": 5, "pass2_count": 3, "files": {"include/qpOASES.hpp": {"path": "layer-1/qpOASES/include/qpOASES.hpp", "filename": "qpOASES.hpp", "file": "qpOASES.hpp", "brief": "Main include file for qpOASES quadratic programming solver\n\nqpOASES solves convex quadratic programs (QPs) of the form:", "math": "lbx <= x <= ubx    (bounds)\n\nKey features:\n- Online active set strategy (efficient for parametric QPs)\n- Hot-starting from previous solutions\n- Handles dense and sparse problems\n- Real-time capable (bounded iteration counts)\n\nMain classes:\n- QProblemB: QP with only box constraints (no A matrix)\n- QProblem: Full QP with linear constraints\n- SQProblem: Sequential QP with varying Hessian", "algorithm": "Online Active Set Strategy - works well for model\n           predictive control (MPC) where QPs are solved in sequence", "ref": ["Ferreau et al. (2014). \"qpOASES: A parametric active-set algorithm\n     for quadratic programming\". Math. Prog. Comp. 6(4):327-363."], "see": ["QProblem for general QP solving", "QProblemB for box-constrained QPs (faster)"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}, "include/qpOASES/QProblemB.hpp": {"path": "layer-1/qpOASES/include/qpOASES/QProblemB.hpp", "filename": "QProblemB.hpp", "file": "QProblemB.hpp", "brief": "Box-constrained QP solver (bounds only, no linear constraints)\n\nSolves QPs with only variable bounds (no constraint matrix A):", "math": "s.t. lb <= x <= ub\n\nThis is more efficient than QProblem when there are no general\nlinear constraints. The active set consists only of bounds.\n\nKey methods:\n- init(): First QP solve (cold start)\n- hotstart(): Subsequent solves with modified g, lb, ub", "algorithm": "Solves KKT conditions directly when active set is known.\n           Uses Cholesky factorization of reduced Hessian.", "see": ["QProblem for QPs with general linear constraints"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}, "include/qpOASES/SQProblem.hpp": {"path": "layer-1/qpOASES/include/qpOASES/SQProblem.hpp", "filename": "SQProblem.hpp", "file": "SQProblem.hpp", "brief": "Sequential QP solver with varying Hessian and constraint matrices\n\nSQProblem extends QProblem to handle QPs where the Hessian H and\nconstraint matrix A change between solves. This is common in:\n- Nonlinear MPC (linearization changes each step)\n- Sequential Quadratic Programming (SQP) for NLP\n- Moving horizon estimation\n\nThe hotstart method accepts new H and A matrices, efficiently\nupdating internal factorizations.", "see": ["QProblem for fixed H and A", "Ipopt for full NLP solving that uses QP subproblems"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": false}, "include/qpOASES/Options.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Options.hpp", "filename": "Options.hpp", "file": "Options.hpp", "brief": "Configuration options for qpOASES QP solver\n\nThe Options class controls solver behavior including:\n- Termination criteria (tolerances, iteration limits)\n- Numerical parameters (regularization, pivoting)\n- Output/printing verbosity\n- Initial homotopy and scaling options\n\nPreset configurations:\n- setToDefault(): Balanced defaults\n- setToReliable(): Maximum robustness\n- setToMPC(): Fast for model predictive control\n- setToFast(): Maximum speed, less robust", "see": ["QProblem::setOptions for applying options"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": false}, "include/qpOASES/QProblem.hpp": {"path": "layer-1/qpOASES/include/qpOASES/QProblem.hpp", "filename": "QProblem.hpp", "file": "QProblem.hpp", "brief": "QP solver with general linear constraints\n\nSolves convex QPs with bounds and linear constraints:", "math": "lb  <= x <= ub       (bounds)\n\nKey methods:\n- init(): First QP solve (cold start)\n- hotstart(): Subsequent solves with modified data", "algorithm": "Online Active Set Strategy:\n  1. Start from a primal-dual feasible point\n  2. Track active set (constraints at bounds)\n  3. Update active set incrementally when parameters change\n  Efficient for parametric QPs where data varies smoothly.", "complexity": "O(n^3) worst case per iteration, but typically much\n            faster due to hot-starting and sparse updates.", "see": ["QProblemB for box-constrained QPs (no A matrix)", "SQProblem for varying Hessian"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}}}}}, "layer-2": {"name": "layer-2", "library_count": 4, "libraries": {"ADOL-C": {"name": "ADOL-C", "file_count": 20, "pass2_count": 1, "files": {"ADOL-C/include/adolc/checkpointing.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/checkpointing.h", "filename": "checkpointing.h", "file": "checkpointing.h", "brief": "Checkpointing support for memory-efficient reverse mode AD\n\nImplements checkpointing (also known as \"time-stepping\" or \"revolve\")\nfor computing adjoints of long time-stepping computations with bounded\nmemory. Instead of storing all intermediate states, only selected\n\"checkpoints\" are stored, and segments are recomputed as needed.\n\nUse cases:\n- Time-stepping ODEs/PDEs where storing all steps would exhaust memory\n- Long computational chains with limited storage budget\n- Trade-off between memory usage and recomputation cost\n\nThe implementation uses the optimal binomial checkpointing schedule\n(Griewank & Walther, Algorithm 799: Revolve).", "see": ["revolve.h for the underlying scheduling algorithm", "externfcts.h for external function differentiation"], "has_pass2": false}, "ADOL-C/include/adolc/adolc.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adolc.h", "filename": "adolc.h", "file": "adolc.h", "brief": "Master include file for ADOL-C automatic differentiation library\n\nThis header provides the complete C/C++ interface for ADOL-C, a tape-based\nautomatic differentiation library using operator overloading. Include this\nfile to access:\n\n- adouble/pdouble types for traced computation\n- Tape management (trace_on, trace_off, createNewTape)\n- Forward and reverse mode interfaces\n- Easy-to-use driver routines (gradient, jacobian, hessian)\n\n@note Content included depends on whether compiling as C or C++", "see": ["adtb_types.h for adouble/pdouble class definitions", "interfaces.h for low-level forward/reverse mode calls", "drivers/drivers.h for high-level derivative computation", "tape_interface.h for tape management functions"], "has_pass2": false}, "ADOL-C/include/adolc/adolc_sparse.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adolc_sparse.h", "filename": "adolc_sparse.h", "file": "adolc_sparse.h", "brief": "Convenience header for sparse derivative computation\n\nInclude this file to access all sparse derivative functionality:\n- sparse_jac(): Sparse Jacobian computation\n- sparse_hess(): Sparse Hessian computation\n- jac_pat() / hess_pat(): Sparsity pattern detection\n- Bit-pattern forward/reverse modes", "see": ["adolc.h for main ADOL-C include (does not include sparse by default)", "sparse/sparsedrivers.h for detailed sparse driver documentation"], "has_pass2": false}, "ADOL-C/include/adolc/revolve.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/revolve.h", "filename": "revolve.h", "file": "revolve.h", "brief": "Optimal binomial checkpointing for memory-efficient reverse mode\n\nImplements the revolve algorithm (Griewank & Walther) for optimal\ncheckpoint placement in reverse mode automatic differentiation.\n\n**Problem:** Reverse mode AD requires storing intermediate values from\nthe forward sweep to use during the reverse sweep. For long computations\nwith N steps, this requires O(N) memory.\n\n**Solution:** The revolve algorithm trades memory for recomputation.\nWith s checkpoints and allowing r repetitions, it can handle up to\nbinomial(s+r, r) steps optimally.\n\n**Actions returned by revolve():**\n- revolve_advance: Run forward computation\n- revolve_takeshot: Save checkpoint\n- revolve_restore: Restore from checkpoint\n- revolve_firsturn/youturn: Start/continue reverse sweep\n- revolve_terminate: Computation complete", "see": ["checkpointing.h for ADOL-C checkpointing interface"], "has_pass2": false}, "ADOL-C/include/adolc/externfcts.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/externfcts.h", "filename": "externfcts.h", "file": "externfcts.h", "brief": "Support for externally differentiated functions in ADOL-C tapes\n\nProvides mechanisms to incorporate user-supplied derivative code for\nfunctions that cannot or should not be traced (e.g., library calls,\nspecialized solvers, discontinuous functions). The user registers\ncallback functions for forward and reverse mode evaluation.\n\nUse cases:\n- Integrating external solvers (linear algebra, sparse systems)\n- Hand-coded derivatives for performance-critical sections\n- Functions from non-ADOL-C libraries\n- Discontinuous or non-differentiable operations with custom handling\n\nCallback modes:\n- zos_forward: zero-order scalar (function evaluation only)\n- fos_forward/fos_reverse: first-order scalar\n- fov_forward/fov_reverse: first-order vector\n- hos_forward/hos_reverse: higher-order scalar\n- hov_forward/hov_reverse: higher-order vector", "see": ["interfaces.h for standard AD interfaces", "checkpointing.h for memory-efficient reverse mode"], "has_pass2": false}, "ADOL-C/include/adolc/adolcerror.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adolcerror.h", "filename": "adolcerror.h", "brief": "Exception class for ADOL-C errors with source location tracking.\n\nThis exception captures the error message and the source location (file,\nline, column) where it was thrown. Integrates with the C++ standard exception\nhierarchy through std::runtime_error.\n\n@example\nThrow example:\n@code\nthrow ADOLCError(\"wrong number of independents\");  // Auto-captures source\nlocation\n@endcode\n\nHandle example:\n@code\ntry {\n  // ADOL-C operations...\n}\ncatch (ADOLCError& e) {\n  std::cerr << e.what() << std::endl;         // Formatted message\n  std::cerr << \"Error occurred at: \"\n            << e.where().file() << \":\"\n            << e.where().func() << \":\"\n            << e.where().line() << std::endl; // Direct source location\naccess\n}\n@endcode", "param": ["message Error description (will be stored as std::string)", "info Source location"], "return": "Const reference to the source_location object", "has_pass2": false}, "ADOL-C/include/adolc/adtl.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adtl.h", "filename": "adtl.h", "file": "adtl.h", "brief": "Tape-less (traceless) forward-mode automatic differentiation\n\nProvides the adtl::adouble class for direct forward-mode AD without\ntape recording. Each adouble carries both its value and directional\nderivatives, which are propagated immediately through operations.\n\nAdvantages over tape-based mode:\n- No memory overhead from tape storage\n- Lower per-operation cost for simple functions\n- Simpler for computing few directional derivatives\n\nLimitations:\n- Only forward mode (no reverse/adjoint mode)\n- Memory scales as O(n·p) for n variables and p directions\n- Must set numDir before creating adoubles via setNumDir(p)\n\nTypical usage:\n@code\n  adtl::setNumDir(3);  // 3 directions\n  adtl::adouble x = 2.0;\n  x.setADValue(0, 1.0);  // ∂x/∂d0 = 1\n  adtl::adouble y = x*x;\n  // y.getValue() = 4.0, y.getADValue(0) = 4.0 (2*x*1)\n@endcode", "see": ["adtb_types.h for tape-based adouble (supports reverse mode)", "interfaces.h for tape evaluation routines"], "has_pass2": false}, "ADOL-C/include/adolc/tape_interface.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/tape_interface.h", "filename": "tape_interface.h", "brief": "Returns a thread-local vector that holds unique pointers to ValueTape\ninstances.\n\nThis buffer serves as storage for all tapes created in the current thread.\nEach thread has its own isolated buffer, enabling thread-safe tape\nmanagement.", "return": "Reference to the thread-local vector of unique pointers to ValueTape.", "param": ["tapeId The ID of the tape to search for.", "tapeId The ID of the tape to locate.", "tapeId The ID of the tape to retrieve.", "tapeId The ID of the tape to set as current.\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId The ID of the new tape to create.\n\n@throws ADOLCError::ErrorType::TAPE_ALREADY_EXIST if a tape with the same ID\nalready exists.", "tapeId ID of the tape to activate for tracing.", "keepTaylors Flag indicating whether to keep Taylor coefficients\n(non-zero to keep).", "tapeId ID of the tape to trace.", "keepTaylors Whether to store Taylor coefficients.", "obs Operation buffer size.", "lbs Location buffer size.", "vbs Value buffer size.", "tbs Taylor buffer size.", "skipFileCleanup Whether to skip cleaning up tape files after tracing.", "flag If non-zero, forces the tape to write op/loc/val files.\n\n@throws ADOLCError::ErrorType::TAPING_NOT_ACTUALLY_TAPING if no active\ntracing is detected.\n\n@note assert in debug mode if currentTape is nullptr", "result vector to store the IDs\n\n@note the vector is resized in this function to the size of the tapeBuffer", "tapeId the ID of the tape to set nested", "nested char to set\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId the ID of the tape to set nested", "tapeId the ID of the tape to read the tapestats", "tapeId the ID of the tape to print the stats\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId ID of the tape to read the number of parameters", "tapeId ID of the tape to store the jacobian information", "sJinfos the information to store\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId ID of the tape to store the hessian information", "sHInfos the information to store\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist."], "has_pass2": false}, "ADOL-C/include/adolc/adtb_types.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adtb_types.h", "filename": "adtb_types.h", "file": "adtb_types.h", "brief": "Core AD types: adouble, pdouble, and tape_location\n\nDefines the fundamental types for tape-based automatic differentiation:\n\n- **adouble**: Active double that records operations on the tape.\n  Use for variables whose derivatives you want to compute.\n\n- **pdouble**: Parameter double for non-differentiable constants that\n  can be changed without re-taping. Use for parameters you want to\n  vary across multiple derivative evaluations.\n\n- **tape_location<T>**: RAII wrapper managing tape location allocation\n  and deallocation for adouble/pdouble.\n\nAlso provides overloaded operators and math functions (sin, cos, exp, etc.)\nthat record operations and enable derivative computation.", "see": ["adolc.h for main include", "interfaces.h for derivative computation functions", "adtl.h for tape-less forward mode alternative"], "tparam": ["T The type to check.", "T Either `adouble` or `pdouble`."], "param": ["loc_ Location on the tape", "valid_ Specifies whether `tape_location` was moved. Used to decide if\n`loc_` can be free'd or not.", "tape The tape to retrieve the next location from.", "other The `tape_location` to move from.", "other The `tape_location` to move from.", "coval Value that is stored on the tape at the new location.", "a The `adouble` to copy.", "other The `adouble` to transfer.", "coval The value to assign.", "a The `adouble` to assign.", "other The `adouble` to transfer.", "p The `pdouble` to assign.", "coval the new value to assign.", "in Value that is assigned to the `adouble`", "out Value that will get the value of `adouble`", "other The `pdouble` to transfer.", "other The `pdouble` to transfer.", "pval The initial value for the `pdouble` on the parameter tape.", "pval The new value to assign.", "a adouble to check whether its value on the tape is normal", "a adouble to check whether its value on the tape is nan", "a adouble to check whether its value on the tape is inf", "a adouble to check whether its value on the tape is finite", "p pdouble to check whether its value on the tape is normal", "p pdouble to check whether its value on the tape is nan", "p pdouble to check whether its value on the tape is inf", "p pdouble to check whether its value on the tape is finite"], "return": "The next location index.", "has_pass2": false}, "ADOL-C/include/adolc/fixpoint.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/fixpoint.h", "filename": "fixpoint.h", "file": "fixpoint.h", "brief": "Differentiation through fixed-point iterations\n\nEnables automatic differentiation of implicit functions defined as\nfixed points x* = G(x*, u), where u are parameters.\n\n**Problem:** Many numerical methods involve iterative solvers:\n- Newton's method: x_{k+1} = x_k - f(x_k)/f'(x_k)\n- Nonlinear system solvers\n- Implicit time integrators\n\n**Solution:** fp_iteration() differentiates through the converged result\nwithout differentiating through all iteration steps, using the implicit\nfunction theorem: dx", "has_pass2": false}, "ADOL-C/include/adolc/advector.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/advector.h", "filename": "advector.h", "file": "advector.h", "brief": "Traceable vector with differentiable subscript operations\n\nProvides advector, a vector of adoubles that can trace subscripting\nwith adouble indices. This enables automatic differentiation through\narray lookups where the index itself depends on independent variables.\n\n**Key classes:**\n- advector: Vector container with contiguous tape locations\n- adubref: Reference type returned when indexing with adouble (lvalue case)\n\n**Use case example:**\n@code\nadvector table(n);        // Create table of adoubles\nadouble index = ...;      // Index depends on independent variables\nadouble result = table[index];  // Differentiation tracks index dependency\n@endcode\n\n@note This is only for taped (not tapeless) ADOL-C mode", "see": ["adtb_types.h for adouble and pdouble definitions"], "has_pass2": false}, "ADOL-C/include/adolc/edfclasses.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/edfclasses.h", "filename": "edfclasses.h", "file": "edfclasses.h", "brief": "Object-oriented interface for external differentiated functions\n\nProvides C++ class-based wrappers for external functions with user-supplied\nderivatives. Inherit from these base classes to integrate external code\n(e.g., legacy Fortran, optimized BLAS, specialized solvers) into AD.\n\n**Classes:**\n- EDFobject: Basic external function with fixed array sizes\n- EDFobject_iArr: External function with integer array parameters\n- EDFobject_v2: Version 2 interface with variable-size arrays and context\n\n**Required overrides:**\n- function(): Evaluate the external function\n- zos_forward(): Zero-order scalar (function evaluation during taping)\n- fos_forward(): First-order scalar forward mode\n- fov_forward(): First-order vector forward mode\n- fos_reverse(): First-order scalar reverse mode\n- fov_reverse(): First-order vector reverse mode", "see": ["externfcts.h for C-style external function interface", "externfcts2.h for v2 C-style interface"], "has_pass2": false}, "ADOL-C/include/adolc/interfaces.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/interfaces.h", "filename": "interfaces.h", "file": "interfaces.h", "brief": "Low-level forward and reverse mode interfaces for tape evaluation\n\nProvides the core differentiation routines that evaluate recorded tapes\nin forward mode (computing directional derivatives) and reverse mode\n(computing adjoints/gradients). These are the building blocks used by\nhigher-level drivers like gradient() and hessian().\n\nMode naming conventions:\n- zos: zero-order scalar - function evaluation only\n- fos: first-order scalar - single direction/adjoint\n- hos: higher-order scalar - Taylor series to order d\n- fov: first-order vector - p directions/adjoints simultaneously\n- hov: higher-order vector - p Taylor series to order d\n- wk: with keep - retain intermediate values for reverse\n- ti: Taylor input - provide input Taylor coefficients", "see": ["drivers/drivers.h for easy-to-use wrapper functions", "tape_interface.h for tape management (trace_on, trace_off)"], "has_pass2": false}, "ADOL-C/include/adolc/sparse/sparsedrivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/sparse/sparsedrivers.h", "filename": "sparsedrivers.h", "file": "sparse/sparsedrivers.h", "brief": "High-level drivers for sparse Jacobian and Hessian computation\n\nProvides efficient computation of sparse derivatives by exploiting\nsparsity structure. Uses a two-phase approach:\n\n1. **Sparsity detection** (jac_pat, hess_pat): Determine which entries\n   are structurally non-zero using bit-pattern propagation\n\n2. **Seed generation** (generate_seed_jac, generate_seed_hess): Compute\n   optimal seed matrices via graph coloring to minimize directional\n   derivative evaluations\n\n3. **Sparse evaluation** (sparse_jac, sparse_hess): Compute non-zero\n   values using compressed row storage (CRS) format\n\nComplexity advantage: For a matrix with O(n) non-zeros and O(1) colors,\nsparse_jac costs O(c) vs O(n·c) for dense jacobian().", "see": ["drivers/drivers.h for dense derivative computation", "sparse/sparse_fo_rev.h for bit-pattern forward/reverse modes", "generate_seed_jac for computing seed matrix from pattern", "sparse_jac for computing values", "jac_pat for sparsity detection only", "jacobian for dense Jacobian computation", "generate_seed_hess for computing seed matrix from pattern", "sparse_hess for computing values", "hess_pat for sparsity detection only", "hessian for dense Hessian computation"], "param": ["tag Tape identifier", "m Number of dependent variables (rows)", "n Number of independent variables (columns)", "x Evaluation point x[n]", "crs Output sparsity pattern crs[m][*], where crs[i][0] = count\n           and crs[i][1..count] = column indices of non-zeros in row i", "options Control options[3] for propagation mode", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "repeat If non-zero, reuse sparsity pattern from previous call", "x Evaluation point x[n]", "nnz Output: number of non-zeros", "row_ind Output: row indices (COO format)", "col_ind Output: column indices (COO format)", "values Output: non-zero values", "options Control options[3]", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Evaluation point x[n]", "crs Output sparsity pattern crs[n][*], where crs[i][0] = count\n           and crs[i][1..count] = column indices of non-zeros in row i", "option Control option for propagation mode", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "repeat If non-zero, reuse sparsity pattern from previous call", "x Evaluation point x[n]", "nnz Output: number of non-zeros (lower triangle only)", "row_ind Output: row indices (COO format)", "col_ind Output: column indices (COO format)", "values Output: non-zero values", "options Control options[2]"], "return": "0 on success", "has_pass2": false}, "ADOL-C/include/adolc/sparse/sparse_fo_rev.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/sparse/sparse_fo_rev.h", "filename": "sparse_fo_rev.h", "file": "sparse/sparse_fo_rev.h", "brief": "Bit-pattern propagation for sparsity detection\n\nProvides forward and reverse mode interfaces that propagate bit patterns\ninstead of numerical values. Used to efficiently determine the sparsity\nstructure of Jacobian and Hessian matrices.\n\nBit patterns are packed into size_t words for efficiency. For n independent\nvariables, the seed matrix X[n][p] uses p = ceil(n / bits_per_long) words\nper row, where bits_per_long = 8 * sizeof(size_t).\n\nTwo modes:\n- **Tight**: Uses actual values x[] during propagation (more accurate)\n- **Safe**: Uses only bit patterns (faster, may overestimate sparsity)", "see": ["sparse/sparsedrivers.h for high-level sparse derivative computation"], "has_pass2": false}, "ADOL-C/include/adolc/drivers/taylor.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/taylor.h", "filename": "taylor.h", "file": "drivers/taylor.h", "brief": "Higher-order derivative tensors and implicit function differentiation\n\nProvides drivers for computing higher-order derivative tensors and\ndifferentiating through implicit/inverse functions.\n\n**Tensor computation:**\n- Computes d-th order derivative tensor using p seed vectors\n- Tensor dimension: C(p+d,d) = \"p+d choose d\" entries per output\n\n**Inverse/implicit function:**\n- Derivatives of x = F^{-1}(y) when tape computes y = F(x)\n- Taylor series propagation through inverse function\n- jac_solv(): Solve J*b = rhs where J is the Jacobian at x\n\n@note These are advanced drivers for specialized applications", "see": ["drivers/drivers.h for standard gradient/Jacobian/Hessian", "drivers/odedrivers.h for ODE-specific Taylor propagation"], "has_pass2": false}, "ADOL-C/include/adolc/drivers/odedrivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/odedrivers.h", "filename": "odedrivers.h", "file": "drivers/odedrivers.h", "brief": "Taylor-based ODE integration drivers\n\nProvides drivers for solving and differentiating ODEs of the form\nx' = f(x) using Taylor series expansion. The tape records f(x),\nthen these drivers compute higher-order Taylor coefficients.\n\n**Forward ODE integration:**\n- forode(): Given x[0] = x0, compute Taylor coefficients x[1]..x[d]\n  such that x(t) = x[0] + x[1]*t + x[2]*t^2/2! + ...\n- Uses recurrence: x[k+1] = f'(x)*x[k] / (k+1)\n\n**Accumulated code (sensitivity):**\n- accode(): Given partial Jacobians dF/dx at each Taylor order,\n  accumulate total sensitivities via chain rule", "param": ["tau Time scaling factor (default 1.0)", "dold Previous Taylor degree computed (default 0)", "dnew New target Taylor degree"], "see": ["interfaces.h for forward/reverse mode primitives", "drivers/taylor.h for general tensor computation"], "has_pass2": false}, "ADOL-C/include/adolc/drivers/psdrivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/psdrivers.h", "filename": "psdrivers.h", "file": "drivers/psdrivers.h", "brief": "Drivers for piecewise smooth (PS) functions with abs-normal form\n\nProvides differentiation tools for functions containing absolute values\nand other piecewise linear operations. These functions are not classically\ndifferentiable at kink points, but have well-defined generalized derivatives.\n\n**Abs-normal form:** Represents piecewise linear functions as:\n- y = cy + Y*x + J*z\n- z = cz + Z*x + L*|z|\n\nwhere z are \"switching variables\" capturing |·| operations, and L is\nstrictly lower triangular (ensuring the implicit equation has unique solution).\n\nThis representation enables:\n- Generalized derivative computation at non-smooth points\n- Algorithmic piecewise differentiation\n- Optimization of non-smooth objectives", "see": ["drivers/drivers.h for smooth function derivatives", "interfaces.h for underlying forward/reverse modes"], "param": ["tag Tape identifier", "n Number of independent variables", "x Evaluation point x[n]", "d Direction vector d[n]", "g Output directional active gradient g[n]", "sigma_g Output signature indicating which piece is active", "tag Tape identifier", "m Number of dependent variables (outputs)", "n Number of independent variables (inputs)", "swchk Number of switching variables s (abs operations in tape)", "x Base point x[n] for linearization", "y Output function value y[m] at base point", "z Output switching variable values z[s]", "cz Output constant vector cz[s] for z equation", "cy Output constant vector cy[m] for y equation", "Y Output Jacobian Y[m][n] of y w.r.t. x", "J Output Jacobian J[m][s] of y w.r.t. z", "Z Output Jacobian Z[s][n] of z (before abs) w.r.t. x", "L Output lower triangular L[s][s] coupling between switching vars"], "return": "0 on success\n\n@note The signature sigma_g encodes which branch of each abs() is taken", "has_pass2": false}, "ADOL-C/include/adolc/drivers/drivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/drivers.h", "filename": "drivers.h", "file": "drivers/drivers.h", "brief": "High-level driver functions for derivative computation\n\nProvides convenient functions for computing common derivative quantities:\n- gradient(): First derivative of scalar function (∇f)\n- jacobian(): First derivative of vector function (∂F/∂x)\n- hessian(): Second derivative of scalar function (∇²f)\n- hess_vec(): Hessian-vector product (∇²f · v)\n- jac_vec(): Jacobian-vector product (J · v)\n- vec_jac(): Vector-Jacobian product (u^T · J)\n\nThese drivers wrap the lower-level forward/reverse interfaces and handle\nmemory allocation and mode selection automatically. All functions require\na pre-recorded tape (via trace_on/trace_off).", "see": ["interfaces.h for low-level forward/reverse mode calls", "tape_interface.h for tape recording functions", "drivers/taylor.h for higher-order Taylor coefficient drivers", "gradient for computing first derivatives", "jacobian for vector-valued functions", "hessian for second derivatives", "gradient for scalar functions (m=1)", "vec_jac for vector-Jacobian products", "jac_vec for Jacobian-vector products", "jac_vec for Jacobian-vector product (forward mode)", "vec_jac for vector-Jacobian product (reverse mode)", "hessian2 for alternative using Hessian-matrix product", "hess_vec for Hessian-vector product only", "hessian for standard approach", "hess_mat for multiple Hessian-vector products", "hess_mat for multiple directions", "lagra_hess_vec for Lagrangian Hessian-vector product", "hess_vec for single direction", "hess_vec for single scalar function"], "param": ["tag Tape identifier", "m Number of dependent variables (outputs)", "n Number of independent variables (inputs)", "x Input point x[n]", "y Output values y[m] = F(x)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "g Output gradient g[n] = ∇f(x)", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "x Input point x[n]", "J Output Jacobian J[m][n]", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "k Strip size (number of columns computed per forward sweep)", "x Input point x[n]", "y Output function values y[m] = F(x)", "J Output Jacobian J[m][n]", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "repeat If non-zero, reuse Taylor coefficients from previous call", "x Input point x[n]", "u Adjoint seed vector u[m]", "v Output vector v[n] = u^T · J = Σ_i u_i · ∇F_i", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "x Input point x[n]", "v Tangent direction vector v[n]", "u Output vector u[m] = J · v (directional derivative)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "H Output Hessian H[n][n] (lower triangle filled)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "H Output Hessian H[n][n] (lower triangle filled)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "v Direction vector v[n]", "w Output vector w[n] = H · v = ∇²f(x) · v", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "q Number of direction vectors", "x Input point x[n]", "V Direction matrix V[n][q] (q column vectors)", "W Output matrix W[n][q] where W[:,j] = H · V[:,j]", "tag Tape identifier", "m Number of dependent variables (constraints)", "n Number of independent variables", "x Input point x[n]", "v Direction vector v[n]", "u Lagrange multipliers u[m]", "w Output vector w[n] = ∇²(Σ u_i F_i) · v"], "return": "0 on success, non-zero on error", "algorithm": "Reverse mode automatic differentiation", "complexity": "O(c·n) where c = cost of function evaluation (typically c ≈ 4-5)", "has_pass2": true}, "ADOL-C/include/adolc/lie/drivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/lie/drivers.h", "filename": "drivers.h", "file": "lie/drivers.h", "brief": "Lie derivative computation for nonlinear control systems\n\nComputes Lie derivatives used in nonlinear control theory for:\n- Observability analysis (lie_scalar, lie_gradient)\n- Controllability analysis (lie_covector, lie_bracket)\n\n**Mathematical background:**\nGiven vector field f(x) and scalar function h(x), the Lie derivative is:\n  L_f h = ∇h · f = Σ (∂h/∂x_i) f_i(x)\n\nHigher-order Lie derivatives (L_f^k h) reveal observability structure.\nLie brackets [f,g] = ∂g/∂x·f - ∂f/∂x·g reveal controllability structure.\n\n**Functions:**\n- lie_scalar(): Compute L_f^k h (scalar Lie derivatives)\n- lie_gradient(): Compute ∇(L_f^k h) (gradients of Lie derivatives)\n- lie_covector(): Compute covector fields\n- lie_bracket(): Compute Lie bracket [f,g]", "see": ["drivers/drivers.h for standard derivative computation"], "has_pass2": false}}}, "Cbc": {"name": "Cbc", "file_count": 83, "pass2_count": 4, "files": {"src/CbcConsequence.hpp": {"path": "layer-2/Cbc/src/CbcConsequence.hpp", "filename": "CbcConsequence.hpp", "file": "CbcConsequence.hpp", "brief": "Abstract base for bound implications from branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcConsequence: Abstract interface for side effects when branching.\nWhen a variable is fixed, other variables may have implied bounds\nthat go beyond what LP constraint propagation would derive.\n\nExample: Binary y controls whether continuous x can be nonzero.\nWhen y=0, we know x=0 even if the constraint linking them is weak.\n\nThe applyToSolver() method is called after branching to impose\nthese derived bounds on the solver, based on the branch state.", "see": ["CbcFixVariable for concrete implementation", "CbcObject for objects that may have consequences\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": false}, "src/CbcSOS.hpp": {"path": "layer-2/Cbc/src/CbcSOS.hpp", "filename": "CbcSOS.hpp", "file": "CbcSOS.hpp", "brief": "Special Ordered Sets (SOS) Type 1 and Type 2 branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSOS: Branching for Special Ordered Sets (Beale & Tomlin, 1970):\n\nSOS Type 1: At most ONE variable can be nonzero\n- Common for selection between alternatives (choose one option)\n- SUM x_i <= 1 (or = 1 for exactly one)\n- Binary SOS1 is a special case of clique\n\nSOS Type 2: At most TWO CONSECUTIVE variables can be nonzero\n- Used for piecewise linear approximation (interpolation)\n- Variables ordered by weights; sum to 1\n- Represents point between two breakpoints\n\nBranching partitions the ordered set at a separator weight,\nforcing variables on one side to zero.", "see": ["CbcSOSBranchingObject for branching action", "CbcClique for related binary set branching", "OsiSOS for OSI-level representation\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcBranchDecision.hpp": {"path": "layer-2/Cbc/src/CbcBranchDecision.hpp", "filename": "CbcBranchDecision.hpp", "file": "CbcBranchDecision.hpp", "brief": "Abstract base for branching variable selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchDecision defines HOW TO CHOOSE between branching candidates:\n- betterBranch(): Compare two CbcBranchingObjects after strong branching\n- bestBranch(): Compare N candidates and return best index\n\nUsed after strong branching evaluates candidates. Decision considers:\n- changeUp/changeDown: Objective change estimates\n- numInfeasibilitiesUp/Down: Remaining infeasibilities", "see": ["CbcBranchDefaultDecision for simple selection", "CbcBranchDynamicDecision for pseudocost-based selection", "CbcBranchingObject for what gets compared", "OsiChooseVariable for variable selection integration\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": false}, "src/CbcHeuristicVND.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicVND.hpp", "filename": "CbcHeuristicVND.hpp", "file": "CbcHeuristicVND.hpp", "brief": "VND - Variable Neighborhood Descent\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicVND: Variable Neighborhood Descent metaheuristic.\nSystematically explores neighborhoods of increasing size.\n\nAlgorithm (Hansen & Mladenovic):\n1. Start from baseSolution_\n2. Search in neighborhood k (stepSize_ variables)\n3. If improved, reset k=1; otherwise k++\n4. Repeat until k > kmax_\n\nParameters:\n- stepSize_: Base neighborhood size\n- k_, kmax_: Current and maximum neighborhood index\n- nDifferent_: Tracks solution diversity\n- baseSolution_: Starting point for descent\n\nUses solutionFix() to solve restricted MIPs.", "see": ["CbcHeuristicRINS for related neighborhood search", "CbcHeuristicLocal for simpler local search\n\nEdwin 12/5/09 carved out of CbcHeuristicRINS"], "has_pass2": false}, "src/CbcFathom.hpp": {"path": "layer-2/Cbc/src/CbcFathom.hpp", "filename": "CbcFathom.hpp", "file": "CbcFathom.hpp", "brief": "Fathoming methods to complete subproblems\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFathom: Abstract base for specialized fathoming techniques.\nAfter branching, the subproblem may be small enough for specialized\nexact methods faster than general B&C.\n\nfathom() return codes:\n- 0: No fathoming attempted\n- 1: Fully fathomed (optimal for subtree)\n- 2: Incomplete search\n- 3: Incomplete but treat as complete\n\nExample implementations:\n- Presolve to smaller problem, then B&C\n- Dynamic programming for special structure\n\nAlso contains CbcOsiSolver: OsiClpSolverInterface that knows about\nCbcModel (base class for CbcLinked).", "see": ["CbcFathomDynamicProgramming for DP fathoming", "CbcLinked for linked solver interface"], "has_pass2": false}, "src/CbcDebug.hpp": {"path": "layer-2/Cbc/src/CbcDebug.hpp", "filename": "CbcDebug.hpp", "file": "CbcDebug.hpp", "brief": "Debugging utilities and test message handlers\n\nCbcDebug: Contains debug memory allocation routines and test message handlers.\n\nFeatures when CLP_DEBUG_MALLOC defined:\n- clp_memory(): Memory debugging\n- clp_malloc()/clp_free(): Custom allocation with tracking\n\nFeatures when TEST_MESSAGE_HANDLER defined:\n- MyMessageHandler2: Example custom message handler\n- Demonstrates trapping and customizing Cbc messages\n- Prefixes messages with \"==\" for identification\n\nThis file is primarily for testing and debugging purposes.\nProduction builds typically have neither macro defined.", "see": ["CoinMessageHandler for message handling base", "CbcMessage for Cbc message definitions"], "has_pass2": false}, "src/CbcGenMessages.hpp": {"path": "layer-2/Cbc/src/CbcGenMessages.hpp", "filename": "CbcGenMessages.hpp", "file": "CbcGenMessages.hpp", "brief": "Message IDs for cbc-generic standalone solver\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcGenMsgCode: Enum of message IDs for the cbc-generic application.\nThese are distinct from CbcMessage (library messages) and are used\nonly by the standalone command-line solver.\n\nMessage codes:\n- CBCGEN_TEST_MSG: Test/debug message\n- CBCGEN_NEW_SOLVER: Solver change notification\n- CBCGEN_CONFUSION: Error/confusion state\n\nThe actual message text is defined in CbcGenMessages.cpp.", "see": ["CbcMessage for library-level messages", "CoinMessageHandler for message infrastructure"], "has_pass2": false}, "src/CbcHeuristic.hpp": {"path": "layer-2/Cbc/src/CbcHeuristic.hpp", "filename": "CbcHeuristic.hpp", "file": "CbcHeuristic.hpp", "brief": "Base class for MIP primal heuristics\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristic: Abstract base for primal heuristics that find feasible\nsolutions during branch-and-cut search. Key method is solution() which\nreturns 1 if a valid solution is found, 0 otherwise.\n\nExecution control:\n- when_: 0=off, 1=root only, 2=non-root, 3=always\n- whereFrom_: Bit field for timing (before/during/after cuts)\n- howOften_: Frequency of invocation\n\nIncludes several concrete heuristics:\n- CbcRounding: Simple rounding with lock-based feasibility\n- CbcHeuristicPartial: Fix priority-based partial solution\n- CbcSerendipity: Capture solver-found solutions\n- CbcHeuristicJustOne: Randomly select one from multiple heuristics\n\nAlso includes CbcHeuristicNode/NodeList for tracking where heuristics\nhave been invoked (distance-based diversification).", "see": ["CbcHeuristicFPump for Feasibility Pump", "CbcHeuristicDive for diving heuristics", "CbcHeuristicRINS, CbcHeuristicRENS for neighborhood search", "CbcHeuristicLocal for local search"], "has_pass2": false}, "src/CbcGeneralDepth.hpp": {"path": "layer-2/Cbc/src/CbcGeneralDepth.hpp", "filename": "CbcGeneralDepth.hpp", "file": "CbcGeneralDepth.hpp", "brief": "Depth-limited partial evaluation branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcGeneralDepth: Advanced branching using partial evaluation (lookahead).\nInstead of simple two-way branch, evaluates the search tree to a\nspecified depth and creates subproblems from the resulting nodes.\n\nUses ClpNode infrastructure (from Clp) for efficient node representation\nduring the lookahead phase. Can generate multiple subproblems from a\nsingle branching decision.\n\nAlso includes:\n- CbcGeneralBranchingObject: Multi-subproblem branching action\n- CbcOneGeneralBranchingObject: Single subproblem selection\n\nSupports orbital branching for symmetry handling.", "see": ["CbcGeneral for base class", "CbcSubProblem for subproblem representation", "ClpNode, ClpNodeStuff for Clp integration\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcHeuristicLocal.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicLocal.hpp", "filename": "CbcHeuristicLocal.hpp", "file": "CbcHeuristicLocal.hpp", "brief": "Local search and related improvement heuristics\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nContains several improvement heuristics for MIP:\n\nCbcHeuristicLocal: 2-opt swap-based local search.\nStarting from incumbent, tries setting variables to better values,\nthen swaps pairs of integer variables. Uses matrix_ for constraint\nanalysis and used_ array to track variable participation in solutions.\n\nCbcHeuristicProximity: Proximity search (Fischetti & Monaci 2014).\nUses Feasibility Pump infrastructure to find solutions close to\nincumbent. Blends objective with distance minimization.\n\nCbcHeuristicNaive: Simple construction heuristic.\n(a) Fix integers close to zero\n(b) Fix integers with small costs to zero\n(c) Tighten continuous bounds and optimize\n\nCbcHeuristicCrossover: Path relinking / solution combination.\nFixes variables that agree across multiple solutions (useNumber_).\nUses random_ array for diversification.", "see": ["CbcHeuristicFPump for Feasibility Pump", "CbcHeuristicRINS for neighborhood search", "CbcHeuristic for base class"], "has_pass2": false}, "src/CbcObject.hpp": {"path": "layer-2/Cbc/src/CbcObject.hpp", "filename": "CbcObject.hpp", "file": "CbcObject.hpp", "brief": "Abstract base for branching entities (variables, SOS, etc.)\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcObject defines WHAT can be branched on:\n- infeasibility(): Measure of violation (0 = feasible)\n- feasibleRegion(): Bound tightening without branching\n- createCbcBranch(): Create CbcBranchingObject for this entity\n\nDerived classes: CbcSimpleInteger, CbcSOS, CbcClique, CbcNWay, etc.\n\nCbcStrongInfo struct holds strong branching results:\nup/down movement, infeasibilities, solver status.\n\nNote: Current implementation assumes binary branching (2 arms).", "see": ["CbcSimpleInteger for integer variable objects", "CbcSOS for SOS Type 1/2 constraints", "CbcBranchingObject for branching actions", "OsiObject for OSI-level base class\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": false}, "src/CbcCompare.hpp": {"path": "layer-2/Cbc/src/CbcCompare.hpp", "filename": "CbcCompare.hpp", "file": "CbcCompare.hpp", "brief": "Functor wrapper for node comparison\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompare: Thin wrapper making CbcCompareBase usable as functor.\nUsed by standard heap algorithms for node priority queue.\n\nHolds pointer to CbcCompareBase strategy object (test_).\noperator() delegates to test_->test(x,y).", "see": ["CbcCompareBase for strategy interface", "CbcTree for usage in node management"], "has_pass2": false}, "src/CbcCompareDefault.hpp": {"path": "layer-2/Cbc/src/CbcCompareDefault.hpp", "filename": "CbcCompareDefault.hpp", "file": "CbcCompareDefault.hpp", "brief": "Default adaptive node comparison strategy\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareDefault: Sophisticated hybrid search strategy.\nAdapts behavior based on search phase and solution history.\n\nStrategy phases:\n- Before solution: Use depth-first (weight_ < 0) or breadth-first\n- After solution: Weight-based hybrid of objective and infeasibilities\n\nweight_ special values:\n- 0.0: Auto-compute from first solution\n- -1.0: Fewest infeasibilities (before solution)\n- -2.0: Breadth-first for first 1000 nodes\n- -3.0: Depth-first before solution\n\nAdaptive callbacks:\n- newSolution(): Recomputes weight from solution gap\n- every1000Nodes(): Adjusts for tree size, triggers diving\n\nDiving support:\n- startDive()/cleanDive(): Focused depth search from promising node", "see": ["CbcCompareBase for interface", "CbcCompareDepth for simple depth-first\n\nEdwin 11/25/09 carved out of CbcCompareActual"], "has_pass2": false}, "src/CbcHeuristicDINS.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDINS.hpp", "filename": "CbcHeuristicDINS.hpp", "file": "CbcHeuristicDINS.hpp", "brief": "DINS - Distance-Induced Neighborhood Search\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDINS: Uses multiple solutions to define neighborhoods.\nMaintains a pool of solutions and fixes variables based on\nagreement across the solution pool (Ghosh 2007).\n\nAlgorithm:\n1. Maintain pool of recent integer solutions (values_ array)\n2. Fix variables that have same value across solutions\n3. Add constraint on distance from solutions (localSpace_)\n4. Solve restricted MIP\n\nKey parameters:\n- maximumKeepSolutions_: Size of solution pool\n- localSpace_: Tightness of distance constraint\n- howOften_: Frequency of application", "see": ["CbcHeuristicRINS for two-solution comparison", "CbcHeuristic for base class\n\nEdwin 12/5/09 carved out of CbcHeuristicRINS"], "has_pass2": false}, "src/CbcNodeInfo.hpp": {"path": "layer-2/Cbc/src/CbcNodeInfo.hpp", "filename": "CbcNodeInfo.hpp", "file": "CbcNodeInfo.hpp", "brief": "Persistent information for recreating search tree nodes\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcNodeInfo stores information to reconstruct a subproblem:\n- Warm start basis\n- Variable bound changes\n- Cutting planes added at this node\n- Parent linkage and reference counting\n\nTwo flavors:\n- CbcFullNodeInfo: Complete state (used at root)\n- CbcPartialNodeInfo: Differences from parent (saves memory)\n\nReference counting: Sum of potential + actual children.\nNode deleted when count reaches zero (subtree pruned).", "see": ["CbcNode for live node information", "CbcFullNodeInfo for complete state storage", "CbcPartialNodeInfo for incremental storage\n\nEdwin 11/24/09 carved from CbcNode"], "has_pass2": false}, "src/CbcHeuristicDiveFractional.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveFractional.hpp", "filename": "CbcHeuristicDiveFractional.hpp", "file": "CbcHeuristicDiveFractional.hpp", "brief": "Dive heuristic selecting most fractional variable\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveFractional: Simplest diving strategy.\nSelects the variable with value closest to 0.5 (most fractional).\n\nselectVariableToBranch() picks variable i maximizing:\n  min(x[i] - floor(x[i]), ceil(x[i]) - x[i])\n\nSimple but effective for breaking ties and finding feasibility.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicDivePseudoCost for pseudocost-guided variant"], "has_pass2": false}, "src/CbcCutModifier.hpp": {"path": "layer-2/Cbc/src/CbcCutModifier.hpp", "filename": "CbcCutModifier.hpp", "file": "CbcCutModifier.hpp", "brief": "Abstract base class for cut modification\nCopyright (C) 2003, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCutModifier: Allows post-processing of generated cuts.\nCalled after cut generation to strengthen, weaken, or remove cuts.\n\nmodify() return codes:\n- 0: Unchanged\n- 1: Strengthened (tightened)\n- 2: Weakened (loosened)\n- 3: Deleted (should be discarded)\n\nUse cases:\n- Strengthen cuts using problem-specific knowledge\n- Remove cuts that are too dense or weak\n- Apply numerical cleanup", "see": ["CbcCutGenerator for cut generation framework", "CbcCutSubsetModifier for subset-specific modification\n\nEdwin 11/25/09 carved out of CbcCutGenerator"], "has_pass2": false}, "src/CbcCompareActual.hpp": {"path": "layer-2/Cbc/src/CbcCompareActual.hpp", "filename": "CbcCompareActual.hpp", "file": "CbcCompareActual.hpp", "brief": "Aggregator header for concrete comparison classes\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nConvenience header including all concrete node comparison strategies.\nSimilar pattern to CbcBranchActual.hpp.", "see": ["CbcCompareBase for abstract interface", "CbcCompareDepth for depth-first", "CbcCompareDefault for adaptive hybrid"], "has_pass2": false}, "src/CbcSolverHeuristics.hpp": {"path": "layer-2/Cbc/src/CbcSolverHeuristics.hpp", "filename": "CbcSolverHeuristics.hpp", "file": "CbcSolverHeuristics.hpp", "brief": "Heuristic setup and execution routines for cbc-generic\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nHelper functions used by the standalone solver for heuristic management.\n\ncrunchIt(): Compress/simplify a ClpSimplex model.\n\nfixVubs(): Fix variable upper bound constraints.\n- doAction values:\n  - 0: Just fix in original, return NULL\n  - 1: Return fixed non-presolved solver\n  - 2: Use presolve internally\n  - 3: Presolve and fix high-cost variables\n  - 10+: Use lastSolution and relax a few\n  - -2: Cleanup after mode 2\n- Returns number of variables fixed\n\ndoHeuristics(): Configure and run heuristics.\n- type 1: Add heuristics to model\n- type 2: Run heuristics (set cutoff and solution)\n- type 3: For miplib testing (skip some)", "see": ["CbcHeuristic for heuristic base class", "CbcParameters for parameter management"], "has_pass2": false}, "src/CbcHeuristicGreedy.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicGreedy.hpp", "filename": "CbcHeuristicGreedy.hpp", "file": "CbcHeuristicGreedy.hpp", "brief": "Greedy construction heuristics for set covering/partitioning\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nContains greedy heuristics suited for set covering/partitioning models:\n\nCbcHeuristicGreedyCover: Classic greedy set cover.\nVariables fixed at one stay at one. Others initially zero.\nIteratively selects variable with best cost/coverage ratio\n(with randomization for tie-breaking).\n\nCbcHeuristicGreedyEquality: Greedy for equality constraints.\nSimilar to cover but handles set partitioning (exact cover).\nUses fraction_ parameter to set partial coverage before B&C.\n\nCbcHeuristicGreedySOS: Greedy for SOS and <= constraints.\nSpecialized for models with GUB/SOS structure and positive elements.\noriginalRhs_ = -1 indicates SOS row, otherwise <= constraint.\nalgorithm_ bits control: current vs original model, pure vs\nsolution-guided greedy, merit calculation, dual usage.\n\nAll three use algorithm_ flags:\n- 0: Use current upper bounds\n- 1: Use original upper bounds\n- +10: More perturbation in ratios\n- +100: Round up all >= 0.5", "see": ["CbcHeuristic for base class", "CbcSOS for SOS constraint branching"], "has_pass2": false}, "src/CbcSymmetry.hpp": {"path": "layer-2/Cbc/src/CbcSymmetry.hpp", "filename": "CbcSymmetry.hpp", "file": "CbcSymmetry.hpp", "brief": "Symmetry detection and orbital branching using nauty\nAuthors: Pietro Belotti (Lehigh), Andreas Waechter (IBM)\nAdapted from Couenne (Carnegie-Mellon University, 2006-11)\nThis file is licensed under the Eclipse Public License (EPL)\n\nCbcSymmetry: Detects problem symmetry and exploits it for faster solving.\nUses the nauty library for automorphism group computation.\n\nKey concepts:\n- Orbits: Sets of symmetric variables\n- Orbital fixing: If one variable in orbit is fixed, fix others by symmetry\n- Orbital branching: Branch to break symmetry\n\nBuild requirements (when CBC_HAS_NAUTY defined):\n- nauty library (configure with --enable-tls --enable-wordsize=32)\n- Add -DCBC_HAS_NAUTY and -DNTY_TRACES (optional) to CXXDEFS\n- Link with -lnauty\n\nRuntime: Use -orbit on to enable\n\nCbcNauty: Wrapper for nauty graph automorphism library.\nHandles graph construction, partition refinement, and orbit computation.\n\nCbcOrbitalBranchingObject: Branching object that exploits symmetry.\nOn one branch, fix variable to 1 and symmetric variables to 0.", "see": ["CbcBranchingObject for branching base class", "nauty documentation for algorithm details"], "has_pass2": false}, "src/CbcBranchActual.hpp": {"path": "layer-2/Cbc/src/CbcBranchActual.hpp", "filename": "CbcBranchActual.hpp", "file": "CbcBranchActual.hpp", "brief": "Aggregator for concrete branching classes\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nConvenience header that includes all concrete branching implementations:\n- CbcSimpleInteger, CbcSimpleIntegerPseudoCost: Integer variables\n- CbcClique, CbcSOS, CbcNWay: Constraint branching\n- CbcFollowOn, CbcFixVariable: Derived bound manipulation\n- CbcGeneral, CbcGeneralDepth: Advanced branching strategies\n\nMost code should include CbcBranchBase.hpp instead for just the\nabstract interfaces; use this header when concrete classes are needed.", "see": ["CbcBranchBase.hpp for abstract interface aggregation"], "has_pass2": false}, "src/CbcHeuristicDiveCoefficient.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveCoefficient.hpp", "filename": "CbcHeuristicDiveCoefficient.hpp", "file": "CbcHeuristicDiveCoefficient.hpp", "brief": "Dive heuristic based on objective coefficients\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveCoefficient: Selects variables based on objective impact.\nPrioritizes fractional variables with large objective coefficients.\n\nselectVariableToBranch() considers objective coefficient magnitude\nto choose variables whose fixing has greatest objective impact.\nUseful when objective-driven rounding is desired.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicDivePseudoCost for estimate-based variant"], "has_pass2": false}, "src/CbcBranchBase.hpp": {"path": "layer-2/Cbc/src/CbcBranchBase.hpp", "filename": "CbcBranchBase.hpp", "file": "CbcBranchBase.hpp", "brief": "Base includes for CBC branching model\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nAggregates the three-class branching model:\n- CbcObject: What to branch on (integer vars, SOS, cliques)\n- CbcBranchingObject: How to branch (bound changes per arm)\n- CbcBranchDecision: How to choose (compare candidates)\n\nAlso defines CbcRangeCompare enum for bound comparisons\nand CbcCompareRanges() utility function.", "see": ["CbcObject for branching candidates", "CbcBranchingObject for branching actions", "CbcBranchDecision for selection criteria", "OsiBranchingObject for OSI-level interface"], "has_pass2": false}, "src/CbcGeneral.hpp": {"path": "layer-2/Cbc/src/CbcGeneral.hpp", "filename": "CbcGeneral.hpp", "file": "CbcGeneral.hpp", "brief": "Abstract base for general multi-way branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcGeneral: Abstract base class for advanced branching strategies\nthat create lists of subproblems beyond simple two-way branching.\n\nDesigned for:\n- Partial evaluation (lookahead) branching\n- Multi-way branching strategies\n- Complex branching objects that need customized subproblem generation\n\nPure virtual class - see CbcGeneralDepth for main implementation.", "see": ["CbcGeneralDepth for depth-limited partial evaluation", "CbcNWay for simpler N-way branching\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcHeuristicDiveGuided.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveGuided.hpp", "filename": "CbcHeuristicDiveGuided.hpp", "file": "CbcHeuristicDiveGuided.hpp", "brief": "Dive heuristic guided by incumbent solution\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveGuided: Uses existing incumbent to guide diving.\nRequires a feasible solution (canHeuristicRun checks this).\n\nselectVariableToBranch() selects fractional variables and\nrounds toward incumbent solution values. This focuses search\nin neighborhoods likely to contain good solutions.\n\nPart of the \"Guided Dive\" approach from Danna et al.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicRINS for related incumbent-guided search"], "has_pass2": false}, "src/CbcLinked.hpp": {"path": "layer-2/Cbc/src/CbcLinked.hpp", "filename": "CbcLinked.hpp", "file": "CbcLinked.hpp", "brief": "Extended solver for nonlinear and bilinear problems\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nThis file contains classes for handling nonlinear/bilinear optimization:\n\nOsiSolverLink: Extended OsiSolver that handles coefficient updates.\n- Nonlinear SLP (Successive Linear Programming)\n- Quadratic objective linearization\n- Bilinear term handling via lambda formulations\n\nOsiLinkedBound: Tracks bounds that depend on other bounds.\n- Coefficient updates when bounds change\n- Used for McCormick-style relaxations\n\nOsiBiLinear: Represents x*y bilinear terms.\n- Constructs 4-lambda SOS2 formulation\n- Handles mesh refinement for tighter bounds\n- Supports branching on either x or y\n\nOsiLink/OsiOldLink: Linked ordered sets for structured problems.\n- Models y and x*f(y) relationships\n- SOS-style branching with linked variables\n\nCbcHeuristicDynamic3: Heuristic for picking up feasible solutions.\nCglTemporary: Temporary cut storage (destroyed after first use).\nOsiSolverLinearizedQuadratic: Solver with linearized quadratic objective.", "see": ["CbcOsiSolver for base solver-model link", "OsiBranchingObject for branching interface"], "has_pass2": false}, "src/CbcCutGenerator.hpp": {"path": "layer-2/Cbc/src/CbcCutGenerator.hpp", "filename": "CbcCutGenerator.hpp", "file": "CbcCutGenerator.hpp", "brief": "Interface between Cbc and Cut Generation Library (CGL)\nCopyright (C) 2003, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCutGenerator: Wraps a CglCutGenerator with Cbc-specific controls.\nManages when and how cut generation is called during B&C search.\n\nKey control parameters:\n- whenCutGenerator_: Frequency (every N nodes, -100=disabled, -99=root only)\n- depthCutGenerator_: Depth-based triggering\n- switches_: Bit flags for various options (normal, atSolution, timing, etc.)\n\nStatistics tracked:\n- numberTimes_: Times generator was called\n- numberCuts_: Total cuts generated\n- numberCutsActive_: Cuts still binding at end\n- timeInCutGenerator_: CPU time spent\n\nSupports adaptive behavior:\n- Can be switched off if ineffective\n- Tracks cuts at root vs tree\n- Global cuts and Lagrangean relaxation support", "algorithm": "Root Node vs. Tree Strategy:\n  - Root: Aggressive cuts (high whenCutGenerator_, all generators active)\n  - Tree: Selective cuts (numberActiveCutsAtRoot_ guides which to keep)\n  - Deep tree: Often disable most generators (computational cost)\n  Trade-off: Stronger root LP → fewer nodes vs. more time per node.", "ref": ["Cornuejols, G. (2008). \"Valid Inequalities for Mixed Integer\n       Linear Programs\". Math. Programming 112(1):3-44."], "complexity": "generateCuts: O(generator_cost × frequency)\n  Generator costs vary: Clique O(n), Gomory O(n²), Probing O(n × LP_solve)\n  Overall B&C impact: cuts reduce nodes but add per-node overhead", "see": ["CglCutGenerator for cut generation algorithms", "CbcCutModifier for post-generation cut modification", "CbcModel::addCutGenerator()"], "has_pass2": true}, "src/CbcEventHandler.hpp": {"path": "layer-2/Cbc/src/CbcEventHandler.hpp", "filename": "CbcEventHandler.hpp", "file": "CbcEventHandler.hpp", "brief": "Event handling and callbacks for Cbc\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcEventHandler: Callback mechanism for monitoring and controlling B&C.\nProvides hooks at key points in the search process.\n\nEvents (CbcEvent enum):\n- node: After processing a node\n- solution: Integer solution found\n- heuristicSolution: Solution from heuristic\n- treeStatus: Periodic tree status report\n- endSearch: Search termination\n- afterRootCuts: Root node complete\n\nActions (CbcAction enum):\n- noAction: Continue normally\n- stop: Abort search\n- restart: Restart from current point\n- restartRoot: Restart from scratch\n- killSolution: Reject found solution\n\nUsage: Derive subclass, override event() method.\nDefault implementation uses event/action map (eaMap_).\n\nDesign matches ClpEventHandler for Clp compatibility.", "see": ["CbcModel::setEventHandler()", "ClpEventHandler for Clp equivalent"], "has_pass2": false}, "src/CbcSolverAnalyze.hpp": {"path": "layer-2/Cbc/src/CbcSolverAnalyze.hpp", "filename": "CbcSolverAnalyze.hpp", "file": "CbcSolverAnalyze.hpp", "brief": "Problem analysis for integer constraint detection\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nanalyze(): Examines model to detect integer structure.\n\nLooks for constraints where:\n- All variables are integer\n- All coefficients are integer\n- RHS is integer\n\nSuch constraints can be strengthened or exploited specially.\nIf changeInt is true, may change continuous variables to integer\nwhen they only appear in integer-coefficient constraints.\n\nReturns array of changed variable indices.\nSets numberChanged to count of modifications.\nSets increment to objective coefficient GCD for integer solutions.", "see": ["CbcModel for problem analysis during solve", "CoinPackedMatrix for constraint access"], "has_pass2": false}, "src/CbcPartialNodeInfo.hpp": {"path": "layer-2/Cbc/src/CbcPartialNodeInfo.hpp", "filename": "CbcPartialNodeInfo.hpp", "file": "CbcPartialNodeInfo.hpp", "brief": "Incremental subproblem storage as differences from parent\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcPartialNodeInfo stores subproblem state as changes from parent:\n- CoinWarmStartDiff for basis changes\n- Only modified variable bounds\n- New cuts added at this node\n\nMemory-efficient: Stores only differences, reconstructs via parent chain.\napplyToModel() walks ancestor chain to rebuild complete state.", "see": ["CbcNodeInfo for base class", "CbcFullNodeInfo for complete state storage", "CoinWarmStartDiff for basis difference representation\n\nEdwin 11/24/09 carved from CbcNode"], "has_pass2": false}, "src/CbcCutSubsetModifier.hpp": {"path": "layer-2/Cbc/src/CbcCutSubsetModifier.hpp", "filename": "CbcCutSubsetModifier.hpp", "file": "CbcCutSubsetModifier.hpp", "brief": "Cut modifier that filters cuts based on variable indices\nCopyright (C) 2003, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCutSubsetModifier: Filters cuts to exclude those involving certain variables.\nUsed in decomposition-style approaches where only a subset of variables\nshould appear in cuts.\n\nPrimary use case:\n- Remove cuts that reference variables >= firstOdd_\n- Useful when auxiliary variables shouldn't appear in cuts\n- Can weaken cuts instead of deleting (future enhancement)\n\nmodify() return values:\n- 0: Cut unchanged\n- 1: Cut strengthened\n- 2: Cut weakened\n- 3: Cut deleted (set to NULL)", "see": ["CbcCutModifier for base class", "CbcCutGenerator for cut generation"], "has_pass2": false}, "src/CbcSimpleIntegerDynamicPseudoCost.hpp": {"path": "layer-2/Cbc/src/CbcSimpleIntegerDynamicPseudoCost.hpp", "filename": "CbcSimpleIntegerDynamicPseudoCost.hpp", "file": "CbcSimpleIntegerDynamicPseudoCost.hpp", "brief": "Integer variable with dynamic (learning) pseudocosts\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSimpleIntegerDynamicPseudoCost: Implements reliability branching\nbased on Achterberg, Koch & Martin's work. Pseudocosts are learned\nfrom actual branching history rather than being static estimates.\n\nKey statistics tracked per variable:\n- sumUpCost_/sumDownCost_: Cumulative objective changes\n- numberTimesUp_/Down_: Branch count for averaging\n- numberBeforeTrust_: Initialization threshold before trusting estimates\n\nReliability branching concept:\n- Initially uses strong branching until numberBeforeTrust_ reached\n- Then relies on accumulated pseudocost estimates\n- Balances exploration (strong branching) vs exploitation (estimates)\n\nAlso includes CbcIntegerPseudoCostBranchingObject and optionally\nCbcSwitchingBinary for coupled binary/continuous variables.", "see": ["CbcSimpleIntegerPseudoCost for static pseudocost version", "CbcBranchDynamicDecision for decision making with dynamic costs\n\nEdwin 11/17/2009 carved out of CbcBranchDynamic"], "has_pass2": false}, "src/CbcSimpleIntegerPseudoCost.hpp": {"path": "layer-2/Cbc/src/CbcSimpleIntegerPseudoCost.hpp", "filename": "CbcSimpleIntegerPseudoCost.hpp", "file": "CbcSimpleIntegerPseudoCost.hpp", "brief": "Integer variable with static pseudocosts\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSimpleIntegerPseudoCost: Extends CbcSimpleInteger with static\npseudocost estimates for branch direction preference:\n- upPseudoCost_: Estimated objective increase per unit ceiling\n- downPseudoCost_: Estimated objective increase per unit floor\n\nUnlike CbcSimpleIntegerDynamicPseudoCost, these values are fixed\n(typically from problem structure or user-provided estimates).\n\nMethod modes for infeasibility calculation:\n- 0: Return min(up, down) - conservative\n- 1: Return max before any solution\n- 2: Return max before branched solution\n- 3: Always return max - aggressive", "see": ["CbcSimpleInteger for base class", "CbcSimpleIntegerDynamicPseudoCost for learning-based version\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcSubProblem.hpp": {"path": "layer-2/Cbc/src/CbcSubProblem.hpp", "filename": "CbcSubProblem.hpp", "file": "CbcSubProblem.hpp", "brief": "Compact subproblem state for diving heuristics\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSubProblem stores a lightweight subproblem state:\n- objectiveValue_, sumInfeasibilities_\n- Variable bound changes\n- Basis status changes\n\nUsed by diving heuristics to backtrack efficiently.\napply() restores state to solver (bounds, basis, or both).", "see": ["CbcHeuristicDive for diving heuristics using this", "CbcNodeInfo for full search tree node storage", "ClpNode for Clp-specific node representation\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcMipStartIO.hpp": {"path": "layer-2/Cbc/src/CbcMipStartIO.hpp", "filename": "CbcMipStartIO.hpp", "file": "CbcMipStartIO.hpp", "brief": "Read MIP starting solutions from files\n\nCbcMipStartIO: Utilities for reading partial or complete MIP solutions.\nWarm-starting B&C with a known solution can dramatically speed up solving.\n\nread(): Parses solution file in format: varname value\n- Returns 0 on success, 1 on failure\n- Fills colValues with (name, value) pairs\n- Reports objective value if available\n\ncomputeCompleteSolution(): Extends partial solution to all variables.\n- extraActions controls how unmentioned integers are set:\n  - 0: Default handling\n  - 1: Set to lower bound\n  - 2: Set to upper bound\n  - 3,5: Without costs as 1,2; with costs to cheapest\n  - 4,6: Without costs as 1,2; with costs to expensive\n\nUse case: Providing initial solutions from heuristics, previous runs,\nor domain knowledge to accelerate MIP solving.", "see": ["CbcModel::setBestSolution for using solutions", "CbcHeuristic for algorithmic solution generation"], "has_pass2": false}, "src/CbcDummyBranchingObject.hpp": {"path": "layer-2/Cbc/src/CbcDummyBranchingObject.hpp", "filename": "CbcDummyBranchingObject.hpp", "file": "CbcDummyBranchingObject.hpp", "brief": "No-op branching object for special cases\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcDummyBranchingObject: A placeholder branch that does nothing.\nUsed when the branching framework requires a branching object\nbut no actual branching action is needed.\n\nUse cases:\n- Continue tree exploration when LP appears feasible but isn't\n- Placeholder for deferred branching decisions\n- Testing and debugging the branching infrastructure\n\nThe branch() method returns immediately without modifying bounds.", "see": ["CbcBranchingObject for base class\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcHeuristicDiveVectorLength.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveVectorLength.hpp", "filename": "CbcHeuristicDiveVectorLength.hpp", "file": "CbcHeuristicDiveVectorLength.hpp", "brief": "Dive heuristic based on constraint participation\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveVectorLength: Selects variables by column density.\nVariables appearing in many constraints are fixed first.\n\nselectVariableToBranch() prioritizes variables with longer\ncolumns (more nonzeros in constraint matrix). Fixing high-density\nvariables propagates more and reveals feasibility faster.\n\nUses matrix_ from CbcHeuristicDive base class for column lengths.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicDiveFractional for simpler selection"], "has_pass2": false}, "src/CbcCompareDepth.hpp": {"path": "layer-2/Cbc/src/CbcCompareDepth.hpp", "filename": "CbcCompareDepth.hpp", "file": "CbcCompareDepth.hpp", "brief": "Depth-first node selection strategy\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareDepth: Implements depth-first search (DFS).\nDefault strategy before first solution is found.\n\ntest(x,y) returns true if y is deeper than x in the tree.\nDeepest nodes explored first -> LIFO stack behavior.\n\nAdvantages:\n- Low memory (linear in depth)\n- Fast to first feasible solution\n- Good for proving feasibility\n\nDisadvantages:\n- May miss better solutions at other branches\n- Weak objective bounds early in search", "see": ["CbcCompareObjective for best-bound search", "CbcCompareDefault for hybrid approach\n\nEdwin 11/24/09 carved out of CbcCompareActual"], "has_pass2": false}, "src/CbcBranchingObject.hpp": {"path": "layer-2/Cbc/src/CbcBranchingObject.hpp", "filename": "CbcBranchingObject.hpp", "file": "CbcBranchingObject.hpp", "brief": "Abstract base for branching actions\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchingObject describes HOW to branch (the action):\n- branch(): Execute next branch arm, modify solver bounds\n- numberBranches(): Total arms (typically 2)\n- way(): Current branch direction (+1 up, -1 down)\n\nCbcBranchObjType enum identifies concrete types:\nSimpleIntegerBranchObj, CliqueBranchObj, SoSBranchObj,\nNWayBranchObj, CutBranchingObj, etc.\n\nExtends OsiBranchingObject with CBC-specific features.", "see": ["CbcObject for what creates branching objects", "CbcBranchDecision for choosing between objects", "CbcIntegerBranchingObject for integer variable branching\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": false}, "src/CbcNWay.hpp": {"path": "layer-2/Cbc/src/CbcNWay.hpp", "filename": "CbcNWay.hpp", "file": "CbcNWay.hpp", "brief": "N-way branching (exactly one variable at upper bound)\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcNWay: Multi-way (N-ary) branching for selection constraints.\nExactly one variable at upper bound, all others at lower bound.\nCreates N children, each fixing one variable to its UB.\n\nDiffers from binary branching:\n- Traditional: 2 children (x <= floor vs x >= ceil)\n- N-way: N children, each selecting a different option\n\nSupports CbcConsequence for each member - when a variable\nis selected (fixed to UB), additional bound changes can be applied.\n\nExample: Facility location with 5 candidate sites.\nN-way branch creates 5 children, each opening a different site.", "see": ["CbcNWayBranchingObject for branching action", "CbcConsequence for side effects", "CbcClique for clique-based (2-way) branching\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcHeuristicPivotAndFix.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicPivotAndFix.hpp", "filename": "CbcHeuristicPivotAndFix.hpp", "file": "CbcHeuristicPivotAndFix.hpp", "brief": "Pivot and Fix heuristic using simplex pivots\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicPivotAndFix: Exploits LP basis structure.\nPerforms simplex pivots to explore nearby basic solutions,\nthen fixes integer variables at their current values.\n\nUses the LP optimal basis and pivots to find integer-feasible\nsolutions nearby in the simplex sense.", "see": ["CbcHeuristic for base class", "CbcHeuristicRandRound for randomized rounding"], "has_pass2": false}, "src/CbcMessage.hpp": {"path": "layer-2/Cbc/src/CbcMessage.hpp", "filename": "CbcMessage.hpp", "file": "CbcMessage.hpp", "brief": "Message IDs and handler for Cbc output\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcMessage: Defines message IDs (CBC_Message enum) for Cbc logging.\nUses COIN message handling framework (CoinMessageHandler).\n\nKey message categories:\n- CBC_SOLUTION: Integer solution found\n- CBC_STATUS/CBC_STATUS2/CBC_STATUS3: Progress reports\n- CBC_GAP: Optimality gap information\n- CBC_ROOT: Root node summary\n- CBC_GENERATOR: Cut generator statistics\n- CBC_BRANCH: Branching decisions\n- CBC_THREAD_STATS: Parallel statistics\n\nText in CbcMessage.cpp supports multiple languages (default: us_en).", "see": ["CoinMessageHandler for message handling infrastructure", "CbcGenMessages.hpp for command-line solver messages"], "has_pass2": false}, "src/CbcHeuristicRandRound.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicRandRound.hpp", "filename": "CbcHeuristicRandRound.hpp", "file": "CbcHeuristicRandRound.hpp", "brief": "Randomized rounding heuristic\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicRandRound: Probabilistic rounding of LP solution.\nRounds fractional variables randomly with probabilities based on\ntheir fractional values (e.g., x=0.7 rounds up with prob 0.7).\n\nMultiple trials with different random seeds can find diverse solutions.\nSimple but can be surprisingly effective for certain problem structures.", "see": ["CbcHeuristic for base class", "CbcHeuristicFPump for deterministic rounding approach"], "has_pass2": false}, "src/CbcModel.hpp": {"path": "layer-2/Cbc/src/CbcModel.hpp", "filename": "CbcModel.hpp", "file": "CbcModel.hpp", "brief": "Main branch-and-cut MIP solver class\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcModel is the central class for COIN-OR branch-and-cut MIP solving.\nKey methods:\n- initialSolve(): Solve LP relaxation\n- branchAndBound(): Run B&C algorithm to optimality\n\nArchitecture:\n- CbcNode/CbcNodeInfo: Subproblem representation in search tree\n- CbcTree: Priority queue of live nodes (heap)\n- CbcCutGenerator: Wrapper for CGL cut generators\n- CbcHeuristic: Primal heuristics for finding solutions\n- CbcBranchingObject: Branching decisions", "algorithm": "Branch-and-Cut (B&C) for Mixed-Integer Programming:\n  1. Solve LP relaxation at root node\n  2. While open nodes remain:\n     a. Select node from priority queue (best-first or depth-first)\n     b. Solve LP relaxation, apply cuts (Gomory, MIR, clique, etc.)\n     c. If fractional: branch on integer variable, create child nodes\n     d. If integer-feasible: update incumbent if improved\n     e. Prune by bound if LP ≥ incumbent\n  3. Return optimal solution when tree exhausted", "complexity": "Worst-case O(2^n) where n = number of integer variables.\n  Practical performance depends heavily on:\n  - Strength of LP relaxation and cutting planes\n  - Quality of branching variable selection\n  - Effectiveness of primal heuristics\n  - Problem structure (e.g., total unimodularity)", "ref": ["Land, A.H. and Doig, A.G. (1960). \"An automatic method of solving\n  discrete programming problems\". Econometrica 28(3):497-520.\n  [Original branch-and-bound algorithm]", "Padberg, M. and Rinaldi, G. (1991). \"A branch-and-cut algorithm\n  for the resolution of large-scale symmetric traveling salesman problems\".\n  SIAM Review 33(1):60-100. [Branch-and-cut methodology]", "Savelsbergh, M.W.P. (1994). \"Preprocessing and probing techniques\n        for mixed integer programming problems\". ORSA J. Computing 6(4):445-454.\n\n      \\todo It remains to work out the cleanest way of getting a solution to\n            the original problem at the end. So this is very preliminary."], "see": ["CbcNode for search tree node representation", "CbcTree for node selection/storage", "CbcCutGenerator for cutting plane management", "CbcHeuristic for primal heuristics", "OsiSolverInterface for underlying LP solver"], "has_pass2": true}, "src/CbcFullNodeInfo.hpp": {"path": "layer-2/Cbc/src/CbcFullNodeInfo.hpp", "filename": "CbcFullNodeInfo.hpp", "file": "CbcFullNodeInfo.hpp", "brief": "Complete subproblem state storage (typically for root node)\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFullNodeInfo stores complete information to recreate a subproblem:\n- Full warm start basis\n- Complete variable bounds\n- All active cutting planes\n\nUsed at root node or when incremental storage isn't beneficial.\nChild nodes typically use CbcPartialNodeInfo to save memory.", "see": ["CbcNodeInfo for base class", "CbcPartialNodeInfo for incremental storage", "CbcNode for live node information\n\nEdwin 11/24/09 carved from CbcNode"], "has_pass2": false}, "src/CbcParamUtils.hpp": {"path": "layer-2/Cbc/src/CbcParamUtils.hpp", "filename": "CbcParamUtils.hpp", "file": "CbcParamUtils.hpp", "brief": "Utility functions for parameter handling in cbc-generic\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcParamUtils namespace: Callback functions for parameter processing.\nThese are invoked when parameters are set via the command line.\n\nSolution I/O:\n- saveSolution(): Write solution to file\n- readSolution(): Load solution from file\n\nAction handlers (doXxxParam):\n- doBaCParam(): Execute branch-and-cut solve\n- doImportParam(): Load problem from file\n- doSolutionParam(): Write/print solution\n- doHelpParam(): Display help information\n- doExitParam(): Exit the program\n\nPush functions (pushCbcSolverXxxParam):\n- Transfer parameter values to solver/model\n- Separate handlers for Dbl, Int, Kwd, Str, Bool types\n- Special handlers for Cut and Heur parameters\n\nModel parameter handlers:\n- pushCbcModelDblParam/IntParam(): Set CbcModel parameters\n- setCbcModelDefaults(): Initialize model with default values", "see": ["CbcParam for parameter definitions", "CbcParameters for parameter storage"], "has_pass2": false}, "src/CbcTreeLocal.hpp": {"path": "layer-2/Cbc/src/CbcTreeLocal.hpp", "filename": "CbcTreeLocal.hpp", "file": "CbcTreeLocal.hpp", "brief": "Local branching search tree (Fischetti-Lodi 2002)\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nImplements local branching neighborhood search:\n1. Start with feasible solution\n2. Add cut: sum |x_i - x*_i| <= k (Hamming distance)\n3. Search k-neighborhood for improvements\n4. If optimal in neighborhood, reverse cut and widen (k+1)\n5. Repeat with new solution center\n\nParameters:\n- range: Initial neighborhood radius k\n- typeCuts: 0 = 0-1 only, 1 = general integer\n- maxDiversification: Max radius increases\n- timeLimit/nodeLimit: Sub-tree limits", "see": ["CbcTree for base tree class", "CbcHeuristicLocal for related local search heuristic", "\"Local Branching\" Fischetti & Lodi, Math. Prog. 2003"], "has_pass2": false}, "src/CbcBranchLotsize.hpp": {"path": "layer-2/Cbc/src/CbcBranchLotsize.hpp", "filename": "CbcBranchLotsize.hpp", "file": "CbcBranchLotsize.hpp", "brief": "Lot-sizing variable with discrete valid values\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcLotsize: Variable restricted to specific discrete values (lot sizes).\nUnlike integers (any value in range), lot-sizing variables can only\ntake values from a predefined set: {v1, v2, ..., vn}.\n\nTwo modes:\n- Points mode (rangeType_=1): Discrete set of valid values\n- Range mode (rangeType_=2): Valid intervals [lo_i, hi_i]\n\nExample: Order quantity must be 0, 100, 250, or 500 units.\n\nBranching creates children that partition the valid value set,\nsimilar to SOS branching but for a single variable.", "see": ["CbcLotsizeBranchingObject for branching action", "CbcSOS for set-based branching on multiple variables"], "has_pass2": false}, "src/CbcHeuristicDivePseudoCost.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDivePseudoCost.hpp", "filename": "CbcHeuristicDivePseudoCost.hpp", "file": "CbcHeuristicDivePseudoCost.hpp", "brief": "Dive heuristic using pseudocost estimates\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDivePseudoCost: Most informed diving strategy.\nUses pseudocosts to estimate objective change from fixing.\n\nselectVariableToBranch() picks variables based on:\n  score = (1-f)*down_pseudocost + f*up_pseudocost\nwhere f is the fractional part. Chooses direction with\nsmaller estimated degradation.\n\ninitializeData() prepares pseudocost arrays (downArray_, upArray_).\nfixOtherVariables() uses reduced costs for additional fixing.\n\nGenerally most effective but requires pseudocost information.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcSimpleIntegerDynamicPseudoCost for pseudocost learning"], "has_pass2": false}, "src/CbcFollowOn.hpp": {"path": "layer-2/Cbc/src/CbcFollowOn.hpp", "filename": "CbcFollowOn.hpp", "file": "CbcFollowOn.hpp", "brief": "Follow-on branching for crew scheduling problems\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFollowOn: Specialized branching for air-crew scheduling and\nsimilar set-partitioning problems. When crew can fly in on flight A\nand out on flight B (or other flights), branch on the connection.\n\nBranch disjunction:\n- Down: Fix all pairings using connection A->B to 0\n- Up: Fix all pairings NOT using A->B to 0\n\nAlso includes:\n- CbcFixingBranchingObject: Fix lists of variables to bounds\n- CbcIdiotBranch: Experimental random-sum branching\n\nShould be used as a supplementary branching rule with high priority,\nbefore standard variable branching.", "see": ["CbcFixingBranchingObject for bulk variable fixing", "CbcBranchCut for related cut-based branching\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcObjectUpdateData.hpp": {"path": "layer-2/Cbc/src/CbcObjectUpdateData.hpp", "filename": "CbcObjectUpdateData.hpp", "file": "CbcObjectUpdateData.hpp", "brief": "Data carrier for updating branching objects after branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcObjectUpdateData: Encapsulates information about a completed branch\nto update the originating CbcObject. Used primarily for learning\ndynamic pseudocosts from actual branching outcomes.\n\nCaptured data:\n- change_: Actual objective change from this branch\n- status_: LP outcome (0=optimal, 1=infeasible, 2=unknown)\n- intDecrease_: Reduction in integer infeasibilities\n- way_: Branch direction taken (-1 down, +1 up)\n\nFlow: After solving a child node, CbcNode creates this object\nand calls object_->updateInformation() to feed back the results.", "see": ["CbcSimpleIntegerDynamicPseudoCost for main consumer", "CbcObject::createUpdateInformation() for creation", "CbcObject::updateInformation() for consumption\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": false}, "src/CbcBranchDynamic.hpp": {"path": "layer-2/Cbc/src/CbcBranchDynamic.hpp", "filename": "CbcBranchDynamic.hpp", "file": "CbcBranchDynamic.hpp", "brief": "Dynamic pseudocost-based branching decision\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchDynamicDecision: Selects branches using dynamic pseudocosts.\n- Before first solution: Uses infeasibility counts\n- After first solution: Uses objective change estimates\n\nPseudocosts are updated during search based on observed\nobjective changes from actual branching decisions.", "see": ["CbcBranchDecision for base class", "CbcSimpleIntegerDynamicPseudoCost for dynamic pseudocost objects", "CbcBranchDefaultDecision for simpler selection"], "has_pass2": false}, "src/CbcCountRowCut.hpp": {"path": "layer-2/Cbc/src/CbcCountRowCut.hpp", "filename": "CbcCountRowCut.hpp", "file": "CbcCountRowCut.hpp", "brief": "Reference-counted row cuts with ownership tracking\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCountRowCut: OsiRowCut with reference counting for memory management.\nTracks which nodes use the cut and auto-deletes when count reaches zero.\n\nBookkeeping:\n- numberPointingToThis_: Reference count\n- owner_/ownerCut_: Back-pointer to creating node\n- whichCutGenerator_: ID of generator (+10000 if global)\n\nUsage pattern:\n- Increment when child nodes inherit the cut\n- Decrement when cut becomes loose or node is fathomed\n- Delete when count reaches zero\n\nCbcRowCuts: Collection class for cuts with hash-based deduplication.\nPrevents duplicate cuts from being added multiple times.", "see": ["CbcNodeInfo for cut ownership", "CbcCutGenerator for cut generation"], "has_pass2": false}, "src/CbcNode.hpp": {"path": "layer-2/Cbc/src/CbcNode.hpp", "filename": "CbcNode.hpp", "file": "CbcNode.hpp", "brief": "Search tree node for branch-and-cut\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcNode represents a live subproblem in the B&C search tree.\nContains information needed while the node is active:\n- Depth in tree, objective value, branching state\n- Links to CbcNodeInfo for reconstruction\n\nLifecycle: Created when branching, deleted when all branches evaluated.\nExtends CoinTreeNode for heap-based storage in CbcTree.\n\nKey methods:\n- createInfo(): Create CbcNodeInfo for subproblem storage\n- branch(): Apply branching and create child subproblem\n- chooseBranch(): Select branching variable/object", "see": ["CbcNodeInfo for persistent subproblem information", "CbcTree for node storage and selection", "CbcBranchingObject for branching decisions"], "has_pass2": false}, "src/CbcHeuristicFPump.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicFPump.hpp", "filename": "CbcHeuristicFPump.hpp", "file": "CbcHeuristicFPump.hpp", "brief": "Feasibility Pump heuristic (Fischetti, Glover & Lodi)\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicFPump: Implements the Feasibility Pump algorithm\n(Fischetti, Glover & Lodi, 2005) for finding initial MIP solutions.\n\nAlgorithm overview:\n1. Solve LP relaxation, get fractional x*\n2. Round x* to nearest integers -> x_bar\n3. Solve LP with objective: minimize distance to x_bar\n4. If new x* = x_bar, done (integer feasible)\n5. Otherwise round and repeat\n\nKey parameters:\n- maximumPasses_: Max iterations before giving up\n- initialWeight_: Blend original objective with distance\n- defaultRounding_: Threshold for rounding (default 0.5)\n\nHandles cycling through randomization and perturbation.\n\nAlso includes CbcDisasterHandler for Clp solver recovery.", "see": ["CbcHeuristic for base class", "CbcHeuristicDive for related diving approaches"], "has_pass2": false}, "src/CbcBranchToFixLots.hpp": {"path": "layer-2/Cbc/src/CbcBranchToFixLots.hpp", "filename": "CbcBranchToFixLots.hpp", "file": "CbcBranchToFixLots.hpp", "brief": "Branch to fix many variables simultaneously\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchToFixLots: Heuristic branching that fixes multiple variables\nin one branch, cutting off the current solution in the other. Useful\nfor reducing problem size when reduced costs indicate fixable variables.\n\nTriggering conditions:\n- Variables at bounds with large reduced costs (dj >= djTolerance_)\n- Sufficient fraction of variables fixable (fractionFixed_)\n- Clean satisfaction of equality/packing constraints (numberClean_)\n\nOne branch arm fixes variables based on reduced cost directions;\nthe other adds a cut excluding the current fractional solution.\n\nControlled by depth_ to avoid excessive use at every node.", "see": ["CbcBranchCut for base class", "CbcBranchAllDifferent for another cut-based branching\n\nEdwin 11/13/2009 carved out of CbcBranchCut"], "has_pass2": false}, "src/CbcBranchAllDifferent.hpp": {"path": "layer-2/Cbc/src/CbcBranchAllDifferent.hpp", "filename": "CbcBranchAllDifferent.hpp", "file": "CbcBranchAllDifferent.hpp", "brief": "All-different constraint for integer variables\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchAllDifferent: Enforces that a set of integer variables\nmust all have different values. When two variables i,j have the\nsame value, creates branching disjunction:\n  x_i <= x_j - 1  OR  x_i >= x_j + 1\n\nCommon in constraint programming problems:\n- Scheduling: No two tasks at same time\n- Assignment: No two agents assigned same job\n- Sudoku: No repeated digits in row/column/box\n\nNot as efficient as specialized CP propagation but integrates\nwith MIP framework for hybrid approaches.", "see": ["CbcBranchCut for base class", "CbcClique for related set constraints\n\nEdwin 11/13/2009 carved out of CbcBranchCut"], "has_pass2": false}, "src/CbcHeuristicRINS.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicRINS.hpp", "filename": "CbcHeuristicRINS.hpp", "file": "CbcHeuristicRINS.hpp", "brief": "RINS - Relaxation Induced Neighborhood Search (Danna et al.)\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicRINS: Implements RINS (Danna, Rothberg & Le Pape, 2005).\nUses LP relaxation to define a neighborhood around the incumbent solution.\n\nAlgorithm:\n1. Compare current LP solution with best known integer solution\n2. Fix variables where LP and incumbent agree\n3. Solve restricted MIP on remaining variables\n4. Accept if improved, update incumbent\n\nTracks which variables have appeared in solutions (used_ array)\nto focus search on promising variables.\n\nAlso includes (via headers) related neighborhood searches:\n- RENS: Relaxation Enforced Neighborhood Search\n- DINS: Distance-Induced Neighborhood Search\n- VND: Variable Neighborhood Descent", "see": ["CbcHeuristic for base class", "CbcHeuristicRENS, CbcHeuristicDINS, CbcHeuristicVND", "CbcHeuristicLocal for pure local search"], "has_pass2": false}, "src/CbcCompareEstimate.hpp": {"path": "layer-2/Cbc/src/CbcCompareEstimate.hpp", "filename": "CbcCompareEstimate.hpp", "file": "CbcCompareEstimate.hpp", "brief": "Estimate-based node selection strategy\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareEstimate: Node selection using solution estimates.\nUsed during rounding phases where estimated objective matters.\n\ntest(x,y) compares node estimates (guessedObjectiveValue).\nEstimates typically come from pseudocost extrapolation or\nother predictive methods.\n\nUseful when LP relaxation bound is weak but estimates\nfrom branching information are more informative.", "see": ["CbcCompareObjective for LP bound comparison", "CbcCompareDefault for adaptive strategy", "CbcNode::guessedObjectiveValue() for estimate source\n\nEdwin 11/25/09 carved out of CbcCompareActual"], "has_pass2": false}, "src/CbcFixVariable.hpp": {"path": "layer-2/Cbc/src/CbcFixVariable.hpp", "filename": "CbcFixVariable.hpp", "file": "CbcFixVariable.hpp", "brief": "Fix variable bounds as branching consequence\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFixVariable: Concrete CbcConsequence that fixes variable bounds\nbased on branching decisions. Implements state-dependent bound changes\nthat go beyond LP propagation.\n\nState encoding:\n- -9999: Variable at lower bound\n- +9999: Variable at upper bound\n- Other: Variable fixed to that specific value\n\nFor each state, stores arrays of:\n- Variables to modify (variable_)\n- New bounds to apply (newBound_)\n- Ranges for lower vs upper bound changes (startLower_, startUpper_)\n\nExample: Binary y controls continuous x. When y=0 (state -9999),\nfix x to 0; when y=1 (state +9999), x can be in [0, 100].", "see": ["CbcConsequence for base class", "CbcObject for objects with consequences\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcHeuristicDW.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDW.hpp", "filename": "CbcHeuristicDW.hpp", "file": "CbcHeuristicDW.hpp", "brief": "Dantzig-Wolfe decomposition based heuristic\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDW: Advanced heuristic exploiting block structure.\nVery compute-intensive - detects and exploits Dantzig-Wolfe\ndecomposable structure in the constraint matrix.\n\nAlgorithm:\n1. findStructure() detects block-angular structure\n2. setupDWStructures() prepares decomposition data\n3. Solves subproblems by block (numberBlocks_)\n4. Combines block solutions via master problem\n5. Callback mechanism for customization (functionPointer_)\n\nKey data structures:\n- whichRowBlock_/whichColumnBlock_: Block membership\n- fingerPrint_: Bitmask for integer patterns per block\n- affinity_: Block similarity for combination\n- dwSolver_: DW master problem solver\n\nParameters controllable via setters:\n- nNeeded_/nNodes_: Search intensity\n- numberPasses_/numberBadPasses_: Iteration limits", "see": ["CbcHeuristic for base class", "CbcHeuristicRINS for simpler neighborhood search"], "has_pass2": false}, "src/CbcHeuristicDive.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDive.hpp", "filename": "CbcHeuristicDive.hpp", "file": "CbcHeuristicDive.hpp", "brief": "Abstract base for diving heuristics\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDive: Abstract base for diving heuristics that explore\nthe tree greedily by fixing variables and re-solving LPs.\n\nDiving process:\n1. Solve LP relaxation\n2. Select a fractional variable via selectVariableToBranch()\n3. Fix variable in direction indicated by bestRound\n4. Re-solve LP, repeat until integer feasible or infeasible\n5. If infeasible, backtrack (limited depth) or fail\n\nSubclasses implement selectVariableToBranch() with different strategies:\n- Fractional: Most fractional variable\n- PseudoCost: Best pseudocost estimate\n- VectorLength: Based on constraint participation\n- Coefficient: Based on objective coefficients\n- Guided: Guided by incumbent solution\n- LineSearch: Along line to LP optimum\n\nUses lock counting for feasibility analysis.", "see": ["CbcHeuristicDiveFractional, CbcHeuristicDivePseudoCost, etc.", "CbcHeuristic for base class", "CbcHeuristicFPump for related approach"], "has_pass2": false}, "src/CbcClique.hpp": {"path": "layer-2/Cbc/src/CbcClique.hpp", "filename": "CbcClique.hpp", "file": "CbcClique.hpp", "brief": "Clique branching for binary variable sets\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcClique: Branching for cliques (sets of binary variables with\nat most one at its \"strong\" value). Generalizes binary SOS1.\n\nStandard form: x1 + x2 + ... + xn <= 1 (all strong at 1)\nGeneral form allows negated variables (y_j = 1 - x_j).\n\nMember types (type_[i]):\n- 1: SOS-style, coefficient +1, strong value is 1\n- 0: Non-SOS, coefficient -1, strong value is 0\n\nBranching: Pick a fractional variable and partition the set.\nUses bitmasks for efficient subset representation (up to 64 members\nwith CbcCliqueBranchingObject, unlimited with CbcLongCliqueBranchingObject).", "see": ["CbcCliqueBranchingObject for sets <= 64 members", "CbcLongCliqueBranchingObject for larger sets", "CbcSOS for ordered set branching\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcHeuristicRENS.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicRENS.hpp", "filename": "CbcHeuristicRENS.hpp", "file": "CbcHeuristicRENS.hpp", "brief": "RENS - Relaxation Enforced Neighborhood Search\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicRENS: Fixes variables based on LP relaxation solution.\nUnlike RINS (which needs an incumbent), RENS works from LP alone.\n\nAlgorithm:\n1. Solve LP relaxation\n2. Fix integer variables that are at bounds in LP\n3. Solve restricted MIP on remaining free variables\n\nrensType_ controls fixing strategy:\n- 0: Fix at lower bound only\n- 1: Fix based on reduced costs\n- 2: Fix at upper bound as well\n- 3: Fix based on 0.01*average reduced cost\n- +16: Allow two tries\n- +32: Use existing solution to keep more variables\n- +64: Honor high priority variables\n- +128: Honor low priority variables", "see": ["CbcHeuristicRINS for incumbent-based neighborhood", "CbcHeuristic for base class\n\nEdwin 12/5/09 carved out of CbcHeuristicRINS"], "has_pass2": false}, "src/CbcTree.hpp": {"path": "layer-2/Cbc/src/CbcTree.hpp", "filename": "CbcTree.hpp", "file": "CbcTree.hpp", "brief": "Heap-based storage for live search tree nodes\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcTree manages the set of live (unexplored) nodes as a priority heap.\nThe comparison function (CbcCompare) determines node selection strategy:\n- Best-first: Select node with best bound\n- Depth-first: Select deepest node\n- Best-estimate: Use pseudocost estimates\n\nKey methods:\n- push(): Add node to heap\n- top()/pop(): Get and remove best node\n- setComparison(): Change selection strategy (re-heapifies)\n- cleanTree(): Remove nodes worse than cutoff", "see": ["CbcCompareBase for comparison function interface", "CbcNode for node representation", "CbcTreeLocal for local search tree variant"], "has_pass2": false}, "src/CbcParameters.hpp": {"path": "layer-2/Cbc/src/CbcParameters.hpp", "filename": "CbcParameters.hpp", "file": "CbcParameters.hpp", "brief": "Central parameter collection for Cbc algorithm control\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcParameters: Master class holding all Cbc solver parameters.\nUsed by cbc-generic to configure and control the MIP solver.\n\nMajor subsystems configured:\n- Cut generators: Gomory, MIR, Probing, Clique, FlowCover, etc.\n- Heuristics: FPump, RINS, RENS, DINS, Diving variants, VND, etc.\n- Search strategy: Node selection, preprocessing, orbital branching\n- Limits: Time, nodes, gap tolerance, solution count\n\nIncludes instances of CGL cut generators and Cbc heuristics.\nLinks to ClpParameters for underlying LP solver control.\n\nKey methods:\n- init(): Initialize with strategy preset\n- setModel(): Associate with CbcModel\n- Various getters/setters for each parameter category\n\nStrategy presets provide common configurations:\n- Default aggressive cutting at root\n- Balanced cut/heuristic emphasis\n- Heuristic-focused for quick solutions", "see": ["CbcParam for individual parameter definitions", "CbcParamUtils for parameter push/pull callbacks", "CbcModel for solver integration"], "algorithm": "Preprocessing Pipeline (IPPMode):\n    Controls integer preprocessing before B&C:\n    - Probing: fix variables by logical implication\n    - Clique detection: identify mutual exclusion constraints\n    - Coefficient strengthening: tighten constraint coefficients\n    - SOS extraction: identify special ordered sets from structure", "ref": ["Margot, F. (2002). \"Pruning by isomorphism in branch-and-cut\".\n    Math. Programming 94:71-90. (Orbital branching foundations)"], "has_pass2": true}, "src/CbcThread.hpp": {"path": "layer-2/Cbc/src/CbcThread.hpp", "filename": "CbcThread.hpp", "file": "CbcThread.hpp", "brief": "Multi-threaded parallel B&C support\nCopyright (C) 2009, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nParallel execution modes:\n- Opportunistic: Threads process nodes as available (nondeterministic)\n- Deterministic: Controlled node batching for reproducibility\n- Parallel cuts: Multiple cut generators run concurrently\n\nCbcSpecificThread: Low-level thread API wrapper (currently pthreads).\n- Mutex locking for thread synchronization\n- Condition variables for thread signaling\n- Thread lifecycle management\n\nCbcThread: Per-thread state and statistics.\n- Node being processed (node_) and created child (createdNode_)\n- Timing: timeInThread_, timeWaitingToStart_, timeLocked_\n- Return codes: -1 available, 0 busy, 1 finished\n\nCbcBaseModel: Coordinates multiple worker threads.\n- Manages thread pool for tree exploration\n- Handles deterministic vs opportunistic parallelism\n- Collects statistics across threads\n\nIf CBC_THREAD not defined, provides dummy stub classes.", "see": ["CbcModel for single-threaded B&C", "CbcTree for node management"], "has_pass2": false}, "src/CbcSolver.hpp": {"path": "layer-2/Cbc/src/CbcSolver.hpp", "filename": "CbcSolver.hpp", "file": "CbcSolver.hpp", "brief": "Top-level driver class for standalone CBC solver\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSolver wraps CbcModel with command-line parsing and scripting:\n- solve(): Execute command sequence (argc/argv interface)\n- fillParameters(): Centralized parameter initialization\n- Supports CbcUser callbacks for extensibility\n- Supports CbcStopNow for termination control\n\nNote: This class is designed to wrap CbcMain0/CbcMain1 functionality.\nFor direct API usage, prefer CbcModel directly.", "see": ["CbcModel for the core B&C solver", "CbcParameters for parameter definitions", "Cbc_C_Interface.h for C language bindings"], "has_pass2": false}, "src/CbcSimpleInteger.hpp": {"path": "layer-2/Cbc/src/CbcSimpleInteger.hpp", "filename": "CbcSimpleInteger.hpp", "file": "CbcSimpleInteger.hpp", "brief": "Integer variable branching object\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcIntegerBranchingObject: Standard floor/ceiling branch on integer x:\n- Down arm: lb <= x <= floor(x*)\n- Up arm: ceil(x*) <= x <= ub\n\nCbcSimpleInteger: CbcObject for integer variables.\nCreates CbcIntegerBranchingObject instances.\n\nAlso supports one-way branching for fixing variables.", "see": ["CbcObject for base class", "CbcSimpleIntegerPseudoCost for pseudocost variant", "CbcSimpleIntegerDynamicPseudoCost for dynamic pseudocosts", "CbcBranchingObject for branching action base\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcStrategy.hpp": {"path": "layer-2/Cbc/src/CbcStrategy.hpp", "filename": "CbcStrategy.hpp", "file": "CbcStrategy.hpp", "brief": "Strategy pattern for configuring CbcModel components\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcStrategy provides a strategy pattern for bundling solver configuration.\nSubclasses define complete solving strategies by implementing:\n- setupCutGenerators(): Configure cutting plane generators\n- setupHeuristics(): Configure primal heuristics\n- setupPrinting(): Configure output/logging\n- setupOther(): Strong branching, preprocessing, etc.\n\nSupports nested models (depth_) and preprocessing state tracking.", "see": ["CbcStrategyDefault for default strategy", "CbcModel::setStrategy() to apply a strategy", "CglPreProcess for preprocessing integration"], "has_pass2": false}, "src/CbcBranchCut.hpp": {"path": "layer-2/Cbc/src/CbcBranchCut.hpp", "filename": "CbcBranchCut.hpp", "file": "CbcBranchCut.hpp", "brief": "Branching by adding cuts (split disjunctions)\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchCut and CbcCutBranchingObject: Branch by adding cuts\nrather than tightening variable bounds. Implements split disjunctions\nwhere each branch arm adds a different cut to the LP.\n\nUse cases:\n- Gomory cuts can be viewed as branching on row slack integrality\n- Split cuts: ax <= b OR ax >= b+1 for integer a, fractional b\n- General disjunctive programming branches\n\nCbcCutBranchingObject stores two OsiRowCut objects:\n- down_: Cut added when way_ = -1\n- up_: Cut added when way_ = 1\n\nBase class for CbcBranchToFixLots and CbcBranchAllDifferent.", "see": ["CbcBranchToFixLots for reduced-cost-based fixing", "CbcBranchAllDifferent for all-different constraint", "OsiRowCut for cut representation"], "has_pass2": false}, "src/CbcParam.hpp": {"path": "layer-2/Cbc/src/CbcParam.hpp", "filename": "CbcParam.hpp", "file": "CbcParam.hpp", "brief": "Individual parameter definitions for cbc-generic\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcParam: Extends CoinParam with Cbc-specific parameter codes.\nUsed by the standalone cbc-generic solver for command-line processing.\n\nParameter categories (CbcParamCode enum):\n- Help: GENERALQUERY, HELP\n- Action: BAB, IMPORT, EXPORT, PRINTSOL, READMIPSTART\n- File: IMPORTFILE, EXPORTFILE, SOLUTIONFILE, MIPSTARTFILE\n- Cut: GOMORYCUTS, MIRCUTS, PROBINGCUTS, KNAPSACKCUTS, etc.\n- Heuristic: FPUMP, RINS, RENS, DIVINGC, GREEDY, etc.\n- Boolean: CPX, DOHEURISTIC, SOS, USESOLUTION\n- Keyword: STRATEGY, PREPROCESS, NODESTRATEGY, ORBITAL\n- Integer: LOGLEVEL, THREADS, CUTDEPTH, MAXNODES, etc.\n- Double: CUTOFF, ALLOWABLEGAP, TIMELIMIT, INCREMENT, etc.\n\nEach parameter links to:\n- parameters_: Parent CbcParameters collection\n- model_: Associated CbcModel (for model parameters)", "see": ["CoinParam for base parameter class", "CbcParameters for parameter collection", "CbcParamUtils for push/pull functions"], "has_pass2": false}, "src/CbcCompareBase.hpp": {"path": "layer-2/Cbc/src/CbcCompareBase.hpp", "filename": "CbcCompareBase.hpp", "file": "CbcCompareBase.hpp", "brief": "Abstract base class for node comparison/selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareBase: Defines interface for search tree node prioritization.\nThe node list is stored as a heap where test(x,y) returns true if\nnode y is \"better\" (higher priority) than node x.\n\nKey methods:\n- test(x,y): Core comparison - returns true if y preferred over x\n- newSolution(): Called when incumbent found, can change strategy\n- every1000Nodes(): Periodic callback for strategy adjustment\n- equalityTest(): Tiebreaker using node numbers\n\nNode selection strategies critically affect B&C performance:\n- Depth-first: Low memory, finds feasible solutions quickly\n- Best-first: Optimal objective bound, may use much memory\n- Hybrid: Combine based on search phase", "see": ["CbcCompareDepth for depth-first search", "CbcCompareObjective for best-bound search", "CbcCompareDefault for adaptive hybrid strategy", "CbcTree for tree management"], "has_pass2": false}, "src/CbcSolverExpandKnapsack.hpp": {"path": "layer-2/Cbc/src/CbcSolverExpandKnapsack.hpp", "filename": "CbcSolverExpandKnapsack.hpp", "file": "CbcSolverExpandKnapsack.hpp", "brief": "Knapsack constraint expansion for tighter formulations\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nexpandKnapsack(): Transforms x*y products (both integer) into knapsack form.\n\nCreates tighter formulation by:\n- Enumerating possible x*y values\n- Creating auxiliary binary variables for each value\n- Adding SOS constraints for mutual exclusion\n\nOutputs:\n- whichColumn: Mapping from new to original columns\n- knapsackStart/knapsackRow: Structure of expanded knapsacks\n- stored: Generated cuts from expansion\n- tightenedModel: Reformulated CoinModel\n\nafterKnapsack(): Recovers original solution from expanded form.\nMaps knapsackSolution back to original variable values.", "see": ["CoinModel for model representation", "CglStored for storing generated cuts"], "has_pass2": false}, "src/CbcCompareObjective.hpp": {"path": "layer-2/Cbc/src/CbcCompareObjective.hpp", "filename": "CbcCompareObjective.hpp", "file": "CbcCompareObjective.hpp", "brief": "Best-bound (objective-based) node selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareObjective: Implements best-first search.\nAlways explores node with best (lowest for min) LP bound.\n\ntest(x,y) returns true if y has smaller objective than x.\nPrioritizes most promising nodes for optimality proof.\n\nAdvantages:\n- Optimal search order for proving optimality\n- Best global bound progression\n- Minimizes total nodes for proof (in theory)\n\nDisadvantages:\n- Can use exponential memory\n- May take long to find first feasible solution\n- Poor anytime behavior", "see": ["CbcCompareDepth for depth-first alternative", "CbcCompareDefault for practical hybrid\n\nEdwin 11/25/09 carved out of CbcCompareActual"], "has_pass2": false}, "src/CbcFathomDynamicProgramming.hpp": {"path": "layer-2/Cbc/src/CbcFathomDynamicProgramming.hpp", "filename": "CbcFathomDynamicProgramming.hpp", "file": "CbcFathomDynamicProgramming.hpp", "brief": "Dynamic programming fathoming for special problem structures\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFathomDynamicProgramming: Exact enumeration via DP for small subproblems.\nAfter branching reduces the problem sufficiently, DP may be faster than B&C.\n\nRequirements for DP applicability:\n- Small integral RHS values\n- All-integer problem with positive integral coefficients\n- Currently: set partitioning (not general covering)\n\nState space considerations:\n- Each RHS=1 doubles state space\n- RHS=2,3 quadruples; RHS=4-7 multiplies by 8\n- maximumSizeAllowed_ controls maximum tractable size\n\nAlgorithm types:\n- Type 0: All coefficients and RHS are 1\n- Type 1: General case with coefficients > 1 or RHS > 1\n\nUses bit patterns to encode constraint satisfaction states.\nFinds optimal solution by enumeration over all feasible states.", "see": ["CbcFathom for base class", "CbcModel::fathom for integration point"], "has_pass2": false}, "src/CbcHeuristicDiveLineSearch.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveLineSearch.hpp", "filename": "CbcHeuristicDiveLineSearch.hpp", "file": "CbcHeuristicDiveLineSearch.hpp", "brief": "Dive heuristic along line to LP optimum\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveLineSearch: Geometric diving approach.\nSelects variables along the line from current point to LP optimum.\n\nselectVariableToBranch() considers how rounding affects\nmovement toward the LP optimal solution. Picks variables\nwhere rounding makes most progress along this direction.\n\nCombines geometric intuition with integer rounding.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicFPump for related LP-based approach"], "has_pass2": false}, "src/CbcBranchDefaultDecision.hpp": {"path": "layer-2/Cbc/src/CbcBranchDefaultDecision.hpp", "filename": "CbcBranchDefaultDecision.hpp", "file": "CbcBranchDefaultDecision.hpp", "brief": "Default branching variable selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchDefaultDecision: Simple selection algorithm.\n- Before first solution: Minimize infeasibility count\n- After first solution: Maximize objective degradation\n\nReturns +1 for up branch preferred, -1 for down branch.\nRemembers best candidate for comparison chain.", "see": ["CbcBranchDecision for base class", "CbcBranchDynamicDecision for pseudocost-based selection", "CbcBranchingObject for what gets compared\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": false}, "src/CbcFeasibilityBase.hpp": {"path": "layer-2/Cbc/src/CbcFeasibilityBase.hpp", "filename": "CbcFeasibilityBase.hpp", "file": "CbcFeasibilityBase.hpp", "brief": "User-defined feasibility checking\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFeasibilityBase: Allows user to override default feasibility tests.\nCalled after LP solve to let user examine solution and declare status.\n\nfeasible() return values:\n- 0: Use default determination\n- 1: Pretend solution is integer feasible\n- -1: Pretend problem is infeasible\n\nmode parameter:\n- 0: Called after solve, before cuts\n- -1: Called after strong branching\n\nUse cases:\n- Accept near-integer solutions as integer\n- Reject solutions violating problem-specific constraints\n- Implement custom tolerances", "see": ["CbcModel::setFeasibilityTest()"], "has_pass2": false}, "src/CbcStatistics.hpp": {"path": "layer-2/Cbc/src/CbcStatistics.hpp", "filename": "CbcStatistics.hpp", "file": "CbcStatistics.hpp", "brief": "Statistics gathering for node processing\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcStatistics: Captures detailed data about each branching operation.\nUsed for analysis and debugging of B&C search behavior.\n\nPer-node data:\n- id_/parentId_: Node identification\n- depth_: Tree depth\n- way_: Branch direction (-1/+1 first, -10/+10 second)\n- sequence_: Variable branched on\n\nObjective tracking:\n- startingObjective_/endingObjective_: LP values\n- startingInfeasibility_/endingInfeasibility_: Integer violations\n- numberIterations_: LP iterations used\n\nUseful for performance tuning and understanding B&C behavior.", "see": ["CbcModel for statistics collection", "CbcNode for node data"], "has_pass2": false}, "src/OsiCbc/OsiCbcSolverInterface.hpp": {"path": "layer-2/Cbc/src/OsiCbc/OsiCbcSolverInterface.hpp", "filename": "OsiCbcSolverInterface.hpp", "param": ["indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/Cbc_C_Interface.h": {"path": "layer-2/Cbc/src/Cbc_C_Interface.h", "filename": "Cbc_C_Interface.h", "file": "Cbc_C_Interface.h", "author": "COIN-OR CBC Development team", "date": "15 Aug 2019\n\nThe C API for the COIN-OR Branch-and-Cut solver", "algorithm": "LP Method Selection (LPMethod enum):\n  - LPM_Dual: Dual simplex (default, best for warm starts)\n  - LPM_Primal: Primal simplex\n  - LPM_Barrier: Interior-point with crossover to basis\n  - LPM_BarrierNoCross: IPM without crossover (for large LPs)", "complexity": "MIP: Exponential worst-case, highly structure-dependent\n  Each LP solve: O(m·n·iterations) for simplex", "ref": ["Land, A.H. and Doig, A.G. (1960). \"An automatic method of solving\n  discrete programming problems\". Econometrica 28(3):497-520."], "brief": "Creates an empty problem", "param": ["model problem object", "array string with problem name", "model problem object", "store: 1 maintain indexes of column and constraints names for searching indexes, 0 not", "model problem object", "name variable name", "lb column lower bound", "ub column upper bound", "obj objective function coefficient", "isInteger 1 if variable is integral, 0 otherwise", "nz number of rows (constraints) where this column appears, can be 0 if constraints will be added later", "rows index of rows where this column appears, NULL if rows will be added later", "coefs coefficients that this column appears in its rows, NULL if rows will be added later\n*", "model problem object", "numCols number of columns that will be deleted", "cols Vector with indexes of columns that will be deleted", "model problem object", "name constraint name", "nz number of variables with non-zero coefficients in this row", "cols index of variables that appear in this row", "coefs coefficients that that variables appear", "sense constraint sense: L if <=, G if >=, E if =, R if ranged and N if free", "rhs right hand size", "model problem object", "numRows number of rows", "rows rows to be deleted", "model problem object", "iColumn column index", "column name", "model problem object", "iRow row index", "name row name", "model problem object", "sense: direction of optimization (1 - minimize, -1 - maximize, 0 - ignore)", "model problem object", "index row index", "value new row lower bound", "model problem object", "index row index", "value new row upper bound", "model problem object", "row row index", "rhs value of the new RHS", "model problem object", "index variable index", "value new objective function coefficient for this variable", "model problem object", "index variable index", "value variable lower bound", "model problem object", "index variable index", "value new variable upper bound", "model problem object", "iColumn column index", "model problem object", "iColumn column index", "model problem object", "row row index", "column column index", "newValue new value of the coefficient", "model problem object", "model problem object", "count number of variables", "colNames names of variables", "colValues variable values", "model problem object", "count number of variables", "colIdxs indexes of variables", "colValues variable values", "model problem object", "fileName problem object", "model problem object", "model problem object", "model problem object", "maxNumberCharacters space in string array", "array string where problem name will be saved", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "row index", "name string where row name will be stored", "string where row name will be stored", "model problem object", "iColumn column index", "name where name will be stored", "maxLength maximum length of name string", "model problem object", "name column (variable) name", "model problem object", "name row (constraint) name", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "rowIdx row index", "model problem object", "col column index", "model problem object", "col column index", "model problem object", "col column index", "model problem object", "idx column index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "colIdx column index", "model problem object", "model problem object", "colIdx column index", "model problem object", "model problem object", "colIdx column index", "model problem object", "i variable index", "model problem object", "features vector of size Cbc_nFeatures() that will be filled with problem features", "model problem object", "model problem object", "model problem object", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "writeValues If writeValues = 1 writes values of structurals (and adds VALUES to end of NAME card), 0 otherwise", "formatType 0 - normal,   1 - extra accuracy,   2 - IEEE hex(later)", "model problem object", "name parameter name, e.g. cuts", "name parameter value, e.g. off", "model problem object", "which which integer parameter", "model problem object", "which which integer parameter", "val  value", "model problem object", "which which integer parameter", "val  value", "model problem object", "which which double parameter", "model model object", "model model object", "allowedGap the maximum allowable gap between the lower bound and the upper bound, when\n        the gap decrease to a smaller value the search is concluded", "model mip model", "cutcb cut callback function", "name cut generator name", "appData optional pointer to some additional information that the cut generator may need", "howOften 1 if the cut generator should be called at every node, > 1 at every howOften nodes negative\n       values have the same meaning but in this case the cut generator may be disable if not bound improvement\n       was obtained with these cuts. -99 for cut generators that will be called only at the root node", "atSolution if the cut generator must to be called also when an integer solution if found (=1) or zero otherwise", "model problem object", "model problem object", "model problem object", "n number of rows", "rows rows indices", "model problem object", "model problem object", "x solution vector", "maxViolRow pointer to double where max violation in rows will be stored", "rowIdx pointer to integer where index of most violated row will be stored", "maxViolCol pointer to double where max violation in columns will be stored", "colIdx pointer to integer where index of most violated column will be stored", "model problem object", "model problem object", "model problem object", "model problem object", "whichSol index of the solution to be retrieved", "model problem object", "whichSol solution index", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "osi OsiSolverInterface object", "sense: direction of optimization (1 - minimize, -1 - maximize, 0 - ignore)", "model problem object", "col column index", "model problem object", "col column index", "model problem object", "col column index", "osi OsiSolverInterface object", "name variable name", "lb column lower bound", "ub column upper bound", "obj objective function coefficient", "isInteger 1 if variable is integral, 0 otherwise", "nz number of rows (constraints) where this column appears, can be 0 if constraints will be added later", "rows index of rows where this column appears, NULL if rows will be added later", "coefs coefficients that this column appears in its rows, NULL if rows will be added later\n*", "osi OsiSolverInterface object", "name constraint name", "nz number of variables with non-zero coefficients in this row", "cols index of variables that appear in this row", "coefs cofficients that that variables appear", "sense constraint sense: L if <=, G if >=, E if =, R if ranged and N if free", "rhs right hand size", "cbcModel problem object", "ct cut type", "oc an OsiCuts object where cuts will be stored", "depth current three depth, cuts may use this info to decide which strategy to use", "pass cut pass number"], "return": "model copy", "has_pass2": true}}}, "Cgl": {"name": "Cgl", "file_count": 37, "pass2_count": 6, "files": {"src/CglResidualCapacity/CglResidualCapacity.hpp": {"path": "layer-2/Cgl/src/CglResidualCapacity/CglResidualCapacity.hpp", "filename": "CglResidualCapacity.hpp", "file": "CglResidualCapacity.hpp", "brief": "Residual capacity cuts for network design", "author": "Francisco Barahona (IBM)\n\nImplements residual capacity inequalities from:\n- Magnanti, Mirchandani, Vachani \"The convex hull of two core\n  capacitated network design problems\" (Math Programming 60, 1993)\n- Atamturk, Rajan \"On splittable and unsplittable flow capacitated\n  network design arc-set polyhedra\" (Math Programming 92, 2002)\n\nRow types detected:\n- ROW_L: a_1*c_1 + ... + a_k*c_k - d*z_1 - ... - d*z_p <= b\n  (continuous c_i, integer z_j with common coefficient d)\n- ROW_G: Same structure with >= sense\n- ROW_BOTH: Equations treatable as both\n- ROW_OTHER: Doesn't fit the pattern\n\nAlgorithm:\n1. resCapPreprocess(): Classify rows by type\n2. generateResCapCuts(): For each ROW_L/ROW_G row\n3. resCapSeparation(): Apply residual capacity separation\n\nTarget problems: Capacitated network design where flows are\ncontinuous but arc installation decisions are integer.", "see": ["CglFlowCover for related flow-based cuts", "CglMixedIntegerRounding for general MIR approach"], "has_pass2": false}, "src/CglAllDifferent/CglAllDifferent.hpp": {"path": "layer-2/Cgl/src/CglAllDifferent/CglAllDifferent.hpp", "filename": "CglAllDifferent.hpp", "file": "CglAllDifferent.hpp", "brief": "All-different constraint propagation for CSP-style constraints\n\nPropagates all-different constraints: variables in a set must all\ntake different integer values. Common in constraint satisfaction\nproblems (CSP) mapped to MIP.\n\nThis is a column cut generator (fixes/tightens variable bounds),\nnot a row cut generator. mayGenerateRowCutsInTree() returns false.\n\nEach set defines: x_i != x_j for all i,j in the set.\n\nAlgorithm: Simple propagation (primitive compared to CSP):\n- If variable is fixed, remove its value from others' domains\n- If domain shrinks to one value, fix that variable\n- Continue until no more propagation possible\n\nConstructor takes:\n- numberSets: How many all-different sets\n- starts: Start indices into which array (size numberSets+1)\n- which: Variable indices for all sets (referenced via starts)", "see": ["CglProbing for more general variable fixing via implications"], "has_pass2": false}, "src/CglFlowCover/CglFlowCover.hpp": {"path": "layer-2/Cgl/src/CglFlowCover/CglFlowCover.hpp", "filename": "CglFlowCover.hpp", "file": "CglFlowCover.hpp", "brief": "Lifted Simple Generalized Flow Cover cuts", "author": "Yan Xu (SAS), Jeff Linderoth (Lehigh), Martin Savelsbergh (Georgia Tech)\n\nGenerates flow cover cuts for constraints with variable upper bounds.\nSpecialization of MIR for single-node flow polytopes:\n  sum_j x_j - sum_k y_k <= b, where y_k <= u_k * d_k (d_k binary)\n\nRow classification (CglFlowRowType):\n- CGLFLOW_ROW_VARUB/VARLB: Variable bound defining rows\n- CGLFLOW_ROW_MIXUB/MIXEQ: Mixed integer rows suitable for cuts\n- CGLFLOW_ROW_SUMVARUB: Binary with 2+ other variables\n\nVariable bounds (CglFlowVUB/CglFlowVLB):\n- VUB: x_j <= a * y_k where y_k is binary indicator\n- VLB: x_j >= a * y_k\n- Enable stronger flow cover cuts than simple bounds\n\nAlgorithm:\n1. flowPreprocess(): Classify rows, identify VUBs/VLBs\n2. For each MIXUB row, try to generate flow cover\n3. generateOneFlowCut(): Find cover, apply lifting\n4. liftMinus()/liftPlus(): Strengthen coefficients\n\nColumn status (CglFlowColCut):\n- INCUT: Variable in cover set\n- INLMIN/INLMINMIN: Variable in L-/L-- for lifting\n- PRIME/SECONDARY: Candidate priority", "see": ["CglMixedIntegerRounding for general MIR approach", "CglKnapsackCover for simpler cover cuts"], "algorithm": "Row Classification (determineOneRowType):\n  Categories for cut generation:\n  - VARUB/VARLB: Two-variable rows defining variable bounds\n  - MIXUB/MIXEQ: Main candidates for flow cover generation\n  - NOBINUB: No binary → cannot generate flow cuts", "math": "VUB enables stronger cuts than simple bounds:\n        x_j ≤ u_j vs x_j ≤ u_j d_j gives tighter when d_j = 0\n  Stored in vubs_[] array indexed by continuous variable.", "ref": ["Padberg, Van Roy, Wolsey (1985) - Valid inequalities for 0-1 programs", "Gu, Nemhauser, Savelsbergh (1999) - Lifted flow cover inequalities"], "complexity": "O(nnz) per row for classification", "has_pass2": true}, "src/CglGomory/CglGomory.hpp": {"path": "layer-2/Cgl/src/CglGomory/CglGomory.hpp", "filename": "CglGomory.hpp", "file": "CglGomory.hpp", "brief": "Gomory mixed-integer cuts from optimal LP basis\n\nGenerates Gomory fractional cuts, the classic MIP cutting planes\nderived by Ralph Gomory in the 1950s-60s. For each fractional basic\nvariable, derives a valid inequality from the simplex tableau row.\n\nAlgorithm:\n1. Requires optimal LP basis (needsOptimalBasis() = true)\n2. For each basic integer variable with fractional value:\n   - Get tableau row: x_i + sum_j a_ij * x_j = b_i\n   - Apply fractional parts: sum_j f(a_ij)*x_j >= f(b_i)\n   - For mixed-integer: separate continuous/integer coefficients\n3. Strengthen using integrality of non-basic integers\n\nKey parameters:\n- limit_/limitAtRoot_: Max nonzeros in cut (dense cuts are expensive)\n- away_/awayAtRoot_: Min distance from integrality to generate cut\n- conditionNumberMultiplier_: Relax cut if basis is ill-conditioned\n- gomoryType_: 0=normal, 1=add original, 2=replace with original matrix\n\nNumerical considerations:\n- Basis condition number affects cut reliability\n- largestFactorMultiplier_ controls cut relaxation for stability\n- alternateFactorization_ uses different factorization for accuracy\n\nGomory cuts are finite and can solve pure integer programs in theory,\nbut numerically fragile. Modern solvers combine with other cuts.", "see": ["CglCutGenerator for base interface", "ClpFactorization for tableau row computation"], "algorithm": "Integrality Distance Threshold (away_):\n  Skip variables near-integer to avoid numerical issues:", "math": "Generate cut only if away_ ≤ f_0 ≤ 1 - away_\n  Default 0.05; tighter at root for more cuts.", "complexity": "O(m) per cut for m nonzeros in tableau row", "ref": ["Gomory (1958) - Outline of an algorithm for integer solutions", "Cornuéjols (2007) - Revival of GMI cuts in MIP", "Cook et al. (2009) - Numerically safe GMI cuts"], "has_pass2": true}, "src/CglTwomir/CglTwomir.hpp": {"path": "layer-2/Cgl/src/CglTwomir/CglTwomir.hpp", "filename": "CglTwomir.hpp", "file": "CglTwomir.hpp", "brief": "Two-step MIR (TMIR) cut generator\n\nGenerates tMIR and 2-step MIR cuts by applying the MIR inequality\nrecursively with different scaling factors. More general than\nsimple Gomory or MIR cuts.\n\nCut types generated (controlled by do_mir_, do_2mir_, do_tab_, do_form_):\n- tMIR: Apply MIR with scaling t (t_min_ to t_max_)\n- 2-step MIR: Apply MIR twice with scaling q (q_min_ to q_max_)\n- Tableau cuts: From simplex tableau rows\n- Formulation cuts: From original constraint rows\n\nKey data structures:\n- DGG_data_t: Problem data including bounds, solution, basis info\n- DGG_constraint_t: Sparse constraint representation\n- DGG_list_t: Collection of generated cuts\n\nAlgorithm flow:\n1. DGG_getData(): Extract LP data from solver\n2. DGG_generateTabRowCuts()/DGG_generateFormulationCuts(): Build base rows\n3. DGG_transformConstraint(): Shift/complement variables\n4. DGG_buildMir()/DGG_build2step(): Apply MIR function\n5. DGG_unTransformConstraint(): Restore original variables\n\nNumerical precision:\n- DGG_GOMORY_THRESH: Min fractionality for cut generation (0.005)\n- DGG_MIN_TABLEAU_COEFFICIENT: Zero threshold (1e-12)\n- DGG_MIN_STEEPNESS: Cut quality threshold", "see": ["CglMixedIntegerRounding for simpler single-step MIR", "CglGomory for basic Gomory cuts"], "has_pass2": false}, "src/CglRedSplit/CglRedSplit.hpp": {"path": "layer-2/Cgl/src/CglRedSplit/CglRedSplit.hpp", "filename": "CglRedSplit.hpp", "file": "CglRedSplit.hpp", "brief": "Reduce-and-Split cuts for enhanced Gomory generation", "author": "Francois Margot (Carnegie Mellon)\n\nBased on Anderson, Cornuejols, Li \"Reduce-and-Split Cuts: Improving\nthe Performance of Mixed Integer Gomory Cuts\" (Management Science, 2005).\n\nKey idea: Combine simplex tableau rows to reduce coefficients on\ncontinuous non-basic variables before applying Gomory formula.\nThis produces stronger cuts with better numerical properties.\n\nAlgorithm:\n1. Identify fractional integer basic variables (intBasicVar_frac)\n2. Build contNonBasicTab: continuous nonbasic coefficients\n3. reduce_contNonBasicTab(): Apply row reduction using pi_mat\n4. generate_cgcut(): Derive Chvatal-Gomory cut from reduced row\n5. eliminate_slacks(): Convert to structural variables only\n\nReduction process:\n- pi_mat: Integer multipliers for row combinations\n- test_pair(r1,r2): Check if combining rows reduces norm\n- find_step(): Optimal integer step for reduction\n- update_pi_mat()/update_redTab(): Apply reduction\n\nComplexity control:\n- maxTab_: Limits mTab * mTab * max(mTab,nTab) computation\n- Reducing maxTab makes generator faster but weaker\n\nRequires optimal basis (Clp or CPLEX).", "see": ["CglRedSplitParam for parameters (away, LUB, tolerances)", "CglRedSplit2 for enhanced version", "CglGomory for basic Gomory cuts"], "has_pass2": false}, "src/CglRedSplit/CglRedSplitParam.hpp": {"path": "layer-2/Cgl/src/CglRedSplit/CglRedSplitParam.hpp", "filename": "CglRedSplitParam.hpp", "file": "CglRedSplitParam.hpp", "brief": "Parameters for the Reduce-and-Split cut generator", "author": "Francois Margot (Tepper School of Business, Carnegie Mellon)\n\nKey parameters control numerical tolerances and algorithm behavior:\n- LUB: Threshold for \"large\" bounds (default 1000)\n- MAXDYN: Max coefficient ratio in cuts (default 1e8)\n- away: Min fractionality for row selection (default 0.05)\n- maxTab: Controls complexity (mTab^2 * max(mTab,nTab))\n- normIsZero, minReduc: Reduction termination criteria", "see": ["CglRedSplit for the generator using these parameters", "CglRedSplit2Param for enhanced version parameters", "CglParam for base parameter class\n\nDate:     11/24/06\n\n-----------------------------------------------------------------------------\nCopyright (C) 2006, Francois Margot and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglPreProcess/CglPreProcess.hpp": {"path": "layer-2/Cgl/src/CglPreProcess/CglPreProcess.hpp", "filename": "CglPreProcess.hpp", "file": "CglPreProcess.hpp", "brief": "MIP preprocessing with cut generator integration\n\nComprehensive MIP preprocessing that combines OsiPresolve with\ncut generators to strengthen the formulation before branch-and-cut.\nUnlike simple presolve, can add cuts that replace/strengthen constraints.\n\nMain operations:\n- preProcess(): Default strategy preprocessing\n- preProcessNonDefault(): User-configured cut generators\n- postProcess(): Reconstruct solution in original space\n- tightenPrimalBounds(): Bound tightening for faster dual\n\nPreprocessing features:\n- Variable fixing from reduced costs\n- Bound tightening propagation\n- Clique detection and strengthening (x + y <= 1 → x + y == 1)\n- SOS (Special Ordered Set) detection\n- Constraint row replacement with stronger versions\n- Duplicate row elimination\n\nHelper classes:\n- CglBK: Bron-Kerbosch algorithm for maximal clique finding\n- CglUniqueRowCuts: Hash-based duplicate cut detection\n- CglHashLink: Hash table support structure\n\nOption flags (options_):\n- 1: Original had integer bounds\n- 2: Don't do probing\n- 4: Don't do duplicate rows\n- 8: Don't do cliques\n- 16/64: Heavy probing options", "see": ["OsiPresolve for basic LP presolve", "CglProbing for variable fixing cuts", "CglClique for clique cut generation"], "has_pass2": false}, "src/CglLandP/CglLandP.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandP.hpp", "filename": "CglLandP.hpp", "file": "CglLandP.hpp", "brief": "Lift-and-Project cuts via simplex pivoting", "author": "Pierre Bonami (Carnegie Mellon/IBM)\n\nAdvanced lift-and-project implementation that generates cuts by\npivoting in the extended (lifted) space. More sophisticated than\nCglLiftAndProject.\n\nAlgorithm: For each fractional integer variable:\n1. Lift problem to higher dimension with disjunction\n2. Pivot in lifted space using CglLandPSimplex\n3. Project resulting constraint back to original space\n\nPivot strategies (SelectionRules):\n- mostNegativeRc: Most negative reduced cost (Dantzig-like)\n- bestPivot: Best possible pivot element\n- initialReducedCosts: Only rows with initially zero reduced cost\n\nCut generation modes (ExtraCutsMode):\n- none: Just primary L&P cuts\n- AtOptimalBasis: Also generate from optimal lifted basis\n- WhenEnteringBasis: Generate as structurals enter basis\n- AllViolatedMigs: Generate all violated GMI cuts during optimization\n\nNormalization options:\n- Unweighted, WeightRHS, WeightLHS, WeightBoth\n- LHS norms: L1, L2, Infinity, Average, Uniform, SupportSize\n\nKey parameters (Parameters class):\n- pivotLimit/pivotLimitInTree: Max pivots before generating cut\n- degeneratePivotLimit: Handle degenerate pivots\n- modularize/strengthen: Cut strengthening options\n\nRequires optimal basis.", "see": ["LAP::CglLandPSimplex for pivoting implementation", "LAP::Validator for cut validation", "CglLiftAndProject for simpler implementation"], "has_pass2": false}, "src/CglLandP/CglLandPMessages.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPMessages.hpp", "filename": "CglLandPMessages.hpp", "file": "CglLandPMessages.hpp", "brief": "Message handler for Lift-and-Project simplex", "author": "Pierre Bonami (CNRS, Aix-Marseille Universites)\n\nDefines LAP_messages enum for L&P algorithm status reporting:\n- Separating, FoundImprovingRow, FoundBestImprovingCol\n- PivotLog, FinishedOptimal, HitLimit\n- Warning messages for numerical issues\n\nLandPMessages class extends CoinMessages for log formatting.", "see": ["CglLandPSimplex for the algorithm using these messages", "CglLandP for the main cut generator", "CoinMessageHandler for message infrastructure\n\nDate:     02/23/08\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglLandP/CglLandPSimplex.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPSimplex.hpp", "filename": "CglLandPSimplex.hpp", "file": "CglLandPSimplex.hpp", "brief": "Simplex algorithm for Lift-and-Project cut generation", "author": "Pierre Bonami (Tepper School of Business, Carnegie Mellon)\n\nCore CGLP (Cut Generation LP) solver for finding optimal L&P cuts.\nImplements the pivoting procedure to optimize the intersection cut\nover adjacent bases using a specialized simplex method.\n\nKey operations:\n- optimize(): Find best cut by pivoting\n- generateMig(): Generate Gomory cuts without pivoting\n- createIntersectionCut(): Build cut from tableau row\n\nUses TabRow for tableau manipulation, Validator for cut quality.\nSupports OsiClp for efficient basis operations when available.", "algorithm": "Lift-and-Project Cutting Planes:\n  Given MIP with integrality on x_j, the L&P disjunction is:\n    (x_j ≤ ⌊x*_j⌋) OR (x_j ≥ ⌈x*_j⌉)\n\n  Cut generation via CGLP (Cut Generation LP):\n  1. Formulate normalization: ||cut|| = 1 (various norms)\n  2. For each disjunction term, lift the LP relaxation\n  3. Find cut valid for intersection of lifted polyhedra\n  4. Project cut back to original variable space\n\n  This class optimizes the CGLP by pivoting through adjacent bases,\n  seeking the most violated intersection cut.", "math": "For source row i with f_i fractional:\n  Intersection cut: Σ (a_j/f_i if a_j ≥ 0, else a_j/(f_i-1)) x_j ≥ 1\n  L&P strengthens by optimizing choice of disjunction via pivoting.", "complexity": "O(pivots × m × n) where pivots typically O(m).\n  Each pivot requires tableau row computation O(m) and ratio test O(n).\n  Heuristics limit pivot count to avoid excessive computation.", "ref": ["Balas, E. (1979). \"Disjunctive Programming\". Annals of Discrete\n  Mathematics 5:3-51. [Theoretical foundation]", "Balas, E. and Perregaard, M. (2003). \"A precise correspondence between\n  lift-and-project cuts, simple disjunctive cuts, and mixed integer\n  Gomory cuts for 0-1 programming\". Math. Programming 94:221-245."], "see": ["CglLandP for the main cut generator interface", "CglLandPTabRow for tableau row structure", "CglLandPValidator for cut acceptance criteria\n\nDate:     21/07/05\n\nCopyright (C) 2005-2009 Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": true}, "src/CglLandP/CglLandPValidator.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPValidator.hpp", "filename": "CglLandPValidator.hpp", "file": "CglLandPValidator.hpp", "brief": "Cut validation and cleaning for Lift-and-Project", "author": "Pierre Bonami (Tepper School of Business, Carnegie Mellon)\n\nValidator class rejects numerically unstable cuts based on:\n- RejectionsReasons enum: SmallViolation, SmallCoefficient,\n  BigDynamic, DenseCut, EmptyCut\n\nParameters:\n- maxFillIn_: Maximum density (fraction of columns)\n- maxRatio_: Maximum coefficient dynamic range\n- minViolation_: Minimum cut violation\n\ncleanCut()/cleanCut2() methods scale and filter cuts.\nTracks rejection statistics by reason.", "see": ["CglLandPSimplex which uses this validator", "CglLandP for the main cut generator", "CglGMIParam for similar validation parameters\n\nDate:     11/22/05\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglLandP/CglLandPTabRow.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPTabRow.hpp", "filename": "CglLandPTabRow.hpp", "file": "CglLandPTabRow.hpp", "brief": "Tableau row structure for Lift-and-Project simplex", "author": "Pierre Bonami (CNRS, Aix-Marseille Universites)\n\nTabRow struct extends CoinIndexedVector for sparse tableau rows:\n- num: Row number in the basis\n- rhs: Right-hand side value\n- modularized_: Flag for GMI strengthening applied\n\nProvides modularize() method for computing fractional parts\nused in intersection cut coefficient derivation.", "see": ["CglLandPSimplex for the simplex using these rows", "CglLandPUtils for row manipulation utilities", "CoinIndexedVector for sparse vector base\n\nDate:     02/23/08\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglLandP/CglLandPUtils.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPUtils.hpp", "filename": "CglLandPUtils.hpp", "file": "CglLandPUtils.hpp", "brief": "Utility functions for Lift-and-Project cut generation", "author": "Pierre Bonami (CNRS, Aix-Marseille Universites)\n\nKey functions for intersection cut computation:\n- intersectionCutCoef(): Computes cut coefficient from tableau entry\n- modularizedCoef(): Fractional part for integer variables\n- normCoef(): Row normalization factor\n- scale(): Cut scaling utilities\n\nCuts struct: Storage for extra cuts generated during pivoting,\nindexed by generating disjunction variable.", "see": ["CglLandPSimplex for the algorithm using these utilities", "CglLandPTabRow for tableau row structure", "CglLandP for the main cut generator\n\nDate:     02/23/08\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglKnapsackCover/CglKnapsackCover.hpp": {"path": "layer-2/Cgl/src/CglKnapsackCover/CglKnapsackCover.hpp", "filename": "CglKnapsackCover.hpp", "file": "CglKnapsackCover.hpp", "brief": "Knapsack cover cuts for binary packing constraints\n\nGenerates cover inequalities for knapsack constraints of the form\nsum_j a_j * x_j <= b where x_j are binary. A cover C is a subset\nwhere sum_{j in C} a_j > b, making x_j = 1 for all j in C infeasible.\n\nCover inequality: sum_{j in C} x_j <= |C| - 1\n\nAlgorithm phases:\n1. deriveAKnapsack(): Convert constraint to canonical binary knapsack\n2. Find minimal violated cover using:\n   - findExactMostViolatedMinCover(): Exact via Horowitz-Sahni\n   - findLPMostViolatedMinCover(): LP relaxation heuristic\n   - findGreedyCover(): Simple greedy heuristic\n3. Lifting: Strengthen cover to full-dimensional inequality\n   - liftCoverCut(): Sequence-independent lifting\n   - seqLiftAndUncomplementAndAdd(): Sequence-dependent lifting\n   - liftUpDownAndUncomplementAndAdd(): Up/down lifting for binaries\n\nAdditional features:\n- createCliques(): Build clique structure for probing\n- Horowitz-Sahni algorithm for exact knapsack solving\n- John-Ellis cover finding from OSL (IBM's solver)\n\nKey parameters:\n- maxInKnapsack_: Maximum variables in processed knapsack\n- expensiveCuts_: Enable/disable costly exact methods\n- rowsToCheck_: Restrict which rows to examine", "see": ["CglProbing for variable implications derived from covers", "CglClique for clique cuts (special case of covers)"], "algorithm": "Greedy Cover Heuristic (findGreedyCover):\n  Order variables by a_j/x̄_j ratio (LP solution weighted):", "math": "Add variables greedily until ∑_{j∈C} a_j > b\n  Fast but may miss most violated cover.", "complexity": "O(2^n) worst case, often O(n * c) practical", "ref": ["Balas & Zemel (1978) - Facets of the knapsack polytope", "Crowder, Johnson, Padberg (1983) - Solving large-scale 0-1 LPs", "Zemel (1989) - Easily computable facets of the knapsack polytope", "Horowitz & Sahni (1974) - Computing partitions", "Martello & Toth (1990) - Knapsack Problems (p. 30)"], "has_pass2": true}, "src/CglOddHole/CglOddHole.hpp": {"path": "layer-2/Cgl/src/CglOddHole/CglOddHole.hpp", "filename": "CglOddHole.hpp", "file": "CglOddHole.hpp", "brief": "Odd hole cuts from conflict graphs\n\nGenerates odd hole inequalities based on the method from\nGrotschel, Lovasz, and Schrijver (1988). An odd hole is a\nchordless cycle of odd length in the conflict graph.\n\nFor set packing rows sum x_i <= 1 (or == 1):\n- Build conflict graph where edge (i,j) means x_i + x_j <= 1\n- Find odd cycles C of length 2k+1\n- Odd hole inequality: sum_{i in C} x_i <= k\n\nLifting via Chvatal-Gomory:\n- Sum all rows in cycle (RHS is odd)\n- Weaken odd coefficients: 1 → 0, 3 → 2, etc.\n- Divide by 2 for stronger cut: sum (even(j)/2)*x_j <= (odd-1)/2\n\nPerformance considerations:\n- Only examines rows with unsatisfied 0-1 variables\n- Can be slow with many fractional variables\n- Uses shortest path algorithm (could be improved)\n- createRowList() to prefilter candidate rows\n- createCliqueList() for external clique information", "see": ["CglClique for clique cuts (simpler structure)", "CglOddWheel for related odd wheel cuts"], "has_pass2": false}, "src/CglRedSplit2/CglRedSplit2Param.hpp": {"path": "layer-2/Cgl/src/CglRedSplit2/CglRedSplit2Param.hpp", "filename": "CglRedSplit2Param.hpp", "file": "CglRedSplit2Param.hpp", "brief": "Parameters for the enhanced Reduce-and-Split cut generator", "author": "Giacomo Nannicini (Singapore University of Technology and Design)\n\nExtends CglRedSplitParam with combinable strategy selection:\n- RowSelectionStrategy: RS1-RS8 for choosing reduction rows\n- ColumnSelectionStrategy: CS1-CS21 for continuous variable selection\n- ColumnScalingStrategy: SC_* for Lift & Project integration\n\nMultiple strategies can be combined (3 row × 2 col = 6 combinations).\nSee Cornuejols & Nannicini, \"Practical strategies for generating\nrank-1 split cuts in MIP\", Math. Prog. Computation.", "see": ["CglRedSplit2 for the generator using these parameters", "CglRedSplitParam for simpler parameter version", "CglLandP for Lift & Project integration\n\nDate:     03/09/09\n-----------------------------------------------------------------------------\nCopyright (C) 2010, Giacomo Nannicini and others.  All Rights Reserved."], "has_pass2": false}, "src/CglRedSplit2/CglRedSplit2.hpp": {"path": "layer-2/Cgl/src/CglRedSplit2/CglRedSplit2.hpp", "filename": "CglRedSplit2.hpp", "file": "CglRedSplit2.hpp", "brief": "Enhanced Reduce-and-Split cuts with multiple strategies", "author": "Giacomo Nannicini (SUTD), based on CglRedSplit by F. Margot\n\nBased on papers:\n- \"Practical strategies for generating rank-1 split cuts\" (MPC)\n- \"Combining Lift-and-Project and Reduce-and-Split\" (INFORMS JOC)\n\nEnhanced version of CglRedSplit with:\n- Multiple column selection strategies (fill_workNonBasicTab)\n- Multiple row selection strategies (get_list_rows_reduction)\n- Integration with Lift-and-Project (tiltLandPcut)\n- Time limits and more parameters\n\nRow selection strategies:\n- sort_rows_by_nonzeroes: BRS1 - fewest nonzeros first\n- sort_rows_by_nonzeroes_greedy: BRS2 - greedy nonzero selection\n- sort_rows_by_cosine: BRS3 - angle-based (most effective)\n\nL&P integration:\n- generateMultipliers(): Get row multipliers for L&P\n- tiltLandPcut(): Improve L&P cut via reduction\n\nDoes NOT generate same cuts as CglRedSplit - use both for best results.", "see": ["CglRedSplit2Param for extensive parameter control", "CglRedSplit for original implementation", "CglLandP for combined L&P + R&S cuts"], "has_pass2": false}, "src/CglBKClique/CglBKClique.hpp": {"path": "layer-2/Cgl/src/CglBKClique/CglBKClique.hpp", "filename": "CglBKClique.hpp", "file": "CglBKClique.hpp", "brief": "Clique cut separator using Bron-Kerbosch algorithm", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\nUses CoinBronKerbosch for maximal clique enumeration on the conflict\ngraph. More efficient than CglClique for large graphs due to the\nwell-known BK algorithm with pivoting.\n\nExtension methods (extMethod_):\n- 0: No extension\n- 1: Random extension\n- 2: Max degree extension\n- 3: Max modified degree\n- 4: Reduced cost (inversely proportional)\n- 5: Reduced cost + modified degree\n\nPivoting strategies delegate to CoinBronKerbosch::PivotingStrategy.", "see": ["CoinBronKerbosch for the BK algorithm implementation", "CoinConflictGraph for the graph data structure", "CglClique for alternative clique separator", "CglCliqueStrengthening for preprocessing with cliques\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "has_pass2": false}, "src/CglCommon/CglMessage.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglMessage.hpp", "filename": "CglMessage.hpp", "file": "CglMessage.hpp", "brief": "Message definitions for CGL cut generators\n\nDefines CGL_Message enum codes and CglMessage class for\ninternationalized message output through CoinMessageHandler.\n\nMessage categories:\n- Infeasibility detection: CGL_INFEASIBLE, CGL_UNBOUNDED\n- Preprocessing results: CGL_FIXED, CGL_MADE_INTEGER, CGL_ADDED_INTEGERS\n- Statistics: CGL_PROCESS_STATS, CGL_PROCESS_STATS2\n- Clique processing: CGL_CLIQUES, CGL_PROCESS_CLQSTR, CGL_WARNING_CLQSTR\n- SOS handling: CGL_PROCESS_SOS1, CGL_PROCESS_SOS2\n- Postsolve: CGL_POST_INFEASIBLE, CGL_POST_CHANGED\n- Matrix changes: CGL_ELEMENTS_CHANGED1, CGL_ELEMENTS_CHANGED2", "see": ["CoinMessageHandler for the message output system", "CoinMessages for base message collection class"], "has_pass2": false}, "src/CglCommon/CglParam.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglParam.hpp", "filename": "CglParam.hpp", "file": "CglParam.hpp", "brief": "Base parameter class for cut generator configuration", "author": "Francois Margot (Carnegie Mellon)\n\nProvides common numerical parameters used across cut generators.\nIndividual generators extend this with algorithm-specific parameters.\n\nParameters:\n- INFINIT: Value representing infinity (default: COIN_DBL_MAX)\n- EPS: Tolerance for double comparisons (default: 1e-6)\n- EPS_COEFF: Minimum absolute coefficient in generated cuts (default: 1e-5)\n  Coefficients smaller than this are zeroed to avoid numerical issues\n- MAX_SUPPORT: Maximum nonzeros allowed in a cut (default: unlimited)\n  Dense cuts are expensive to store and may cause fill-in\n\nDesign pattern: Base class with virtual setters so derived classes\ncan validate parameter combinations or apply constraints.", "see": ["CglGMIParam, CglRedSplitParam for extended parameter classes"], "has_pass2": false}, "src/CglCommon/CglCutGenerator.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglCutGenerator.hpp", "filename": "CglCutGenerator.hpp", "file": "CglCutGenerator.hpp", "brief": "Abstract base class for all CGL cutting plane generators\n\nDefines the interface that all cut generators must implement. In MIP\nbranch-and-cut, cutting planes tighten the LP relaxation to cut off\nfractional solutions while keeping all integer-feasible points.\n\nPure virtual interface:\n- generateCuts(): Main entry point - examines LP solution and adds violated cuts\n- clone(): Deep copy for use in parallel B&C\n\nConfiguration:\n- aggressive_: Hint for cut generation intensity (0=minimal, 100=root node default)\n- canDoGlobalCuts_: Whether cuts are valid globally (requires no general integers)\n- originalSolver_: Some generators need unpreprocessed model\n\nIntegration with branch-and-cut:\n- mayGenerateRowCutsInTree(): Whether to call at tree nodes (affects matrix structure)\n- needsOptimalBasis(): Whether LP must be solved to optimality first\n- maximumLengthOfCutInTree(): Limits cut density for efficiency\n\nDerived classes implement specific separation algorithms:\n- Gomory, MIR, knapsack covers (general MIP)\n- Clique, odd hole, zero-half (0-1 structure)\n- Flow cover, lift-and-project (specialized)", "see": ["OsiCuts for the cut collection interface", "CglTreeInfo for branch-and-cut context"], "has_pass2": false}, "src/CglCommon/CglTreeInfo.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglTreeInfo.hpp", "filename": "CglTreeInfo.hpp", "file": "CglTreeInfo.hpp", "brief": "Context information for cut generation in branch-and-cut\n\nPasses branch-and-cut tree state to cut generators so they can adapt\ntheir behavior based on tree depth, pass number, and other factors.\n\nCglTreeInfo fields:\n- level: Search tree depth (0 = root)\n- pass: Which cut generation round at this node\n- formulation_rows: Original row count (before generated cuts)\n- inTree: True after first branch\n- options: Bit flags for various modes (see below)\n\nOption flags (can be combined):\n- 1: Treat costed integers as important\n- 4/8: Set global cut flag at root\n- 16: Make cuts globally valid\n- 32: Last round ineffective - be more aggressive\n- 64: In preprocessing stage\n- 128: Looks like we found a solution\n- 8192: Problem is infeasible\n\nCglTreeProbingInfo (derived class):\nExtends CglTreeInfo with variable fixing implications from probing.\n- fixEntries_: What gets fixed when a variable is set\n- toZero_/toOne_: Starts of implication lists\n- Supports analyze() to derive implied bounds", "see": ["CglCutGenerator for the generator interface", "CglProbing which populates probing info"], "has_pass2": false}, "src/CglCommon/CglStored.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglStored.hpp", "filename": "CglStored.hpp", "file": "CglStored.hpp", "brief": "Cut generator that stores and replays previously found cuts\n\nInstead of computing cuts, CglStored maintains a collection of cuts\nfrom prior solves or other sources, and adds violated ones to the LP.\n\nUse cases:\n- Warm-starting branch-and-cut with cuts from a similar problem\n- Cut pool management across nodes in the search tree\n- Importing cuts computed externally or in a preprocessing phase\n- Debugging by replaying a known cut sequence\n\nCut selection:\n- Only adds cuts violated by more than requiredViolation_ (default 1e-5)\n- Violation = constraint LHS evaluated at current LP solution\n- Avoids adding nearly-satisfied cuts that waste basis updates\n\nCan also store:\n- Best known solution (for primal bounds)\n- Tight variable bounds (from probing/preprocessing)\n- Probing implication information (CglTreeProbingInfo)", "see": ["CglCutGenerator for the base interface", "CglTreeProbingInfo for implication data from probing", "OsiCuts for the cut collection interface"], "has_pass2": false}, "src/CglMixedIntegerRounding/CglMixedIntegerRounding.hpp": {"path": "layer-2/Cgl/src/CglMixedIntegerRounding/CglMixedIntegerRounding.hpp", "filename": "CglMixedIntegerRounding.hpp", "file": "CglMixedIntegerRounding.hpp", "brief": "Mixed Integer Rounding (MIR) cuts via Marchand-Wolsey procedure", "author": "Joao Goncalves (Lehigh), Laszlo Ladanyi (IBM)\n\nImplements c-MIR (complemented MIR) cuts from the seminal paper by\nMarchand and Wolsey (Operations Research, 2001). These generalize\nGomory cuts by:\n- Aggregating multiple constraint rows\n- Applying bound substitution heuristics\n- Using the MIR function to round coefficients\n\nAlgorithm phases:\n1. mixIntRoundPreprocess(): Classify rows (ROW_MIX, ROW_VARUB, etc.)\n2. Row aggregation: Combine rows to create mixed-integer base inequality\n3. boundSubstitution(): Replace continuous vars using VUB/VLB\n4. cMirSeparation(): Apply MIR function to generate cut\n\nKey structures:\n- CglMixIntRoundVUB/VLB: Variable upper/lower bounds x_j <= a*y_k\n- RowType enum: Categorizes constraints for aggregation\n\nParameters:\n- MAXAGGR_: Maximum rows to aggregate (default: 3)\n- MULTIPLY_: Whether to try row * (-1) during aggregation\n- CRITERION_: Bound substitution rule (1, 2, or 3)\n- doPreproc_: When to run preprocessing (-1, 0, or 1)\n\nMIR function: For f(b)*x integer, MIR creates floor/ceil combination\nthat cuts off fractional solutions while preserving integer feasibility.", "see": ["CglMixedIntegerRounding2 for alternative implementation", "CglGomory for the base Gomory cuts MIR generalizes"], "has_pass2": false}, "src/CglGMI/CglGMI.hpp": {"path": "layer-2/Cgl/src/CglGMI/CglGMI.hpp", "filename": "CglGMI.hpp", "file": "CglGMI.hpp", "brief": "Gomory Mixed-Integer cuts with numerical safety testing", "author": "Giacomo Nannicini (SUTD)\n\nEnhanced Gomory cut generator with multiple cleaning procedures\nto test numerical safety of generated cuts. More robust than\nbasic CglGomory for difficult problems.\n\nRejection tracking (via RejectionType enum):\n- failureFractionality: Variable not sufficiently fractional\n- failureDynamism: Coefficient ratio too large\n- failureViolation: Cut doesn't violate LP solution enough\n- failureSupport: Too many nonzeros in cut\n- failureScale: Scaling failed to produce good cut\n\nCleaning procedures (param.CLEANING_PROCEDURE):\n- Check violation, dynamism, support\n- Remove small coefficients\n- Scale cuts (integral, norm-based, or largest=1)\n- nearestRational() for converting to exact fractions\n\nNumerical enhancements:\n- areEqual()/isZero()/isIntegerValue() with tolerances\n- factorize() to recompute basis for accuracy\n- flip()/unflipOrig()/unflipSlack() for bound handling\n\nRequires optimal basis (Clp or CPLEX 9.0+).", "see": ["CglGMIParam for parameter settings", "CglGomory for simpler Gomory implementation"], "algorithm": "Slack Elimination (eliminateSlack):\n  GMI from tableau uses slack variables; convert to original:", "math": "Substitute s_i = b_i - a_i'x using original constraint\n  Produces cut in terms of structural variables only.", "complexity": "O(nnz * log(maxdenom)) for rational approximation", "ref": ["Gomory (1960) - An algorithm for mixed-integer problems", "Cornuéjols (2008) - Valid inequalities for MIP", "Cook, Kannan, Schrijver (1990) - Chvátal closures", "Continued fraction algorithm for rational approximation"], "has_pass2": true}, "src/CglGMI/CglGMIParam.hpp": {"path": "layer-2/Cgl/src/CglGMI/CglGMIParam.hpp", "filename": "CglGMIParam.hpp", "file": "CglGMIParam.hpp", "brief": "Parameters for the GMI (Gomory Mixed-Integer) cut generator", "author": "Giacomo Nannicini (based on CglRedSplitParam by Francois Margot)\n\nKey parameters for cut quality and numerical safety:\n- AWAY: Min fractionality for row selection (default 0.005)\n- MAXDYN: Max coefficient ratio (default 1e6)\n- MINVIOL: Min cut violation (default 1e-4)\n- CleaningProcedure: CP_CGLLANDP1/2, CP_CGLREDSPLIT, CP_INTEGRAL_CUTS\n\nScaling options: INTEGRAL_SCALE_CONT, ENFORCE_SCALING\nDuplicate checking: CHECK_DUPLICATES (can be slow)", "see": ["CglGMI for the generator using these parameters", "CglGomory for the classic Gomory generator", "CglParam for base parameter class\n\nDate:     11/17/09\n-----------------------------------------------------------------------------\nCopyright (C) 2009, Giacomo Nannicini and others.  All Rights Reserved."], "has_pass2": false}, "src/CglClique/CglClique.hpp": {"path": "layer-2/Cgl/src/CglClique/CglClique.hpp", "filename": "CglClique.hpp", "file": "CglClique.hpp", "brief": "Clique cuts from set packing structure\n\nGenerates clique inequalities by finding maximal cliques in the\nconflict graph of binary variables. Two binaries conflict if they\ncannot both be 1 in any feasible solution.\n\nClique inequality: sum_{j in C} x_j <= 1 (or = 1 if equality)\n\nGraph construction (frac_graph):\n- Nodes: Binary variables at fractional LP values\n- Edges: Variables that conflict (appear in same set packing row)\n- Edge costs: 1 - x_i - x_j (for odd hole detection)\n\nTwo clique-finding methods:\n1. Star clique (do_star_clique):\n   - Pick a center node, find all neighbors (star)\n   - Enumerate maximal cliques in the star\n   - SCL_MIN_DEGREE/MAX_DEGREE/MAX_XJ_MAX_DEG selection rules\n2. Row clique (do_row_clique):\n   - Start from existing set packing row\n   - Extend to larger clique using adjacent variables\n\nThresholds:\n- scl_candidate_length_threshold: Max size for exact enumeration\n- rcl_candidate_length_threshold: Same for row clique method\n- Beyond threshold: use greedy heuristic\n\nCglFakeClique (derived class):\n- Works on \"fake\" solver with invented rows from probing\n- Integrates with CglProbing for stronger implications", "see": ["CglOddHole for odd-hole cuts (related conflict graph structure)", "CglProbing for variable fixing from cliques"], "algorithm": "Greedy Maximal Clique (greedy_maximal_clique):\n  When candidate list exceeds threshold:", "math": "Greedily add highest-degree nodes until no more fit\n  Fast but may miss maximum violated clique.", "complexity": "O(n² * d) for greedy approach", "ref": ["Padberg (1973) - On the facial structure of set packing", "Bron-Kerbosch (1973) - Algorithm for finding all cliques"], "has_pass2": true}, "src/CglProbing/CglProbing.hpp": {"path": "layer-2/Cgl/src/CglProbing/CglProbing.hpp", "filename": "CglProbing.hpp", "file": "CglProbing.hpp", "brief": "Probing cut generator - variable fixing and implications\n\nProbing systematically tests setting integer variables to bounds,\npropagating implications to derive:\n- Variable fixings (column cuts)\n- Bound tightenings\n- Disaggregation cuts (row cuts)\n- Coefficient strengthening\n- Clique structures\n\nProbing outcomes when testing x_j = 0 vs x_j = 1:\n1. One direction infeasible → fix variable (column cut)\n2. Both infeasible → problem infeasible\n3. Both feasible, y fixed same way → y can be fixed\n4. Both feasible, y fixed opposite → can substitute y = 1-x_j\n5. Constraint went slack by c → strengthen coefficient\n\nKey structures:\n- disaggregationAction: What gets fixed when probing a variable\n- cutVector_: Stores implications for each integer\n- Clique arrays: oneFixStart_, zeroFixStart_ for clique membership\n\nModes:\n- mode_ 0: Only unsatisfied integers, use snapshot (fast, global)\n- mode_ 1: Unsatisfied integers with current bounds\n- mode_ 2: All integers with current bounds\n\nCglImplication (separate class):\n- Generates cuts directly from stored implication info\n- Uses CglTreeProbingInfo built during probing", "see": ["CglTreeProbingInfo for storing probing results", "CglClique for cliques discovered during probing", "CglKnapsackCover which also builds clique info"], "has_pass2": false}, "src/CglZeroHalf/CglZeroHalf.hpp": {"path": "layer-2/Cgl/src/CglZeroHalf/CglZeroHalf.hpp", "filename": "CglZeroHalf.hpp", "file": "CglZeroHalf.hpp", "brief": "Zero-half ({0,1/2}) cutting planes\n\nGenerates {0,1/2}-cuts by taking mod-2 combinations of constraint rows.\nBased on Andreello, Caprara, Fischetti (INFORMS J. Computing, 2007).\n\nTheory: If we combine constraints with {0, 1/2} multipliers such that\nall LHS coefficients become even, we get a valid cut by dividing by 2\nand rounding down the RHS.\n\nAlgorithm outline:\n1. Convert constraint matrix to integers (scaling)\n2. Reduce coefficients mod 2 (0-1 matrix)\n3. Find combinations where LHS sums to 0 mod 2 per column\n4. These yield valid {0,1/2}-cuts when RHS is odd\n\nInternal representation:\n- mr_, mc_, mnz_: Matrix dimensions and nonzeros\n- mtbeg_, mtcnt_, mtind_, mtval_: Sparse row storage\n- vlb_, vub_: Variable bounds (integer scaled)\n- Cgl012Cut cutInfo_: Separation algorithm state\n\nUses Dijkstra shortest path (cglShortestPath) for separation.", "see": ["Cgl012cut.hpp for internal separation machinery", "CglGomory for related MIP cutting plane approach"], "has_pass2": false}, "src/CglZeroHalf/Cgl012cut.hpp": {"path": "layer-2/Cgl/src/CglZeroHalf/Cgl012cut.hpp", "filename": "Cgl012cut.hpp", "file": "Cgl012cut.hpp", "brief": "Low-level C-style implementation for {0,1/2}-cut separation\nCopyright (C) 2010, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCore data structures for mod-2 arithmetic based separation:\n- ilp: Original ILP matrix in row format\n- parity_ilp: Parity (mod-2) representation with weakening info\n- separation_graph: Graph for finding odd cycles\n- auxiliary_graph, cgl_graph: Shortest path computation\n\nCgl012Cut class provides:\n- sep_012_cut(): Main separation routine (Caprara-Fischetti method)\n- basic_separation(): Original MP paper algorithm\n- tabu_012(): Tabu search for violated cuts\n\nUses local search with scoring to find good {0,1/2}-cuts.", "see": ["CglZeroHalf for the OO wrapper around this implementation", "CglOddHole for related odd-cycle based cuts"], "has_pass2": false}, "src/CglDuplicateRow/CglDuplicateRow.hpp": {"path": "layer-2/Cgl/src/CglDuplicateRow/CglDuplicateRow.hpp", "filename": "CglDuplicateRow.hpp", "file": "CglDuplicateRow.hpp", "brief": "Detect and eliminate duplicate/dominated rows\n\nPreprocessing generator that identifies:\n- Duplicate rows (identical constraints)\n- Dominated rows (one implies another)\n- Variables fixable due to coefficient > effective RHS\n\nDesigned for problems with few rows, many integer variables,\n<= or == constraints, and small integer coefficients/RHS.\n\nduplicate_ array interpretation:\n- -1: Row still active\n- -2: Row removed (all variables fixed)\n- k >= 0: Row is same as row k\n\nModes (mode_):\n- 1: Process rows only\n- 2: Process columns only (dominated columns)\n- 3: Process both rows and columns\n- 4/8: Alternative algorithms\n\nAlgorithm for duplicate detection:\n- If row A is subset of row B with same effective RHS\n- Can fix variables in B not in A\n- Then rows become identical\n\nShould be called before first solve (preprocessing phase).", "see": ["CglPreProcess which uses this for preprocessing", "CglStored for storing cuts from dominated columns"], "has_pass2": false}, "src/CglMixedIntegerRounding2/CglMixedIntegerRounding2.hpp": {"path": "layer-2/Cgl/src/CglMixedIntegerRounding2/CglMixedIntegerRounding2.hpp", "filename": "CglMixedIntegerRounding2.hpp", "file": "CglMixedIntegerRounding2.hpp", "brief": "Enhanced MIR (Mixed Integer Rounding) cut generator", "author": "Joao Goncalves (Lehigh), Laszlo Ladanyi (IBM)\n\nAlternative implementation of MIR cuts with different heuristics\nthan CglMixedIntegerRounding. Uses CoinIndexedVector for efficiency.\n\nRow classification (RowType):\n- ROW_VARUB/VARLB: Variable bound rows (x <= ay or x >= ay)\n- ROW_VAREQ: Equality variable bound\n- ROW_MIX: Mixed continuous/integer\n- ROW_CONT/ROW_INT: All continuous or all integer\n\nAlgorithm steps:\n1. mixIntRoundPreprocess(): Identify VUBs/VLBs and row types\n2. generateMirCuts(): For each ROW_MIX row\n3. selectRowToAggregate(): Choose rows to combine\n4. aggregateRow(): Combine rows\n5. boundSubstitution(): Apply VUB/VLB substitution\n6. cMirSeparation(): Generate c-MIR inequality\n\nKey classes:\n- CglMixIntRoundVUB2/VLB2: Variable bound structures\n\nParameters:\n- MAXAGGR_: Max rows to aggregate (-1/-2 for unlimited)\n- MULTIPLY_: Also try row * (-1)\n- CRITERION_: Bound substitution strategy (1, 2, or 3)", "see": ["CglMixedIntegerRounding for original implementation", "CglTwomir for two-step MIR"], "has_pass2": false}, "src/CglSimpleRounding/CglSimpleRounding.hpp": {"path": "layer-2/Cgl/src/CglSimpleRounding/CglSimpleRounding.hpp", "filename": "CglSimpleRounding.hpp", "file": "CglSimpleRounding.hpp", "brief": "Simple rounding cuts via GCD (greatest common divisor)\n\nGenerates simple rounding cuts using the GCD method from\nNemhauser and Wolsey (1988, p.211).\n\nAlgorithm for each constraint:\n1. Derive a <= inequality in all integer variables\n   (net out continuous variables if possible)\n2. Find GCD of all integer coefficients\n3. Divide constraint by GCD\n4. Round down the RHS\n\nExample: 6x + 9y <= 20 with x,y integer\n- GCD(6,9) = 3\n- Divide: 2x + 3y <= 6.67\n- Round: 2x + 3y <= 6\n\nHelper methods:\n- gcd(a,b): Euclid's algorithm for two integers\n- gcdv(n,vi): GCD of integer vector\n- power10ToMakeDoubleAnInt(): Scale coefficients to integers\n- deriveAnIntegerRow(): Create integer-only version of constraint\n\nWarning: Sensitive to numerical precision. The power10 scaling\nmust not overflow 32-bit integers.", "see": ["CglGomory for more sophisticated cut generation"], "has_pass2": false}, "src/CglCliqueStrengthening/CglCliqueStrengthening.hpp": {"path": "layer-2/Cgl/src/CglCliqueStrengthening/CglCliqueStrengthening.hpp", "filename": "CglCliqueStrengthening.hpp", "file": "CglCliqueStrengthening.hpp", "brief": "Preprocessing to extend and merge clique constraints", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\nNot a CglCutGenerator - this is preprocessing that modifies the model.\nStrengthens set packing constraints sum x_i <= 1 by:\n1. Extending: Add variables that conflict with all in the clique\n2. Merging: Remove dominated cliques (subsets of stronger ones)\n\nExtension methods:\n- 0: No extension\n- 1: Random\n- 2: Max degree\n- 3: Max modified degree\n- 4: Reduced cost (inversely proportional) - default\n- 5: Reduced cost + modified degree\n\nHelper classes:\n- CliqueRows: Storage for clique constraints\n- CliqueRowStatus: NotDominated or Dominated\n\nUses CoinConflictGraph to find extension candidates.", "see": ["CglBKClique for cut generation (vs preprocessing)", "CoinCliqueSet for clique storage", "CglPreProcess which may call this\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["extMethod Extension method: 0 = no extension;1 = random;\n2 = max degree; 3 = max modified degree;\n4 = reduced cost (inversely proportional);\n5 = reduced cost (inversely proportional) + modified degree.", "n number of rows to be strengthened", "rows rows to be strengthened", "extMethod Extension method: 0 = no extension;\n1 = random; 2 = max degree; 3 = max modified degree;\n4 = reduced cost (inversely proportional);\n5 = reduced cost (inversely proportional) + modified degree."], "has_pass2": false}, "src/CglLiftAndProject/CglLiftAndProject.hpp": {"path": "layer-2/Cgl/src/CglLiftAndProject/CglLiftAndProject.hpp", "filename": "CglLiftAndProject.hpp", "file": "CglLiftAndProject.hpp", "brief": "Lift-and-project cuts for 0-1 programming\n\nImplements lift-and-project cuts based on Balas, Ceria, and Cornuejols.\nCreates cuts by \"lifting\" the problem into higher dimensions where\nthe LP relaxation is tighter, then \"projecting\" back.\n\nFor a disjunction x_j = 0 OR x_j = 1:\n- Create two copies of constraints: one with x_j=0, one with x_j=1\n- Convexify the union of these two polyhedra\n- Project back to original space\n- The projection gives a valid cut\n\nNormalization:\n- beta_ = +1 or -1 controls the normalization constraint\n- Choice affects which cuts are generated\n\nThis is a foundational method - computationally expensive but\ntheoretically powerful. Modern implementations often use\nspecialized variants like split cuts.", "see": ["CglLandP for related Lift-and-Project implementation (Bonami)", "CglGomory which can be viewed as a special case"], "has_pass2": false}, "src/CglOddWheel/CglOddWheel.hpp": {"path": "layer-2/Cgl/src/CglOddWheel/CglOddWheel.hpp", "filename": "CglOddWheel.hpp", "file": "CglOddWheel.hpp", "brief": "Odd-wheel cut separator (odd cycles with lifting)", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\nAn odd wheel is an odd hole (chordless odd cycle) with a \"center\"\nvertex connected to all cycle vertices. The wheel inequality:\n  sum_{i in cycle} x_i + (k)*x_center <= k\nwhere k = (cycle_length - 1) / 2.\n\nLifting strategies (extMethod_):\n- 0: No lifting (just odd holes)\n- 1: Single variable as wheel center\n- 2: Clique as wheel center (strongest)\n\nUses CoinConflictGraph to find odd cycles via shortest paths.", "see": ["CglOddHole for simpler odd-hole cuts", "CglBKClique for clique cuts from same conflict graph", "CoinConflictGraph for conflict graph structure\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["extMethod strategy that will be used to lift odd cycles,\ntransforming them into odd wheels: 0 = no lifting, 1 = only one\nvariable as wheel center, 2 = a clique as wheel center."], "has_pass2": false}}}, "Ipopt": {"name": "Ipopt", "file_count": 142, "pass2_count": 1, "files": {"src/Interfaces/IpTNLP.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpTNLP.hpp", "filename": "IpTNLP.hpp", "file": "IpTNLP.hpp", "brief": "User interface for defining NLP problems in standard form\n\nTNLP (Templated NLP) is the primary user-facing class for defining\noptimization problems. Users inherit from TNLP and implement:\n\nRequired methods:\n- get_nlp_info(): Problem dimensions and sparsity\n- get_bounds_info(): Variable and constraint bounds\n- get_starting_point(): Initial primal/dual values\n- eval_f(): Objective function value\n- eval_grad_f(): Objective gradient\n- eval_g(): Constraint function values\n- eval_jac_g(): Constraint Jacobian (sparse triplet format)\n- finalize_solution(): Receive final solution\n\nOptional methods:\n- eval_h(): Hessian of Lagrangian (or use quasi-Newton)\n- intermediate_callback(): Monitor progress per iteration", "see": ["IpIpoptApplication.hpp for solving TNLP problems", "IpTNLPAdapter.hpp for internal conversion to NLP interface"], "param": ["n           (out) Storage for the number of variables \\f$x\\f$", "m           (out) Storage for the number of constraints \\f$g(x)\\f$", "nnz_jac_g   (out) Storage for the number of nonzero entries in the Jacobian", "nnz_h_lag   (out) Storage for the number of nonzero entries in the Hessian", "index_style (out) Storage for the index style,\n                    the numbering style used for row/col entries in the sparse matrix format\n                    (TNLP::C_STYLE: 0-based, TNLP::FORTRAN_STYLE: 1-based; see also \\ref TRIPLET)", "n   (in) the number of variables \\f$x\\f$ in the problem", "x_l (out) the lower bounds \\f$x^L\\f$ for the variables \\f$x\\f$", "x_u (out) the upper bounds \\f$x^U\\f$ for the variables \\f$x\\f$", "m   (in) the number of constraints \\f$g(x)\\f$ in the problem", "g_l (out) the lower bounds \\f$g^L\\f$ for the constraints \\f$g(x)\\f$", "g_u (out) the upper bounds \\f$g^U\\f$ for the constraints \\f$g(x)\\f$", "n      (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "init_x (in) if true, this method must provide an initial value for \\f$x\\f$", "x      (out) the initial values for the primal variables \\f$x\\f$", "init_z (in) if true, this method must provide an initial value for the bound multipliers \\f$z^L\\f$ and \\f$z^U\\f$", "z_L    (out) the initial values for the bound multipliers \\f$z^L\\f$", "z_U    (out) the initial values for the bound multipliers \\f$z^U\\f$", "m      (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "init_lambda (in) if true, this method must provide an initial value for the constraint multipliers \\f$\\lambda\\f$", "lambda (out) the initial values for the constraint multipliers, \\f$\\lambda\\f$", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the values for the primal variables \\f$x\\f$ at which the objective function \\f$f(x)\\f$ is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise.\n                   This can be helpful when users have efficient implementations that calculate multiple outputs at once.\n                   %Ipopt internally caches results from the TNLP and generally, this flag can be ignored.", "obj_value (out) storage for the value of the objective function \\f$f(x)\\f$", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the values for the primal variables \\f$x\\f$ at which the gradient \\f$\\nabla f(x)\\f$ is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "grad_f (out) array to store values of the gradient of the objective function \\f$\\nabla f(x)\\f$.\n                     The gradient array is in the same order as the \\f$x\\f$ variables\n                     (i.e., the gradient of the objective with respect to `x[2]` should be put in `grad_f[2]`).", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the values for the primal variables \\f$x\\f$ at which the constraint functions \\f$g(x)\\f$ are to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "g     (out) array to store constraint function values \\f$g(x)\\f$, do not add or subtract the bound values \\f$g^L\\f$ or \\f$g^U\\f$.", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) first call: NULL; later calls: the values for the primal variables \\f$x\\f$ at which the constraint Jacobian \\f$\\nabla g(x)^T\\f$ is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "nele_jac (in) the number of nonzero elements in the Jacobian; it will have the same value that was specified in TNLP::get_nlp_info", "iRow  (out) first call: array of length nele_jac to store the row indices of entries in the Jacobian of the constraints; later calls: NULL", "jCol  (out) first call: array of length nele_jac to store the column indices of entries in the Jacobian of the constraints; later calls: NULL", "values (out) first call: NULL; later calls: array of length nele_jac to store the values of the entries in the Jacobian of the constraints", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) first call: NULL; later calls: the values for the primal variables \\f$x\\f$ at which the Hessian is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "obj_factor (in) factor \\f$\\sigma_f\\f$ in front of the objective term in the Hessian", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "lambda (in) the values for the constraint multipliers \\f$\\lambda\\f$ at which the Hessian is to be evaluated", "new_lambda (in) false if any evaluation method was previously called with the same values in lambda, true otherwise", "nele_hess (in) the number of nonzero elements in the Hessian; it will have the same value that was specified in TNLP::get_nlp_info", "iRow  (out) first call: array of length nele_hess to store the row indices of entries in the Hessian; later calls: NULL", "jCol  (out) first call: array of length nele_hess to store the column indices of entries in the Hessian; later calls: NULL", "values (out) first call: NULL; later calls: array of length nele_hess to store the values of the entries in the Hessian", "status @parblock (in) gives the status of the algorithm\n  - SUCCESS: Algorithm terminated successfully at a locally optimal\n    point, satisfying the convergence tolerances (can be specified\n    by options).\n  - MAXITER_EXCEEDED: Maximum number of iterations exceeded (can be specified by an option).\n  - CPUTIME_EXCEEDED: Maximum number of CPU seconds exceeded (can be specified by an option).\n  - STOP_AT_TINY_STEP: Algorithm proceeds with very little progress.\n  - STOP_AT_ACCEPTABLE_POINT: Algorithm stopped at a point that was converged, not to \"desired\" tolerances, but to \"acceptable\" tolerances (see the acceptable-... options).\n  - LOCAL_INFEASIBILITY: Algorithm converged to a point of local infeasibility. Problem may be infeasible.\n  - USER_REQUESTED_STOP: The user call-back function TNLP::intermediate_callback returned false, i.e., the user code requested a premature termination of the optimization.\n  - DIVERGING_ITERATES: It seems that the iterates diverge.\n  - RESTORATION_FAILURE: Restoration phase failed, algorithm doesn't know how to proceed.\n  - ERROR_IN_STEP_COMPUTATION: An unrecoverable error occurred while %Ipopt tried to compute the search direction.\n  - INVALID_NUMBER_DETECTED: Algorithm received an invalid number (such as NaN or Inf) from the NLP; see also option check_derivatives_for_nan_inf).\n  - INTERNAL_ERROR: An unknown internal error occurred.\n  @endparblock", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the final values for the primal variables", "z_L   (in) the final values for the lower bound multipliers", "z_U   (in) the final values for the upper bound multipliers", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "g     (in) the final values of the constraint functions", "lambda (in) the final values of the constraint multipliers", "obj_value (in) the final value of the objective function", "ip_data (in) provided for expert users", "ip_cq (in) provided for expert users", "ip_data (in)  Ipopt Data", "ip_cq   (in)  Ipopt Calculated Quantities", "scaled  (in)  whether to retrieve scaled or unscaled iterate", "n       (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping x, z_L, and z_U", "x       (out) buffer to store value of primal variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving x", "z_L     (out) buffer to store the lower bound multipliers \\f$z_L\\f$, must have length at least n; pass NULL to skip retrieving z_L and z_U", "z_U     (out) buffer to store the upper bound multipliers \\f$z_U\\f$, must have length at least n; pass NULL to skip retrieving z_U and z_U", "m       (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping g and lambda", "g       (out) buffer to store the constraint values \\f$g(x)\\f$, must have length at least m; pass NULL to skip retrieving g", "lambda  (out) buffer to store the constraint multipliers \\f$\\lambda\\f$, must have length at least m; pass NULL to skip retrieving lambda", "ip_data    (in)  Ipopt Data", "ip_cq      (in)  Ipopt Calculated Quantities", "scaled     (in)  whether to retrieve scaled or unscaled violations", "n          (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping compl_x_L, compl_x_U, and grad_lag_x", "x_L_violation (out) buffer to store violation of original lower bounds on variables (max(orig_x_L-x,0)), must have length at least n; pass NULL to skip retrieving orig_x_L", "x_U_violation (out) buffer to store violation of original upper bounds on variables (max(x-orig_x_U,0)), must have length at least n; pass NULL to skip retrieving orig_x_U", "compl_x_L  (out) buffer to store violation of complementarity for lower bounds on variables (\\f$(x-x_L)z_L\\f$), must have length at least n; pass NULL to skip retrieving compl_x_L", "compl_x_U  (out) buffer to store violation of complementarity for upper bounds on variables (\\f$(x_U-x)z_U\\f$), must have length at least n; pass NULL to skip retrieving compl_x_U", "grad_lag_x (out) buffer to store gradient of Lagrangian w.r.t. variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving grad_lag_x", "m          (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping lambda", "nlp_constraint_violation (out) buffer to store violation of constraints \\f$max(g_l-g(x),g(x)-g_u,0)\\f$, must have length at least m; pass NULL to skip retrieving constraint_violation", "compl_g    (out) buffer to store violation of complementarity of constraint (\\f$(g(x)-g_l)*\\lambda^+ + (g_l-g(x))*\\lambda^-\\f$, where \\f$\\lambda^+=max(0,\\lambda)\\f$ and \\f$\\lambda^-=max(0,-\\lambda)\\f$ (componentwise)), must have length at least m; pass NULL to skip retrieving compl_g"], "return": "true if success, false otherwise.\n\nThe values of `n` and `m` that were specified in TNLP::get_nlp_info are passed\nhere for debug checking. Setting a lower bound to a value less than or\nequal to the value of the option \\ref OPT_nlp_lower_bound_inf \"nlp_lower_bound_inf\"\nwill cause %Ipopt to assume no lower bound. Likewise, specifying the upper bound above or\nequal to the value of the option \\ref OPT_nlp_upper_bound_inf \"nlp_upper_bound_inf\"\nwill cause %Ipopt to assume no upper bound. These options are set to -10<sup>19</sup> and\n10<sup>19</sup>, respectively, by default, but may be modified by changing these\noptions.", "has_pass2": false}, "src/Interfaces/IpTNLPAdapter.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpTNLPAdapter.hpp", "filename": "IpTNLPAdapter.hpp", "file": "IpTNLPAdapter.hpp", "brief": "Adapter converting user TNLP interface to internal NLP\n\nTNLPAdapter is a Design Pattern Adapter that converts from the\nuser-facing TNLP format to Ipopt's internal NLP representation:\n\nTNLP (user):           NLP (internal):\n  g_L <= g(x) <= g_U  ->  c(x) = 0           (equalities)\n                          d_L <= d(x) <= d_U (inequalities)\n\nKey responsibilities:\n- Separate mixed constraints into equalities and inequalities\n- Handle fixed variables (as parameters or relaxed bounds)\n- Convert sparse triplet (i,j,val) to internal matrix format\n- Detect linear dependencies in constraints\n- Map between TNLP multipliers and internal dual variables", "see": ["IpTNLP.hpp for user problem definition", "IpNLP.hpp for internal representation"], "return": "True, if bound multipliers could be assigned. False if there was an evaluation error when calculating bound multipliers for fixed variables.\n@since 3.14.0", "has_pass2": false}, "src/Interfaces/IpSolveStatistics.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpSolveStatistics.hpp", "filename": "IpSolveStatistics.hpp", "file": "IpSolveStatistics.hpp", "brief": "Statistics collected during optimization run\n\nSolveStatistics provides post-solve information accessible via\nIpoptApplication::Statistics(). Captures optimization outcomes\nat the time of construction from internal Ipopt objects.\n\nAvailable statistics:\n- Iteration count\n- Final objective value\n- Final constraint violation (primal infeasibility)\n- Final optimality error (dual infeasibility)\n- Final complementarity\n- Timing information", "see": ["IpIpoptApplication.hpp for accessing statistics after solve", "TNLP::finalize_solution() for receiving solution data"], "has_pass2": false}, "src/Interfaces/IpInterfacesRegOp.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpInterfacesRegOp.hpp", "filename": "IpInterfacesRegOp.hpp", "file": "IpInterfacesRegOp.hpp", "brief": "Option registration for Interfaces subsystem\n\nDeclares RegisterOptions_Interfaces() which registers all options\nspecific to the Interfaces module with the RegisteredOptions object.\n\nCalled during IpoptApplication initialization to register options\nlike fixed_variable_treatment, dependency_detection_with_rhs, etc.", "see": ["IpRegOptions.hpp for the registration framework"], "has_pass2": false}, "src/Interfaces/IpAlgTypes.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpAlgTypes.hpp", "filename": "IpAlgTypes.hpp", "file": "IpAlgTypes.hpp", "brief": "Algorithm-level return codes and exception types\n\nDefines SolverReturn enum for internal algorithm status:\n- SUCCESS: Converged to optimality tolerances\n- MAXITER_EXCEEDED, CPUTIME_EXCEEDED, WALLTIME_EXCEEDED\n- STOP_AT_ACCEPTABLE_POINT: Met acceptable (looser) tolerances\n- LOCAL_INFEASIBILITY: Problem appears locally infeasible\n- USER_REQUESTED_STOP: Callback requested termination\n- RESTORATION_FAILURE: Feasibility restoration failed\n- DIVERGING_ITERATES, INVALID_NUMBER_DETECTED\n\nAlso declares standard exceptions thrown during solve:\nLOCALLY_INFEASIBLE, TOO_FEW_DOF, TINY_STEP_DETECTED, etc.", "see": ["IpReturnCodes.hpp for application-level return codes"], "has_pass2": false}, "src/Interfaces/IpReturnCodes.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpReturnCodes.hpp", "filename": "IpReturnCodes.hpp", "file": "IpReturnCodes.hpp", "brief": "C++ namespace wrapper for Ipopt return codes\n\nWraps IpReturnCodes_inc.h in Ipopt namespace for C++ usage.\nDefines ApplicationReturnStatus enum used by IpoptApplication:\n\n- Solve_Succeeded: Optimal solution found\n- Solved_To_Acceptable_Level: Acceptable tolerance met\n- Infeasible_Problem_Detected: Locally infeasible\n- Maximum_Iterations_Exceeded, Maximum_CpuTime_Exceeded\n- Restoration_Failed, Error_In_Step_Computation\n- Invalid_Option, Invalid_Number_Detected\n- Not_Enough_Degrees_Of_Freedom, Internal_Error", "see": ["IpReturnCodes_inc.h for actual enum definitions", "IpIpoptApplication.hpp for usage context"], "has_pass2": false}, "src/Interfaces/IpNLP.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpNLP.hpp", "filename": "IpNLP.hpp", "file": "IpNLP.hpp", "brief": "Internal NLP representation using Vector/Matrix abstractions\n\nNLP is Ipopt's internal problem representation with equality constraints\nseparated from inequalities:\n  min  f(x)\n  s.t. c(x) = 0           (equality constraints)\n       d_L <= d(x) <= d_U (inequality constraints)\n       x_L <= x <= x_U    (variable bounds)\n\nUnlike TNLP (user interface with arrays), NLP uses Vector and Matrix\nobjects for all operations. TNLPAdapter converts TNLP to this form.\n\nKey methods provide:\n- Problem dimensions and bounds\n- Function/gradient/Jacobian/Hessian evaluation\n- Space factories for creating appropriately-sized vectors", "see": ["IpTNLP.hpp for user-facing problem definition", "IpTNLPAdapter.hpp for TNLP-to-NLP conversion"], "has_pass2": false}, "src/Interfaces/IpTNLPReducer.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpTNLPReducer.hpp", "filename": "IpTNLPReducer.hpp", "file": "IpTNLPReducer.hpp", "brief": "Wrapper to solve subproblems with fewer constraints\n\nTNLPReducer wraps an existing TNLP and presents a reduced problem\nwith specified constraints, lower bounds, and upper bounds removed.\nUseful for experimentation with constraint subsets.\n\nCan skip:\n- Constraints (by index)\n- Lower bounds on variables\n- Upper bounds on variables\n- Fix certain variables\n\n@note Not efficient - still evaluates full problem internally.\nPrimarily for debugging and experimentation.", "see": ["IpTNLP.hpp for the underlying interface"], "has_pass2": false}, "src/Interfaces/IpStdInterfaceTNLP.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpStdInterfaceTNLP.hpp", "filename": "IpStdInterfaceTNLP.hpp", "file": "IpStdInterfaceTNLP.hpp", "brief": "TNLP implementation wrapping C function pointer interface\n\nStdInterfaceTNLP implements TNLP by delegating to C function pointers\nprovided through IpStdCInterface.h. This enables C programs to use\nIpopt without C++ inheritance.\n\nThe C interface passes:\n- Problem dimensions (n_var, n_con, nele_jac, nele_hess)\n- Bounds arrays (x_L, x_U, g_L, g_U)\n- Starting points (start_x, start_lam, start_z_L, start_z_U)\n- Function pointers for eval_f, eval_g, eval_grad_f, eval_jac_g, eval_h\n\nStdInterfaceTNLP converts C callbacks to TNLP virtual method calls.", "see": ["IpStdCInterface.h for C API documentation", "IpTNLP.hpp for the underlying interface"], "has_pass2": false}, "src/Interfaces/IpIpoptApplication.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpIpoptApplication.hpp", "filename": "IpIpoptApplication.hpp", "file": "IpIpoptApplication.hpp", "brief": "Main application class for calling Ipopt from C++\n\nIpoptApplication is the entry point for C++ applications using Ipopt.\nProvides methods for initialization, option handling, and solving.\n\nTypical usage:\n  SmartPtr<IpoptApplication> app = IpoptApplicationFactory();\n  app->Initialize();\n  app->Options()->SetStringValue(\"linear_solver\", \"ma57\");\n  ApplicationReturnStatus status = app->OptimizeTNLP(mynlp);\n\nKey methods:\n- Initialize(): Read options from ipopt.opt file\n- OptimizeTNLP(): Solve a TNLP problem\n- ReOptimizeTNLP(): Warm-start solve (reuses algorithm state)\n- Options(): Access OptionsList for setting parameters\n- Statistics(): Get solve statistics after optimization", "see": ["IpTNLP.hpp for defining optimization problems", "IpReturnCodes.hpp for ApplicationReturnStatus values"], "return": "Solve_Succeeded or something else if there was a\n problem in the initialization (such as an invalid option).\n\n You should call one of the initialization methods at some\n point before the first optimize call.\n Set @par allow_clobber to true if you want to allow\n overwriting options that are set by the input stream.", "has_pass2": false}, "src/Common/IpSmartPtr.hpp": {"path": "layer-2/Ipopt/src/Common/IpSmartPtr.hpp", "filename": "IpSmartPtr.hpp", "file": "IpSmartPtr.hpp", "brief": "Intrusive reference-counting smart pointer for Ipopt\n\nProvides SmartPtr<T> template class implementing automatic memory\nmanagement via reference counting. All managed objects must inherit\nfrom ReferencedObject. Used throughout Ipopt for safe object lifetime\nmanagement without manual delete calls.\n\nKey design: intrusive (count stored in object) vs non-intrusive\n(count stored externally). Intrusive chosen to avoid double-counting\nwhen raw pointers are passed between SmartPtr instances.\n\n@note Does NOT handle circular references - use weak pointers or\nexplicit release methods for bidirectional object graphs.", "see": ["IpReferenced.hpp for ReferencedObject base class", "IpObserver.hpp for observer pattern using SmartPtr"], "has_pass2": false}, "src/Common/IpTaggedObject.hpp": {"path": "layer-2/Ipopt/src/Common/IpTaggedObject.hpp", "filename": "IpTaggedObject.hpp", "file": "IpTaggedObject.hpp", "brief": "Base class for objects with change-tracking tags\n\nTaggedObject provides efficient change detection for expensive\ncomputations. Each object maintains a unique tag that changes whenever\nthe object's internal state is modified. Consumers compare stored tags\nto detect changes and avoid redundant recalculation.\n\nUsage pattern:\n1. Store last-used tag locally\n2. Before computation, call HasChanged(stored_tag)\n3. If changed: recompute and update stored tag via GetTag()\n4. If unchanged: reuse cached result\n\nInherits from both ReferencedObject (for SmartPtr) and Subject\n(to notify Observers of changes).", "see": ["IpCachedResults.hpp for cached computation with auto-invalidation", "IpObserver.hpp for the Observer pattern"], "has_pass2": false}, "src/Common/IpReferenced.hpp": {"path": "layer-2/Ipopt/src/Common/IpReferenced.hpp", "filename": "IpReferenced.hpp", "file": "IpReferenced.hpp", "brief": "Reference counting base class for SmartPtr management\n\nDefines ReferencedObject - the base class that any object must inherit\nfrom to be managed by SmartPtr<T>. Implements intrusive reference counting\nwhere the count is stored in the object itself rather than externally.\n\nAlso defines Referencer pseudo-class used to track which SmartPtr\ninstances hold references (useful for debugging circular references).", "see": ["IpSmartPtr.hpp for the SmartPtr template class"], "has_pass2": false}, "src/Common/IpException.hpp": {"path": "layer-2/Ipopt/src/Common/IpException.hpp", "filename": "IpException.hpp", "file": "IpException.hpp", "brief": "Exception base class and macros for Ipopt error handling\n\nProvides IpoptException base class and convenience macros:\n- DECLARE_STD_EXCEPTION(Type): Define new exception class\n- THROW_EXCEPTION(Type, msg): Throw with file/line info\n- ASSERT_EXCEPTION(cond, Type, msg): Conditional throw\n\nExceptions carry source location (file, line) and message for\ndebugging. Can be reported via Journalist::ReportException().\n\nUses special visibility attributes on macOS to ensure exceptions\ncan be caught across shared library boundaries.", "see": ["IpJournalist.hpp for exception reporting"], "has_pass2": false}, "src/Common/IpUtils.hpp": {"path": "layer-2/Ipopt/src/Common/IpUtils.hpp", "filename": "IpUtils.hpp", "file": "IpUtils.hpp", "brief": "General utility functions for Ipopt\n\nProvides platform-independent utility functions:\n- Min/Max template functions for up to 4 arguments\n- IsFiniteNumber() for NaN/Inf detection\n- CpuTime(), SysTime(), WallclockTime() for profiling\n- IpRandom01() for random number generation\n- Snprintf() for safe formatted output\n- Compare_le() for numerical comparison with tolerance\n- Signal handling for graceful interrupt", "see": ["IpTypes.hpp for Number and Index type definitions", "IpTimedTask.hpp for high-level timing wrapper"], "return": "whether registering the handler was successful\n@since 3.14.17", "has_pass2": false}, "src/Common/IpOptionsList.hpp": {"path": "layer-2/Ipopt/src/Common/IpOptionsList.hpp", "filename": "IpOptionsList.hpp", "file": "IpOptionsList.hpp", "brief": "Container for user-specified algorithmic options\n\nOptionsList stores runtime-configurable parameters as string key-value\npairs. Supports integer, numeric, and string option types with automatic\nconversion. Tracks how many times each option has been accessed (useful\nfor detecting unused options).\n\nOptions are case-insensitive. Values can be set via:\n- SetNumericValue(), SetIntegerValue(), SetStringValue()\n- ReadFromStream() for ipopt.opt format files\n\nIntegrates with RegisteredOptions for validation and default values.", "see": ["IpRegOptions.hpp for option registration and validation", "IpIpoptApplication.hpp for initialization from files"], "return": "false, if an error was encountered", "has_pass2": false}, "src/Common/IpTypes.hpp": {"path": "layer-2/Ipopt/src/Common/IpTypes.hpp", "filename": "IpTypes.hpp", "file": "IpTypes.hpp", "brief": "Fundamental type definitions for Ipopt (C++ namespace wrapper)\n\nWraps the C type definitions from IpTypes.h in the Ipopt namespace:\n- Number (ipnumber): floating-point type for all numerical values\n- Index (ipindex): integer type for array indices and dimensions\n\nActual types depend on compile-time configuration (IPOPT_SINGLE,\nIPOPT_INT64). Default is double for Number and int for Index.", "see": ["IpTypes.h for underlying C definitions", "IpoptConfig.h for configuration macros"], "has_pass2": false}, "src/Common/IpTimedTask.hpp": {"path": "layer-2/Ipopt/src/Common/IpTimedTask.hpp", "filename": "IpTimedTask.hpp", "file": "IpTimedTask.hpp", "brief": "Timer class for profiling algorithm components\n\nTimedTask provides RAII-style timing measurement for code blocks.\nTracks CPU time, system time, and wall clock time. Multiple\nStart()/End() pairs accumulate total time across invocations.\n\nUsage:\n  TimedTask timer;\n  timer.Start();\n  // ... expensive operation ...\n  timer.End();\n  double elapsed = timer.TotalCpuTime();\n\nUsed throughout Ipopt to report timing breakdown by category\n(function evaluation, linear solve, line search, etc.).", "see": ["IpUtils.hpp for CpuTime(), WallclockTime() functions"], "has_pass2": false}, "src/Common/IpJournalist.hpp": {"path": "layer-2/Ipopt/src/Common/IpJournalist.hpp", "filename": "IpJournalist.hpp", "file": "IpJournalist.hpp", "brief": "Logging and output management system for Ipopt\n\nImplements a flexible logging framework with multiple output destinations\n(Journals) and fine-grained control over verbosity by category.\n\nPrint levels (EJournalLevel): J_NONE < J_ERROR < J_WARNING < J_SUMMARY\n  < J_ITERSUMMARY < J_DETAILED < J_VECTOR < J_MATRIX < J_ALL\n\nCategories (EJournalCategory): J_MAIN, J_LINE_SEARCH, J_LINEAR_ALGEBRA,\n  J_BARRIER_UPDATE, J_SOLVE_PD_SYSTEM, etc. plus J_USER1-J_USER17 for apps.\n\nEach Journal (console, file, stream) can have independent level settings\nper category. Printf-style and stream-style output both supported.", "see": ["IpIpoptApplication.hpp for setting up output journals", "IpOptionsList.hpp for print_level option"], "return": "the Journal pointer so you can set specific acceptance criteria, or NULL if there was a problem creating a new Journal.", "has_pass2": false}, "src/Common/IpLibraryLoader.hpp": {"path": "layer-2/Ipopt/src/Common/IpLibraryLoader.hpp", "filename": "IpLibraryLoader.hpp", "file": "IpLibraryLoader.hpp", "brief": "Dynamic library loading for runtime solver selection\n\nLibraryLoader provides platform-independent dynamic library loading:\n- dlopen()/dlsym() on POSIX systems\n- LoadLibrary()/GetProcAddress() on Windows\n\nUsed to load linear solver libraries (HSL, Pardiso, etc.) at runtime\nwithout compile-time linkage. Allows users to provide proprietary\nsolvers without recompiling Ipopt.\n\nIPOPT_SHAREDLIBEXT defined to \"so\", \"dylib\", or \"dll\" per platform.\n\n@since 3.14.0", "see": ["Linear solver interface files for usage examples"], "has_pass2": false}, "src/Common/IpCachedResults.hpp": {"path": "layer-2/Ipopt/src/Common/IpCachedResults.hpp", "filename": "IpCachedResults.hpp", "file": "IpCachedResults.hpp", "brief": "Template class for caching computed results with dependency tracking\n\nCachedResults<T> stores computed values along with their dependencies\n(TaggedObjects and scalar parameters). Results are automatically\ninvalidated when any dependency changes via the Observer pattern.\n\nKey features:\n- LRU-style cache with configurable maximum size\n- Automatic staleness detection via TaggedObject notifications\n- Support for multiple dependency types (objects, scalars)\n- Thread-safe invalidation of dependent results\n\nTypical usage: IpoptCalculatedQuantities stores gradients, Hessians,\nand constraint values, recomputing only when the iterate changes.", "see": ["IpTaggedObject.hpp for change-tracking base class", "IpIpoptCalculatedQuantities.hpp for primary user of this class"], "return": "true, if the result was found", "has_pass2": false}, "src/Common/IpRegOptions.hpp": {"path": "layer-2/Ipopt/src/Common/IpRegOptions.hpp", "filename": "IpRegOptions.hpp", "file": "IpRegOptions.hpp", "brief": "Option registration, validation, and documentation system\n\nProvides infrastructure for declaring algorithm options with:\n- Type information (OT_Number, OT_Integer, OT_String)\n- Valid ranges (bounds for numerics, allowed values for strings)\n- Default values\n- Documentation strings for help generation\n- Categories for organizing options in documentation\n\nRegisteredOptions aggregates all RegisteredOption instances and\nprovides lookup, validation, and documentation generation.\n\nEach Ipopt component registers its options in RegisterAllOptions()\nduring initialization.", "see": ["IpOptionsList.hpp for runtime option storage"], "return": "NULL, if the option does not exist", "has_pass2": false}, "src/Common/IpObserver.hpp": {"path": "layer-2/Ipopt/src/Common/IpObserver.hpp", "filename": "IpObserver.hpp", "file": "IpObserver.hpp", "brief": "Observer design pattern implementation for change notification\n\nImplements Subject/Observer pattern for automatic cache invalidation.\nTaggedObjects (vectors, matrices) act as Subjects; dependent computed\nquantities (CachedResults) act as Observers. When a Subject changes,\nall attached Observers are notified to invalidate their cached values.\n\nNotification types:\n- NT_Changed: Subject data has been modified\n- NT_BeingDestroyed: Subject is being deleted\n- NT_All: Receive all notification types", "see": ["IpTaggedObject.hpp for tagged subjects", "IpCachedResults.hpp for cached observers"], "has_pass2": false}, "src/Common/IpDebug.hpp": {"path": "layer-2/Ipopt/src/Common/IpDebug.hpp", "filename": "IpDebug.hpp", "file": "IpDebug.hpp", "brief": "Debug assertion and verbose tracing macros\n\nProvides compile-time controlled debugging facilities:\n\nIPOPT_CHECKLEVEL > 0 enables:\n- DBG_ASSERT(cond): Runtime assertions\n- DBG_ASSERT_EXCEPTION(cond, type, msg): Conditional exceptions\n- DBG_DO(cmd): Execute only in debug mode\n\nIPOPT_VERBOSITY >= 1 enables tracing:\n- DBG_START_FUN/DBG_START_METH: Trace function entry/exit\n- DBG_PRINT: Printf-style debug output with indentation\n- DBG_EXEC: Conditional execution based on verbosity\n\nAll macros compile to nothing when disabled (zero overhead).", "see": ["IpJournalist.hpp for production-level logging"], "has_pass2": false}, "src/Algorithm/IpLeastSquareMults.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLeastSquareMults.hpp", "filename": "IpLeastSquareMults.hpp", "file": "IpLeastSquareMults.hpp", "brief": "Least-squares estimation of equality constraint multipliers\n\nLeastSquareMultipliers computes initial estimates for the equality\nconstraint multipliers y_c and y_d by solving a least-squares problem.\n\nFormulation: Find y minimizing ||∇_x L(x,y)||^2 where\n  ∇_x L = ∇f(x) + J_c^T y_c + J_d^T y_d - z_L + z_U\n\nThis is equivalent to solving the normal equations:\n  [J_c J_c^T    0    ] [y_c]   [-J_c (∇f - z_L + z_U)]\n  [   0     J_d J_d^T] [y_d] = [-J_d (∇f - z_L + z_U)]\n\nActually solved via augmented system:\n  [0  J_c^T  J_d^T] [r  ]   [∇f - z_L + z_U]\n  [J_c  0     0   ] [y_c] = [     0        ]\n  [J_d  0     0   ] [y_d]   [     0        ]\n\nUses AugSystemSolver to solve the linear system with W=0.\n\nUsage:\n- DefaultIterateInitializer: Initial multiplier estimates\n- MinC_1NrmRestorationPhase: Post-restoration multiplier reset", "see": ["IpEqMultCalculator.hpp for base interface", "IpAugSystemSolver.hpp for the linear solver", "IpDefaultIterateInitializer.hpp for initialization usage"], "return": "false, if the least square system could not be solved (the linear system is singular)", "has_pass2": false}, "src/Algorithm/IpGenAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpGenAugSystemSolver.hpp", "filename": "IpGenAugSystemSolver.hpp", "file": "IpGenAugSystemSolver.hpp", "brief": "Augmented system solver using GenKKTSolverInterface\n\nGenAugSystemSolver adapts the AugSystemSolver interface to use\nGenKKTSolverInterface, which provides a more generic linear solver\ninterface supporting iterative methods.\n\nThis class:\n- Extracts raw Number* arrays from Vector objects\n- Passes Matrix objects directly from the NLP\n- Manages caching to avoid redundant matrix updates\n\nMultiSolve() implementation:\n1. Check if augmented system matrices have changed (via tags)\n2. If changed, update solver_interface_ with new matrices\n3. Extract RHS vectors to raw arrays\n4. Call solver_interface_->Solve()\n5. Copy solutions back to Vector objects\n\nTag-based caching tracks:\n- W matrix and W_factor\n- Diagonal matrices D_x, D_s, D_c, D_d\n- Jacobians J_c, J_d\n- Regularization deltas\n\nInertia and quality:\n- NumberOfNegEVals(): Query solver for eigenvalue count\n- ProvidesInertia(): Check if solver supports this\n- IncreaseQuality(): Request better pivoting/tolerance", "see": ["IpAugSystemSolver.hpp for base interface", "IpGenKKTSolverInterface.hpp for the solver interface"], "return": "the number of negative eigenvalues of the most recent factorized matrix.", "has_pass2": false}, "src/Algorithm/IpLineSearch.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLineSearch.hpp", "filename": "IpLineSearch.hpp", "file": "IpLineSearch.hpp", "brief": "Strategy interface for globalization via line search\n\nLineSearch is the abstract base for all line search strategies\nin Ipopt's globalization framework. Given a search direction\n(from IpData.delta()), finds an acceptable trial point.\n\nKey methods:\n- FindAcceptableTrialPoint(): Main entry - searches along direction\n- Reset(): Called when barrier parameter changes\n- ActivateFallbackMechanism(): Triggers restoration phase\n- SetRigorousLineSearch(): Controls acceptance strictness\n\nImplementations:\n- BacktrackingLineSearch: Filter or merit function with backtracking\n- FilterLineSearch: Filter-based acceptance (Wächter-Biegler)", "see": ["IpBacktrackingLineSearch.hpp for the main implementation", "IpFilterLSAcceptor.hpp for filter-based acceptance", "IpRestoPhase.hpp for the fallback restoration mechanism"], "return": "false, if no fallback mechanism is available.", "has_pass2": false}, "src/Algorithm/IpLimMemQuasiNewtonUpdater.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLimMemQuasiNewtonUpdater.hpp", "filename": "IpLimMemQuasiNewtonUpdater.hpp", "file": "IpLimMemQuasiNewtonUpdater.hpp", "brief": "Limited-memory quasi-Newton Hessian approximation (L-BFGS/SR1)\n\nLimMemQuasiNewtonUpdater maintains a low-rank approximation of the\nLagrangian Hessian using limited-memory BFGS or SR1 updates.\n\nCompact representation: W = σI + V*M^(-1)*V^T where\n- σ: Scalar initialization factor (B0_ if diagonal)\n- V, U: Low-rank update matrices derived from S, Y history\n- S_: Matrix of step vectors s_k = x_{k+1} - x_k\n- Y_: Matrix of gradient differences y_k = ∇L_{k+1} - ∇L_k\n\nUpdate types (limited_memory_update_type_):\n- BFGS: Positive definite secant update, may skip if s'y <= 0\n- SR1: Symmetric rank-1, can capture negative curvature\n\nInitialization (limited_memory_initialization_):\n- SCALAR1-4: Various heuristics for σ based on y'y/s'y\n- CONSTANT: Fixed σ = limited_memory_init_val_\n\nKey parameters:\n- limited_memory_max_history_: Maximum stored (s,y) pairs\n- limited_memory_max_skipping_: Reset after N consecutive skips\n- sigma_safe_min/max_: Safeguards for initialization factor\n\nRestoration phase (update_for_resto_):\n- Structured update accounting for η*D_r*x quadratic term\n- Ypart_ stores constraint-only gradient differences", "see": ["IpHessianUpdater.hpp for base interface", "IpExactHessianUpdater.hpp for exact alternative", "IpLowRankUpdateSymMatrix.hpp for matrix representation"], "return": "true, if no update is to be performed this time.", "has_pass2": false}, "src/Algorithm/IpExactHessianUpdater.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpExactHessianUpdater.hpp", "filename": "IpExactHessianUpdater.hpp", "file": "IpExactHessianUpdater.hpp", "brief": "Hessian updater using exact second derivatives from NLP\n\nExactHessianUpdater is a trivial HessianUpdater implementation that\nsimply retrieves the exact Hessian of the Lagrangian from the NLP\nat each iteration.\n\nThe Hessian is:\n  W = ∇²f(x) + Σ y_c[i] * ∇²c_i(x) + Σ y_d[j] * ∇²d_j(x)\n\nUpdateHessian() method:\n- Calls IpoptNLP::h() with current x and multipliers\n- Stores result in IpData for use by PDSystemSolver\n\nRequirements:\n- User NLP must provide eval_h callback (TNLP) or h() method\n- Option hessian_approximation must be \"exact\"\n\nAdvantages over quasi-Newton:\n- Quadratic local convergence rate\n- Accurate curvature for non-convex problems\n- Better inertia detection\n\nDisadvantages:\n- User must implement second derivatives\n- More function evaluations per iteration\n- Hessian evaluation can be expensive", "see": ["IpHessianUpdater.hpp for the base interface", "IpLimMemQuasiNewtonUpdater.hpp for L-BFGS alternative"], "has_pass2": false}, "src/Algorithm/IpProbingMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpProbingMuOracle.hpp", "filename": "IpProbingMuOracle.hpp", "file": "IpProbingMuOracle.hpp", "brief": "Mehrotra's probing heuristic for barrier parameter selection\n\nProbingMuOracle implements Mehrotra's predictor-corrector approach\nto compute the barrier parameter mu. This is the strategy used in\nmany interior point codes like LOQO and PCx.\n\nAlgorithm:\n1. Compute affine scaling direction (predictor step) with sigma=0\n2. Find maximum step to boundary (alpha_aff)\n3. Compute complementarity after affine step: mu_aff\n4. Compute centering parameter: sigma = (mu_aff/mu)^3\n5. Use sigma in corrector step\n\nThe centering parameter sigma controls the balance between:\n- sigma=0: Pure affine scaling (aggressive, may overshoot)\n- sigma=1: Pure centering (conservative, slow progress)\n\nKey parameter:\n- sigma_max_: Upper bound on sigma to prevent excessive centering", "see": ["IpMuOracle.hpp for base interface", "IpQualityFunctionMuOracle.hpp for alternative mu strategy"], "has_pass2": false}, "src/Algorithm/IpMonotoneMuUpdate.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpMonotoneMuUpdate.hpp", "filename": "IpMonotoneMuUpdate.hpp", "file": "IpMonotoneMuUpdate.hpp", "brief": "Standard monotone barrier parameter update strategy\n\nMonotoneMuUpdate implements the classical IPM approach where the\nbarrier parameter mu is reduced monotonically as the subproblem\nconverges.\n\nUpdate rule:\n1. Solve barrier subproblem to tolerance barrier_tol_factor_ * mu\n2. Reduce mu: new_mu = max(mu_target_, min(kappa_mu * mu, mu^theta_mu))\n   where kappa_mu = mu_linear_decrease_factor_\n   and theta_mu = mu_superlinear_decrease_power_\n3. Update tau (fraction-to-boundary): tau = max(tau_min_, 1 - mu)\n\nKey parameters:\n- mu_init_: Initial barrier parameter\n- mu_linear_decrease_factor_: Linear decrease factor kappa_mu\n- mu_superlinear_decrease_power_: Superlinear power theta_mu\n- tau_min_: Minimum fraction-to-boundary parameter\n- mu_target_: Target mu for termination\n\nFast decrease heuristic:\n- mu_allow_fast_monotone_decrease_: Allow faster decrease when\n  complementarity is already small\n\nInteractions:\n- Calls linesearch_->Reset() when mu changes\n- Filter is cleared on barrier parameter update", "see": ["IpMuUpdate.hpp for the base interface", "IpAdaptiveMuUpdate.hpp for non-monotone alternative"], "has_pass2": false}, "src/Algorithm/IpUserScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpUserScaling.hpp", "filename": "IpUserScaling.hpp", "file": "IpUserScaling.hpp", "brief": "NLP scaling using user-provided scaling factors\n\nUserScaling obtains scaling factors directly from the NLP interface\nvia the get_scaling_parameters callback (TNLP) or GetScalingParameters\nmethod (NLP).\n\nThis allows users to specify problem-specific scaling based on:\n- Prior knowledge of variable magnitudes\n- Physical units of constraints\n- Problem structure\n\nScaling factors:\n- df: Objective function scaling\n- dx: Variable scaling (per-variable)\n- dc: Equality constraint scaling (per-constraint)\n- dd: Inequality constraint scaling (per-constraint)\n\nThe NLP should implement get_scaling_parameters() returning:\n- use_x_scaling, use_g_scaling flags\n- obj_scaling factor\n- x_scaling array (length n)\n- g_scaling array (length m)\n\nNote: If user doesn't provide scaling, falls back to no scaling.", "see": ["IpNLPScaling.hpp for scaling framework", "IpGradientScaling.hpp for automatic scaling"], "has_pass2": false}, "src/Algorithm/IpOrigIterationOutput.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpOrigIterationOutput.hpp", "filename": "IpOrigIterationOutput.hpp", "file": "IpOrigIterationOutput.hpp", "brief": "Standard iteration output for original NLP problem\n\nOrigIterationOutput displays the per-iteration summary line\nfor the original (non-restoration) NLP problem.\n\nOutput format (one line per iteration):\n  iter  objective  inf_pr  inf_du  lg(mu)  ||d||  lg(rg)  alpha_du  alpha_pr  ls\n\nWhere:\n- iter: Iteration number\n- objective: Current objective value (scaled)\n- inf_pr: Primal infeasibility (see inf_pr_output_)\n- inf_du: Dual infeasibility\n- lg(mu): Log10 of barrier parameter\n- ||d||: Norm of search direction\n- lg(rg): Log10 of regularization delta_x\n- alpha_du: Dual step size\n- alpha_pr: Primal step size\n- ls: Line search trials\n\nConfiguration:\n- print_info_string_: Append info character (e.g., 'r' for resto)\n- inf_pr_output_: What to show in inf_pr column\n- print_frequency_iter_: Print every N iterations\n- print_frequency_time_: Print every T seconds", "see": ["IpIterationOutput.hpp for base interface", "IpRestoIterationOutput.hpp for restoration phase output"], "has_pass2": false}, "src/Algorithm/IpRestoRestoPhase.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoRestoPhase.hpp", "filename": "IpRestoRestoPhase.hpp", "file": "IpRestoRestoPhase.hpp", "brief": "Recursive restoration for separable n,p variable initialization\n\nRestoRestorationPhase provides a specialized \"restoration within\nrestoration\" procedure for the MinC_1NrmRestorationPhase. It computes\noptimal values for the slack variables (n_c, p_c, n_d, p_d) by\ntreating them as separable from x and s.\n\nThe restoration feasibility problem has structure:\n  min ||p + n||_1  s.t.  c(x) + n - p = 0\n\nFor fixed x, optimal n and p can be computed by solving a quadratic\nequation v^2 + 2a*v - b = 0 element-wise, where:\n- a relates to constraint values\n- b relates to complementarity conditions\n\nThis avoids full nested optimization when only n,p need updating.", "see": ["IpRestoMinC_1Nrm.hpp for main restoration phase", "IpRestoPhase.hpp for base restoration interface"], "has_pass2": false}, "src/Algorithm/IpIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIterateInitializer.hpp", "filename": "IpIterateInitializer.hpp", "file": "IpIterateInitializer.hpp", "brief": "Strategy interface for computing initial iterates\n\nIterateInitializer is the abstract base for strategies that\ncompute the starting point (x, s, y_c, y_d, z_L, z_U, v_L, v_U)\nfor the interior point algorithm.\n\nInitialization tasks:\n- Project x into bounds (push away from bounds by slack_bound_push)\n- Initialize slack variables s from d(x)\n- Compute initial dual estimates (y_c, y_d) via least squares\n- Initialize bound multipliers (z, v) from complementarity\n\nImplementations:\n- DefaultIterateInitializer: Standard cold-start initialization\n- WarmStartIterateInitializer: Uses provided starting point", "see": ["IpDefaultIterateInitializer.hpp for standard initialization", "IpWarmStartIterateInitializer.hpp for warm starts"], "has_pass2": false}, "src/Algorithm/IpMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpMuOracle.hpp", "filename": "IpMuOracle.hpp", "file": "IpMuOracle.hpp", "brief": "Strategy interface for suggesting barrier parameter values\n\nMuOracle is the abstract interface for components that compute\nsuggested values for the barrier parameter mu in adaptive (non-monotone)\nbarrier updates.\n\nInterface:\n- CalculateMu(mu_min, mu_max, new_mu): Compute suggested mu in [mu_min, mu_max]\n- Returns false if suggestion cannot be computed (e.g., linear solve fails)\n\nImplementations:\n- QualityFunctionMuOracle: Minimize quality function over sigma\n- ProbingMuOracle: Try candidate mu values and select best\n- LoqoMuOracle: LOQO-style adaptive rule\n\nUsage in AdaptiveMuUpdate:\n- free_mu_oracle_: Computes mu in free (non-monotone) mode\n- fix_mu_oracle_: Optional, computes mu when switching to fixed mode\n\nThe oracle is called each iteration in free mode. If it returns\nfalse or the suggested mu doesn't satisfy globalization, the\nalgorithm may switch to fixed (monotone) mode.", "see": ["IpAdaptiveMuUpdate.hpp for usage", "IpQualityFunctionMuOracle.hpp for quality function approach"], "has_pass2": false}, "src/Algorithm/IpRestoIpoptNLP.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoIpoptNLP.hpp", "filename": "IpRestoIpoptNLP.hpp", "file": "IpRestoIpoptNLP.hpp", "brief": "IpoptNLP adapter for restoration phase feasibility problem\n\nRestoIpoptNLP transforms the original NLP into a feasibility problem\nfor the restoration phase. It introduces slack variables to allow\nconstraint violations and penalizes them in the objective.\n\nVariable structure (CompoundVector):\n- x: Original primal variables\n- p_c, n_c: Positive/negative slacks for equality constraints\n- p_d, n_d: Positive/negative slacks for inequality constraints\n\nObjective function:\n  f_resto = ρ * (e'p_c + e'n_c + e'p_d + e'n_d)\n          + (η/2) * ||D_r(x - x_ref)||_2^2\nWhere η = η_factor * μ^η_mu_exponent (mu-dependent)\n\nModified constraints:\n- c(x) - p_c + n_c = 0  (equality)\n- d(x) - p_d + n_d (bounded by d_L, d_U)\n\nMatrix structure:\n- Jacobians: Block structure with original Jac and identity blocks\n- Hessian: Original Hessian + diagonal D_r^2 for proximity term\n\nKey methods:\n- f(), grad_f(): Depend on mu (objective_depends_on_mu() = true)\n- Eta(): Computes proximity weight from current mu\n- DR_x(): Scaling factors 1/max(1, |x_ref_i|)", "see": ["IpRestoMinC_1Nrm.hpp for the restoration phase algorithm", "IpIpoptNLP.hpp for the base interface"], "has_pass2": false}, "src/Algorithm/IpFilter.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpFilter.hpp", "filename": "IpFilter.hpp", "file": "IpFilter.hpp", "brief": "Multi-dimensional filter for globalization (Wächter-Biegler)\n\nThe Filter implements the filter globalization mechanism where a\npoint is acceptable if it improves in at least one of multiple\nobjectives (typically constraint violation θ and objective φ).\n\nFilter acceptance rule: A point (θ, φ) is acceptable to the filter\nif for every entry (θ_i, φ_i) in the filter, either θ < θ_i or φ < φ_i.\n\nKey concepts:\n- FilterEntry: Single entry storing coordinates and iteration number\n- Dominated entries are removed when new entries are added\n- Margin parameters (gamma_theta, gamma_phi) provide sufficient decrease\n\nClasses:\n- FilterEntry: Single point in the filter with coordinates\n- Filter: Container managing list of entries with add/query operations\n\nUsed by FilterLSAcceptor for filter line search acceptance tests.", "see": ["IpFilterLSAcceptor.hpp for filter line search usage", "IpBacktrackingLineSearch.hpp for the line search framework"], "return": "true, if pair is acceptable", "has_pass2": false}, "src/Algorithm/IpIpoptNLP.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptNLP.hpp", "filename": "IpIpoptNLP.hpp", "file": "IpIpoptNLP.hpp", "brief": "Internal NLP interface with caching and scaling\n\nIpoptNLP abstracts the optimization problem for internal use:\n  min f(x)  s.t.  c(x) = 0, d_L <= d(x) <= d_U, x_L <= x <= x_U\n\nProvides:\n- f(), grad_f(), c(), jac_c(), d(), jac_d(), h() - NLP evaluations\n- Bounds: x_L, x_U, d_L, d_U with expansion matrices Px_L, Px_U, etc.\n- Scaling via NLPScalingObject\n- Caching of evaluated quantities\n- Barrier-dependent objectives (for restoration phase)\n\nImplementations: OrigIpoptNLP (standard), RestoIpoptNLP (restoration)", "see": ["IpOrigIpoptNLP.hpp for standard implementation", "IpTNLPAdapter.hpp for user interface adapter", "IpNLPScaling.hpp for scaling strategies"], "has_pass2": false}, "src/Algorithm/IpConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpConvCheck.hpp", "filename": "IpConvCheck.hpp", "file": "IpConvCheck.hpp", "brief": "Strategy interface for checking algorithm termination\n\nConvergenceCheck is the abstract base for convergence testing\nstrategies. Called each iteration to determine if optimization\nshould continue, has converged, or has failed.\n\nConvergenceStatus enum values:\n- CONTINUE: Keep iterating\n- CONVERGED: Optimal solution found (tol satisfied)\n- CONVERGED_TO_ACCEPTABLE_POINT: Acceptable solution (acceptable_tol)\n- MAXITER_EXCEEDED, CPUTIME_EXCEEDED, WALLTIME_EXCEEDED: Limits hit\n- DIVERGING: Objective unbounded or iterates diverging\n- USER_STOP: User callback requested termination\n- FAILED: Unrecoverable error\n\nAlso provides CurrentIsAcceptable() for early termination logic.", "see": ["IpOptErrorConvCheck.hpp for the main implementation", "IpRestoConvCheck.hpp for restoration phase convergence"], "has_pass2": false}, "src/Algorithm/IpWarmStartIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpWarmStartIterateInitializer.hpp", "filename": "IpWarmStartIterateInitializer.hpp", "file": "IpWarmStartIterateInitializer.hpp", "brief": "Warm start initialization from previous solution\n\nWarmStartIterateInitializer initializes IPM iterates from a\npreviously computed solution, enabling faster convergence for\nrelated problems (e.g., MPC, parametric optimization).\n\nWarm start sources:\n- warm_start_entire_iterate_: Use GetWarmStartIterate() from NLP\n- Otherwise: Use initialization vectors from NLP\n\nProcessing steps:\n1. Push primals away from bounds (warm_start_bound_push/frac_)\n2. Push slacks (warm_start_slack_bound_push/frac_)\n3. Clip multipliers (warm_start_mult_init_max_)\n4. Ensure bound multipliers positive (warm_start_mult_bound_push_)\n\nTarget mu adjustment (warm_start_target_mu_):\n- Adjusts slack/multiplier pairs toward target complementarity\n- process_target_mu(): Scales to achieve s*z ≈ target_mu\n- adapt_to_target_mu(): Fine-tunes pairing\n\nKey parameters:\n- warm_start_bound_push_: Absolute bound push\n- warm_start_bound_frac_: Relative bound push\n- warm_start_mult_init_max_: Maximum multiplier magnitude\n- warm_start_target_mu_: Target barrier parameter", "see": ["IpIterateInitializer.hpp for the base interface", "IpDefaultIterateInitializer.hpp for cold start"], "has_pass2": false}, "src/Algorithm/IpEqMultCalculator.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpEqMultCalculator.hpp", "filename": "IpEqMultCalculator.hpp", "file": "IpEqMultCalculator.hpp", "brief": "Strategy interface for computing equality constraint multipliers\n\nEqMultiplierCalculator is the abstract base for computing estimates\nof the equality constraint multipliers y_c and y_d. These estimates\nare used for:\n- Initial multiplier estimation in DefaultIterateInitializer\n- Multiplier recalculation when recalc_y option is set\n- Computing multipliers for feasibility problems\n\nImplementations:\n- LeastSquareMultipliers: Solves min ||y||^2 s.t. KKT conditions\n  (requires solving augmented system)", "see": ["IpLeastSquareMults.hpp for the main implementation", "IpDefaultIterateInitializer.hpp for usage in initialization", "IpIpoptAlg.hpp for recalc_y usage"], "has_pass2": false}, "src/Algorithm/IpLowRankSSAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLowRankSSAugSystemSolver.hpp", "filename": "IpLowRankSSAugSystemSolver.hpp", "file": "IpLowRankSSAugSystemSolver.hpp", "brief": "Low-rank Hessian handling via single backsolve (iterative solver friendly)\n\nLowRankSSAugSystemSolver handles LowRankUpdateSymMatrix Hessians\n(from L-BFGS) by augmenting the system rather than using\nSherman-Morrison. This requires only ONE factorization/backsolve.\n\nL-BFGS Hessian: W = sigma*I + V*M*V^T (compact representation)\n\nStandard approach (LowRankAugSystemSolver):\n- Uses Sherman-Morrison formula\n- Requires multiple backsolves (one per rank)\n- Better for direct solvers\n\nThis approach (LowRankSSAugSystemSolver):\n- Augments the KKT system with V columns\n- Extends J_c with ExpandedMultiVectorMatrix rows\n- Single larger factorization\n- Better for iterative solvers\n\nSystem augmentation:\n- J_c_ext_ = [J_c; V^T] (extended constraint Jacobian)\n- D_c_ext_ includes regularization for V rows\n- y_c_ext_space_ for extended multiplier space", "see": ["IpLowRankAugSystemSolver.hpp for Sherman-Morrison version", "IpLimMemQuasiNewtonUpdater.hpp for L-BFGS Hessian", "ProvidesInertia)."], "return": "the number of negative eigenvalues of the most recent factorized matrix", "has_pass2": false}, "src/Algorithm/IpLoqoMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLoqoMuOracle.hpp", "filename": "IpLoqoMuOracle.hpp", "file": "IpLoqoMuOracle.hpp", "brief": "LOQO formula for barrier parameter selection\n\nLoqoMuOracle computes the barrier parameter using the heuristic\nfrom the LOQO solver (Vanderbei). This provides a simple formula\nbased on current complementarity.\n\nThe LOQO formula typically uses:\n  sigma = min(0.1, 100*mu)\n  mu_new = sigma * (current_complementarity / n)\n\nThis is a simple, stateless oracle that doesn't require solving\nan additional linear system (unlike probing or quality function).\n\nCompared to other strategies:\n- ProbingMuOracle: Requires affine step computation\n- QualityFunctionMuOracle: Requires 1D optimization\n- LoqoMuOracle: Direct formula, no extra computation", "see": ["IpMuOracle.hpp for base interface", "IpProbingMuOracle.hpp for Mehrotra's predictor-corrector"], "has_pass2": false}, "src/Algorithm/IpPDSearchDirCalc.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDSearchDirCalc.hpp", "filename": "IpPDSearchDirCalc.hpp", "file": "IpPDSearchDirCalc.hpp", "brief": "Newton search direction computation via primal-dual system\n\nPDSearchDirCalculator is the standard SearchDirectionCalculator\nimplementation that computes the Newton step by solving the\nprimal-dual KKT system.\n\nComputeSearchDirection():\n1. Assembles right-hand side from current residuals\n2. Calls PDSystemSolver::Solve() to get direction\n3. Stores result in IpData::delta()\n\nAlgorithm options:\n- fast_step_computation_: Skip residual verification (trust solver)\n- mehrotra_algorithm_: Mehrotra predictor-corrector mode\n  - Forces specific settings for adaptive mu\n  - Disables various globalization features\n\nThe PDSolver() accessor allows other components (e.g., second-order\ncorrection in line search) to compute additional Newton solves.\n\nDirection stored in delta:\n- delta_x, delta_s: Primal steps\n- delta_y_c, delta_y_d: Equality multiplier steps\n- delta_z_L, delta_z_U, delta_v_L, delta_v_U: Bound multiplier steps", "see": ["IpSearchDirCalculator.hpp for base interface", "IpPDSystemSolver.hpp for the system solver interface", "IpPDFullSpaceSolver.hpp for full-space implementation"], "has_pass2": false}, "src/Algorithm/IpGradientScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpGradientScaling.hpp", "filename": "IpGradientScaling.hpp", "file": "IpGradientScaling.hpp", "brief": "NLP scaling based on gradient magnitudes at initial point\n\nGradientScaling computes scaling factors for the NLP based on the\nmaximum gradient norms at the user-provided starting point. This\nimproves problem conditioning by normalizing objective and constraint\nmagnitudes.\n\nScaling computation:\n- Evaluate gradients at x0\n- s_f = 1 / max(1, ||∇f||_∞ / target_grad)\n- s_c[i] = 1 / max(1, ||∇c_i||_∞ / target_grad)\n- s_d[i] = 1 / max(1, ||∇d_i||_∞ / target_grad)\n\nKey parameters:\n- scaling_max_gradient_: Skip scaling if gradients below this\n- scaling_obj_target_gradient_: Target norm for objective gradient\n- scaling_constr_target_gradient_: Target norm for constraint gradients\n- scaling_min_value_: Lower bound on scaling factors\n\nNote: Variable scaling (d_x) is not computed by this class;\nuses identity scaling for x.", "see": ["IpNLPScaling.hpp for the scaling framework", "IpEquilibrationScaling.hpp for MC19-based alternative"], "has_pass2": false}, "src/Algorithm/IpIpoptCalculatedQuantities.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptCalculatedQuantities.hpp", "filename": "IpIpoptCalculatedQuantities.hpp", "file": "IpIpoptCalculatedQuantities.hpp", "brief": "Cached computation of all derived quantities from iterates\n\nIpoptCalculatedQuantities is the central caching layer for computed\nvalues derived from the current/trial iterates in IpoptData:\n\nCached quantities include:\n- Slacks: slack_x_L, slack_x_U, slack_s_L, slack_s_U\n- Objective: f, grad_f, barrier_obj, grad_barrier_obj\n- Constraints: c, d, d-s, jac_c, jac_d, J^T*y products\n- Primal-dual: grad_lag_x/s, complementarity, relaxed_compl\n- Errors: primal/dual infeasibility, complementarity, nlp_error\n- Line search: frac_to_bound, sigma matrices\n\nAll methods check dependencies via CachedResults and recompute\nonly when inputs have changed (tracked via TaggedObject tags).", "see": ["IpIpoptData.hpp for the raw iterate storage", "IpIpoptNLP.hpp for NLP function evaluations", "IpCachedResults.hpp for the caching mechanism"], "return": "number of corrected slacks", "has_pass2": false}, "src/Algorithm/IpHessianUpdater.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpHessianUpdater.hpp", "filename": "IpHessianUpdater.hpp", "file": "IpHessianUpdater.hpp", "brief": "Strategy interface for Hessian computation/approximation\n\nHessianUpdater is the abstract base for strategies that provide\nthe Hessian of the Lagrangian (or an approximation) to the algorithm.\nThe result is stored in IpData.W().\n\nOptions:\n- Exact Hessian: Uses second derivatives from NLP (hessian_approximation=exact)\n- Limited-memory BFGS: L-BFGS approximation (hessian_approximation=limited-memory)\n\nFor quasi-Newton methods, the updater maintains history of\n{s_k, y_k} pairs and computes B_k or H_k approximations.\n\nImplementations:\n- ExactHessianUpdater: Evaluates exact Hessian from NLP\n- LimMemQuasiNewtonUpdater: L-BFGS or L-SR1 approximation", "see": ["IpExactHessianUpdater.hpp for exact Hessian", "IpLimMemQuasiNewtonUpdater.hpp for quasi-Newton", "IpLowRankUpdateSymMatrix.hpp for L-BFGS matrix representation"], "has_pass2": false}, "src/Algorithm/IpRestoMinC_1Nrm.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoMinC_1Nrm.hpp", "filename": "IpRestoMinC_1Nrm.hpp", "file": "IpRestoMinC_1Nrm.hpp", "brief": "Restoration phase minimizing 1-norm of constraint violation\n\nMinC_1NrmRestorationPhase is the main restoration phase implementation.\nWhen the line search cannot make progress, it minimizes constraint\nviolation to find a feasible point from which optimization can continue.\n\nRestoration NLP formulation:\n  min  ρ * ||[p_c; n_c; p_d; n_d]||_1 + (η/2) * ||D_r(x - x_ref)||_2^2\n  s.t. c(x) - p_c + n_c = 0\n       d_L <= d(x) - p_d + n_d <= d_U\n       x_L <= x <= x_U\n       p_c, n_c, p_d, n_d >= 0\n\nWhere:\n- ρ: Penalty on infeasibility (resto_penalty_parameter)\n- η: Proximity weight (resto_proximity_weight * sqrt(mu))\n- D_r: Diagonal scaling based on reference point\n- x_ref: Starting point for restoration\n\nKey behaviors:\n- Uses nested IpoptAlgorithm to solve restoration NLP\n- eq_mult_calculator_ reinitializes multipliers after restoration\n- bound_mult_reset_threshold_: Limits post-restoration bound multipliers\n- count_restorations_: Tracks restoration phase calls", "see": ["IpRestoPhase.hpp for the base interface", "IpRestoIpoptNLP.hpp for the restoration NLP formulation", "IpRestoFilterConvCheck.hpp for restoration convergence criteria"], "has_pass2": false}, "src/Algorithm/IpBacktrackingLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpBacktrackingLSAcceptor.hpp", "filename": "IpBacktrackingLSAcceptor.hpp", "file": "IpBacktrackingLSAcceptor.hpp", "brief": "Strategy interface for step acceptance in backtracking line search\n\nBacktrackingLSAcceptor defines the interface for step acceptance\ntests used by BacktrackingLineSearch. Implementations decide whether\na trial point should be accepted based on various criteria.\n\nKey responsibilities:\n- CheckAcceptabilityOfTrialPoint(): Main acceptance test\n- CalculateAlphaMin(): Minimum step size before restoration phase\n- TrySecondOrderCorrection(): Improve constraint satisfaction\n- TryCorrector(): Higher-order corrector for fast local convergence\n- PrepareRestoPhaseStart(): Augment filter before restoration\n\nImplementations:\n- FilterLSAcceptor: Wächter-Biegler filter method\n- PenaltyLSAcceptor: Penalty function approach\n\nWatchdog support:\n- StartWatchDog()/StopWatchDog(): Store/restore acceptor state\n- InitThisLineSearch(): Called at start of each line search", "see": ["IpBacktrackingLineSearch.hpp for the line search framework", "IpFilterLSAcceptor.hpp for filter-based implementation", "IpFilter.hpp for filter data structure"], "return": "a character for the info_alpha_primal_char field in IpData", "has_pass2": false}, "src/Algorithm/IpPenaltyLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPenaltyLSAcceptor.hpp", "filename": "IpPenaltyLSAcceptor.hpp", "file": "IpPenaltyLSAcceptor.hpp", "brief": "Penalty function line search using Waltz-Morales-Nocedal-Orban method\n\nPenaltyLSAcceptor implements exact penalty function globalization\nas an alternative to filter line search. Based on the paper by\nWaltz, Morales, Nocedal, and Orban.\n\nMerit function: phi_nu(x) = f(x) + nu * ||c(x)||_1\n\nAcceptance criterion:\n  phi_nu(x + alpha*d) <= phi_nu(x) + eta * alpha * pred\nwhere pred is the predicted reduction.\n\nPenalty parameter nu:\n- Automatically updated to ensure descent direction\n- Increased by nu_inc_ when direction is not descent\n- nu_init_ provides starting value\n\nKey parameters:\n- nu_init_, nu_inc_: Penalty parameter initialization/increment\n- eta_: Sufficient decrease parameter (Armijo)\n- rho_: Parameter for predicted reduction calculation\n- max_soc_: Maximum second order correction steps", "see": ["IpFilterLSAcceptor.hpp for filter-based alternative", "IpBacktrackingLSAcceptor.hpp for base interface"], "param": ["alpha_primal_test value of alpha that has been used for in the acceptence test ealier"], "has_pass2": false}, "src/Algorithm/IpRestoFilterConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoFilterConvCheck.hpp", "filename": "IpRestoFilterConvCheck.hpp", "file": "IpRestoFilterConvCheck.hpp", "brief": "Restoration phase convergence check for filter line search\n\nRestoFilterConvergenceCheck specializes restoration phase termination\nfor use with the filter globalization mechanism. It checks whether\nthe current restoration point would be acceptable to the original\nproblem's filter.\n\nTermination criteria (via TestOrigProgress):\n1. Current point acceptable to original filter\n2. Current point shows sufficient decrease vs original iterate\n3. Constraint violation reduced below tolerance\n\nThe orig_filter_ls_acceptor_ reference is used to query:\n- IsAcceptableToCurrentIterate(): Armijo or θ-type improvement\n- IsAcceptableToCurrentFilter(): Point not dominated by filter\n\nImportant: SetOrigLSAcceptor() must be called before Initialize()\nto establish the link to the original problem's filter.\n\nNote: Uses raw pointer to avoid circular reference with line search.", "see": ["IpRestoConvCheck.hpp for the base restoration convergence class", "IpFilterLSAcceptor.hpp for filter acceptance methods", "IpRestoMinC_1Nrm.hpp for the restoration phase algorithm"], "has_pass2": false}, "src/Algorithm/IpBacktrackingLineSearch.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpBacktrackingLineSearch.hpp", "filename": "IpBacktrackingLineSearch.hpp", "file": "IpBacktrackingLineSearch.hpp", "brief": "General backtracking line search with filter/restoration support\n\nBacktrackingLineSearch is the main LineSearch implementation,\nproviding a flexible framework for globalization strategies:\n\nCore algorithm:\n1. Start with full step alpha = alpha_max (fraction-to-boundary)\n2. Test acceptability via BacktrackingLSAcceptor\n3. If rejected, reduce alpha *= alpha_red_factor and retry\n4. If alpha becomes too small, trigger restoration phase\n\nAdvanced features:\n- Watchdog mechanism: Accept poor steps temporarily to escape local minima\n- Second-order correction (SOC): Improve constraint satisfaction\n- Soft restoration phase: Accept steps that reduce primal-dual error\n- Magic steps: Improve slack/bound multiplier pairing\n- Corrector steps: Improve local convergence rate\n\nKey parameters:\n- alpha_for_y: How to step in equality multipliers\n- watchdog_trial_iter_max: Watchdog iteration limit\n- max_soc: Maximum second-order corrections", "see": ["IpLineSearch.hpp for the interface", "IpBacktrackingLSAcceptor.hpp for acceptance test strategies", "IpFilterLSAcceptor.hpp for filter-based acceptance", "IpRestoPhase.hpp for restoration phase"], "return": "false, if that is not possible.", "has_pass2": false}, "src/Algorithm/IpEquilibrationScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpEquilibrationScaling.hpp", "filename": "IpEquilibrationScaling.hpp", "file": "IpEquilibrationScaling.hpp", "brief": "NLP scaling using MC19 matrix equilibration\n\nEquilibrationScaling computes scaling factors using the HSL MC19\nsymmetric indefinite matrix equilibration routine. This produces\nwell-conditioned scaling by analyzing the Jacobian structure.\n\nAlgorithm:\n1. Perturb initial point randomly (point_perturbation_radius_)\n2. Evaluate Jacobians at perturbed points\n3. Build combined KKT-like matrix structure\n4. Apply MC19 to compute row/column scaling\n5. Extract scaling for x, c, d from equilibration\n\nMC19 routine (HSL):\n- Computes diagonal scaling D such that |D*A*D|_ij ≈ 1\n- Uses infinity-norm equilibration\n- Loaded dynamically via LibraryLoader\n\nPointPerturber helper class:\n- Generates random perturbations within bounds\n- Used to avoid evaluating at singular points\n- MakeNewPerturbedPoint(): Returns randomly perturbed x", "see": ["IpNLPScaling.hpp for the scaling framework", "IpGradientScaling.hpp for gradient-based alternative", "IpMc19TSymScalingMethod.hpp for MC19 interface"], "has_pass2": false}, "src/Algorithm/IpRestoPenaltyConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoPenaltyConvCheck.hpp", "filename": "IpRestoPenaltyConvCheck.hpp", "file": "IpRestoPenaltyConvCheck.hpp", "brief": "Restoration phase convergence for penalty line search\n\nRestoPenaltyConvergenceCheck extends RestoConvergenceCheck for use\nwhen the original algorithm uses penalty function globalization\n(as opposed to filter method).\n\nPurpose:\nDuring restoration phase, check if current point would be\nacceptable to the original penalty line search acceptor.\n\nTermination:\n- Restoration succeeds when IsAcceptableToCurrentIterate() returns\n  true for the original penalty acceptor\n- This tests if trial_barr and trial_theta would be accepted\n\nSetup requirement:\n- SetOrigLSAcceptor() must be called before Initialize()\n- Links to the PenaltyLSAcceptor from the original algorithm\n\nNote: Uses raw pointer (not SmartPtr) to avoid circular reference\nbetween restoration phase and original algorithm objects.", "see": ["IpRestoConvCheck.hpp for base restoration convergence", "IpRestoFilterConvCheck.hpp for filter-based restoration", "IpPenaltyLSAcceptor.hpp for penalty line search"], "has_pass2": false}, "src/Algorithm/IpRestoPhase.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoPhase.hpp", "filename": "IpRestoPhase.hpp", "file": "IpRestoPhase.hpp", "brief": "Strategy interface for restoration phase fallback\n\nRestorationPhase is the abstract base for the fallback mechanism\nwhen the line search cannot make progress. The restoration phase\nminimizes constraint violation to find a feasible point.\n\nTriggered when:\n- Line search step size becomes too small\n- No search direction can be computed (singular KKT system)\n- Algorithm explicitly requests fallback (ActivateFallbackMechanism)\n\nExceptions thrown on restoration failure:\n- RESTORATION_CONVERGED_TO_FEASIBLE_POINT: Success, found feasible point\n- RESTORATION_FAILED: Could not reduce infeasibility\n- RESTORATION_MAXITER_EXCEEDED: Hit iteration limit\n- RESTORATION_USER_STOP: User callback requested stop\n\nMain implementation: RestoIterationOutput minimizes ||c(x)||^2 + ||d(x)-s||^2\nusing the interior point method on a modified feasibility problem.", "see": ["IpRestoMinC_1Nrm.hpp for the main implementation", "IpRestoIpoptNLP.hpp for restoration phase NLP formulation", "IpBacktrackingLineSearch.hpp for restoration phase trigger"], "has_pass2": false}, "src/Algorithm/IpNLPScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpNLPScaling.hpp", "filename": "IpNLPScaling.hpp", "file": "IpNLPScaling.hpp", "brief": "NLP scaling framework for problem conditioning\n\nNLPScalingObject is the abstract base for applying diagonal scaling\nto the NLP to improve numerical conditioning. Transforms:\n  min s_f*f(S_x^{-1}*x̃)  s.t.  s_c*c(S_x^{-1}*x̃)=0, ...\n\nProvides methods to:\n- apply_obj_scaling / unapply_obj_scaling: Scale objective\n- apply_vector_scaling_x/c/d: Scale primal/constraint vectors\n- apply_jac_c/d_scaling: Scale Jacobians (row/col scaling)\n- apply_hessian_scaling: Scale Hessian\n\nClass hierarchy:\n- NLPScalingObject: Abstract interface\n- StandardScalingBase: Common implementation with scaling vectors\n- NoNLPScalingObject: Identity (no scaling)\n- GradientScaling: Scale by gradient norm (nlp_scaling_method=gradient-based)\n- EquilibrationScaling: Row/column equilibration\n\nScaling matrices are represented via ScaledMatrix/SymScaledMatrix.", "see": ["IpGradientScaling.hpp for gradient-based implementation", "IpEquilibrationScaling.hpp for equilibration", "IpScaledMatrix.hpp for scaled matrix representation"], "has_pass2": false}, "src/Algorithm/IpFilterLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpFilterLSAcceptor.hpp", "filename": "IpFilterLSAcceptor.hpp", "file": "IpFilterLSAcceptor.hpp", "brief": "Filter line search acceptor using Wächter-Biegler method\n\nFilterLSAcceptor implements the filter globalization strategy where\nstep acceptance is based on improvement in either constraint violation\n(theta) or barrier objective (phi).\n\nAcceptance criteria:\n- Armijo sufficient decrease in barrier function, OR\n- Sufficient reduction in constraint violation (switching condition)\n- Point must be acceptable to current filter\n\nKey parameters:\n- theta_max_fact_: Upper bound factor on infeasibility\n- gamma_phi_, gamma_theta_: Margin parameters for filter\n- eta_phi_: Armijo parameter\n- s_phi_, s_theta_: Exponents in switching condition\n\nSecond-order correction (SOC):\n- When step is rejected, solves for constraint linearization error\n- Up to max_soc_ corrections tried\n- kappa_soc_ controls required constraint reduction between SOCs\n\nCorrector steps:\n- Affine or primal-dual corrector for fast local convergence\n- Applied even when step is acceptable\n\nFilter management:\n- Dominated entries removed on filter augmentation\n- Filter reset heuristic when repeatedly rejected by filter", "see": ["IpBacktrackingLSAcceptor.hpp for the base interface", "IpFilter.hpp for the filter data structure", "IpBacktrackingLineSearch.hpp for usage"], "param": ["in_watchdog indicates if we are currently in an active watchdog procedure"], "has_pass2": false}, "src/Algorithm/IpRestoConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoConvCheck.hpp", "filename": "IpRestoConvCheck.hpp", "file": "IpRestoConvCheck.hpp", "brief": "Base class for restoration phase convergence checking\n\nRestoConvergenceCheck extends OptimalityErrorConvergenceCheck for\nthe restoration phase, adding termination criteria based on\nacceptability to the original problem's globalization mechanism.\n\nTermination conditions (checked in order):\n1. Standard optimality error convergence (inherited)\n2. Acceptable to original problem (via TestOrigProgress)\n3. Sufficient reduction in infeasibility (kappa_resto_)\n4. Iteration limits (maximum_iters_, maximum_resto_iters_)\n\nTestOrigProgress (pure virtual):\n- Implemented by subclasses for specific globalization\n- RestoFilterConvergenceCheck: Check filter acceptability\n- Returns CONVERGED if original algorithm can accept point\n\nKey parameters:\n- kappa_resto_: Required infeasibility reduction fraction\n- maximum_resto_iters_: Max successive restoration iterations\n- orig_constr_viol_tol_: Original constraint tolerance\n\nSetOrigLSAcceptor():\n- Must be called to link restoration check to original problem\n- Provides access to original filter/acceptor state", "see": ["IpOptErrorConvCheck.hpp for base convergence check", "IpRestoFilterConvCheck.hpp for filter implementation"], "has_pass2": false}, "src/Algorithm/IpDefaultIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpDefaultIterateInitializer.hpp", "filename": "IpDefaultIterateInitializer.hpp", "file": "IpDefaultIterateInitializer.hpp", "brief": "Standard initialization procedure for IPM iterates\n\nDefaultIterateInitializer computes starting points for all primal\nand dual variables based on user options and problem bounds.\n\nPrimal initialization (x, s):\n- Start from user-provided x0 or NLP default\n- Push away from bounds: x_new = max(x_L + ε, min(x, x_U - ε))\n- bound_push_, bound_frac_: Absolute/relative push parameters\n- least_square_init_primal_: Fit linearized constraints\n\nDual initialization:\n- Equality multipliers (y_c, y_d): Least-squares or zero\n- eq_mult_calculator_: Computes min ||y|| s.t. KKT gradient\n- constr_mult_init_max_: Reject large multiplier estimates\n- Bound multipliers (z_L, z_U, v_L, v_U):\n  - B_CONSTANT: bound_mult_init_val_\n  - B_MU_BASED: mu_init_ / slack\n\nWarm start:\n- warm_start_init_point_: Use warm_start_initializer_ instead\n- Delegates to WarmStartIterateInitializer\n\nStatic utilities:\n- push_variables(): Move point away from bounds\n- least_square_mults(): Compute y from gradient conditions", "see": ["IpIterateInitializer.hpp for the base interface", "IpWarmStartIterateInitializer.hpp for warm start", "IpEqMultCalculator.hpp for multiplier computation"], "has_pass2": false}, "src/Algorithm/IpPDSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDSystemSolver.hpp", "filename": "IpPDSystemSolver.hpp", "file": "IpPDSystemSolver.hpp", "brief": "Abstract interface for solving the full primal-dual KKT system\n\nPDSystemSolver defines the interface for solving the 8x8 block\nprimal-dual linear system that arises at each Newton iteration:\n\n  [W    0   Jc^T  Jd^T  -Px_L  Px_U   0     0   ] [dx  ]   [rx  ]\n  [0    0    0    -I     0     0    -Pd_L  Pd_U ] [ds  ]   [rs  ]\n  [Jc   0    0     0     0     0     0     0   ] [dyc ] = [rc  ]\n  [Jd  -I    0     0     0     0     0     0   ] [dyd ]   [rd  ]\n  [Zl  Px_L^T ...                              ] [dzL ]   [rzL ]\n  [...]                                          [dzU ]   [rzU ]\n  [...]                                          [dvL ]   [rvL ]\n  [...]                                          [dvU ]   [rvU ]\n\nImplementations typically reduce this to the 4x4 augmented system\nby eliminating the bound multiplier equations, then solve via\nAugSystemSolver. The key implementation is PDFullSpaceSolver.", "see": ["IpPDFullSpaceSolver.hpp for the main implementation", "IpAugSystemSolver.hpp for the reduced 4x4 system", "IpIteratesVector.hpp for the rhs/solution structure"], "return": "false, if a solution could not be computed (for\n example, when the Hessian regularization parameter becomes too\n large)", "has_pass2": false}, "src/Algorithm/IpMuUpdate.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpMuUpdate.hpp", "filename": "IpMuUpdate.hpp", "file": "IpMuUpdate.hpp", "brief": "Strategy interface for updating barrier parameter mu\n\nMuUpdate is the abstract base for strategies that determine the\nbarrier parameter mu and fraction-to-boundary parameter tau for\neach iteration of the interior point method.\n\nThe barrier parameter controls the tradeoff between optimality\nand centrality: smaller mu → closer to optimality, larger mu →\nstays closer to central path.\n\nImplementations:\n- MonotoneMuUpdate: mu decreases monotonically (classical path-following)\n- AdaptiveMuUpdate: mu varies non-monotonically (LOQO-style)\n- QualityFunctionMuOracle: Mehrotra predictor-corrector style\n\nAlso sets tau = 1 - mu^theta (fraction-to-boundary safeguard).", "see": ["IpMonotoneMuUpdate.hpp for monotone strategy", "IpAdaptiveMuUpdate.hpp for adaptive strategy", "IpMuOracle.hpp for mu computation oracles"], "has_pass2": false}, "src/Algorithm/IpNLPBoundsRemover.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpNLPBoundsRemover.hpp", "filename": "IpNLPBoundsRemover.hpp", "file": "IpNLPBoundsRemover.hpp", "brief": "NLP adapter that converts variable bounds to inequality constraints\n\nNLPBoundsRemover is an NLP adapter primarily used for inexact/iterative\nlinear solvers that require the KKT system to have a specific structure\nwithout bound constraints.\n\nTransformation:\n  Original: x_L <= x <= x_U\n  Transformed: d(x) = x with d_L = x_L, d_U = x_U (added inequalities)\n  Result: No bounds on x, but equivalent inequality constraints\n\nThe resulting NLP has:\n- No variable bounds (x unbounded)\n- Additional inequality constraints d(x) = x\n- Corresponding d_L, d_U bounds\n\nKey restrictions:\n- Original inequalities must be one-sided (unless allow_twosided_inequalities_)\n- Jacobian structure changes (identity blocks added)\n\nMatrix structure modification:\n- J_d becomes [J_d_orig; I] (stacked)\n- Hessian unchanged (d=x is linear)\n\nUses:\n- Inexact Newton methods requiring bound-free formulation\n- Research into alternative KKT structures", "see": ["IpNLP.hpp for the base NLP interface"], "has_pass2": false}, "src/Algorithm/IpAlgorithmRegOp.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAlgorithmRegOp.hpp", "filename": "IpAlgorithmRegOp.hpp", "file": "IpAlgorithmRegOp.hpp", "brief": "Registration function for all Algorithm module options\n\nDeclares RegisterOptions_Algorithm() which registers all options\nfor the Ipopt algorithm components with the RegisteredOptions system.\n\nThis function is called during IpoptApplication initialization to\nmake all algorithm options available. It aggregates option registration\nfrom numerous strategy classes.\n\nRegistered option categories include:\n- Main algorithm options (IpoptAlgorithm)\n- Line search options (BacktrackingLineSearch, FilterLSAcceptor)\n- Barrier parameter options (MonotoneMuUpdate, AdaptiveMuUpdate)\n- Linear solver options (PDFullSpaceSolver, AugSystemSolver)\n- Initialization options (DefaultIterateInitializer)\n- Scaling options (NLPScaling, GradientScaling)\n- Convergence options (OptimalityErrorConvergenceCheck)\n- Output options (OrigIterationOutput)", "see": ["IpRegOptions.hpp for the options registration framework", "IpoptApplication::RegisterAllOptions"], "has_pass2": false}, "src/Algorithm/IpRestoIterationOutput.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoIterationOutput.hpp", "filename": "IpRestoIterationOutput.hpp", "file": "IpRestoIterationOutput.hpp", "brief": "Iteration output during restoration phase\n\nRestoIterationOutput provides per-iteration summary output while\nthe algorithm is in restoration phase. It displays metrics for the\nORIGINAL NLP (not the restoration feasibility problem).\n\nOutput format:\n- Iteration number marked with 'r' prefix to indicate restoration\n- Objective value from original NLP\n- Constraint violation (theta) for original constraints\n- Dual infeasibility for original problem\n\nDual output mode:\nIf resto_orig_iteration_output is provided, produces two lines:\n1. Restoration phase problem metrics\n2. Original NLP metrics (using original scaling)\n\nConfiguration:\n- print_info_string_: Whether to print info at end of line\n- inf_pr_output_: What to show in inf_pr column\n- print_frequency_iter_: Iteration print frequency\n- print_frequency_time_: Time-based print frequency", "see": ["IpOrigIterationOutput.hpp for regular iteration output", "IpIterationOutput.hpp for base interface"], "has_pass2": false}, "src/Algorithm/IpAdaptiveMuUpdate.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAdaptiveMuUpdate.hpp", "filename": "IpAdaptiveMuUpdate.hpp", "file": "IpAdaptiveMuUpdate.hpp", "brief": "Adaptive (non-monotone) barrier parameter update strategy\n\nAdaptiveMuUpdate implements the free-mode/fixed-mode approach for\nbarrier parameter updates. In free mode, mu is computed adaptively\neach iteration. In fixed mode, monotone decrease is enforced.\n\nTwo operating modes:\n- Free mode: MuOracle suggests mu (e.g., Mehrotra predictor-corrector\n  or quality function minimization). Allows temporary mu increases.\n- Fixed mode: Monotone decrease enforced. Triggered when free mode\n  fails to make sufficient progress.\n\nGlobalization strategies (adaptive_mu_globalization_):\n- KKT_ERROR: Track reduction in primal-dual KKT error\n- FILTER_OBJ_CONSTR: Use filter on (theta, phi)\n- NEVER_MONOTONE_MODE: Always stay in free mode\n\nFree mode oracles (via MuOracle):\n- QualityFunctionMuOracle: Minimize quality function\n- ProbingMuOracle: Try candidate mu values\n- LoqoMuOracle: LOQO-style adaptive rule\n\nFixed mode behavior:\n- Uses fix_mu_oracle_ or average complementarity\n- restore_accepted_iterate_: Can restore last good free-mode point\n\nReference value tracking:\n- refs_vals_: List of recent KKT error values\n- refs_red_fact_: Required reduction factor\n- num_refs_max_: Maximum stored references", "see": ["IpMuUpdate.hpp for the base interface", "IpMuOracle.hpp for oracle interface", "IpQualityFunctionMuOracle.hpp for quality function approach"], "has_pass2": false}, "src/Algorithm/IpQualityFunctionMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpQualityFunctionMuOracle.hpp", "filename": "IpQualityFunctionMuOracle.hpp", "file": "IpQualityFunctionMuOracle.hpp", "brief": "Barrier parameter selection via quality function minimization\n\nQualityFunctionMuOracle computes the barrier parameter by minimizing\na quality function that measures how close the predictor-corrector\nstep brings the iterate to optimality.\n\nAlgorithm:\n1. Compute affine scaling direction (mu = 0)\n2. Compute centering direction (pure mu term)\n3. For combined step (sigma * centering + affine), find optimal sigma\n4. Convert optimal sigma to mu = sigma * average_complementarity\n\nQuality function Q(sigma) measures (configurable):\n- Primal-dual error norm after step\n- Centrality deviation from mu-centered path\n- Balancing term for primal/dual progress\n\nConfiguration options:\n- quality_function_norm_: NM_NORM_1, NM_NORM_2, NM_NORM_MAX\n- quality_function_centrality_: CEN_NONE, CEN_LOG, CEN_RECIPROCAL\n- quality_function_balancing_term_: BT_NONE, BT_CUBIC\n\nGolden section search:\n- Finds optimal sigma in [sigma_min_, sigma_max_]\n- Tolerances: quality_function_section_sigma_tol_, _qf_tol_\n- Maximum iterations: quality_function_max_section_steps_", "see": ["IpMuOracle.hpp for the base interface", "IpAdaptiveMuUpdate.hpp for usage"], "has_pass2": false}, "src/Algorithm/IpAugRestoSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAugRestoSystemSolver.hpp", "filename": "IpAugRestoSystemSolver.hpp", "file": "IpAugRestoSystemSolver.hpp", "brief": "Augmented system solver exploiting restoration phase structure\n\nAugRestoSystemSolver is a decorator that exploits the known structure\nof the restoration phase problem to reduce the augmented system to\nthe original problem size.\n\nRestoration problem structure:\n- Variables: (x, s, n_c, p_c, n_d, p_d)\n- Constraints: c(x) - n_c + p_c = 0, d(x) - s - n_d + p_d = 0\n- Objective: penalty on ||n|| + ||p||\n\nThe Hessian and Jacobian have special structure that allows the\nn, p variables to be eliminated analytically. The resulting system\nhas the same dimension as the original problem.\n\nKey quantities computed and cached:\n- sigma_tilde_*_inv_: Inverse of perturbed bound multiplier ratios\n- Neg_Omega_*_plus_D_*: Combined diagonal terms\n- Rhs_*R: Reduced right-hand sides\n\nThe skip_orig_aug_solver_init flag allows reusing symbolic\nfactorization from the main algorithm.", "see": ["IpRestoIpoptNLP.hpp for restoration problem formulation", "IpAugSystemSolver.hpp for base interface"], "return": "true, if linear solver provides inertia", "has_pass2": false}, "src/Algorithm/IpAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAugSystemSolver.hpp", "filename": "IpAugSystemSolver.hpp", "file": "IpAugSystemSolver.hpp", "brief": "Abstract interface for solving the 4x4 augmented KKT system\n\nAugSystemSolver defines the interface for solving the reduced\naugmented system obtained by eliminating bound multiplier equations:\n\n  [W + Dx + δx*I      0        Jc^T      Jd^T  ] [sol_x]   [rhs_x]\n  [    0         Ds + δs*I     0         -I   ] [sol_s] = [rhs_s]\n  [   Jc             0      Dc - δc*I    0    ] [sol_c]   [rhs_c]\n  [   Jd            -I         0      Dd - δd*I] [sol_d]   [rhs_d]\n\nKey parameters:\n- W: Hessian of Lagrangian (or approximation)\n- Dx, Ds: Barrier diagonal contributions (Σx, Σs)\n- δx, δs, δc, δd: Regularization parameters for inertia control\n- Jc, Jd: Constraint Jacobians\n\nImplementations:\n- StdAugSystemSolver: Forms explicit matrix, uses SymLinearSolver\n- GenAugSystemSolver: For general (non-symmetric) matrices", "see": ["IpStdAugSystemSolver.hpp for the main implementation", "IpSymLinearSolver.hpp for the underlying sparse solver", "IpPDFullSpaceSolver.hpp for the caller"], "return": "return value of the linear solver object.", "has_pass2": false}, "src/Algorithm/IpPDPerturbationHandler.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDPerturbationHandler.hpp", "filename": "IpPDPerturbationHandler.hpp", "file": "IpPDPerturbationHandler.hpp", "brief": "Inertia correction via primal-dual perturbation (regularization)\n\nPDPerturbationHandler manages the regularization parameters\n(delta_x, delta_s, delta_c, delta_d) added to the KKT system to\nensure correct inertia (n positive, m+p negative eigenvalues).\n\nThe perturbed augmented system is:\n  [W + D_x + δ_x I   ...  ]\n  [  ...   D_s + δ_s I  ...]\n  [  ...   ... D_c - δ_c I]\n  [  ...   ...   D_d - δ_d I]\n\nPerturbation strategies:\n- ConsiderNewSystem(): Called for each new matrix, may add δ_c, δ_d\n  if structurally singular Jacobian is detected\n- PerturbForSingularity(): Handle numerically singular system\n- PerturbForWrongInertia(): Correct incorrect eigenvalue count\n\nHeuristics:\n- Track structural degeneracy (hess_degenerate_, jac_degenerate_)\n- Increase perturbation exponentially until acceptable\n- Decrease perturbation when not needed", "see": ["IpPDFullSpaceSolver.hpp for usage", "IpAugSystemSolver.hpp for where perturbations are applied"], "return": "false, no suitable perturbation could be found.", "has_pass2": false}, "src/Algorithm/IpStdAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpStdAugSystemSolver.hpp", "filename": "IpStdAugSystemSolver.hpp", "file": "IpStdAugSystemSolver.hpp", "brief": "Standard augmented system solver forming explicit matrix\n\nStdAugSystemSolver is the main implementation of AugSystemSolver\nfor sparse triple-format matrices (SymTMatrix). It explicitly\nassembles the 4x4 augmented system as a CompoundSymMatrix:\n\n  [W + D_x + δ_x I      0         J_c^T      J_d^T  ]\n  [     0          D_s + δ_s I    0          -I    ]\n  [    J_c             0       D_c - δ_c I    0     ]\n  [    J_d            -I          0       D_d - δ_d I]\n\nImplementation details:\n- Uses CompoundSymMatrixSpace with SumSymMatrix for (1,1) block\n- DiagMatrix for D_x, D_s, D_c, D_d contributions\n- IdentityMatrix for scalar delta regularization\n- Tracks matrix tags to avoid unnecessary reassembly\n- Delegates factorization/solve to SymLinearSolver", "see": ["IpAugSystemSolver.hpp for the interface", "IpSymLinearSolver.hpp for the underlying sparse solver", "IpCompoundSymMatrix.hpp for the matrix representation"], "return": "the number of negative eigenvalues of the\nmost recent factorized matrix", "has_pass2": false}, "src/Algorithm/IpIpoptAlg.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptAlg.hpp", "filename": "IpIpoptAlg.hpp", "file": "IpIpoptAlg.hpp", "brief": "Main Ipopt algorithm orchestrating the interior point method\n\nIpoptAlgorithm is the central class implementing the primal-dual\ninterior point method. Each iteration:\n1. UpdateHessian() - Evaluate or update Hessian approximation\n2. UpdateBarrierParameter() - Adjust mu using chosen strategy\n3. ComputeSearchDirection() - Solve KKT system for Newton step\n4. ComputeAcceptableTrialPoint() - Line search with filter/merit\n5. AcceptTrialPoint() - Update current iterate\n\nUses Strategy pattern: line search, mu update, convergence check\nare all pluggable components configured via AlgorithmBuilder.", "see": ["IpAlgBuilder.hpp for algorithm construction", "IpIpoptData.hpp for iteration data storage"], "return": "false, if the algorithm can't continue with the\n regular procedure and needs to revert to a fallback\n mechanism in the line search (such as restoration phase)", "has_pass2": false}, "src/Algorithm/IpRestoIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoIterateInitializer.hpp", "filename": "IpRestoIterateInitializer.hpp", "file": "IpRestoIterateInitializer.hpp", "brief": "Iterate initialization for restoration phase\n\nRestoIterateInitializer computes starting values for all variables\nin the restoration phase feasibility problem. This includes the\noriginal variables (x, s) and the slack variables (n_c, p_c, n_d, p_d).\n\nInitialization procedure:\n1. Take x, s from the point where main algorithm entered restoration\n2. Compute n, p slacks to satisfy: c(x) + n - p = 0\n3. Initialize bound multipliers (z, v) for new bounds\n4. Optionally compute equality multipliers via least squares\n\nThe slack computation solves quadratic equation v^2 + 2a*v - b = 0\nto find feasible n, p values.\n\nKey parameter:\n- constr_mult_init_max_: If initial multiplier estimate exceeds\n  this, reset to zero to avoid ill-conditioning", "see": ["IpDefaultIterateInitializer.hpp for standard cold start", "IpRestoIpoptNLP.hpp for restoration problem structure"], "has_pass2": false}, "src/Algorithm/IpIteratesVector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIteratesVector.hpp", "filename": "IpIteratesVector.hpp", "file": "IpIteratesVector.hpp", "brief": "Combined primal-dual iterate vector for IPM algorithm\n\nIteratesVector is a specialized CompoundVector with fixed 8 components\nrepresenting the complete primal-dual iterate of the interior point method:\n\nComponents (indices 0-7):\n- x (0): Primal variables\n- s (1): Inequality slacks\n- y_c (2): Equality constraint multipliers\n- y_d (3): Inequality constraint multipliers\n- z_L (4): Lower bound multipliers on x\n- z_U (5): Upper bound multipliers on x\n- v_L (6): Lower bound multipliers on s\n- v_U (7): Upper bound multipliers on s\n\nThis allows the iterate to participate in vector operations (norms,\nscaling, axpy) while providing type-safe accessors for each component.\nIpoptData stores curr_, trial_, delta_ as IteratesVectors.", "see": ["IpCompoundVector.hpp for the base class", "IpIpoptData.hpp for storage of current/trial/delta iterates"], "return": "NULL, if none is currently set", "has_pass2": false}, "src/Algorithm/IpPDFullSpaceSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDFullSpaceSolver.hpp", "filename": "IpPDFullSpaceSolver.hpp", "file": "IpPDFullSpaceSolver.hpp", "brief": "Full-space primal-dual system solver with inertia correction\n\nPDFullSpaceSolver is the main implementation of PDSystemSolver.\nIt reduces the 8x8 primal-dual system to the 4x4 augmented system\nby eliminating bound multiplier equations:\n  d_z = S^{-1}(rhs_z - Z*P^T*d_x)\n\nKey features:\n- Iterative refinement with quality monitoring (residual_ratio)\n- Inertia correction via PDPerturbationHandler (adds delta_x, delta_c)\n- Automatic retries with increased pivot tolerance\n- Handles singular systems by adding regularization\n\nParameters:\n- min/max_refinement_steps: Iterative refinement bounds\n- residual_ratio_max: Acceptable solution quality threshold\n- neg_curv_test_tol: Tolerance for inertia heuristics", "see": ["IpPDSystemSolver.hpp for the interface", "IpAugSystemSolver.hpp for the reduced system solver", "IpPDPerturbationHandler.hpp for inertia correction"], "return": "false, if for some reason the linear system\n could not be solved (e.g. when the regularization parameter\n becomes too large)", "has_pass2": false}, "src/Algorithm/IpTimingStatistics.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpTimingStatistics.hpp", "filename": "IpTimingStatistics.hpp", "file": "IpTimingStatistics.hpp", "brief": "Collection of timing statistics for algorithm profiling\n\nTimingStatistics aggregates TimedTask objects for all major\nalgorithm components, enabling performance profiling and\nbottleneck identification.\n\nTiming categories:\n- Algorithm phases: InitializeIterates, UpdateHessian, OutputIteration,\n  UpdateBarrierParameter, ComputeSearchDirection, etc.\n- Linear system: PDSystemSolverTotal, LinearSystemFactorization,\n  LinearSystemBackSolve, LinearSystemScaling\n- NLP evaluations: f_eval_time, grad_f_eval_time, c_eval_time,\n  jac_c_eval_time, d_eval_time, jac_d_eval_time, h_eval_time\n- Auxiliary: Task1-Task6 for ad-hoc profiling\n\nKey methods:\n- ResetTimes(): Clear all accumulated times\n- EnableTimes()/DisableTimes(): Control timing overhead\n- PrintAllTimingStatistics(): Output formatted timing report\n- TotalFunctionEvaluationCpuTime(): Sum of NLP evaluation times\n\nEach TimedTask tracks CPU time, system time, and wall-clock time.", "see": ["IpTimedTask.hpp for individual task timing", "IpIpoptAlg.hpp for timing during main loop"], "has_pass2": false}, "src/Algorithm/IpSearchDirCalculator.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpSearchDirCalculator.hpp", "filename": "IpSearchDirCalculator.hpp", "file": "IpSearchDirCalculator.hpp", "brief": "Strategy interface for computing the Newton search direction\n\nSearchDirectionCalculator is the strategy interface for computing\nthe search direction at each IPM iteration. The computed direction\nis stored in IpData().delta() (an IteratesVector).\n\nImplementations:\n- PDSearchDirCalculator: Standard primal-dual direction via PDSystemSolver\n- SensitivityStepCalculator: For parametric sensitivity analysis\n\nThe direction computation involves solving the KKT system with the\ncurrent Hessian and Jacobians, applying any regularization needed\nfor inertia correction.", "see": ["IpPDSearchDirCalc.hpp for the standard implementation", "IpPDSystemSolver.hpp for the linear system solver", "IpIpoptAlg.hpp for where ComputeSearchDirection() is called"], "has_pass2": false}, "src/Algorithm/IpAlgBuilder.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAlgBuilder.hpp", "filename": "IpAlgBuilder.hpp", "file": "IpAlgBuilder.hpp", "brief": "Builder pattern for constructing configured IpoptAlgorithm\n\nAlgorithmBuilder assembles a complete IpoptAlgorithm from components\nbased on user options. This implements the Builder design pattern,\ncentralizing the complex construction logic.\n\nBuild order (with dependency chain):\n1. SymLinearSolver (MA27/57/77/86/97, MUMPS, Pardiso, WSMP, etc.)\n2. AugSystemSolver (wraps SymLinearSolver for augmented system)\n3. PDSystemSolver (solves full primal-dual system)\n4. SearchDirectionCalculator, EqMultiplierCalculator\n5. IterateInitializer, LineSearch, MuUpdate, ConvergenceCheck\n\nCustomization: Subclass and override virtual Build* methods,\nor provide custom_solver in constructor.", "algorithm": "Linear Solver Selection (SymLinearSolverFactory):\n  HSL solvers (preferred): MA27/57 (multifrontal), MA77/86/97 (out-of-core)\n  Alternatives: MUMPS (parallel), Pardiso (MKL/project), WSMP, SPRAL\n  Key requirement: handle indefinite symmetric matrices with inertia detection.", "math": "[W + Σ  A'] [Δx]   [r_x]\n        [A      0 ] [Δy] = [r_c]\n  where W = ∇²L (Hessian of Lagrangian), Σ = X⁻¹Z (diagonal),\n  A = ∇c(x)' (Jacobian). Solve via sparse symmetric factorization.", "ref": ["Nocedal, J. and Wright, S.J. (2006). \"Numerical Optimization\".\n       Springer, Chapter 19 (Interior-Point Methods).", "Fiacco, A.V. and McCormick, G.P. (1968). \"Nonlinear Programming:\n       Sequential Unconstrained Minimization Techniques\". Wiley.", "Mehrotra, S. (1992). \"On the implementation of a primal-dual\n       interior point method\". SIAM J. Optimization 2(4):575-601.", "Wächter, A. and Biegler, L.T. (2006). \"On the implementation\n       of an interior-point filter line-search algorithm for large-scale\n       nonlinear programming\". Math. Programming 106(1):25-57."], "complexity": "Per iteration: O(n³) for dense, O(nnz^{1.5-2}) for sparse\n  Total: O(iterations × linear_solve_cost) where iterations ~ O(√n) typical", "see": ["IpIpoptAlg.hpp for resulting algorithm", "IpIpoptApplication.hpp for high-level usage"], "has_pass2": true}, "src/Algorithm/IpIterationOutput.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIterationOutput.hpp", "filename": "IpIterationOutput.hpp", "file": "IpIterationOutput.hpp", "brief": "Strategy interface for per-iteration output display\n\nIterationOutput is the abstract base for strategies that produce\nthe iteration summary line and optional detailed output.\n\nStandard output line (from OrigIterationOutput):\n  iter  objective   inf_pr   inf_du   lg(mu)  ||d||  lg(rg) alpha_du alpha_pr ls\n\nInfPrOutput enum controls whether inf_pr shows internal (with slacks)\nor original NLP constraint violation.\n\nImplementations:\n- OrigIterationOutput: Standard one-line summary for original problem\n- RestoIterationOutput: Output during restoration phase\n\nOutput controlled by print_level option (0-12).", "see": ["IpOrigIterationOutput.hpp for standard implementation", "IpRestoIterationOutput.hpp for restoration phase", "IpJournalist.hpp for output channel management"], "has_pass2": false}, "src/Algorithm/IpIpoptData.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptData.hpp", "filename": "IpIpoptData.hpp", "file": "IpIpoptData.hpp", "brief": "Central storage for all iteration data in Ipopt\n\nIpoptData holds the algorithmic state across iterations:\n- curr_: Current iterate (x, s, y_c, y_d, z_L, z_U, v_L, v_U)\n- trial_: Trial point from line search\n- delta_: Search direction from KKT solve\n- delta_aff_: Affine-scaling step (for Mehrotra predictor-corrector)\n- W_: Hessian or Hessian approximation\n\nAlso tracks: barrier parameter mu, iteration count, timing stats,\nconvergence tolerance, and iteration output information.\n\nIpoptAdditionalData allows algorithm extensions (CG penalty, inexact)\nto store their specialized data alongside the core data.", "see": ["IpIteratesVector.hpp for iterate vector structure", "IpIpoptAlg.hpp for algorithm using this data"], "has_pass2": false}, "src/Algorithm/IpLowRankAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLowRankAugSystemSolver.hpp", "filename": "IpLowRankAugSystemSolver.hpp", "file": "IpLowRankAugSystemSolver.hpp", "brief": "Low-rank Hessian handling via Sherman-Morrison (multiple backsolves)\n\nLowRankAugSystemSolver handles LowRankUpdateSymMatrix Hessians\n(from L-BFGS quasi-Newton) using the Sherman-Morrison formula.\n\nL-BFGS Hessian: W = sigma*I + V*M*V^T (compact representation)\n- V: n x 2k matrix of gradient/step differences\n- M: 2k x 2k small dense matrix\n- k: number of stored corrections (limited memory)\n\nSherman-Morrison approach:\n(A + UV^T)^{-1} = A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1}\n\nImplementation:\n1. Solve diagonal system: Vtilde = A^{-1}*V (2k backsolves)\n2. Form small dense matrix: J = I + V^T*Vtilde\n3. Solve J*y = V^T*A^{-1}*rhs (small dense solve)\n4. Correct: x = A^{-1}*rhs - Vtilde*y\n\nStorage:\n- J1_, J2_: Dense matrices for correction\n- Vtilde1_, Utilde2_: MultiVectorMatrix backsolve results\n- Wdiag_: Diagonal part passed to base solver", "see": ["IpLowRankSSAugSystemSolver.hpp for single-backsolve version", "IpLimMemQuasiNewtonUpdater.hpp for L-BFGS Hessian"], "return": "number of negative eigenvalues of the most recent factorized matrix", "has_pass2": false}, "src/Algorithm/IpOrigIpoptNLP.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpOrigIpoptNLP.hpp", "filename": "IpOrigIpoptNLP.hpp", "file": "IpOrigIpoptNLP.hpp", "brief": "Standard IpoptNLP implementation wrapping user's NLP\n\nOrigIpoptNLP is the concrete implementation of IpoptNLP for standard\noptimization problems. It wraps the user's NLP (via TNLP/TNLPAdapter)\nand applies scaling transformations:\n\n  min s_f·f(S_x^{-1}·x̃)  s.t.  s_c·c(S_x^{-1}·x̃)=0, ...\n\nResponsibilities:\n- Initialize vector/matrix spaces from NLP structure\n- Evaluate f, grad_f, c, d, jac_c, jac_d, h with caching\n- Apply/unapply scaling via NLPScalingObject\n- Handle bound relaxation (bound_relax_factor)\n- Track function evaluation counts\n- Call intermediate callback and finalize solution\n\nKey options:\n- hessian_approximation: EXACT or LIMITED_MEMORY\n- bound_relax_factor: Relax bounds for numerical safety\n- honor_original_bounds: Project solution to original bounds", "see": ["IpIpoptNLP.hpp for the abstract interface", "IpRestoIpoptNLP.hpp for restoration phase variant", "IpNLPScaling.hpp for scaling strategies"], "has_pass2": false}, "src/Algorithm/IpAlgStrategy.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAlgStrategy.hpp", "filename": "IpAlgStrategy.hpp", "file": "IpAlgStrategy.hpp", "brief": "Base class for all pluggable algorithm components\n\nAlgorithmStrategyObject is the abstract base for Ipopt's Strategy pattern\nimplementation. All pluggable algorithm components inherit from this:\n- LineSearch, MuUpdate, ConvergenceCheck\n- SearchDirectionCalculator, HessianUpdater\n- PDSystemSolver, AugSystemSolver\n\nProvides:\n- Initialize() template method with common setup\n- Access to IpoptNLP, IpoptData, IpoptCalculatedQuantities\n- Consistent initialization pattern across all strategies", "see": ["IpAlgBuilder.hpp for strategy composition", "IpIpoptAlg.hpp for strategy usage"], "has_pass2": false}, "src/Algorithm/IpOptErrorConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpOptErrorConvCheck.hpp", "filename": "IpOptErrorConvCheck.hpp", "file": "IpOptErrorConvCheck.hpp", "brief": "Standard convergence check based on optimality error tolerances\n\nOptimalityErrorConvergenceCheck implements the standard termination\ncriteria for Ipopt based on KKT optimality conditions.\n\nOptimal convergence (tol):\n- Dual infeasibility <= dual_inf_tol_\n- Constraint violation <= constr_viol_tol_\n- Complementarity <= compl_inf_tol_\n- Overall scaled error <= tol\n\nAcceptable convergence (acceptable_tol):\n- Less stringent tolerances applied for acceptable_iter_ iterations\n- Terminates early if stuck near acceptable solution\n- acceptable_obj_change_tol_: Detects stagnation in objective\n\nFailure conditions:\n- max_iterations_: Iteration limit exceeded\n- max_wall_time_, max_cpu_time_: Time limits\n- diverging_iterates_tol_: Primal variables diverging\n- mu_target_: Target barrier parameter reached\n\nCurrentIsAcceptable():\n- Returns true if acceptable-level tolerances are met\n- Used by line search to decide on restoration phase", "see": ["IpConvCheck.hpp for the base interface", "IpRestoConvCheck.hpp for restoration phase convergence"], "has_pass2": false}, "src/LinAlg/IpExpansionMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpExpansionMatrix.hpp", "filename": "IpExpansionMatrix.hpp", "file": "IpExpansionMatrix.hpp", "brief": "Expansion/projection matrices for dimension mapping\n\nExpansionMatrix maps between vectors of different sizes:\n- MultVector: Embeds small vector into larger space (zero-fills)\n- TransMultVector: Projects large vector to smaller space (extracts)\n\nUsed in Ipopt for:\n- Mapping slack variables to constraint space\n- Handling fixed variables (remove from optimization)\n- Extracting subsets of inequality constraints\n\nStored as index mapping (no explicit matrix storage).\nVery efficient for large problems with many fixed variables.", "see": ["IpMatrix.hpp for base interface", "IpTNLPAdapter.hpp for primary usage"], "has_pass2": false}, "src/LinAlg/IpMultiVectorMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpMultiVectorMatrix.hpp", "filename": "IpMultiVectorMatrix.hpp", "file": "IpMultiVectorMatrix.hpp", "brief": "Tall-skinny matrix stored as collection of column Vectors\n\nMultiVectorMatrix represents an m x k matrix (k << m) where each\ncolumn is stored as a separate Vector. Efficient for:\n- Limited-memory quasi-Newton: store recent gradient differences\n- Low-rank updates: V*V^T matvec via two sequential operations\n\nKey methods:\n- LRMultVector: computes y = alpha*V*V^T*x + beta*y\n- AddRightMultMatrix: V = alpha*U*C + beta*V for small dense C\n\nUsed in Ipopt for:\n- L-BFGS/L-SR1 Hessian approximations\n- Storing {s_i, y_i} pairs for quasi-Newton", "see": ["IpLowRankUpdateSymMatrix.hpp for matrix using this", "IpExpandedMultiVectorMatrix.hpp for row-vector variant"], "has_pass2": false}, "src/LinAlg/IpVector.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpVector.hpp", "filename": "IpVector.hpp", "file": "IpVector.hpp", "brief": "Abstract base class for all vector types in Ipopt\n\nVector is the foundation of Ipopt's linear algebra abstraction.\nProvides BLAS-1 style operations (Copy, Scal, Axpy, Dot, Nrm2, etc.)\nplus IPM-specific operations (AddOneVector, FracToBound, etc.).\n\nKey design features:\n- Inherits TaggedObject for automatic cache invalidation\n- Uses VectorSpace factory pattern for creating compatible vectors\n- Template Method pattern: public methods call protected *_Impl methods\n- Implementations: DenseVector, CompoundVector\n\nDerived classes override protected *_Impl methods to provide\nactual computation (e.g., CopyImpl, AxpyImpl, DotImpl).", "see": ["IpDenseVector.hpp for primary implementation", "IpCompoundVector.hpp for composite vectors", "IpMatrix.hpp for matrix abstraction"], "has_pass2": false}, "src/LinAlg/IpBlas.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpBlas.hpp", "filename": "IpBlas.hpp", "file": "IpBlas.hpp", "brief": "C++ wrappers for BLAS (Basic Linear Algebra Subprograms)\n\nProvides platform-independent access to BLAS Level 1 and 2 routines:\n\nBLAS Level 1 (vector operations):\n- IpBlasDot: Dot product (DDOT/SDOT)\n- IpBlasNrm2: Euclidean norm (DNRM2/SNRM2)\n- IpBlasAsum: Sum of absolute values (DASUM/SASUM)\n- IpBlasScal: Vector scaling (DSCAL/SSCAL)\n- IpBlasAxpy: y = α*x + y (DAXPY/SAXPY)\n- IpBlasCopy: Vector copy (DCOPY/SCOPY)\n- IpBlasIamax: Index of max element (IDAMAX/ISAMAX)\n\nBLAS Level 2 (matrix-vector):\n- IpBlasGemv: General matrix-vector multiply (DGEMV/SGEMV)\n- IpBlasSymv: Symmetric matrix-vector multiply (DSYMV/SSYMV)", "see": ["IpLapack.hpp for higher-level linear algebra (factorization, solve)"], "has_pass2": false}, "src/LinAlg/IpCompoundMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpCompoundMatrix.hpp", "filename": "IpCompoundMatrix.hpp", "file": "IpCompoundMatrix.hpp", "brief": "Block-structured matrix composed of sub-matrices\n\nCompoundMatrix implements the Composite pattern for matrices,\nrepresenting a block matrix: M = [M_00, M_01, ...; M_10, ...]\n\nUsed in Ipopt for KKT system structure:\n  [H    A^T ] [dx]   [r_d]\n  [A    -D  ] [dy] = [r_p]\n\nNull components are treated as zero blocks. Operations recurse\nto sub-matrices. Each block can be from different MatrixSpace.", "see": ["IpMatrix.hpp for base interface", "IpCompoundSymMatrix.hpp for symmetric version", "IpCompoundVector.hpp for vector counterpart"], "has_pass2": false}, "src/LinAlg/IpIdentityMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpIdentityMatrix.hpp", "filename": "IpIdentityMatrix.hpp", "file": "IpIdentityMatrix.hpp", "brief": "Scalar multiple of the identity matrix (factor * I)\n\nIdentityMatrix represents matrices of the form factor*I where\nI is the identity. Stores only the scalar factor - O(1) storage.\nMatrix-vector multiply is just a scale operation: y = factor*x.\n\nUsed in Ipopt for:\n- Regularization terms (delta_x * I, delta_c * I)\n- Initial Hessian approximations in quasi-Newton\n- Trust region constraints", "see": ["IpDiagMatrix.hpp for general diagonal matrices", "IpSymMatrix.hpp for base class"], "has_pass2": false}, "src/LinAlg/IpSumSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSumSymMatrix.hpp", "filename": "IpSumSymMatrix.hpp", "file": "IpSumSymMatrix.hpp", "brief": "Symmetric matrix as weighted sum: M = sum(alpha_i * M_i)\n\nSumSymMatrix represents a symmetric matrix as a sum of symmetric terms,\neach with its own scalar factor. Preserves symmetry of components.\n\nUsed in Ipopt for KKT Hessian structure:\n- W + sum of barrier contributions + regularization\n- Combining user Hessian with barrier terms without explicit formation", "see": ["IpSumMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": false}, "src/LinAlg/IpLowRankUpdateSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpLowRankUpdateSymMatrix.hpp", "filename": "IpLowRankUpdateSymMatrix.hpp", "file": "IpLowRankUpdateSymMatrix.hpp", "brief": "Symmetric matrix as low-rank update: M = D + V*V^T - U*U^T\n\nLowRankUpdateSymMatrix represents matrices in factored form:\n  M = P_LR * (D + V*V^T - U*U^T) * P_LR^T  (if reduced_diag)\n  M = D + P_LR * (V*V^T - U*U^T) * P_LR^T  (otherwise)\n\nWhere D is diagonal, V and U are MultiVectorMatrices (few columns),\nand P_LR is an optional ExpansionMatrix for dimension lifting.\n\nThis representation is fundamental for limited-memory quasi-Newton:\n- L-BFGS: Hessian approximation as low-rank updates\n- L-SR1: Symmetric rank-1 updates\n- Efficient matvec without forming full matrix", "see": ["IpMultiVectorMatrix.hpp for V, U storage", "IpExpansionMatrix.hpp for P_LR"], "has_pass2": false}, "src/LinAlg/IpScaledMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpScaledMatrix.hpp", "filename": "IpScaledMatrix.hpp", "file": "IpScaledMatrix.hpp", "brief": "Matrix decorator applying row and column scaling\n\nScaledMatrix wraps an unscaled matrix M to represent D_r * M * D_c\nwhere D_r and D_c are diagonal scaling matrices (stored as vectors).\n\nImplements the Decorator pattern - operations transparently apply\nscaling before/after delegating to the underlying matrix.\n\nUsed in Ipopt for:\n- NLP scaling (user-provided or automatic gradient-based)\n- Constraint Jacobian scaling\n- Hessian scaling for numerical stability", "see": ["IpSymScaledMatrix.hpp for symmetric version", "IpMatrix.hpp for base interface"], "has_pass2": false}, "src/LinAlg/IpZeroMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpZeroMatrix.hpp", "filename": "IpZeroMatrix.hpp", "file": "IpZeroMatrix.hpp", "brief": "Matrix with all zero entries (null matrix)\n\nZeroMatrix represents a matrix of zeros with given dimensions.\nNo storage required. Matrix-vector multiply just scales y by beta.\n\nUsed in Ipopt for:\n- Placeholder in CompoundMatrix for absent blocks\n- Problems with no constraints (zero Jacobian)\n- Structural zeros in KKT system", "see": ["IpZeroSymMatrix.hpp for symmetric version", "IpCompoundMatrix.hpp where null blocks appear"], "has_pass2": false}, "src/LinAlg/IpDenseVector.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDenseVector.hpp", "filename": "IpDenseVector.hpp", "file": "IpDenseVector.hpp", "brief": "Dense vector implementation storing contiguous Number arrays\n\nDenseVector is the primary Vector implementation in Ipopt.\nTwo storage modes:\n- Heterogeneous: Full array of values (Values() method)\n- Homogeneous: Single scalar repeated for all elements (Scalar() method)\n\nCheck IsHomogeneous() before accessing values to use appropriate method.\n\nFeatures:\n- Efficient BLAS operations via IpBlas wrappers\n- Metadata support for debugging (variable names)\n- DenseVectorSpace factory for creating compatible vectors\n\nOperations delegate to BLAS for performance when heterogeneous.", "see": ["IpVector.hpp for abstract interface", "IpCompoundVector.hpp for composite vectors"], "has_pass2": false}, "src/LinAlg/IpTransposeMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpTransposeMatrix.hpp", "filename": "IpTransposeMatrix.hpp", "file": "IpTransposeMatrix.hpp", "brief": "Matrix wrapper representing transpose of another matrix\n\nTransposeMatrix wraps an existing matrix M to represent M^T without\nexplicitly forming the transpose. MultVector becomes TransMultVector\non the original, and vice versa.\n\nImplements the Adapter pattern - provides transposed interface while\ndelegating to the original matrix's operations.\n\nUsed in Ipopt for:\n- Accessing Jacobian as J or J^T without storing both\n- Building KKT system blocks from constraint Jacobian", "see": ["IpMatrix.hpp for base interface"], "has_pass2": false}, "src/LinAlg/IpSymScaledMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSymScaledMatrix.hpp", "filename": "IpSymScaledMatrix.hpp", "file": "IpSymScaledMatrix.hpp", "brief": "Symmetric matrix with symmetric (congruent) scaling\n\nSymScaledMatrix wraps a symmetric matrix M to represent D * M * D\nwhere D is a diagonal scaling matrix. Unlike ScaledMatrix which has\nseparate row/column scaling, symmetric scaling uses the same vector.\n\nPreserves symmetry: if M is symmetric, D * M * D is also symmetric.\nDecorator pattern - operations transparently apply scaling.\n\nUsed in Ipopt for:\n- Scaled Hessian of Lagrangian\n- Symmetrically scaled KKT system", "see": ["IpScaledMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": false}, "src/LinAlg/IpSumMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSumMatrix.hpp", "filename": "IpSumMatrix.hpp", "file": "IpSumMatrix.hpp", "brief": "Matrix representing weighted sum of matrices: M = sum(alpha_i * M_i)\n\nSumMatrix represents a matrix as a sum of terms, each with its own\nscalar factor: M = alpha_0*M_0 + alpha_1*M_1 + ... + alpha_n*M_n\n\nMatrix-vector multiply distributes across terms. This avoids forming\nexplicit sums for matrices that are only accessed via matvec.\n\nUsed in Ipopt for:\n- Combining multiple contributions to Jacobians or Hessians\n- Building composite matrix structures lazily", "see": ["IpSumSymMatrix.hpp for symmetric version", "IpCompoundMatrix.hpp for block structure (vs linear combination)"], "has_pass2": false}, "src/LinAlg/IpZeroSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpZeroSymMatrix.hpp", "filename": "IpZeroSymMatrix.hpp", "file": "IpZeroSymMatrix.hpp", "brief": "Symmetric matrix with all zero entries\n\nZeroSymMatrix represents a symmetric zero matrix (n x n).\nNo storage required. Inherits from SymMatrix for type safety.\n\nUsed in Ipopt for:\n- Problems with linear objectives (zero Hessian of objective)\n- Placeholder in CompoundSymMatrix for zero blocks\n- Default Hessian when user doesn't provide one", "see": ["IpZeroMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": false}, "src/LinAlg/IpMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpMatrix.hpp", "filename": "IpMatrix.hpp", "file": "IpMatrix.hpp", "brief": "Abstract base class for all (unsymmetric) matrix types\n\nMatrix provides the abstraction for general (non-symmetric) matrices.\nPrimary operations: MultVector (y = α*A*x + β*y) and TransMultVector.\n\nKey design features:\n- MatrixSpace factory for creating compatible matrices\n- Template Method pattern: override *_Impl methods\n- Dimension information via OwnerSpace()\n- Optional methods: ComputeRowAMax, ComputeColAMax, HasValidNumbers\n\nImplementations include:\n- Dense: DenseGenMatrix\n- Structured: CompoundMatrix, SumMatrix, ScaledMatrix\n- Special: ExpansionMatrix, TransposeMatrix, DiagMatrix", "see": ["IpSymMatrix.hpp for symmetric matrices", "IpVector.hpp for vector abstraction"], "has_pass2": false}, "src/LinAlg/IpCompoundSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpCompoundSymMatrix.hpp", "filename": "IpCompoundSymMatrix.hpp", "file": "IpCompoundSymMatrix.hpp", "brief": "Block-structured symmetric matrix (lower triangle storage)\n\nCompoundSymMatrix implements a symmetric block matrix where only\nthe lower triangular blocks are stored: M[i][j] with j <= i.\nDiagonal blocks must themselves be SymMatrix types.\n\nUsed in Ipopt for the full symmetric KKT system:\n  [W + Sigma_x    J_c^T    J_d^T ]\n  [    J_c       -delta_c    0   ]\n  [    J_d          0    -Sigma_s]\n\nNull blocks are treated as zeros. Operations recurse to components.", "see": ["IpCompoundMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": false}, "src/LinAlg/IpExpandedMultiVectorMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpExpandedMultiVectorMatrix.hpp", "filename": "IpExpandedMultiVectorMatrix.hpp", "file": "IpExpandedMultiVectorMatrix.hpp", "brief": "Short-fat matrix V^T*P^T with expansion for KKT construction\n\nExpandedMultiVectorMatrix represents a k x n matrix (k << n) as\nV^T * P^T where V is a MultiVectorMatrix-like collection of row\nvectors and P is an optional ExpansionMatrix.\n\nUsed in Ipopt's KKT system construction for low-rank Hessian:\nWhen Hessian is D + V*V^T, the augmented system includes blocks\ninvolving V^T that map to different dimensions.\n\nNull row vectors are treated as zero rows.", "see": ["IpMultiVectorMatrix.hpp for column-vector variant", "IpExpansionMatrix.hpp for P expansion", "IpLowRankUpdateSymMatrix.hpp for Hessian structure"], "has_pass2": false}, "src/LinAlg/IpCompoundVector.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpCompoundVector.hpp", "filename": "IpCompoundVector.hpp", "file": "IpCompoundVector.hpp", "brief": "Composite vector stacking multiple sub-vectors\n\nCompoundVector implements the Composite pattern for vectors,\nrepresenting: x_compound = [x_0; x_1; ...; x_{n-1}]\n\nUsed in Ipopt for structured iterate vectors combining:\n- Primal variables x\n- Slack variables s\n- Dual variables (y_c, y_d, z_L, z_U)\n\nOperations recurse to components. Each component can be from\na different VectorSpace (heterogeneous composition).", "see": ["IpVector.hpp for base interface", "IpCompoundMatrix.hpp for matrix counterpart"], "has_pass2": false}, "src/LinAlg/IpDiagMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDiagMatrix.hpp", "filename": "IpDiagMatrix.hpp", "file": "IpDiagMatrix.hpp", "brief": "Diagonal matrix stored as a vector\n\nDiagMatrix efficiently represents diagonal matrices by storing only\nthe diagonal elements as a Vector. Matrix-vector multiply is O(n).\n\nUsed extensively in Ipopt's KKT system for:\n- Barrier Hessian contribution: diag(z_L/x - x_L) + diag(z_U/x_U - x)\n- Slack variable Hessians\n- Scaling matrices", "see": ["IpIdentityMatrix.hpp for scalar multiple of identity", "IpSymMatrix.hpp for base class"], "has_pass2": false}, "src/LinAlg/IpSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSymMatrix.hpp", "filename": "IpSymMatrix.hpp", "file": "IpSymMatrix.hpp", "brief": "Abstract base class for symmetric matrices\n\nSymMatrix extends Matrix for symmetric matrices (A = A^T).\nTransMultVector automatically delegates to MultVector since\ntranspose is a no-op for symmetric matrices.\n\nUsed for:\n- Hessian of Lagrangian\n- KKT system matrix blocks\n- Quasi-Newton approximations (BFGS, SR1)\n\nImplementations include:\n- DenseSymMatrix: Full dense storage\n- CompoundSymMatrix, SumSymMatrix: Structured composites\n- LowRankUpdateSymMatrix: Limited-memory representations", "see": ["IpMatrix.hpp for general (unsymmetric) matrices", "IpDenseSymMatrix.hpp for dense implementation"], "has_pass2": false}, "src/LinAlg/IpDenseSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDenseSymMatrix.hpp", "filename": "IpDenseSymMatrix.hpp", "file": "IpDenseSymMatrix.hpp", "brief": "Dense symmetric matrix in BLAS lower-triangular storage\n\nDenseSymMatrix stores only the lower triangle in column-major format,\nfollowing BLAS/LAPACK conventions for symmetric matrices.\n\nKey operations:\n- HighRankUpdate: M = alpha*V*V^T + beta*M (for quasi-Newton updates)\n- SpecialAddForLMSR1: Specialized update for limited-memory SR1\n\nUsed in Ipopt for:\n- Limited-memory quasi-Newton Hessian approximations (L-BFGS, L-SR1)\n- Dense KKT subsystems in reduced-space methods\n- Eigenvalue computations for inertia", "see": ["IpDenseGenMatrix.hpp for general (non-symmetric) variant", "IpMultiVectorMatrix.hpp for high-rank update operands"], "has_pass2": false}, "src/LinAlg/IpDenseGenMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDenseGenMatrix.hpp", "filename": "IpDenseGenMatrix.hpp", "file": "IpDenseGenMatrix.hpp", "brief": "Dense general (non-symmetric) matrix with linear algebra operations\n\nDenseGenMatrix stores elements in column-major (Fortran) format\nand provides direct factorization capabilities:\n- Cholesky factorization (for positive definite matrices)\n- LU factorization with pivoting (for general matrices)\n- Forward/back substitution solves\n\nUsed in Ipopt for:\n- Small dense subsystems in limited-memory quasi-Newton\n- Eigenvalue decomposition for inertia correction\n- Dense reduced-space methods", "see": ["IpDenseSymMatrix.hpp for symmetric variant", "IpLapack.hpp for underlying LAPACK routines"], "return": "false if the factorization could not be done, e.g., when\n the matrix is not positive definite.", "has_pass2": false}, "src/LinAlg/IpLapack.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpLapack.hpp", "filename": "IpLapack.hpp", "file": "IpLapack.hpp", "brief": "C++ wrappers for LAPACK (Linear Algebra PACKage) routines\n\nProvides platform-independent access to LAPACK for dense matrices:\n\nCholesky factorization and solve:\n- IpLapackPotrf: Factor A = L*L^T (DPOTRF/SPOTRF)\n- IpLapackPotrs: Solve using Cholesky factor (DPOTRS/SPOTRS)\n\nLU factorization:\n- IpLapackSytrf: Symmetric indefinite factorization (DSYTRF/SSYTRF)\n- IpLapackSytrs: Solve using symmetric factorization\n\nEigenvalues:\n- IpLapackSyev: Eigenvalues of symmetric matrix (DSYEV/SSYEV)\n\nUsed by dense factorization backends (e.g., ClpCholeskyDense).", "see": ["IpBlas.hpp for lower-level vector operations", "Linear solver interfaces for sparse factorization"], "has_pass2": false}, "src/Apps/AmplSolver/AmplTNLP.hpp": {"path": "layer-2/Ipopt/src/Apps/AmplSolver/AmplTNLP.hpp", "filename": "AmplTNLP.hpp", "return": "a pointer to a char* with the name of the stub", "has_pass2": false}, "src/contrib/CGPenalty/IpCGPenaltyLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyLSAcceptor.hpp", "filename": "IpCGPenaltyLSAcceptor.hpp", "file": "IpCGPenaltyLSAcceptor.hpp", "brief": "Chen-Goldfarb penalty function line search acceptor\n\nCGPenaltyLSAcceptor implements the backtracking line search acceptor\nbased on the Chen-Goldfarb penalty function approach, providing an\nalternative to the standard filter method.\n\nThe penalty function merit is:\n  phi_rho(x) = f(x) + rho * ||c(x)||_1\n\nAcceptance criteria:\n1. Armijo condition on the penalty function\n2. Piecewise linear penalty function (PLPF) acceptability\n\nThe PLPF maintains a list of (penalty_parameter, objective, infeasibility)\ntriples to avoid the Maratos effect while ensuring global convergence.\n\nSupports second-order correction steps and watchdog procedure\nsimilar to the standard filter line search.", "see": ["IpPiecewisePenalty.hpp for the PLPF data structure", "IpCGSearchDirCalc.hpp for direction computation", "IpBacktrackingLSAcceptor.hpp for base interface"], "return": "false, if no such fall back option is available", "has_pass2": false}, "src/contrib/CGPenalty/IpPiecewisePenalty.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpPiecewisePenalty.hpp", "filename": "IpPiecewisePenalty.hpp", "file": "IpPiecewisePenalty.hpp", "brief": "Piecewise linear penalty function (PLPF) data structure\n\nPiecewisePenalty maintains a list of break points for the piecewise\nlinear penalty function used in the Chen-Goldfarb globalization.\n\nEach entry (PiecewisePenEntry) stores:\n- pen_r: Penalty parameter value at this break point\n- barrier_obj: Barrier objective function value\n- infeasi: Constraint violation (infeasibility)\n\nThe PLPF defines an envelope of acceptable points. A trial point\nis acceptable if it lies below the piecewise linear function\ndefined by these break points.\n\nThis approach provides non-monotone globalization that can accept\nsteps rejected by simple Armijo, avoiding the Maratos effect\nwhile maintaining global convergence guarantees.", "see": ["IpCGPenaltyLSAcceptor.hpp for usage in line search", "IpFilterLSAcceptor.hpp for filter-based alternative"], "return": "true, if pair is acceptable", "has_pass2": false}, "src/contrib/CGPenalty/IpCGPerturbationHandler.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPerturbationHandler.hpp", "filename": "IpCGPerturbationHandler.hpp", "file": "IpCGPerturbationHandler.hpp", "brief": "Perturbation handler for Chen-Goldfarb penalty method\n\nCGPerturbationHandler manages the diagonal perturbations (delta_x,\ndelta_s, delta_c, delta_d) for the KKT system when using the\nChen-Goldfarb penalty function approach.\n\nThe perturbations are added to handle:\n- Singular or nearly singular KKT matrices\n- Wrong inertia in the factorization\n- Pure Newton vs penalty-regularized steps\n\nKey methods:\n- ConsiderNewSystem(): Returns initial perturbations for new matrix\n- PerturbForSingularity(): Increases perturbations when singular\n\nThe handler coordinates with CGPenaltyData to track whether pure\nNewton steps should be attempted or if penalty regularization is\nneeded throughout the optimization.", "see": ["IpPDPerturbationHandler.hpp for base class", "IpCGSearchDirCalc.hpp for direction computation"], "return": "false, if no suitable perturbation could be found", "has_pass2": false}, "src/contrib/CGPenalty/IpCGPenaltyCq.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyCq.hpp", "filename": "IpCGPenaltyCq.hpp", "file": "IpCGPenaltyCq.hpp", "brief": "Calculated quantities for Chen-Goldfarb penalty method\n\nCGPenaltyCq provides cached computation of quantities specific to\nthe Chen-Goldfarb penalty function algorithm, extending the base\nIpoptCalculatedQuantities.\n\nKey computed quantities:\n- curr/trial_penalty_function(): phi_rho(x) = f(x) + rho*||c(x)||\n- curr_direct_deriv_penalty_function(): Directional derivative\n- curr_fast_direct_deriv_penalty_function(): For fast CG direction\n- curr_cg_pert_fact(): Perturbation factor for KKT regularization\n- compute_curr_cg_penalty(): Line search penalty parameter\n- compute_curr_cg_penalty_scale(): KKT scaling penalty\n\nAlso computes Jacobian norms and multiplier scaling factors\nneeded for the penalty parameter update rules.\n\nAll quantities use CachedResults for efficiency.", "see": ["IpIpoptCalculatedQuantities.hpp for base class", "IpCGPenaltyData.hpp for algorithm state"], "has_pass2": false}, "src/contrib/CGPenalty/IpCGSearchDirCalc.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGSearchDirCalc.hpp", "filename": "IpCGSearchDirCalc.hpp", "file": "IpCGSearchDirCalc.hpp", "brief": "Search direction calculator for Chen-Goldfarb penalty method\n\nCGSearchDirCalculator computes the search direction for the\nChen-Goldfarb penalty function algorithm. It produces two types\nof directions stored in CGPenaltyData:\n\n1. delta_cgpen: Standard CG direction with penalty regularization\n2. delta_cgfast: \"Fast\" direction attempting pure Newton steps\n\nThe fast direction is tried when the iterate is close to the\nsolution (measured by various criteria like kappa_x_dis_,\nkappa_y_dis_). Using the fast direction can achieve superlinear\nconvergence near the solution.\n\nThe penalty parameter is initialized and updated based on the\nKKT system properties and step quality. Parameters control:\n- penalty_init_min/max: Initial penalty bounds\n- penalty_max_: Maximum allowed penalty\n- pen_des_fact_: Desired decrease factor", "see": ["IpSearchDirCalculator.hpp for base interface", "IpCGPenaltyLSAcceptor.hpp for line search using these directions"], "has_pass2": false}, "src/contrib/CGPenalty/IpCGPenaltyData.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyData.hpp", "filename": "IpCGPenaltyData.hpp", "file": "IpCGPenaltyData.hpp", "brief": "Algorithm state data for Chen-Goldfarb penalty method\n\nCGPenaltyData extends IpoptAdditionalData to store all state\nspecific to the Chen-Goldfarb penalty function algorithm.\n\nStored directions:\n- delta_cgpen_: Standard Chen-Goldfarb search direction\n- delta_cgfast_: Fast (pure Newton) direction when near solution\n\nPenalty parameters:\n- curr_penalty_: Current line search penalty parameter (rho)\n- curr_kkt_penalty_: Penalty for KKT system scaling\n- curr_penalty_pert_: Current perturbation factor\n\nAlgorithm control:\n- never_try_pure_Newton_: Flag to disable fast direction\n- restor_iter_: Iteration count for restoration tracking\n- max_alpha_x_: Recorded primal step size\n\nThe data is accessed via IpData().AdditionalData() and cast\nto CGPenaltyData using the CGPenData() accessor pattern.", "see": ["IpIpoptData.hpp for base iteration data", "IpCGPenaltyCq.hpp for computed quantities"], "has_pass2": false}, "src/contrib/CGPenalty/IpCGPenaltyRegOp.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyRegOp.hpp", "filename": "IpCGPenaltyRegOp.hpp", "file": "IpCGPenaltyRegOp.hpp", "brief": "Registration of Chen-Goldfarb penalty method options\n\nDeclares RegisterOptions_CGPenalty(), which registers all options\nspecific to the Chen-Goldfarb penalty function globalization\nstrategy.\n\nOptions include:\n- Penalty parameter initialization and bounds\n- PLPF (piecewise linear penalty function) parameters\n- Fast direction control parameters\n- Armijo and sufficient decrease factors\n- Perturbation handling parameters\n\nCalled when Ipopt is configured to use the CG penalty method\ninstead of the default filter line search.", "see": ["IpAlgRegOp.hpp for main algorithm options", "IpCGPenaltyLSAcceptor.hpp for option usage"], "has_pass2": false}, "src/Algorithm/Inexact/IpInexactLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactLSAcceptor.hpp", "filename": "IpInexactLSAcceptor.hpp", "param": ["in_watchdog indicates if we are currently in an active watchdog procedure", "alpha_primal_test the value of alpha that has been used\n for in the acceptance test earlier."], "has_pass2": false}, "src/Algorithm/Inexact/IpIterativePardisoSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpIterativePardisoSolverInterface.hpp", "filename": "IpIterativePardisoSolverInterface.hpp", "return": "true, if linear solver provides inertia", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpPardisoMKLSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpPardisoMKLSolverInterface.hpp", "filename": "IpPardisoMKLSolverInterface.hpp", "file": "IpPardisoMKLSolverInterface.hpp", "brief": "Interface to Intel MKL PARDISO sparse solver\n\nPardisoMKLSolverInterface wraps Intel's MKL implementation of PARDISO.\nWhile sharing the same API as pardiso-project.org's version, Intel MKL\nPARDISO has some differences in features and behavior.\n\nKey differences from pardiso-project.org version:\n- Linked directly with Intel MKL (no dynamic loading)\n- No DPARM array (only IPARM for parameters)\n- No iterative solver mode\n- Different default parameters in some cases\n\nInput format: CSR_Format_1_Offset (upper triangular)\nProvides inertia: Yes\nMTYPE=-2: Real symmetric indefinite\n\nMatching strategies same as pardiso-project.org version:\nCOMPLETE, COMPLETE2x2, CONSTRAINT\n\nIntroduced in Ipopt 3.14.0 as a distinct interface from the\npardiso-project.org version.", "see": ["IpPardisoSolverInterface.hpp for pardiso-project.org version", "IpMa97SolverInterface.hpp for HSL alternative"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpTSymLinearSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTSymLinearSolver.hpp", "filename": "IpTSymLinearSolver.hpp", "file": "IpTSymLinearSolver.hpp", "brief": "Driver connecting SymMatrix to sparse linear solver interfaces\n\nTSymLinearSolver is the main driver that connects Ipopt's SymMatrix\nobjects to concrete sparse linear solvers. It handles:\n- Matrix format conversion (SymMatrix to triplet/CSR)\n- Optional matrix scaling\n- Delegation to SparseSymLinearSolverInterface implementations\n\nFormat handling:\n- Queries solver's MatrixFormat() preference\n- Uses TripletToCSRConverter if solver needs CSR format\n- Stores triplet indices in airn_/ajcn_ arrays\n\nScaling support:\n- Optional TSymScalingMethod for equilibration\n- Scales matrix, RHS, and solution: D*A*D, D*b, D^{-1}*x\n- linear_scaling_on_demand_: Enable scaling when quality increase requested\n\nThe tag system (atag_) tracks matrix changes to avoid redundant\nfactorizations when the matrix hasn't changed.", "see": ["IpSparseSymLinearSolverInterface.hpp for solver interface", "IpTripletToCSRConverter.hpp for format conversion", "IpTSymScalingMethod.hpp for scaling"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpTripletToCSRConverter.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTripletToCSRConverter.hpp", "filename": "IpTripletToCSRConverter.hpp", "file": "IpTripletToCSRConverter.hpp", "brief": "Convert sparse matrix from triplet to CSR format\n\nTripletToCSRConverter converts symmetric matrices from triplet\n(COO) format to Compressed Sparse Row (CSR) format. Handles both\nupper-triangular-only and full-matrix storage.\n\nTriplet format: (row[k], col[k], val[k]) for k=0..nnz-1\n- May contain duplicates (summed during conversion)\n- May have entries in either triangle (normalized to upper)\n\nCSR format: ia[0..n], ja[0..nnz_csr-1]\n- ia[i] = start of row i in ja\n- ja = column indices, sorted by row then column\n- Offset 0 (C-style) or 1 (Fortran-style) supported\n\nConversion process:\n1. InitializeConverter(): Build mapping structure (once per pattern)\n2. ConvertValues(): Apply mapping to convert values (each factorization)\n\nDuplicate handling:\n- ipos_first_[i]: First triplet entry contributing to CSR position i\n- ipos_double_*: Additional triplet entries to add", "see": ["IpTSymLinearSolver.hpp for usage"], "return": "number of nonzeros in the condensed matrix.\n (Since nonzero elements can be listed several times\n in the triplet format, it is possible that this value is\n different from the input value nonzeros.)\n\n This method must be called before IA, JA, or ConvertValues.", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMc19TSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMc19TSymScalingMethod.hpp", "filename": "IpMc19TSymScalingMethod.hpp", "file": "IpMc19TSymScalingMethod.hpp", "brief": "Matrix scaling using HSL MC19 equilibration\n\nMc19TSymScalingMethod uses the HSL subroutine MC19 to compute\nequilibration scaling factors for symmetric matrices.\n\nMC19 computes scaling to make row/column norms approximately equal,\nimproving the condition number for factorization. The algorithm\nis based on iterative refinement of scaling factors.\n\nMC19 interface (Fortran):\n  MC19A(N, NZ, A, IRN, ICN, R, C, W)\nNote: R, C, W are single precision (float) even in double version.\n\nLibrary loading:\n- Can be linked at compile time\n- Or loaded dynamically via LibraryLoader at runtime\n- SetFunctions() allows setting function pointer globally\n\nThe scaling factors returned satisfy ||D*A*D||_inf ≈ 1 where\nD = diag(exp(R)) and the matrix is approximately equilibrated.", "see": ["IpTSymScalingMethod.hpp for base interface", "IpEquilibrationScaling.hpp for NLP-level scaling"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpWsmpSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpWsmpSolverInterface.hpp", "filename": "IpWsmpSolverInterface.hpp", "file": "IpWsmpSolverInterface.hpp", "brief": "Interface to IBM WSMP sparse symmetric direct solver\n\nWsmpSolverInterface wraps the Watson Sparse Matrix Package (WSMP),\na high-performance parallel direct solver developed at IBM.\n\nWSMP characteristics:\n- Parallel: Multi-threaded with shared memory\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n- Provides degeneracy detection\n- Supports both direct and iterative modes\n\nKey parameters:\n- wsmp_num_threads_: Number of threads for parallel execution\n- wsmp_pivtol_/wsmp_pivtolmax_: Pivot tolerance bounds\n- wsmp_scaling_: Scaling method selection\n- wsmp_singularity_threshold_: Singularity detection threshold\n- wsmp_no_pivoting_: Use positive definite mode (faster but unsafe)\n\nUses IPARM and DPARM arrays for parameter control, similar to PARDISO.\nPERM/INVP arrays store the fill-reducing ordering.\n\nCan optionally use PARDISO preprocessing for matching.", "see": ["IpIterativeWsmpSolverInterface.hpp for iterative mode", "IpPardisoSolverInterface.hpp for similar direct solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMumpsSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMumpsSolverInterface.hpp", "filename": "IpMumpsSolverInterface.hpp", "file": "IpMumpsSolverInterface.hpp", "brief": "Interface to MUMPS parallel sparse direct solver\n\nMumpsSolverInterface wraps MUMPS (MUltifrontal Massively Parallel\nsparse direct Solver), a freely available solver supporting MPI\nparallelism for distributed memory systems.\n\nMUMPS characteristics:\n- Parallel: MPI-based (also sequential mode)\n- Input format: Triplet_Format (lower triangular)\n- Provides inertia\n- Provides degeneracy detection (ProvidesDegeneracyDetection)\n- Open source (CeCILL-C license)\n\nJob codes in MUMPS:\n- Job 1: Analysis (symbolic factorization)\n- Job 2: Numerical factorization\n- Job 3: Solve phase\n\nKey parameters:\n- mumps_permuting_scaling_: Permutation and scaling method\n- mumps_pivot_order_: Pivot ordering (AMD, METIS, etc.)\n- mumps_scaling_: Additional scaling options\n- mumps_dep_tol_: Threshold for dependency detection\n\nOften the default linear solver for Ipopt when HSL is unavailable.", "see": ["IpMa57TSolverInterface.hpp for HSL alternative", "IpMa97SolverInterface.hpp for parallel HSL solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpTSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTSymScalingMethod.hpp", "filename": "IpTSymScalingMethod.hpp", "file": "IpTSymScalingMethod.hpp", "brief": "Base class for matrix scaling in triplet format\n\nTSymScalingMethod is the abstract base class for computing diagonal\nscaling factors for symmetric matrices. Scaling improves numerical\nconditioning of the linear system.\n\nScaling transformation:\n  Original: A * x = b\n  Scaled:   (D*A*D) * (D^{-1}*x) = D*b\nwhere D = diag(scaling_factors).\n\nThe ComputeSymTScalingFactors method takes:\n- n: matrix dimension\n- nnz: number of nonzeros\n- airn, ajcn: row/column indices (triplet format)\n- a: matrix values\n- scaling_factors: output array of length n\n\nImplementations:\n- Mc19TSymScalingMethod: HSL MC19 equilibration\n- SlackBasedTSymScalingMethod: Simple slack-based scaling", "see": ["IpMc19TSymScalingMethod.hpp for MC19 implementation", "IpTSymLinearSolver.hpp for usage"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpSlackBasedTSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSlackBasedTSymScalingMethod.hpp", "filename": "IpSlackBasedTSymScalingMethod.hpp", "file": "IpSlackBasedTSymScalingMethod.hpp", "brief": "Simple scaling based on current slack values\n\nSlackBasedTSymScalingMethod computes scaling factors using only\nthe current slack values, without requiring external HSL routines.\nDesigned for use with inexact/iterative linear solvers.\n\nUnlike MC19 which performs full equilibration, this method uses\na simpler heuristic based on:\n- Current slack variable values s\n- Diagonal elements of the KKT system\n\nBenefits:\n- No external library dependencies\n- Lightweight computation\n- Suitable when full equilibration is unnecessary\n\nLimitations:\n- May not achieve as good conditioning as MC19\n- Best for problems where slacks dominate scaling needs", "see": ["IpTSymScalingMethod.hpp for base interface", "IpMc19TSymScalingMethod.hpp for full equilibration"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpGenKKTSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpGenKKTSolverInterface.hpp", "filename": "IpGenKKTSolverInterface.hpp", "file": "IpGenKKTSolverInterface.hpp", "brief": "Generic interface for iterative/matrix-free KKT solvers\n\nGenKKTSolverInterface provides an alternative to\nSparseSymLinearSolverInterface for solvers that work with the\nfull KKT structure rather than requiring explicit sparse matrices.\n\nThe 4x4 block KKT system:\n  [W + D_x + delta_x*I,  0,         J_c^T,  J_d^T ] [sol_x]   [rhs_x]\n  [0,          D_s + delta_s*I,   0,     -I     ] [sol_s] = [rhs_s]\n  [J_c,        0,         D_c - delta_c*I,  0  ] [sol_c]   [rhs_c]\n  [J_d,       -I,         0,    D_d - delta_d*I] [sol_d]   [rhs_d]\n\nInterface differences from SparseSymLinearSolverInterface:\n- Receives Matrix/SymMatrix objects directly (not sparse arrays)\n- Diagonal arrays (D_x, D_s, D_c, D_d) as Number* pointers\n- Better suited for iterative solvers (Krylov, GMRES)\n- Can use matrix-vector products without explicit assembly\n\nUsed by GenAugSystemSolver to adapt this interface to AugSystemSolver.", "see": ["IpGenAugSystemSolver.hpp for adapter class", "IpSparseSymLinearSolverInterface.hpp for sparse direct solvers"], "return": "The number of negative eigenvalues of the most recent factorized matrix.\n\n This must not be called if the linear solver does not compute this quantities\n (see ProvidesInertia).", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpSpralSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSpralSolverInterface.hpp", "filename": "IpSpralSolverInterface.hpp", "file": "IpSpralSolverInterface.hpp", "brief": "Interface to SPRAL SSIDS sparse symmetric solver\n\nSpralSolverInterface wraps SPRAL (Sparse Parallel Robust Algorithms\nLibrary), an open-source alternative to HSL solvers developed by\nSTFC RAL. SSIDS is SPRAL's symmetric indefinite direct solver.\n\nSPRAL/SSIDS characteristics:\n- Open source (BSD-3-Clause license)\n- Parallel: GPU (CUDA) and multi-core CPU support\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n- Dynamic scaling strategies (same as MA97)\n\nScaling strategies (scaling_opts):\n- SWITCH_NEVER: No dynamic scaling\n- SWITCH_AT_START: Scale on first factorization\n- SWITCH_ON_DEMAND: Scale when needed\n- SWITCH_NDELAY: Scale based on delayed pivots\n\nSimilar API to MA97 with akeep/fkeep separation for\nanalysis and factorization phases.\n\nRecommended when HSL is unavailable and GPU acceleration is desired.\n\n@since 3.14.0", "see": ["IpMa97SolverInterface.hpp for similar HSL solver", "IpMumpsSolverInterface.hpp for open-source CPU alternative"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMa86SolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa86SolverInterface.hpp", "filename": "IpMa86SolverInterface.hpp", "file": "IpMa86SolverInterface.hpp", "brief": "Interface to HSL MA86 parallel sparse symmetric solver\n\nMa86SolverInterface wraps the HSL MA86 solver, a DAG-based\nparallel direct solver for symmetric indefinite systems.\n\nMA86 characteristics:\n- Parallel: Uses OpenMP for multi-core parallelism\n- DAG-based scheduling for load balancing\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n\nUses MC68 for fill-reducing ordering (AUTO, AMD, or METIS).\n\nOperations:\n- ma86_analyse: Symbolic factorization with ordering\n- ma86_factor: Parallel numerical factorization\n- ma86_factor_solve: Combined factorization and solve\n- ma86_solve: Forward/backward solve\n- ma86_finalise: Release memory\n\nThe order_ array stores the fill-reducing permutation reused\nacross factorizations with the same sparsity pattern.", "see": ["IpMa77SolverInterface.hpp for out-of-core solver", "IpMa97SolverInterface.hpp for newer parallel solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMa28TDependencyDetector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa28TDependencyDetector.hpp", "filename": "IpMa28TDependencyDetector.hpp", "file": "IpMa28TDependencyDetector.hpp", "brief": "Dependency detector using HSL MA28 unsymmetric solver\n\nMa28TDependencyDetector uses the unsymmetric sparse solver MA28\nto detect linearly dependent rows in the constraint Jacobian.\nUnlike the symmetric solvers, MA28 handles general rectangular\nmatrices, making it suitable for analyzing the constraint Jacobian\ndirectly.\n\nThe detection works by attempting LU factorization with threshold\npivoting. When a pivot falls below tolerance (ma28_pivtol_), the\ncorresponding row is flagged as linearly dependent.\n\nInput format: Triplet (row, col, val) for general matrices\n\nUsed by Ipopt's constraint degeneracy detection mechanism.", "see": ["IpTDependencyDetector.hpp for base interface", "IpTSymDependencyDetector.hpp for symmetric solver approach"], "return": "false if there was a problem with the underlying linear solver", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpTSymDependencyDetector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTSymDependencyDetector.hpp", "filename": "IpTSymDependencyDetector.hpp", "file": "IpTSymDependencyDetector.hpp", "brief": "Dependency detection using symmetric linear solver\n\nTSymDependencyDetector detects linearly dependent constraint rows\nby using a TSymLinearSolver that provides degeneracy detection.\n\nMethod:\nSome symmetric linear solvers (e.g., MA57 via ProvidesDegeneracyDetection)\ncan identify dependent rows during factorization. This class\nleverages that capability.\n\nAlgorithm:\n1. Form symmetric matrix J*J^T (or equivalent structure)\n2. Attempt factorization with the TSymLinearSolver\n3. If solver detects singularity, query dependent row indices\n4. Return list of dependent rows in c_deps\n\nRequirements:\n- The underlying linear solver must implement\n  ProvidesDegeneracyDetection() returning true\n- Must implement DetermineDependentRows() for the sparse format\n\nThis is preferred over MA28-based detection when using a solver\nthat already provides this capability.", "see": ["IpTDependencyDetector.hpp for base interface", "IpTSymLinearSolver.hpp for symmetric solver"], "return": "false if there was a problem with the underlying linear solver", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpTDependencyDetector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTDependencyDetector.hpp", "filename": "IpTDependencyDetector.hpp", "file": "IpTDependencyDetector.hpp", "brief": "Base class for detecting linearly dependent constraint rows\n\nTDependencyDetector is the abstract base class for algorithms that\ndetect linearly dependent rows in the constraint Jacobian. This is\nneeded to handle degenerate problems where some constraints are\nredundant.\n\nPurpose:\nIf rank(J_c) < m_c, the KKT system is singular. Detecting and\nremoving dependent rows allows the solver to proceed.\n\nInterface:\n- DetermineDependentRows(): Takes Jacobian in triplet format,\n  returns list of dependent row indices in c_deps\n\nInput format (MA28 style triplet):\n- n_rows, n_cols: Jacobian dimensions\n- n_jac_nz: Number of nonzeros\n- jac_c_vals, jac_c_iRow, jac_c_jCol: Values and indices\n\nThe input arrays may be modified internally (working space).", "see": ["IpTSymDependencyDetector.hpp for symmetric solver-based detection", "IpMa28TDependencyDetector.hpp for MA28-based detection"], "return": "false if there was a problem with the underlying linear solver", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpSymLinearSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSymLinearSolver.hpp", "filename": "IpSymLinearSolver.hpp", "file": "IpSymLinearSolver.hpp", "brief": "Base class for symmetric indefinite linear solvers\n\nSymLinearSolver is the abstract base class for all symmetric linear\nsolvers used in Ipopt's augmented system. The solver must handle\nsymmetric indefinite matrices and optionally provide inertia.\n\nReturn codes (ESymSolverStatus):\n- SYMSOLVER_SUCCESS: Solve completed successfully\n- SYMSOLVER_SINGULAR: Matrix detected as singular\n- SYMSOLVER_WRONG_INERTIA: Negative eigenvalue count doesn't match\n- SYMSOLVER_CALL_AGAIN: Solver needs reallocation (retry with new values)\n- SYMSOLVER_FATAL_ERROR: Unrecoverable error\n\nKey interface methods:\n- MultiSolve(): Factorize and solve for multiple RHS\n- NumberOfNegEVals(): Query inertia after factorization\n- ProvidesInertia(): Check if solver computes inertia\n- IncreaseQuality(): Request higher accuracy (e.g., larger pivots)\n\nThe matrix structure must remain fixed between Initialize and solve calls.", "see": ["IpSparseSymLinearSolverInterface.hpp for sparse format interface", "IpTSymLinearSolver.hpp for triplet format driver"], "return": "the number of negative eigenvalues of the most recent factorized matrix\n\n This must not be called if the linear solver does not compute this quantities\n (see ProvidesInertia).", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMa57TSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa57TSolverInterface.hpp", "filename": "IpMa57TSolverInterface.hpp", "file": "IpMa57TSolverInterface.hpp", "brief": "Interface to HSL MA57 sparse symmetric indefinite solver\n\nMa57TSolverInterface wraps the Harwell MA57 subroutine, an improved\nmultifrontal solver over MA27 with better memory management and\nnumerical stability.\n\nMA57 improvements over MA27:\n- Better pivot selection for numerical stability\n- More efficient memory management\n- Support for multiple right-hand sides\n- Better handling of dense row detection\n\nInput format: Triplet (lower triangular), same as MA27\n\nPhases (MA57A/B/C/E/I):\n- MA57I: Initialize control parameters\n- MA57A: Symbolic factorization\n- MA57B: Numerical factorization\n- MA57C: Solve (job selects A, PLP^T, PDP^T, or PL^T P^T)\n- MA57E: Copy/resize factor arrays\n\nKey parameters:\n- pivtol_/pivtolmax_: Threshold pivoting tolerance\n- ma57_pre_alloc_: Factor for initial workspace allocation", "see": ["IpMa27TSolverInterface.hpp for simpler MA27", "IpMa97SolverInterface.hpp for modern parallel solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpIterativeWsmpSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpIterativeWsmpSolverInterface.hpp", "filename": "IpIterativeWsmpSolverInterface.hpp", "file": "IpIterativeWsmpSolverInterface.hpp", "brief": "Interface to IBM WSMP iterative (WISMP) solver\n\nIterativeWsmpSolverInterface wraps the iterative variant of WSMP\n(called WISMP), which uses incomplete LU factorization as a\npreconditioner for iterative refinement.\n\nKey characteristics:\n- ProvidesInertia: false (unlike direct WSMP)\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Uses incomplete factorization with drop tolerance\n\nIterative-specific parameters:\n- wsmp_inexact_droptol_: Drop tolerance for incomplete factorization\n- wsmp_inexact_fillin_limit_: Maximum fill-in allowed\n\nSince WISMP cannot determine inertia (negative eigenvalue count),\nit may not be suitable for all Ipopt applications that rely on\ninertia correction for KKT system validity.\n\nUseful for very large problems where direct factorization memory\nrequirements are prohibitive.", "see": ["IpWsmpSolverInterface.hpp for direct solver variant", "IpGenKKTSolverInterface.hpp for generic iterative interface"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpPardisoSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpPardisoSolverInterface.hpp", "filename": "IpPardisoSolverInterface.hpp", "file": "IpPardisoSolverInterface.hpp", "brief": "Interface to PARDISO sparse solver from pardiso-project.org\n\nPardisoSolverInterface wraps the PARDISO solver distributed by\npardiso-project.org (not to be confused with Intel MKL's PARDISO).\nPARDISO is a high-performance parallel direct solver for sparse\nsymmetric indefinite systems.\n\nPARDISO characteristics:\n- Parallel: Multi-threaded with OpenMP\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n- Supports iterative refinement\n\nMatching strategies (match_strat_):\n- COMPLETE: Full weighted matching\n- COMPLETE2x2: 2x2 matching for saddle point systems\n- CONSTRAINT: Constraint-preserving matching\n\nPhases (controlled by PHASE parameter):\n- Phase 11: Analysis (reordering, symbolic factorization)\n- Phase 22: Numerical factorization\n- Phase 33: Forward/backward solve\n- Phase -1: Release memory\n\nMTYPE=-2 indicates real symmetric indefinite matrix.", "see": ["IpPardisoMKLSolverInterface.hpp for Intel MKL version", "IpMa97SolverInterface.hpp for alternative parallel solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMa77SolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa77SolverInterface.hpp", "filename": "IpMa77SolverInterface.hpp", "file": "IpMa77SolverInterface.hpp", "brief": "Interface to HSL MA77 out-of-core sparse symmetric solver\n\nMa77SolverInterface wraps the HSL MA77 solver, designed for\nlarge-scale problems that may not fit entirely in memory.\n\nMA77 characteristics:\n- Out-of-core capability: Uses files for factor storage\n- Input format: CSR_Full_Format_1_Offset (both triangles)\n- Provides inertia\n- Supports element-by-element assembly\n\nUses MC68 for fill-reducing ordering (AMD or METIS).\n\nMain operations:\n- ma77_open: Initialize solver and create temporary files\n- ma77_input_vars/reals: Provide matrix structure/values\n- ma77_analyse: Symbolic factorization with MC68 ordering\n- ma77_factor: Numerical factorization\n- ma77_solve: Forward/backward solve\n- ma77_finalise: Cleanup and delete files\n\nThe solver creates 4 temporary files (fname1-4) for factor storage.", "see": ["IpMa86SolverInterface.hpp for in-core parallel solver", "IpMa97SolverInterface.hpp for modern in-core solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpSparseSymLinearSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSparseSymLinearSolverInterface.hpp", "filename": "IpSparseSymLinearSolverInterface.hpp", "file": "IpSparseSymLinearSolverInterface.hpp", "brief": "Interface for sparse symmetric indefinite linear solvers\n\nSparseSymLinearSolverInterface defines the interface that concrete\nlinear solvers (MA27, MA57, MUMPS, Pardiso, etc.) must implement.\n\nMatrix formats (EMatrixFormat):\n- Triplet_Format: (row, col, val) triplets, lower triangular (MA27 style)\n- CSR_Format_0_Offset: CSR with 0-based indexing (C-style)\n- CSR_Format_1_Offset: CSR with 1-based indexing (Fortran-style)\n- CSR_Full_Format_*: Full matrix (both triangles) CSR\n\nUsage protocol:\n1. Initialize(): Set options, allocate internal structures\n2. MatrixFormat(): Query required input format\n3. InitializeStructure(): Provide sparsity pattern, do symbolic factorization\n4. GetValuesArrayPtr(): Get array for matrix values\n5. MultiSolve(): Factorize (if new_matrix) and solve\n\nDuplicate entries in triplet format are summed. Warm start support\nallows reusing symbolic factorization across optimizations.", "see": ["IpMa27TSolverInterface.hpp for Harwell MA27", "IpMumpsSolverInterface.hpp for MUMPS", "IpPardisoSolverInterface.hpp for Pardiso"], "return": "SYMSOLV_SUCCESS if the factorization and\n solves were successful, SYMSOLV_SINGULAR if the linear system\n is singular, and SYMSOLV_WRONG_INERTIA if check_NegEVals is\n true and the number of negative eigenvalues in the matrix does\n not match numberOfNegEVals.  If SYMSOLV_CALL_AGAIN is\n returned, then the calling function will request the pointer\n for the array for storing a again (with GetValuesPtr), write\n the values of the nonzero elements into it, and call this\n MultiSolve method again with the same right-hand sides.  (This\n can be done, for example, if the linear solver realized it\n does not have sufficient memory and needs to redo the\n factorization; e.g., for MA27.)\n\n The number of right-hand sides is given by nrhs, the values of\n the right-hand sides are given in rhs_vals (one full right-hand\n side stored immediately after the other), and solutions are\n to be returned in the same array.\n\n check_NegEVals will not be chosen true, if ProvidesInertia()\n returns false.", "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMa27TSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa27TSolverInterface.hpp", "filename": "IpMa27TSolverInterface.hpp", "file": "IpMa27TSolverInterface.hpp", "brief": "Interface to HSL MA27 sparse symmetric indefinite solver\n\nMa27TSolverInterface wraps the Harwell MA27 subroutine for solving\nsparse symmetric indefinite linear systems using multifrontal\nfactorization with threshold pivoting.\n\nMA27 characteristics:\n- Input format: Triplet (lower triangular)\n- Provides inertia (negative eigenvalue count)\n- Classic serial solver, still widely used\n\nPhases (MA27A/B/C):\n- MA27A: Symbolic factorization, determines pivoting order\n- MA27B: Numerical factorization with threshold pivoting\n- MA27C: Forward/backward solve\n\nKey parameters:\n- pivtol_/pivtolmax_: Pivot tolerance (default 1e-8, max 0.5)\n- liw_init_factor_, la_init_factor_: Workspace sizing\n- meminc_factor_: Memory growth factor when reallocation needed\n\nThe solver may request memory reallocation (SYMSOLVER_CALL_AGAIN)\nif initial workspace estimates prove insufficient.", "see": ["IpMa57TSolverInterface.hpp for improved MA57", "IpSparseSymLinearSolverInterface.hpp for interface"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpMa97SolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa97SolverInterface.hpp", "filename": "IpMa97SolverInterface.hpp", "file": "IpMa97SolverInterface.hpp", "brief": "Interface to HSL MA97 modern parallel sparse symmetric solver\n\nMa97SolverInterface wraps the HSL MA97 solver, the most modern\nHSL solver for symmetric indefinite systems with advanced\nparallelism and scaling strategies.\n\nMA97 characteristics:\n- Modern parallel implementation using OpenMP\n- Advanced scaling options (dynamic, reuse, on-demand)\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n\nOrdering options: AUTO, BEST, AMD, METIS, MATCHED_*\n\nScaling strategies (scale_opts):\n- SWITCH_NEVER: No dynamic scaling\n- SWITCH_AT_START: Scale on first factorization\n- SWITCH_ON_DEMAND: Scale when needed\n- SWITCH_NDELAY: Scale based on delayed pivots\n\nKeeps separate akeep (analysis) and fkeep (factorization) pointers\nto efficiently handle re-factorization with unchanged structure.\n\nRecommended as default HSL solver for parallel systems.", "see": ["IpMa86SolverInterface.hpp for older parallel solver", "IpMa57TSolverInterface.hpp for serial solver"], "has_pass2": false}, "src/Algorithm/LinearSolvers/IpLinearSolversRegOp.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpLinearSolversRegOp.hpp", "filename": "IpLinearSolversRegOp.hpp", "file": "IpLinearSolversRegOp.hpp", "brief": "Registration of all linear solver options\n\nDeclares RegisterOptions_LinearSolvers(), which registers options\nfor all available linear solvers (MA27, MA57, MA77, MA86, MA97,\nMUMPS, Pardiso, WSMP, SPRAL, etc.).\n\nCalled during Ipopt initialization to make solver-specific options\navailable through the OptionsList mechanism. Each solver interface\nhas its own RegisterOptions() method that is invoked here.\n\nOptions typically include:\n- Pivot tolerance bounds\n- Ordering method selection\n- Scaling options\n- Memory allocation factors\n- Parallelism settings (thread counts)", "see": ["IpAlgRegOp.hpp for algorithm options registration", "IpInterfacesRegOp.hpp for interface options"], "has_pass2": false}, "src/Interfaces/IpStdCInterface.h": {"path": "layer-2/Ipopt/src/Interfaces/IpStdCInterface.h", "filename": "IpStdCInterface.h", "return": "false, if the option could not be set (e.g., if keyword is unknown)", "param": ["ipopt_problem (in) Problem that is currently optimized.", "n       (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping x, z_L, and z_U", "scaled  (in)  whether to retrieve scaled or unscaled iterate", "x       (out) buffer to store value of primal variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving x", "z_L     (out) buffer to store the lower bound multipliers \\f$z_L\\f$, must have length at least n; pass NULL to skip retrieving z_L and Z_U", "z_U     (out) buffer to store the upper bound multipliers \\f$z_U\\f$, must have length at least n; pass NULL to skip retrieving z_L and Z_U", "m       (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping g and lambda", "g       (out) buffer to store the constraint values \\f$g(x)\\f$, must have length at least m; pass NULL to skip retrieving g", "lambda  (out) buffer to store the constraint multipliers \\f$\\lambda\\f$, must have length at least m; pass NULL to skip retrieving lambda", "ipopt_problem (in) Problem that is currently optimized.", "scaled     (in)  whether to retrieve scaled or unscaled violations", "n          (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping compl_x_L, compl_x_U, and grad_lag_x", "x_L_violation (out) buffer to store violation of original lower bounds on variables (max(orig_x_L-x,0)), must have length at least n; pass NULL to skip retrieving orig_x_L", "x_U_violation (out) buffer to store violation of original upper bounds on variables (max(x-orig_x_U,0)), must have length at least n; pass NULL to skip retrieving orig_x_U", "compl_x_L  (out) buffer to store violation of complementarity for lower bounds on variables (\\f$(x-x_L)z_L\\f$), must have length at least n; pass NULL to skip retrieving compl_x_L", "compl_x_U  (out) buffer to store violation of complementarity for upper bounds on variables (\\f$(x_U-x)z_U\\f$), must have length at least n; pass NULL to skip retrieving compl_x_U", "grad_lag_x (out) buffer to store gradient of Lagrangian w.r.t. variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving grad_lag_x", "m          (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping lambda", "nlp_constraint_violation (out) buffer to store violation of constraints \\f$max(g_l-g(x),g(x)-g_u,0)\\f$, must have length at least m; pass NULL to skip retrieving constraint_violation", "compl_g    (out) buffer to store violation of complementarity of constraint (\\f$(g(x)-g_l)*\\lambda^+ + (g_l-g(x))*\\lambda^-\\f$, where \\f$\\lambda^+=max(0,\\lambda)\\f$ and \\f$\\lambda^-=max(0,-\\lambda)\\f$ (componentwise)), must have length at least m; pass NULL to skip retrieving compl_g"], "has_pass2": false}}}}}, "layer-3": {"name": "layer-3", "library_count": 10, "libraries": {"Bcp": {"name": "Bcp", "file_count": 31, "pass2_count": 0, "files": {"Applications/Csp/include/CSP_lp.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_lp.hpp", "filename": "CSP_lp.hpp", "param": ["vars       (IN) The variables in the current formulation", "cuts       (IN) The cuts in the current formulation", "var_status (IN) The stati of the variables", "cut_status (IN) The stati of the cuts", "var_changed_pos (OUT) The positions of the variables whose\n       bounds should be tightened", "var_new_bd      (OUT) The new lb/ub of those variables", "cut_changed_pos (OUT) The positions of the cuts whose bounds\n       should be tightened", "cut_new_bd (OUT) The new lb/ub of those cuts", "lpres  the result of the most recent LP optimization", "vars   variables currently in the formulation", "cuts    the cuts currently in the relaxation (IN)", "vars    the variables to be converted (IN/OUT)", "cols    the colums the variables convert into (OUT)", "lpres   solution to the current LP relaxation (IN)", "origin  where the do the cuts come from (IN)", "allow_multiple whether multiple expansion, i.e., lifting, is\n       allowed (IN)\n\n       Default: throw an exception (if this method is invoked then the user\n       must have generated variables and BCP has no way to know how to convert\n       them).", "lpres    solution to the current LP relaxation (IN)", "vars     the variabless currently in the relaxation (IN)", "cuts     the cuts currently in the relaxation (IN)", "new_cuts the vector of generated cuts (OUT)", "new_rows the correspontding rows(OUT)", "lpres         solution to the current LP relaxation (IN)", "vars          the variabless currently in the relaxation (IN)", "cuts          the cuts currently in the relaxation (IN)", "before_fathom if true then BCP is about to fathom the node, so\n       spend some extra effort generating variables if\n       you want to avoid that...", "new_vars      the vector of generated variables (OUT)", "new_cols the correspontding columns(OUT)", "lpres the result of the most recent LP optimization,", "vars the variables in the current formulation,", "status the stati of the variables as known to the system,", "var_bound_changes_since_logical_fixing the number of variables\n       whose bounds have changed (by reduced cost fixing) since the\n       most recent invocation of this method that has actually forced\n       changes returned something in the last two arguments,", "changed_pos the positions of the variables whose bounds should\n       be changed", "new_bd the new bounds (lb/ub pairs) of these variables.", "slack_pool the pool of slacks. (IN)", "to_be_purged the indices of the cuts to be purged. (OUT)"], "has_pass2": false}, "Bcp/src/include/BCP_matrix.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_matrix.hpp", "filename": "BCP_matrix.hpp", "file": "BCP_matrix.hpp", "brief": "Matrix and vector representations for BCP LP relaxation\n\nExtends CoinPackedVector/Matrix with bounds for LP formulation:\n\n**BCP_col (extends CoinPackedVector):**\n- Sparse column with objective coefficient + lb/ub\n- Used for column generation (new variables)\n- Default bounds: [0, +∞), obj = 0\n\n**BCP_row (extends CoinPackedVector):**\n- Sparse row with lb/ub (constraint bounds)\n- Used for cut generation (new constraints)\n- Default bounds: [-∞, +∞]\n\n**BCP_lp_relax (extends CoinPackedMatrix):**\n- Full LP relaxation: matrix + bounds + objective\n- _Objective: Variable costs\n- _ColLowerBound, _ColUpperBound: Variable bounds\n- _RowLowerBound, _RowUpperBound: Constraint bounds\n- Can be row-major or column-major ordered\n- pack()/unpack() for serialization\n\n**Usage:**\nCore matrix defined in initialize_core(), columns added\nvia column generation, rows added via cut generation.", "see": ["BCP_var.hpp for variable representation", "BCP_cut.hpp for cut representation", "CoinPackedMatrix for underlying sparse matrix"], "has_pass2": false}, "Bcp/src/include/BCP_lp_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_user.hpp", "filename": "BCP_lp_user.hpp", "file": "BCP_lp_user.hpp", "brief": "User customization interface for LP process\n\nBCP_lp_user is the main interface for user-defined LP behavior.\nMost BCP customization happens here.\n\n**Initialization:**\n- initialize_solver_interface(): Create OsiSolverInterface\n- initialize_int_and_sos_list(): Define branching objects\n- initialize_new_search_tree_node(): Preprocess before node\n- load_problem(): Load problem into solver\n\n**LP iteration:**\n- process_lp_result(): Main iteration callback (or use individual)\n- test_feasibility(): Check if solution is integer-feasible\n- generate_heuristic_solution(): Run primal heuristics\n- compute_lower_bound(): True lower bound for column generation\n\n**Cut/variable generation:**\n- generate_cuts_in_lp(): Local cut generation\n- generate_vars_in_lp(): Local column generation (pricing)\n- cuts_to_rows(): Convert BCP_cut to matrix rows\n- vars_to_cols(): Convert BCP_var to matrix columns\n\n**Branching:**\n- select_branching_candidates(): Choose branching objects\n- compare_branching_candidates(): Select best candidate\n- set_actions_for_children(): Fathom/Return/Keep decisions\n\n**Packing:**\n- pack_primal_solution(): Send LP solution to CG\n- pack_dual_solution(): Send duals to VG\n- pack_feasible_solution(): Send solution to TM", "see": ["BCP_lp.hpp for LP process", "BCP_tm_user for Tree Manager user interface"], "param": ["vars       (IN) The variables in the current formulation", "cuts       (IN) The cuts in the current formulation", "var_status (IN) The stati of the variables", "cut_status (IN) The stati of the cuts", "var_changed_pos (OUT) The positions of the variables whose\n\t                             bounds should be tightened", "var_new_bd      (OUT) The new lb/ub of those variables", "cut_changed_pos (OUT) The positions of the cuts whose bounds\n\t                             should be tightened", "cut_new_bd (OUT) The new lb/ub of those cuts", "lp_result the result of the most recent LP optimization (IN)", "vars      variables currently in the formulation (IN)", "cuts      variables currently in the formulation (IN)", "old_lower_bound the previously known best lower bound (IN)", "new_cuts  the vector of generated cuts (OUT)", "new_rows  the correspontding rows(OUT)", "new_vars      the vector of generated variables (OUT)", "new_cols the correspontding columns(OUT)", "lp_result the result of the most recent LP optimization", "vars      variables currently in the formulation", "cuts      variables currently in the formulation", "buf (OUT) the buffer to pack into", "sol (IN)  the solution to be packed", "buf       (OUT) the buffer to pack into", "lp_result (IN) the result of the most recent LP optimization", "vars      (IN) variables currently in the formulation", "cuts      (IN) cuts currently in the formulation", "buf       (OUT) the buffer to pack into", "lp_result (IN) the result of the most recent LP optimization", "vars      (IN) variables currently in the formulation", "cuts      (IN) cuts currently in the formulation", "lp_result (IN) the result of the most recent LP optimization", "vars      (IN) variables currently in the formulation", "final_lp_solution (IN) whether the lp solution is final or not.", "vars    the variables currently in the relaxation (IN)", "cuts    the cuts to be converted (IN/OUT)", "rows    the rows into which the cuts are converted (OUT)", "lpres   solution to the current LP relaxation (IN)", "origin  where the cuts come from (IN)", "allow_multiple whether multiple expansion, i.e., lifting, is\n\tallowed (IN)\n\n        Default: throw an exception (if this method is invoked then the user\n        must have generated cuts and BCP has no way to know how to convert\n        them).", "cuts    the cuts currently in the relaxation (IN)", "vars    the variables to be converted (IN/OUT)", "cols    the colums the variables convert into (OUT)", "lpres   solution to the current LP relaxation (IN)", "origin  where the do the cuts come from (IN)", "allow_multiple whether multiple expansion, i.e., lifting, is\n\tallowed (IN)\n\n        Default: throw an exception (if this method is invoked then the user\n        must have generated variables and BCP has no way to know how to convert\n        them).", "lpres    solution to the current LP relaxation (IN)", "vars     the variabless currently in the relaxation (IN)", "cuts     the cuts currently in the relaxation (IN)", "new_cuts the vector of generated cuts (OUT)", "new_rows the correspontding rows(OUT)", "lpres         solution to the current LP relaxation (IN)", "vars          the variabless currently in the relaxation (IN)", "cuts          the cuts currently in the relaxation (IN)", "before_fathom if true then BCP is about to fathom the node, so\n\tspend some extra effort generating variables if\n\tyou want to avoid that...", "new_vars      the vector of generated variables (OUT)", "new_cols the correspontding columns(OUT)", "lpres the result of the most recent LP optimization,", "vars the variables in the current formulation,", "status the stati of the variables as known to the system,", "var_bound_changes_since_logical_fixing the number of variables\n\twhose bounds have changed (by reduced cost fixing) since the\n\tmost recent invocation of this method that has actually forced\n\tchanges returned something in the last two arguments,", "changed_pos the positions of the variables whose bounds should\n\tbe changed", "new_bd the new bounds (lb/ub pairs) of these variables.", "lpres the result of the most recent LP optimization.", "vars the variables in the current formulation.", "cuts the cuts in the current formulation.", "local_var_pool the local pool that holds variables with negative\n\treduced cost. In case of continuing with the node the best so\n\tmany variables will be added to the formulation (those with the\n\tmost negative reduced cost).", "local_cut_pool the local pool that holds violated cuts. In case\n\tof continuing with the node the best so many cuts will be added\n\tto the formulation (the most violated ones).", "cands the generated branching candidates.", "force_branch indicate whether to force branching regardless\n\t       of the size of the local cut/var pools", "slack_pool the pool of slacks. (IN)", "to_be_purged the indices of the cuts to be purged. (OUT)"], "has_pass2": false}, "Bcp/src/include/BCP_vector_change.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_change.hpp", "filename": "BCP_vector_change.hpp", "param": ["new_vec the vector that should result after the change is\n\t       applied to", "old_vec the original vector", "del_pos specifies which entries are to be deleted from old_vec\n               before the change in this object can be applied."], "has_pass2": false}, "Bcp/src/include/BCP_lp_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_param.hpp", "filename": "BCP_lp_param.hpp", "file": "BCP_lp_param.hpp", "brief": "LP process parameters for BCP Branch-Cut-Price\n\nBCP_lp_par defines all configurable parameters for LP processes.\nSet via parameter file: BCP_{param_name} {value}\n\n**Character (boolean) parameters:**\n- BranchOnCuts: Enable branching on slack cuts\n- CompareNew{Cuts,Vars}ToOldOnes: Duplicate detection\n- DoReducedCostFixing*: Variable fixing strategies\n- SendFathomedNodeDesc: Return fathomed nodes to TM\n- LpVerb_*: Verbosity flags for LP events\n\n**Integer parameters:**\n- SlackCutDiscardingStrategy: When to purge slack cuts\n- CutEffectiveCountBeforePool: Iterations before pooling cut\n- CutPoolCheckFrequency: How often to query cut pool\n- IneffectiveConstraints: Definition of ineffective rows\n- MaxCutsAddedPerIteration: Limit on cuts per round\n- MaxPresolveIter: Strong branching iteration limit\n- StrongBranchNum: Candidates for strong branching\n- BranchingObjectComparison: Branch selection rule\n\n**Double parameters:**\n- Granularity: Objective discretization\n- IntegerTolerance: Integer feasibility tolerance\n- *Timeout: Waiting times for cuts/vars\n- MaxRunTime: Time limit", "see": ["BCP_lp.hpp for LP process", "BCP_tm_param.hpp for TM parameters"], "has_pass2": false}, "Bcp/src/include/BCP_branch.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_branch.hpp", "filename": "BCP_branch.hpp", "file": "BCP_branch.hpp", "brief": "Internal branching object for BCP Branch-Cut-Price\n\nRepresents the result of a branching decision as bound changes\non variables and cuts across children nodes.\n\n**BCP_internal_brobj:**\nCreated AFTER any cuts/variables from branching are added.\nStores only bound changes, not the branching object itself.\n\n**Data structure:**\n- _child_num: Number of children (typically 2 for binary branching)\n- _var_positions: Indices of variables with changed bounds\n- _cut_positions: Indices of cuts with changed bounds\n- _var_bounds: New [lb, ub] pairs for each child × each affected var\n- _cut_bounds: New [lb, ub] pairs for each child × each affected cut\n\n**Memory layout:**\nBounds are stored as: child0_var0_lb, child0_var0_ub, child0_var1_lb, ...\nthen child1_var0_lb, child1_var0_ub, ...\nTotal length: 2 × child_num × var_positions.size()\n\n**Usage:**\n- apply_child_bounds(): Modify LP solver for child's bounds\n- pack()/unpack(): Serialize for sending to TM", "see": ["BCP_lp_branch.hpp for LP-side branching object", "BCP_enum_branch.hpp for branching enumerations", "BCP_lp_user::select_branching_candidates() for branching logic"], "has_pass2": false}, "Bcp/src/include/BCP_cut.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cut.hpp", "filename": "BCP_cut.hpp", "file": "BCP_cut.hpp", "brief": "Cut (constraint) representation for BCP Branch-Cut-Price\n\nDefines cut classes used throughout BCP:\n\n**Class hierarchy:**\n- BCP_cut: Abstract base with lb, ub, status, bcpind, effective_count\n- BCP_cut_core: Core cuts that always stay in LP (original constraints)\n- BCP_cut_algo: Algorithmic cuts from cut generation\n- BCP_cut_set: Collection with bulk operations\n\n**Effectiveness tracking:**\n- _eff_cnt: Positive = consecutive iterations cut was tight\n- Negative = consecutive iterations cut was slack\n- Cuts with low effectiveness may be removed from LP\n\n**Cut bounds:**\n- lb, ub: Constraint lb ≤ ax ≤ ub\n- Inactive cut: lb = -∞, ub = +∞ (free constraint)\n\n**Status flags (BCP_obj_status):**\n- BCP_ObjInactive: Cut is free (not binding)\n- BCP_ObjNotRemovable: Cannot be removed (e.g., branching cut)\n- BCP_ObjToBeRemoved: Marked for deletion\n- BCP_ObjDoNotSendToPool: Skip cut pool\n\n**Slack pool:**\nIneffective cuts are moved to slack_pool for potential\nbranching later (branching on cuts).", "see": ["BCP_var.hpp for symmetric variable representation", "BCP_cg.hpp for Cut Generator process", "BCP_lp.hpp for LP process using cuts"], "has_pass2": false}, "Bcp/src/include/BCP_vg.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vg.hpp", "filename": "BCP_vg.hpp", "file": "BCP_vg.hpp", "brief": "Variable Generator process for BCP Branch-Cut-Price (pricing)\n\nThe Variable Generator (VG) performs column generation:\ngenerates variables with negative reduced cost from dual solutions.\n\n**Column generation workflow:**\n1. LP process packs dual solution (cut duals, reduced costs)\n2. VG solves pricing subproblem to find negative reduced cost columns\n3. VG sends new variables back to LP for addition\n\n**BCP_vg_prob members:**\n- cuts, pi: Cuts and their dual values from LP\n- sender: Process ID of requesting LP\n- phase, node_level, node_index, node_iteration: Context info\n- upper_bound: Current best known solution\n- core: Problem core description\n\n**User customization:**\nOverride BCP_vg_user methods to implement pricing:\n- unpack_dual_solution(): Unpack dual values\n- generate_vars(): Main pricing subproblem logic\n- pack_var(): Send variables back to LP\n\n**Relation to LP process:**\nWhen LP becomes primal infeasible during column generation,\nit sends duals to VG. New columns restore feasibility.", "see": ["BCP_vg_user for user override points", "BCP_lp.hpp for LP process that requests columns", "BCP_var.hpp for variable representation"], "has_pass2": false}, "Bcp/src/include/BCP_message.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_message.hpp", "filename": "BCP_message.hpp", "file": "BCP_message.hpp", "brief": "Abstract message passing interface for BCP parallelism\n\nBCP_message_environment is the base class for all message passing\nimplementations. Derived classes implement the protocol:\n\n**Implementations:**\n- BCP_message_single: Single-process (no parallelism)\n- BCP_message_pvm: PVM (Parallel Virtual Machine)\n- BCP_message_mpi: MPI (Message Passing Interface)\n\n**Core operations:**\n- register_process(): Get process ID\n- parent_process(): Get spawning process ID\n- alive(): Test if processes are running\n\n**Point-to-point:**\n- send(): Send message to specific process\n- receive(): Blocking receive with timeout\n- probe(): Non-blocking check for messages\n\n**Collective:**\n- multicast(): Send to multiple processes\n- start_process(): Spawn new process\n- start_processes(): Spawn multiple processes\n\n**Message tags (BCP_message_tag):**\nPredefined tags for different message types (node data,\nsolutions, cuts, variables, etc.)", "see": ["BCP_buffer.hpp for message serialization", "BCP_message_tag.hpp for tag definitions", "BCP_message_single.hpp for single-process implementation"], "has_pass2": false}, "Bcp/src/include/BCP_tm.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm.hpp", "filename": "BCP_tm.hpp", "file": "BCP_tm.hpp", "brief": "Tree Manager process for BCP Branch-Cut-Price framework\n\nThe Tree Manager (TM) coordinates the entire BCP algorithm:\n- Manages the search tree and candidate node selection\n- Tracks global upper/lower bounds\n- Schedules work to LP processes\n- Stores variables/cuts both locally and distributed\n\n**Key classes:**\n- BCP_tm_prob: Central TM state (search tree, bounds, problem core)\n- BCP_tm_stat: Statistics (wait time, queue length by LP count)\n- BCP_slave_params: Parameter sets for LP/CG/VG processes\n\n**Process coordination:**\n- lp_procs: Vector of LP process IDs\n- active_nodes: Map from process ID to current node\n- candidate_list: CoinSearchTreeManager for node selection\n\n**Bound tracking:**\n- lower_bounds: Multiset of unexplored node bounds\n- upper_bound: Best known feasible solution value\n- lb_multiplier: Floor(lb*multiplier) to avoid rounding errors", "see": ["BCP_tm_user for user customization hooks", "BCP_lp.hpp for LP process that receives work"], "has_pass2": false}, "Bcp/src/include/BCP_vg_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vg_param.hpp", "filename": "BCP_vg_param.hpp", "see": ["the man page of the <code>setpriority</code> system function."], "has_pass2": false}, "Bcp/src/include/BCP_cg.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cg.hpp", "filename": "BCP_cg.hpp", "file": "BCP_cg.hpp", "brief": "Cut Generator process for BCP Branch-Cut-Price framework\n\nThe Cut Generator (CG) runs as a separate process that generates\ncutting planes from LP solutions sent by the LP process.\n\n**Workflow:**\n1. LP process packs primal solution (fractional variables)\n2. CG unpacks and generates violated cuts\n3. CG sends cuts back to LP for addition to formulation\n\n**BCP_cg_prob members:**\n- vars, x: Variables and their primal values from LP\n- sender: Process ID of requesting LP\n- phase, node_level, node_index, node_iteration: Context info\n- upper_bound: Current best known solution\n- core: Problem core description\n\n**User customization:**\nOverride BCP_cg_user methods to implement cut generation:\n- unpack_primal_solution(): Unpack LP solution\n- generate_cuts(): Main cut generation logic\n- pack_cut(): Send cuts back to LP", "see": ["BCP_cg_user for user override points", "BCP_lp.hpp for LP process that requests cuts", "BCP_cut.hpp for cut representation"], "has_pass2": false}, "Bcp/src/include/BCP_enum.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_enum.hpp", "filename": "BCP_enum.hpp", "file": "BCP_enum.hpp", "brief": "Core enumerations for BCP Branch-Cut-Price framework\n\nDefines fundamental types controlling BCP algorithm behavior:\n\n**Object types:**\n- BCP_object_t: Core vs. Algorithmic objects\n- BCP_obj_status: Object status flags (removable, inactive, etc.)\n- BCP_var_t: Variable integrality (Binary, Integer, Continuous)\n\n**Algorithm control:**\n- BCP_column_generation: Fathom, Send, or GenerateColumns\n- BCP_storage_t: NoData, Explicit, WrtParent, WrtCore\n- BCP_warmstart_info: None, Root, Parent inheritance\n\n**Feasibility:**\n- BCP_feasibility: NotFeasible, Feasible, HeuristicFeasible\n- BCP_feasibility_test: Binary, Integral, FullTest\n\n**Cut management:**\n- BCP_slack_cut_discarding: AtNewNode, AtNewIteration\n- BCP_CutViolationNorm: Plain, Distance, Directional\n- BCP_IneffectiveConstraints: None, NonzeroSlack, ZeroDualValue", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp.hpp for LP process using these enums"], "has_pass2": false}, "Bcp/src/include/BCP_cg_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cg_param.hpp", "filename": "BCP_cg_param.hpp", "see": ["the man page of the <code>setpriority</code> system function."], "has_pass2": false}, "Bcp/src/include/BCP_cg_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cg_user.hpp", "filename": "BCP_cg_user.hpp", "file": "BCP_cg_user.hpp", "brief": "User customization interface for Cut Generator process\n\nBCP_cg_user is the base class for user-defined cut generation in\na separate process. Users derive and override virtual methods.\n\n**Key methods to override:**\n- unpack_module_data(): Receive initialization from TM\n- unpack_primal_solution(): Receive LP solution\n- generate_cuts(): Main cut generation logic\n\n**Usage pattern:**\n1. LP sends primal solution to CG process\n2. CG unpacks solution via unpack_primal_solution()\n3. generate_cuts() finds violated inequalities\n4. For each cut found, call send_cut() to send to LP\n\n**Informational methods:**\n- upper_bound(): Current best solution value\n- current_phase/level/index/iteration(): Search position\n- get_param()/set_param(): BCP_cg_par parameters\n\n**When to use CG process:**\nUse when cut generation is computationally expensive and\nbenefits from separate process or distributed computation.\nFor cheap cuts, generate locally in BCP_lp_user instead.", "see": ["BCP_cg.hpp for Cut Generator process internals", "BCP_lp_user for LP-side cut generation", "BCP_cg_param.hpp for CG parameters"], "has_pass2": false}, "Bcp/src/include/BCP_obj_change.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_obj_change.hpp", "filename": "BCP_obj_change.hpp", "file": "BCP_obj_change.hpp", "brief": "Delta encoding for variable/cut bound changes\n\nProvides compact representation of how objects (vars/cuts) change\nbetween parent and child nodes in the search tree.\n\n**BCP_obj_change:**\nSingle object's bound change:\n- lb, ub: New lower/upper bounds\n- stat: New BCP_obj_status\n\n**BCP_obj_set_change:**\nEncodes all changes to a set of objects (vars or cuts):\n- _storage: BCP_Storage_Explicit or BCP_Storage_WrtParent\n- _deleted_num: Count of deleted objects\n- _del_change_pos: Indices of deleted/changed objects\n- _change: New (lb, ub, status) for changed objects\n- _new_objs: bcpind values of added objects\n\n**Storage modes:**\n- Explicit: Full list of all objects (for root or checkpoints)\n- WrtParent: Delta from parent - deletions, changes, additions\n\n**Methods:**\n- update(): Apply delta to reconstruct full object set\n- pack()/unpack(): Serialization for inter-process transfer\n- pack_size(): Estimate serialized size\n\nSpace-efficient encoding crucial for large trees where\neach node may have thousands of variables/cuts.", "see": ["BCP_node_change.hpp for full node delta encoding", "BCP_var.hpp, BCP_cut.hpp for object types"], "has_pass2": false}, "Bcp/src/include/BCP_node_change.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_node_change.hpp", "filename": "BCP_node_change.hpp", "file": "BCP_node_change.hpp", "brief": "Complete node delta encoding for tree storage\n\nBCP_node_change aggregates all changes between parent and child nodes\nfor compact tree storage.\n\n**Components:**\n- core_change: Changes to core var/cut bounds (BCP_problem_core_change)\n- var_change: Changes to algorithmic variables (BCP_obj_set_change)\n- cut_change: Changes to algorithmic cuts (BCP_obj_set_change)\n- warmstart: LP warm start information\n\n**Purpose:**\nEnables efficient tree storage by encoding only deltas between\nparent and child nodes instead of full formulations. Critical for\nscaling to large trees where each node may have thousands of vars/cuts.\n\n**Reconstruction:**\nTo reconstruct a node's formulation, start from root (or checkpoint)\nand apply node_change deltas along the path to the target node.\n\n**Serialization:**\npack()/unpack() methods handle inter-process transfer, using\nBCP_user_pack for user-defined warmstart serialization.", "see": ["BCP_problem_core.hpp for core formulation", "BCP_obj_change.hpp for object set deltas", "BCP_tm_node.hpp for TM node representation"], "has_pass2": false}, "Bcp/src/include/BCP_lp.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp.hpp", "filename": "BCP_lp.hpp", "file": "BCP_lp.hpp", "brief": "LP process for BCP Branch-Cut-Price framework\n\nThe LP process is the workhorse that actually solves subproblems:\n- Solves LP relaxations via OsiSolverInterface\n- Generates cuts locally or receives from Cut Generator\n- Generates variables locally or receives from Variable Generator\n- Tests feasibility and applies heuristics\n- Performs branching when stuck\n\n**Key classes:**\n- BCP_lp_prob: Central LP state (solver, pools, current node)\n- BCP_lp_statistics: Timing for feas/cut/var/heuristics/LP/branching\n\n**Problem structure:**\n- core: Variables/cuts that always stay in formulation\n- node/parent: Current search tree node and its parent\n- local_var_pool, local_cut_pool: Generated vars/cuts pending addition\n- slack_pool: Removed cuts kept for potential branching\n\n**LP solving:**\n- master_lp: Original LP solver (template)\n- lp_solver: Working LP solver for current node\n- lp_result: Most recent LP optimization result\n- warmstartRoot: Optional warm start from root for all nodes", "see": ["BCP_lp_user for user customization hooks", "BCP_tm.hpp for Tree Manager that assigns work"], "has_pass2": false}, "Bcp/src/include/BCP_vg_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vg_user.hpp", "filename": "BCP_vg_user.hpp", "file": "BCP_vg_user.hpp", "brief": "User customization interface for Variable Generator process\n\nBCP_vg_user is the base class for user-defined column generation\n(pricing) in a separate process. Users derive and override virtual methods.\n\n**Key methods to override:**\n- unpack_module_data(): Receive initialization from TM\n- unpack_dual_solution(): Receive LP dual values\n- generate_vars(): Main pricing subproblem logic\n\n**Usage pattern:**\n1. LP sends dual solution (pi) to VG process\n2. VG unpacks duals via unpack_dual_solution()\n3. generate_vars() solves pricing problem for negative reduced cost columns\n4. For each variable found, call send_var() to send to LP\n\n**Informational methods:**\n- upper_bound(): Current best solution value\n- current_phase/level/index/iteration(): Search position\n- get_param()/set_param(): BCP_vg_par parameters\n\n**When to use VG process:**\nUse when pricing subproblem is computationally expensive (e.g.,\nshortest path, knapsack) and benefits from separate process.\nFor simple pricing, generate locally in BCP_lp_user instead.", "see": ["BCP_vg.hpp for Variable Generator process internals", "BCP_lp_user for LP-side column generation", "BCP_vg_param.hpp for VG parameters"], "has_pass2": false}, "Bcp/src/include/BCP_solution.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_solution.hpp", "filename": "BCP_solution.hpp", "file": "BCP_solution.hpp", "brief": "Solution representation for BCP Branch-Cut-Price\n\nDefines how MIP feasible solutions are stored and transmitted:\n\n**Class hierarchy:**\n- BCP_solution: Abstract base with objective_value() pure virtual\n- BCP_solution_generic: Default implementation storing (var, value) pairs\n\n**BCP_solution_generic:**\n- _objective: Objective function value\n- _vars: Vector of variables at nonzero level\n- _values: Corresponding values\n- _delete_vars: Whether to delete vars on destruction\n\n**Usage flow:**\n1. LP process finds integer feasible solution\n2. BCP_lp_user::pack_feasible_solution() serializes it\n3. Sent to Tree Manager\n4. BCP_tm_user::unpack_feasible_solution() deserializes\n5. Best solution tracked for upper bound\n\nUsers can derive from BCP_solution for custom solution\nstorage (e.g., to include dual values).", "see": ["BCP_lp_user for packing interface", "BCP_tm_user for unpacking interface"], "has_pass2": false}, "Bcp/src/include/BCP_warmstart.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_warmstart.hpp", "filename": "BCP_warmstart.hpp", "file": "BCP_warmstart.hpp", "brief": "LP warm start information for BCP Branch-Cut-Price\n\nBCP_warmstart is the abstract base for storing LP warm start data.\nEnables efficient LP resolves when moving between tree nodes.\n\n**Storage modes (BCP_storage_t):**\n- Explicit: Full warm start data\n- WrtParent: Changes relative to parent node\n- WrtCore: Changes relative to core formulation\n\n**Key methods:**\n- convert_to_CoinWarmStart(): Get Osi-compatible warm start\n- storage(): Query how data is stored\n- update(): Apply incremental changes\n- as_change(): Compute delta from previous warm start\n- clone(): Deep copy\n- storage_size(): Memory footprint for storage decisions\n\n**Node transitions:**\nWhen LP dives from parent to child, warm start may be stored\nas delta to save space. When backtracking, explicit form needed.\n\nImplementations: BCP_warmstart_basis (simplex basis),\nBCP_warmstart_primaldual (interior point)", "see": ["BCP_lp.hpp for LP process using warm starts", "CoinWarmStart for underlying OSI interface"], "param": ["old_ws the old warmstart info", "del_vars the indices of the variables that are deleted from the\n              formulation <code>old_ws</code> was created for", "del_cuts same for the cuts", "petol primal zero tolerance", "detol dual zero tolerance"], "has_pass2": false}, "Bcp/src/include/BCP_tm_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_user.hpp", "filename": "BCP_tm_user.hpp", "file": "BCP_tm_user.hpp", "brief": "User customization interface for Tree Manager process\n\nBCP_tm_user is the base class for user-defined Tree Manager behavior.\nUsers derive their own class and override virtual methods.\n\n**Setup methods:**\n- initialize_core(): Define core variables, cuts, and matrix\n- create_root(): Set up the root node (extra vars/cuts, user data)\n- init_new_phase(): Initialize each algorithm phase\n\n**Solution handling:**\n- unpack_feasible_solution(): Deserialize solutions from LP\n- display_feasible_solution(): Output best solution\n- replace_solution(): Secondary objective tie-breaking\n\n**Tree management:**\n- change_candidate_heap(): Modify node selection strategy\n- display_node_information(): Status output before/after processing\n- display_final_information(): Summary after search completes\n\n**Communication:**\n- pack_module_data(): Send initialization to LP/CG/VG processes\n- process_message(): Handle user-defined messages\n\n**Parameter access:**\n- get_param()/set_param(): Query/modify BCP_tm_par parameters\n- upper_bound()/lower_bound(): Access global bounds", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp_user for LP process user interface"], "has_pass2": false}, "Bcp/src/include/BCP_var.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_var.hpp", "filename": "BCP_var.hpp", "file": "BCP_var.hpp", "brief": "Variable representation for BCP Branch-Cut-Price\n\nDefines variable classes used throughout BCP:\n\n**Class hierarchy:**\n- BCP_var: Abstract base with obj, lb, ub, type, status, bcpind\n- BCP_var_core: Core variables that always stay in LP\n- BCP_var_algo: Algorithmic variables from column generation\n- BCP_var_set: Collection with bulk operations\n\n**Variable types (BCP_var_t):**\n- BCP_BinaryVar: 0-1 variable\n- BCP_IntegerVar: General integer\n- BCP_ContinuousVar: Continuous\n\n**Status flags (BCP_obj_status):**\n- BCP_ObjInactive: Fixed to a bound\n- BCP_ObjNotRemovable: Cannot be removed (e.g., branched on)\n- BCP_ObjToBeRemoved: Marked for deletion\n- BCP_ObjDoNotSendToPool: Skip variable pool\n\n**Internal index (bcpind):**\nUnique global identifier assigned by TM. Negative bcpind indicates\nthe variable was deleted and should not be referenced.", "see": ["BCP_cut.hpp for symmetric cut representation", "BCP_vg.hpp for Variable Generator process", "BCP_lp.hpp for LP process using variables"], "has_pass2": false}, "Bcp/src/include/BCP_lp_branch.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_branch.hpp", "filename": "BCP_lp_branch.hpp", "file": "BCP_lp_branch.hpp", "brief": "LP-side branching objects for BCP Branch-Cut-Price\n\nUser-facing branching object classes used during strong branching\nand branch selection in the LP process.\n\n**BCP_lp_branching_object:**\n- child_num: Number of children (usually 2)\n- vars_to_add, cuts_to_add: New objects for children\n- forced_var_pos/bd: Variable bounds changed by branching\n- forced_cut_pos/bd: Cut bounds changed by branching\n- implied_var_pos/bd: Additional fixings from logical implications\n- implied_cut_pos/bd: Additional cut bound changes\n\n**Position encoding:**\n- Non-negative: Index in current LP formulation\n- Negative (-i): Index (i-1) in added vars/cuts\n\n**BCP_presolved_lp_brobj:**\nWraps a branching object with strong branching results:\n- _lpres: LP results for each child after presolving\n- _child_action: What to do with each child\n  (BCP_ReturnChild, BCP_KeepChild, BCP_FathomChild)\n- _user_data: User data for child nodes\n\n**Strong branching flow:**\n1. User creates BCP_lp_branching_object candidates\n2. BCP wraps in BCP_presolved_lp_brobj\n3. Each child presolved, results stored in _lpres\n4. Best candidate selected based on results", "see": ["BCP_branch.hpp for internal branching representation", "BCP_lp_user::select_branching_candidates()", "OsiBranchingObject for OSI branching interface"], "has_pass2": false}, "Bcp/src/include/BCP_problem_core.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_problem_core.hpp", "filename": "BCP_problem_core.hpp", "file": "BCP_problem_core.hpp", "brief": "Core problem formulation for BCP Branch-Cut-Price\n\nDefines the permanent part of the MIP formulation that never changes.\n\n**BCP_problem_core:**\nThe immutable core problem:\n- vars: Vector of BCP_var_core* (always in formulation)\n- cuts: Vector of BCP_cut_core* (always in formulation)\n- matrix: Constraint matrix for core vars × core cuts\n\nCreated once via BCP_tm_user::initialize_core() and shared\nacross all tree nodes.\n\n**BCP_problem_core_change:**\nTracks how core object bounds/stati differ from original:\n- _storage: Explicit, WrtCore, or WrtParent\n- var_pos, var_ch: Changed variable indices and new (lb,ub,status)\n- cut_pos, cut_ch: Changed cut indices and new (lb,ub,status)\n\n**Storage modes:**\n- Explicit: Full bounds for all core objects (root node)\n- WrtCore: Changes relative to original core (compact)\n- WrtParent: Incremental changes from parent (for tree traversal)\n\n**Key methods:**\n- ensure_explicit(): Convert delta to full representation\n- make_wrtcore_if_shorter(): Optimize storage size\n- update(): Apply incremental changes", "see": ["BCP_tm_user::initialize_core() for core setup", "BCP_node_change.hpp for full node deltas"], "has_pass2": false}, "Bcp/src/include/BCP_tm_node.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_node.hpp", "filename": "BCP_tm_node.hpp", "file": "BCP_tm_node.hpp", "brief": "Search tree node representation in Tree Manager\n\nDefines how the TM represents and manages the branch-and-bound tree.\n\n**BCP_tm_node_status:**\n- BCP_DefaultNode: Initial state\n- BCP_ProcessedNode: Fully processed\n- BCP_ActiveNode: Currently being processed by LP\n- BCP_PrunedNode_*: Pruned (by bound, infeasibility, or discarded)\n- BCP_CandidateNode: In candidate queue\n- BCP_NextPhaseNode_*: Deferred to next phase\n\n**BCP_tm_node:**\nExtends CoinTreeNode with BCP-specific data:\n- status: Current node status\n- _index: Unique node identifier\n- _parent: Parent node pointer\n- _children: Child node pointers\n- lp, cg, cp, vg, vp: Process assignments\n- _*_storage: How vars/cuts/warmstart stored (explicit vs. delta)\n- _data: Node description and user data\n\n**BCP_tree:**\nContainer for all nodes with:\n- root(): Get root node\n- size(): Total nodes created\n- maxdepth(): Deepest level reached\n- true_lower_bound(): Global lower bound\n\n**BCP_tm_node_to_send:**\nHandles asynchronous node transmission to LP, collecting\nancestor data as needed for explicit storage reconstruction.", "see": ["BCP_tm.hpp for Tree Manager", "BCP_node_change.hpp for node delta encoding", "CoinSearchTree for node selection strategies"], "has_pass2": false}, "Bcp/src/include/BCP_buffer.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_buffer.hpp", "filename": "BCP_buffer.hpp", "file": "BCP_buffer.hpp", "brief": "Message buffer for BCP inter-process communication\n\nBCP_buffer is the serialization mechanism for all BCP messages.\nEach process uses a single buffer for both incoming and outgoing messages.\n\n**Packable types:**\n- POD types copyable via memcpy (int, double, etc.)\n- Structs of POD types\n- BCP_vec<T> of the above\n- BCP_string\n- Objects with pack(BCP_buffer&) method\n\n**Buffer structure:**\n- _data: Raw character array\n- _size: Current message size\n- _pos: Read position (incoming) or write position (outgoing)\n- _max_size: Allocated capacity (grows as needed, never shrinks)\n- _msgtag: Message type (for received messages)\n- _sender: Sender process ID (for received messages)\n\n**Usage pattern:**\n- pack(): Append data to buffer, advances _size\n- unpack(): Read data from buffer, advances _pos\n- clear(): Reset for new message", "see": ["BCP_message.hpp for message passing interface", "BCP_message_tag.hpp for message type tags"], "has_pass2": false}, "Bcp/src/include/BCP_USER.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_USER.hpp", "filename": "BCP_USER.hpp", "file": "BCP_USER.hpp", "brief": "User initialization and packing interface for BCP\n\nDefines the entry points and serialization interfaces users must implement.\n\n**BCP_user_data:**\nBase class for user-defined data attached to tree nodes.\nDerive to store problem-specific information per node.\n\n**BCP_user_class:**\nEmpty base for all *_user classes (BCP_tm_user, BCP_lp_user, etc.).\n\n**BCP_user_pack:**\nSerialization interface for user-defined objects:\n- pack/unpack_warmstart(): Warm start information\n- pack/unpack_var_algo(): Algorithmic variables\n- pack/unpack_cut_algo(): Algorithmic cuts\n- pack/unpack_user_data(): Node-attached user data\n\n**USER_initialize:**\nMain entry point users must implement. Called by bcp_main():\n- msgenv_init(): Create message passing environment\n- tm_init(): Create Tree Manager user object\n- lp_init(): Create LP user object\n- cg_init(), vg_init(): Create CG/VG user objects\n- packer_init(): Create serialization object\n\n**bcp_main(argc, argv, user_init):**\nCall this from main() to start BCP with your USER_initialize.", "see": ["BCP_tm_user.hpp, BCP_lp_user.hpp for user hook classes"], "has_pass2": false}, "Bcp/src/include/BCP_lp_result.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_result.hpp", "filename": "BCP_lp_result.hpp", "file": "BCP_lp_result.hpp", "brief": "LP solve results for BCP Branch-Cut-Price\n\nBCP_lp_result stores the output from solving an LP relaxation.\nUsed throughout BCP for accessing LP solution data.\n\n**BCP_termcode (LP termination codes):**\n- BCP_Abandoned: Solver gave up\n- BCP_ProvenOptimal: Optimal solution found\n- BCP_ProvenPrimalInf: Primal infeasible\n- BCP_ProvenDualInf: Dual infeasible (unbounded)\n- BCP_PrimalObjLimReached: Cutoff triggered\n- BCP_DualObjLimReached: Lower bound exceeded cutoff\n- BCP_IterationLimit: Iteration limit hit\n- BCP_TimeLimit: Time limit hit\n\n**BCP_lp_result data:**\n- _termcode: How LP terminated\n- _iternum: Iterations used\n- _objval: Objective value (or bound)\n- _x: Primal solution vector\n- _pi: Dual solution (row prices)\n- _dj: Reduced costs\n- _lhs: Left-hand side values (Ax)\n- _primal_tolerance, _dual_tolerance: Solver tolerances\n\n**Usage:**\nCall get_results(OsiSolverInterface&) after LP solve to\npopulate all fields from the solver.", "see": ["BCP_lp.hpp for LP process", "BCP_lp_branch.hpp for strong branching results", "OsiSolverInterface for LP solver interface"], "has_pass2": false}, "Bcp/src/include/BCP_tm_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_param.hpp", "filename": "BCP_tm_param.hpp", "file": "BCP_tm_param.hpp", "brief": "Tree Manager parameters for BCP Branch-Cut-Price\n\nBCP_tm_par defines all configurable parameters for the Tree Manager.\nSet via parameter file: BCP_{param_name} {value}\n\n**Character (boolean) parameters:**\n- Debug*Processes: Enable debugging for LP/CG/VG/CP/VP\n- GenerateVars: Enable variable generation\n- MessagePassingIsSerial: Single-process mode\n- TrimTreeBeforeNewPhase: Clean tree between phases\n- RemoveExploredBranches: Free completed subtrees\n- TmVerb_*: Verbosity flags for various events\n\n**Integer parameters:**\n- WarmstartInfo: How to store warm start (None/Root/Parent)\n- MaxHeapSize: Memory limit in MB\n- TreeSearchStrategy: Best-first(0), Breadth-first(1), Depth-first(2)\n- LpProcessNum, CgProcessNum, etc.: Process counts\n- LPscheduler_*: Node scheduling parameters\n\n**Double parameters:**\n- UnconditionalDiveProbability: Dive vs. backtrack randomization\n- QualityRatioToAllowDiving_*: When diving is permitted\n- Granularity: Objective function discretization\n- MaxRunTime: Time limit\n- TerminationGap_*: Optimality gap tolerances\n- UpperBound: Initial upper bound\n\n**String parameters:**\n- SaveRootCutsTo/ReadRootCutsFrom: Root cut persistence\n- ExecutableName: Spawned process executable\n- *Machines: Machine lists for each process type", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp_param.hpp for LP parameters"], "has_pass2": false}, "Bcp/src/include/BCP_vector.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector.hpp", "filename": "BCP_vector.hpp", "file": "BCP_vector.hpp", "brief": "Custom vector class for BCP with performance optimizations\n\nBCP_vec<T> is BCP's custom vector, similar to std::vector but\nguaranteed to be a contiguous memory array with extra methods.\n\n**Key advantages:**\n- Guaranteed contiguous memory (std::vector only \"likely\")\n- unchecked_* methods skip bounds checking for speed\n- Bulk operations: keep_by_index, erase_by_index, update\n- Direct memory access for serialization\n\n**Core interface:**\n- begin()/end(): Iterators (raw T* pointers)\n- size()/capacity()/empty(): Standard queries\n- operator[]: Element access\n- push_back()/pop_back(): End modification\n- insert()/erase(): Position modification\n- reserve()/swap(): Memory management\n\n**BCP-specific methods:**\n- entry(i): Get iterator to i-th element\n- unchecked_push_back(): Skip capacity check\n- keep_by_index(): Keep only specified indices\n- erase_by_index(): Erase specified indices\n- update(): Bulk update at positions\n\n**Helper functions:**\n- purge_ptr_vector(): Delete pointers and clear vector\n- keep_ptr_vector_by_index(): Keep selected, delete rest\n\nSpecializations included for bool, char, short, int, double.", "see": ["std::vector for standard interface reference"], "has_pass2": false}}}, "Bonmin": {"name": "Bonmin", "file_count": 44, "pass2_count": 0, "files": {"src/Interfaces/BonTMINLP2TNLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTMINLP2TNLP.hpp", "filename": "BonTMINLP2TNLP.hpp", "file": "BonTMINLP2TNLP.hpp", "brief": "Adapter converting TMINLP (MINLP) to TNLP (NLP) for Ipopt\n\nThis adapter pattern allows an external caller to modify variable bounds,\nenabling treatment of binary/integer variables as either relaxed (continuous)\nor fixed. Essential for branch-and-bound where each node fixes or bounds\ninteger variables differently.\n\n**Key functionality:**\n- SetVariableBounds(): Modify bounds for branching\n- SetVariableType(): Change variable types (for probing/fixing)\n- x_sol()/duals_sol(): Access solution after optimization\n- get_starting_point(): Provide initial/warm start point\n\n**Usage in B&B:**\n1. Root node: All integers relaxed to continuous\n2. Branching: Fix or bound integer variables\n3. Each node: Solve NLP relaxation via Ipopt\n4. Warm start: Use parent's solution for faster convergence", "see": ["TMINLP for the original MINLP problem interface", "IpTNLP for Ipopt's NLP interface", "OsiTMINLPInterface for the higher-level solver interface"], "has_pass2": false}, "src/Interfaces/BonTMINLP2OsiLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTMINLP2OsiLP.hpp", "filename": "BonTMINLP2OsiLP.hpp", "file": "BonTMINLP2OsiLP.hpp", "brief": "Abstract transformer from MINLP to LP outer approximation\n\nBase class for building linear outer approximations of nonlinear problems.\nUsed to create MILP relaxations for OA-based algorithms.\n\n**Key methods:**\n- extract(): Build initial OA in an OsiSolverInterface\n- get_oas(): Generate OA cuts at a given point\n- get_refined_oa(): Get refined OA cuts\n\n**Coefficient cleaning (cleanNnz):**\nSmall coefficients are handled specially to avoid numerical issues:\n- Values < very_tiny_: Ignored completely\n- Values < tiny_: Try to absorb into RHS if bounds allow", "see": ["BonOaDecBase for OA algorithm driver", "BonOuterApprox for concrete implementation"], "has_pass2": false}, "src/Interfaces/BonExitCodes.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonExitCodes.hpp", "filename": "BonExitCodes.hpp", "file": "BonExitCodes.hpp", "brief": "Error codes for uncatchable errors in Bonmin\n\nDefines error codes for fatal errors that cannot be handled via exceptions.\n\n**Error codes:**\n- ERROR_IN_AMPL_SUFFIXES (111): Invalid AMPL suffix specification\n- UNSUPPORTED_CBC_OBJECT: CbcObject type not supported by Bonmin", "has_pass2": false}, "src/Interfaces/BonCutStrengthener.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonCutStrengthener.hpp", "filename": "BonCutStrengthener.hpp", "file": "BonCutStrengthener.hpp", "brief": "Strengthening OA cuts via NLP optimization\n\nImproves outer approximation cuts by solving auxiliary NLPs to find\ntighter bounds on linearizations. Also supports disjunctive cuts.\n\n**Cut strengthening types (CutStrengtheningType):**\n- CS_None: No strengthening\n- CS_StrengthenedGlobal: Strengthen cuts globally\n- CS_UnstrengthenedGlobal_StrengthenedLocal: Global weak, local strong\n- CS_StrengthenedGlobal_StrengthenedLocal: Both global and local strong\n\n**Strengthening algorithm:**\nGiven OA cut: g(x*) + nabla g(x*)^T (x - x*) <= 0\nSolve: min {nabla g(x*)^T x : g(x) <= 0, x in bounds}\nto find tighter RHS for the linearization.", "see": ["BonOaDecBase for OA cut generation", "StrengtheningTNLP for the internal NLP formulation"], "has_pass2": false}, "src/Interfaces/BonStrongBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonStrongBranchingSolver.hpp", "filename": "BonStrongBranchingSolver.hpp", "file": "BonStrongBranchingSolver.hpp", "brief": "Abstract base class for strong branching NLP solves\n\nProvides interface for efficiently solving modified NLPs during strong\nbranching evaluation. Supports hot-start capability to reuse factorizations.\n\n**Hot-start protocol:**\n1. markHotStart(): Save current solver state before branching evaluations\n2. solveFromHotStart(): Solve with changed bounds (fast, reuses state)\n3. unmarkHotStart(): Restore original state after all evaluations\n\n**Implementations:**\n- LpBranchingSolver: LP-based strong branching\n- QpBranchingSolver: QP approximation for strong branching\n- CurvBranchingSolver: Curvature-based branching", "see": ["BonChooseVariable for the variable selection driver", "LpBranchingSolver, QpBranchingSolver for implementations"], "has_pass2": false}, "src/Interfaces/BonTNLPSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTNLPSolver.hpp", "filename": "BonTNLPSolver.hpp", "file": "BonTNLPSolver.hpp", "brief": "Abstract interface for NLP solvers used in branch-and-bound\n\nProvides a generic interface for calling NLP solvers to solve continuous\nrelaxations of MINLPs. This abstraction allows Bonmin to use different\nNLP solvers (Ipopt, FilterSQP) interchangeably.\n\n**Key methods:**\n- OptimizeTNLP(): Solve an NLP from scratch\n- ReOptimizeTNLP(): Resolve with warm start (for B&B efficiency)\n- setWarmStart()/getWarmStart(): Manage warm start information\n- enableWarmStart()/disableWarmStart(): Control warm starting\n\n**Return statuses:**\n- solvedOptimal: Problem solved to optimality\n- provenInfeasible: Infeasibility proven\n- iterationLimit/timeLimit: Resource limits reached\n- computationError/exception: Solver failures\n\nImplementations: BonIpoptSolver (Ipopt), BonFilterSolver (FilterSQP)", "see": ["BonIpoptSolver.hpp for Ipopt-specific implementation", "BonFilterSolver.hpp for FilterSQP implementation", "IpTNLP.hpp for the NLP problem interface"], "has_pass2": false}, "src/Interfaces/BonRegisteredOptions.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonRegisteredOptions.hpp", "filename": "BonRegisteredOptions.hpp", "file": "BonRegisteredOptions.hpp", "brief": "Extended option registration with algorithm validity tracking\n\nExtends Ipopt's RegisteredOptions to track which options are valid for\nwhich Bonmin algorithms, enabling algorithm-specific option filtering.\n\n**Algorithm validity bits (ExtraOptInfosBits):**\n- validInHybrid (1): B-Hyb algorithm\n- validInQG (2): Quesada-Grossmann B-QG\n- validInOA (4): Pure outer approximation B-OA\n- validInBBB (8): Pure NLP-based branch-and-bound B-BB\n- validInEcp (16): Extended cutting plane B-Ecp\n- validIniFP (32): Iterated feasibility pump B-iFP\n- validInCbc (64): Cbc parallel mode\n\n**Category types (ExtraCategoriesInfo):**\n- BonminCategory, IpoptCategory, FilterCategory, BqpdCategory, CouenneCategory", "see": ["BonBabSetupBase for algorithm configuration", "Ipopt::RegisteredOptions for base functionality"], "has_pass2": false}, "src/Interfaces/BonCurvatureEstimator.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonCurvatureEstimator.hpp", "filename": "BonCurvatureEstimator.hpp", "file": "BonCurvatureEstimator.hpp", "brief": "Curvature estimation for branching decisions (NOT SUPPORTED)\n\n@warning This file is not currently supported (#error directive).\n\nWould compute null-space curvature (d^T H_Lag d) for branching variable\nselection, projecting directions onto constraint null space.\n\n**Intended functionality:**\n- ComputeNullSpaceCurvature(): Project direction, compute Hessian product\n- Handle active bound and constraint detection\n- Use linear solvers for projection systems", "see": ["CurvBranchingSolver for curvature-based branching (if enabled)"], "has_pass2": false}, "src/Interfaces/BonOsiTMINLPInterface.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonOsiTMINLPInterface.hpp", "filename": "BonOsiTMINLPInterface.hpp", "file": "BonOsiTMINLPInterface.hpp", "brief": "OsiSolverInterface wrapper for TMINLP problems\n\nProvides an OsiSolverInterface that wraps a TMINLP problem, enabling\nBonmin to use Cbc's branch-and-bound framework. The continuous relaxations\nare solved by an NLP solver (Ipopt or FilterSQP) rather than LP.\n\n**Key functionality:**\n- initialSolve()/resolve(): Solve NLP relaxation via TNLPSolver\n- extractLinearRelaxation(): Build LP outer approximation (OA)\n- getOuterApproximation(): Generate OA cuts at current point\n- solveFeasibilityProblem(): Find nearest feasible point\n\n**Outer Approximation (OA) support:**\n- Linearizes nonlinear constraints at solution points\n- Generates cutting planes for MIP approximation\n- Enables hybrid NLP-BB / OA-BB algorithms\n\n**Warm starting:**\n- Supports warm starts from previous NLP solutions\n- Multiple retry strategies for failed NLP solves\n\n@note Many OsiSolverInterface methods throw since an NLP lacks an LP matrix", "see": ["BonTMINLP.hpp for the underlying MINLP problem interface", "BonTNLPSolver.hpp for the NLP solver abstraction", "OsiSolverInterface for the base class interface"], "has_pass2": false}, "src/Interfaces/BonTMINLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTMINLP.hpp", "filename": "BonTMINLP.hpp", "file": "BonTMINLP.hpp", "brief": "Base class for Mixed-Integer Nonlinear Programs (MINLP)\n\nDefines the TMINLP interface for specifying optimization problems of the form:\n@f[\n\\begin{array}{rl}\n  \\min & f(x) \\\\\n  \\text{s.t.} & g^L \\le g(x) \\le g^U \\\\\n              & x^L \\le x \\le x^U \\\\\n              & x_i \\in \\{0,1\\} \\text{ for } i \\in \\mathcal{B} \\\\\n              & x_i \\in \\mathbb{Z} \\text{ for } i \\in \\mathcal{I}\n\\end{array}\n@f]\n\nUsers implement this interface to define their MINLP problem. Key methods:\n- get_nlp_info(): Problem dimensions and sparsity\n- get_variables_types(): Identify binary/integer/continuous variables\n- get_bounds_info(): Variable and constraint bounds\n- eval_f(), eval_grad_f(): Objective and gradient\n- eval_g(), eval_jac_g(): Constraints and Jacobian\n- eval_h(): Hessian of the Lagrangian\n\nThe TMINLP2TNLP adapter converts this to an Ipopt-solvable TNLP by\nrelaxing or fixing integer variables.", "see": ["BonTMINLP2TNLP.hpp for TMINLP to TNLP conversion", "BonOsiTMINLPInterface.hpp for OsiSolverInterface wrapper", "IpTNLP.hpp for Ipopt's NLP interface (parent of continuous relaxation)"], "has_pass2": false}, "src/Interfaces/BonTNLP2FPNLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTNLP2FPNLP.hpp", "filename": "BonTNLP2FPNLP.hpp", "file": "BonTNLP2FPNLP.hpp", "brief": "Adapter for Feasibility Pump NLP formulation\n\nWraps a TNLP to create Feasibility Pump subproblems by modifying the\nobjective to minimize distance to a target integer point.\n\n**Modified objective:**\nf_FP(x) = lambda * ||x_I - x̃_I||_p + (1-lambda) * sigma * f(x)\nwhere x̃_I is the target integer point, p is norm (1 or 2).\n\n**Optional constraints:**\n- Cutoff constraint: f(x) <= cutoff (improve on incumbent)\n- Local branching: sum |x_i - x̃_i| <= rhs (neighborhood search)\n\n**Key parameters:**\n- lambda_: Weight on distance (vs original objective)\n- sigma_: Scaling for original objective\n- norm_: L1 or L2 distance metric", "see": ["HeuristicFPump for the Feasibility Pump driver", "HeuristicLocalBranching for local branching heuristic"], "has_pass2": false}, "src/Interfaces/BonAuxInfos.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonAuxInfos.hpp", "filename": "BonAuxInfos.hpp", "file": "BonAuxInfos.hpp", "brief": "Auxiliary information passing between Bonmin B&B components\n\nExtends OsiBabSolver to pass solver-specific information during\nbranch-and-cut, including NLP solutions found by heuristics.\n\n**Key functionality:**\n- nlpSolution_: Store solutions from NLP solves/heuristics\n- infeasibleNode_: Flag for cut generator detected infeasibility\n- bestSolution2_: Alternate objective tracking (for bi-objective)\n\n**Usage pattern:**\nCut generators and heuristics store found solutions here;\nthe B&B framework queries for incumbents.", "see": ["BabInfo for extended version with Bab pointer", "OsiBabSolver for the base class interface"], "has_pass2": false}, "src/CbcBonmin/BonBabInfos.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonBabInfos.hpp", "filename": "BonBabInfos.hpp", "file": "BonBabInfos.hpp", "brief": "Information passing between Bonmin B&B components\n\nExtends OsiAuxInfo to pass Bonmin-specific information between components\nduring branch-and-cut. Provides access to the Bab solver and solution info.\n\n**Key functionality:**\n- babPtr_: Access to the Bab object (and via it, CbcModel)\n- hasSolution(): Check if incumbent exists\n- bestSolution_: Current best known solution (from AuxInfo)\n\nUsed by cut generators and heuristics to query current B&B state.", "see": ["AuxInfo for the base class", "Bab for the branch-and-bound driver"], "has_pass2": false}, "src/CbcBonmin/BonCbcNode.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonCbcNode.hpp", "filename": "BonCbcNode.hpp", "file": "BonCbcNode.hpp", "brief": "Extended node information classes for MINLP branch-and-bound\n\nExtends Cbc's node information to track MINLP-specific data like sequences\nof infeasible or unsolved subproblems along a branch.\n\n**Classes:**\n- BonCbcFullNodeInfo: Complete node recreation from scratch\n- BonCbcPartialNodeInfo: Incremental node changes from parent\n\n**MINLP-specific tracking:**\n- sequenceOfInfeasiblesSize_: Consecutive infeasible NLP subproblems\n- sequenceOfUnsolvedSize_: Consecutive NLP solver failures\n\nThese counts help detect problematic branches where NLP solves consistently\nfail, enabling early pruning heuristics to avoid wasted computation.", "see": ["CbcFullNodeInfo for base class", "CbcPartialNodeInfo for base class"], "has_pass2": false}, "src/CbcBonmin/BonCbc.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonCbc.hpp", "filename": "BonCbc.hpp", "file": "BonCbc.hpp", "brief": "Main branch-and-bound driver for Bonmin using Cbc\n\nThe Bab class is the entry point for solving MINLPs with Bonmin. It wraps\nCbcModel to provide branch-and-bound with NLP solvers for subproblems.\n\n**Key method:**\n- branchAndBound(BabSetupBase&): Main solve routine\n\n**Return statuses (MipStatuses):**\n- FeasibleOptimal: Optimal solution found and proven\n- ProvenInfeasible: Problem proven infeasible\n- Feasible: Integer solution found (not proven optimal)\n- UnboundedOrInfeasible: Continuous relaxation unbounded\n- NoSolutionKnown: No feasible solution found\n\n**Solution access:**\n- bestSolution(): Primal solution vector\n- bestObj(): Objective value of best solution\n- bestBound(): Best known lower bound", "see": ["BabSetupBase for algorithm configuration", "CbcModel for the underlying MIP framework"], "has_pass2": false}, "src/CbcBonmin/BonDiver.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonDiver.hpp", "filename": "BonDiver.hpp", "file": "BonDiver.hpp", "brief": "Diving-based tree traversal strategies for MINLP branch-and-bound\n\nImplements tree traversal strategies that \"dive\" down branches before\nbacktracking, often finding feasible solutions faster than pure best-bound.\n\n**Tree traversal classes:**\n- CbcDiver: Simple diving to leaves before returning to heap\n- CbcProbedDiver: Evaluates both children before choosing dive direction\n- CbcDfsDiver: Depth-first diving with configurable backtracking\n\n**CbcDfsDiver modes (ComparisonModes):**\n- Enlarge: Initial tree exploration\n- FindSolutions: Prioritize finding feasible solutions\n- CloseBound: Prioritize closing optimality gap\n- LimitTreeSize: Control memory usage\n\n**DiverCompare:**\nNode comparison strategy that adapts based on solve progress,\nswitching modes as solutions are found or node count grows.", "see": ["CbcTree for the base tree class", "BabSetupBase for tree traversal configuration"], "has_pass2": false}, "src/Algorithms/BonBonminSetup.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonBonminSetup.hpp", "filename": "BonBonminSetup.hpp", "file": "BonBonminSetup.hpp", "brief": "Main Bonmin algorithm configuration and initialization\n\nExtends BabSetupBase with Bonmin-specific algorithm selection and\ninitialization for all MINLP algorithms.\n\n**Algorithm enum:**\n- B_BB (0): NLP-based branch-and-bound\n- B_OA (1): Pure Outer Approximation decomposition\n- B_QG (2): Quesada-Grossmann branch-and-cut\n- B_Hyb (3): Hybrid OA with NLP at nodes (default)\n- B_Ecp (4): Extended Cutting Plane (FilMINT-style)\n- B_IFP (5): Iterated Feasibility Pump\n\n**Initialization:**\n- initializeBBB(): Pure B&B with NLP at every node\n- initializeBHyb(): Hybrid with OA cuts + occasional NLP", "see": ["BabSetupBase for common configuration", "BonminAmplSetup for AMPL-based initialization"], "has_pass2": false}, "src/Algorithms/BonSubMipSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonSubMipSolver.hpp", "filename": "BonSubMipSolver.hpp", "file": "BonSubMipSolver.hpp", "brief": "Unified interface for solving MILP subproblems in OA decomposition\n\nProvides a common interface for solving MILP subproblems using either\nCbc (via OsiClpSolverInterface) or CPLEX (via OsiCpxSolverInterface).\nUsed by OA decomposition algorithms to solve the linearized master problem.\n\n**Solve strategies (MILP_solve_strategy):**\n- FindGoodSolution: Quickly find a feasible integer solution\n- GetOptimum: Solve to optimality (slower but provides bounds)\n\n**Key methods:**\n- solve(): Dispatch to find_good_sol() or optimize() based on strategy\n- optimize_with_lazy_constraints(): CPLEX lazy cuts for OA\n- getLastSolution(): Retrieve integer solution found\n- lowBound(): Lower bound from MILP solve", "see": ["OaDecompositionBase for OA algorithm using this solver", "OACutGenerator2 for classical OA implementation"], "has_pass2": false}, "src/Algorithms/BonBabSetupBase.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonBabSetupBase.hpp", "filename": "BonBabSetupBase.hpp", "file": "BonBabSetupBase.hpp", "brief": "Base configuration class for branch-and-bound MINLP solvers\n\nAggregates all components needed to run branch-and-bound: NLP solver,\ncut generators, heuristics, branching rules, and tree traversal strategy.\nServes as the central configuration point for Bonmin algorithms.\n\n**Key components:**\n- nonlinearSolver_: OsiTMINLPInterface for NLP subproblems\n- cutGenerators_: List of CglCutGenerator (OA, ECP, Gomory, etc.)\n- heuristics_: List of CbcHeuristic (feasibility pump, rounding, etc.)\n- branchingMethod_: OsiChooseVariable for variable selection\n\n**Tree search strategies:**\n- NodeComparison: bestBound, DFS, BFS, dynamic, bestGuess\n- TreeTraversal: HeapOnly, DiveFromBest, ProbedDive, DfsDiveFromBest\n\n**Branching strategies (VarSelectStra_Enum):**\n- MOST_FRACTIONAL: Simple, fast, often poor\n- STRONG_BRANCHING: Evaluate candidates via LP/NLP solves\n- RELIABILITY_BRANCHING: Use pseudo-costs when trusted", "see": ["BonminSetup for the concrete Bonmin configuration", "CbcBonmin for the main branch-and-bound driver"], "has_pass2": false}, "src/Interfaces/Ipopt/BonIpoptSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ipopt/BonIpoptSolver.hpp", "filename": "BonIpoptSolver.hpp", "file": "BonIpoptSolver.hpp", "brief": "Ipopt implementation of TNLPSolver for NLP subproblems\n\nWraps Ipopt's IpoptApplication to solve continuous NLP relaxations within\nBonmin's branch-and-bound framework. Provides warm starting capabilities\nessential for efficient node processing.\n\n**Key features:**\n- OptimizeTNLP()/ReOptimizeTNLP(): Solve NLPs via Ipopt\n- Warm start strategies: none, optimal point, interior point\n- Status translation: Maps Ipopt return codes to TNLPSolver::ReturnStatus\n\n**Warm start strategies (warmStartStrategy_):**\n- 0: No warm start\n- 1: Simple warm start from optimal point\n- 2: Interior point warm start (preserves barrier info)", "see": ["TNLPSolver for the abstract interface", "IpoptWarmStart for warm start data storage", "IpoptInteriorWarmStarter for advanced interior point warm start"], "has_pass2": false}, "src/Interfaces/Ipopt/BonIpoptWarmStart.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ipopt/BonIpoptWarmStart.hpp", "filename": "BonIpoptWarmStart.hpp", "file": "BonIpoptWarmStart.hpp", "brief": "Warm start storage for Ipopt within branch-and-bound\n\nStores primal and dual solution values for warm starting subsequent NLP\nsolves. Inherits from both CoinWarmStartPrimalDual (for actual data) and\nCoinWarmStartBasis (for Cbc integration compatibility).\n\n**Data layout (dual part has 2*n + m values):**\n- [0, n): Dual multipliers for lower bounds (z_L for x >= l)\n- [n, 2n): Dual multipliers for upper bounds (z_U for x <= u)\n- [2n, 2n+m): Dual multipliers for constraints (lambda for g(x))\n\n**Classes:**\n- IpoptWarmStart: Main warm start container\n- IpoptWarmStartDiff: Differential update for tree traversal", "see": ["IpoptInteriorWarmStarter for interior point specific warm start", "CoinWarmStartPrimalDual for base class data storage"], "has_pass2": false}, "src/Interfaces/Ipopt/BonIpoptInteriorWarmStarter.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ipopt/BonIpoptInteriorWarmStarter.hpp", "filename": "BonIpoptInteriorWarmStarter.hpp", "file": "BonIpoptInteriorWarmStarter.hpp", "brief": "Interior point warm start for Ipopt in branch-and-bound\n\nStores interior point iterates from a solved NLP for warm starting child\nnodes. Unlike simple optimal-point warm start, this preserves barrier\nparameter (mu) and interior structure, enabling faster convergence.\n\n**Key methods:**\n- UpdateStoredIterates(): Called during optimization to save promising iterates\n- WarmStartIterate(): Computes initial point for new problem from stored data\n- Finalize(): Post-processing after parent NLP solve completes\n\n**Stored data per iterate:**\n- Primal/dual iterate vectors\n- Barrier parameter mu\n- NLP error, primal/dual infeasibility, complementarity", "see": ["IpoptWarmStart for simpler primal-dual warm start", "IpoptSolver for the solver that uses this warm starter"], "has_pass2": false}, "src/Interfaces/Filter/BonFilterWarmStart.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonFilterWarmStart.hpp", "filename": "BonFilterWarmStart.hpp", "file": "BonFilterWarmStart.hpp", "brief": "Warm start storage for FilterSQP solver\n\nStores warm start information for FilterSQP, including primal/dual values\nand FilterSQP's internal lws (integer working set) array. Inherits from both\nCoinWarmStartPrimalDual and CoinWarmStartBasis for Cbc compatibility.\n\n**Data stored:**\n- x: Primal variables (from CoinWarmStartPrimalDual)\n- lam: Dual multipliers (from CoinWarmStartPrimalDual)\n- lws: FilterSQP's integer working set array (active set info)\n- istat[14]: FilterSQP status array\n\n**Classes:**\n- FilterWarmStart: Main warm start container\n- FilterWarmStartDiff: Differential update for tree traversal", "see": ["FilterSolver for the solver that uses this warm start", "IpoptWarmStart for the Ipopt equivalent"], "has_pass2": false}, "src/Interfaces/Filter/BonFilterSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonFilterSolver.hpp", "filename": "BonFilterSolver.hpp", "file": "BonFilterSolver.hpp", "brief": "FilterSQP implementation of TNLPSolver for NLP subproblems\n\nWraps the FilterSQP solver (Fortran library) to solve continuous NLP\nrelaxations as an alternative to Ipopt. FilterSQP uses sequential quadratic\nprogramming with a filter-based trust region.\n\n**Key features:**\n- OptimizeTNLP()/ReOptimizeTNLP(): Solve NLPs via FilterSQP\n- cachedInfo: Caches problem structure for efficient re-optimization\n- TMat2RowPMat(): Converts sparse matrix formats for Fortran interface\n\n**Filter algorithm:**\nThe \"filter\" maintains pairs (constraint violation, objective value) and\naccepts steps that improve either measure - more robust than merit functions.", "see": ["TNLPSolver for the abstract interface", "FilterWarmStart for warm start data", "BqpdSolver for the QP subproblem solver used by FilterSQP"], "has_pass2": false}, "src/Interfaces/Filter/BonFilterTypes.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonFilterTypes.hpp", "filename": "BonFilterTypes.hpp", "file": "BonFilterTypes.hpp", "brief": "Fortran type definitions for FilterSQP interface\n\nDefines type aliases for interfacing with the Fortran FilterSQP library.\nUses FORTRAN_INTEGER_TYPE from Ipopt's configuration for portability\nacross different Fortran compilers (32-bit vs 64-bit integers).", "see": ["BonFilterSolver.hpp for the FilterSQP wrapper"], "has_pass2": false}, "src/Interfaces/Ampl/BonAmplInterface.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ampl/BonAmplInterface.hpp", "filename": "BonAmplInterface.hpp", "file": "BonAmplInterface.hpp", "brief": "OsiSolverInterface for AMPL .nl file input\n\nExtends OsiTMINLPInterface to read problems from AMPL .nl files.\nProvides convenient entry point for AMPL-based modeling.\n\n**Usage:**\n@code\nAmplInterface solver;\nsolver.readAmplNlFile(argv, roptions, options, journalist);\n@endcode\n\n**Additional features:**\n- readNames(): Read variable/row names from .col/.row files\n- amplModel(): Access underlying AmplTMINLP", "see": ["AmplTMINLP for the TMINLP implementation", "OsiTMINLPInterface for the base class"], "has_pass2": false}, "src/Interfaces/Ampl/BonAmplTMINLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ampl/BonAmplTMINLP.hpp", "filename": "BonAmplTMINLP.hpp", "file": "BonAmplTMINLP.hpp", "brief": "AMPL interface for MINLP problems via .nl files\n\nImplements TMINLP by reading AMPL .nl files, extracting integer variable\ninformation, SOS constraints, branching priorities, and convexity markers.\n\n**AMPL suffix reading:**\n- read_priorities(): Variable branching priorities\n- read_sos(): SOS1/SOS2 constraint definitions\n- read_convexities(): Constraint convexity markers (for OA)\n- read_onoff(): Perspective reformulation indicators\n- read_obj_suffixes(): Upper bounding objective (UBObj prefix)\n\n**AMPL solve_result_num codes:**\n- 3: Integer optimal\n- 220: Proven infeasible\n- 421: Limit reached with feasible solution\n- 410: Limit reached, no feasible solution\n- 500: Error", "see": ["TMINLP for the base interface", "AmplInterface for the OsiSolverInterface wrapper"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicFPump.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicFPump.hpp", "filename": "BonHeuristicFPump.hpp", "file": "BonHeuristicFPump.hpp", "brief": "Feasibility Pump heuristic for MINLP\n\nImplements the Feasibility Pump algorithm adapted for nonlinear problems.\nAlternates between:\n1. Rounding to nearest integer solution\n2. Projecting back to feasible continuous space via NLP\n\n**Algorithm:**\n1. Solve NLP relaxation → get x*\n2. Round x* to nearest integers → get x̃\n3. Solve NLP minimizing ||x - x̃|| → get new x*\n4. If x* = x̃, found feasible solution; else repeat\n\n**Classes:**\n- HeuristicFPump: Main feasibility pump heuristic (CbcHeuristic)\n- RoundingFPump: Helper for intelligent rounding considering constraints", "see": ["CbcHeuristic for the base class", "BonminSetup for configuration"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonLocalSolverBasedHeuristic.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonLocalSolverBasedHeuristic.hpp", "filename": "BonLocalSolverBasedHeuristic.hpp", "file": "BonLocalSolverBasedHeuristic.hpp", "brief": "Base class for heuristics using local NLP/MINLP solves\n\nProvides infrastructure for heuristics that find feasible solutions by\nsolving restricted MINLP subproblems (local searches) in a neighborhood\nof the current solution.\n\n**Key method:**\n- doLocalSearch(): Solve MINLP subproblem with modified bounds/cutoff\n\n**Parameters:**\n- time_limit_: Maximum time for local search\n- max_number_nodes_: Node limit for local search\n- max_number_solutions_: Stop after finding this many solutions\n\n**Subclasses:**\n- FixAndSolveHeuristic: Fix some variables, solve remaining MINLP\n- HeuristicRINS: Relaxation Induced Neighborhood Search\n- HeuristicLocalBranching: Local branching cuts", "see": ["CbcHeuristic for the base class", "BonminSetup for local search configuration"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonFixAndSolveHeuristic.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonFixAndSolveHeuristic.hpp", "filename": "BonFixAndSolveHeuristic.hpp", "file": "BonFixAndSolveHeuristic.hpp", "brief": "Fix-and-Solve heuristic for MINLP\n\nSimple heuristic that fixes some integer variables to their current\nvalues and solves the resulting smaller MINLP subproblem.\n\n**Algorithm:**\n1. Select subset of integer variables to fix (based on LP solution)\n2. Fix selected variables to their current integer values\n3. Solve reduced MINLP with remaining variables\n4. If feasible and improved, update incumbent\n\nSimpler than RINS (doesn't require incumbent for comparison).", "see": ["LocalSolverBasedHeuristic for base class", "HeuristicRINS for more sophisticated fixing strategy"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicLocalBranching.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicLocalBranching.hpp", "filename": "BonHeuristicLocalBranching.hpp", "file": "BonHeuristicLocalBranching.hpp", "brief": "Local Branching heuristic for MINLP improvement\n\nSearches for improved solutions in a Hamming distance neighborhood\nof the current incumbent by adding a local branching constraint.\n\n**Algorithm:**\n1. Add constraint: sum |x_i - x*_i| <= k (Hamming distance <= k)\n2. Solve restricted MINLP in this neighborhood\n3. If improved solution found, update incumbent\n4. Optionally expand neighborhood if no improvement\n\n**Local branching constraint:**\nFor binary variables: sum_{x*_i=1}(1-x_i) + sum_{x*_i=0}(x_i) <= k", "see": ["LocalSolverBasedHeuristic for base class", "TNLP2FPNLP for local branching constraint implementation"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicDive.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDive.hpp", "filename": "BonHeuristicDive.hpp", "file": "BonHeuristicDive.hpp", "brief": "Base class for diving heuristics in MINLP\n\nImplements diving heuristics that repeatedly fix integer variables and solve\nNLP relaxations until a feasible integer solution is found or infeasibility.\n\n**Algorithm:**\n1. Solve NLP relaxation\n2. Select fractional variable via selectVariableToBranch()\n3. Fix variable to floor or ceil\n4. Resolve NLP, repeat until all integers fixed\n\n**Subclasses implement different selection rules:**\n- HeuristicDiveFractional: Branch on most fractional\n- HeuristicDiveVectorLength: Use gradient information\n- HeuristicDiveMIP: Solve MIP subproblem for selection\n\n**Helper functions:**\n- isNlpFeasible(): Check NLP constraint satisfaction\n- adjustPrimalTolerance(): Handle near-feasible solutions", "see": ["CbcHeuristic for the base class", "HeuristicDiveFractional for a concrete implementation"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicRINS.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicRINS.hpp", "filename": "BonHeuristicRINS.hpp", "file": "BonHeuristicRINS.hpp", "brief": "Relaxation Induced Neighborhood Search (RINS) heuristic for MINLP\n\nRINS finds improving solutions by solving restricted MINLP subproblems\nwhere variables agreeing between LP relaxation and incumbent are fixed.\n\n**Algorithm:**\n1. Compare current LP solution with incumbent\n2. Fix integer variables where both solutions agree\n3. Solve restricted MINLP with remaining variables free\n4. If solution improves incumbent, update\n\n**Parameters:**\n- howOften_: Frequency of heuristic application\n- numberSolutions_: Track solution count for triggering", "see": ["LocalSolverBasedHeuristic for base class", "HeuristicLocalBranching for related neighborhood search"], "has_pass2": false}, "src/Algorithms/Branching/BonPseudoCosts.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonPseudoCosts.hpp", "filename": "BonPseudoCosts.hpp", "file": "BonPseudoCosts.hpp", "brief": "Pseudo-cost storage and update for MINLP branching\n\nExtends OsiPseudoCosts to track branching history for integer variables.\nPseudo-costs estimate the objective change per unit change in a variable,\nenabling efficient branching decisions without expensive strong branching.\n\n**Pseudo-cost formula:**\n- upPseudoCost[i] = sum(upChange) / count(upBranches)\n- downPseudoCost[i] = sum(downChange) / count(downBranches)\n- Expected change = pseudoCost * distance_to_integer\n\n**Update via addInfo():**\n- Called after each branching decision resolves\n- Records objective change and infeasibility change\n- Status indicates: optimal, infeasible, or not finished", "see": ["BonChooseVariable for usage in branching decisions", "OsiPseudoCosts for the base class"], "has_pass2": false}, "src/Algorithms/Branching/BonQpBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonQpBranchingSolver.hpp", "filename": "BonQpBranchingSolver.hpp", "file": "BonQpBranchingSolver.hpp", "brief": "QP-based strong branching solver\n\nImplements strong branching by solving QP approximations of the NLP\nsubproblems. Faster than full NLP but more accurate than LP.\n\n**Algorithm:**\n1. Build QP approximation at current NLP solution (BranchingTQP)\n2. For each branching candidate, solve QP with modified bounds\n3. Use QP objective change to estimate branching quality\n\n**Requires:** FilterSQP or BQPD QP solver (BONMIN_HAS_FILTERSQP)", "see": ["StrongBranchingSolver for base class", "BranchingTQP for the QP subproblem formulation", "BqpdSolver for the underlying QP solver"], "has_pass2": false}, "src/Algorithms/Branching/BonLpBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonLpBranchingSolver.hpp", "filename": "BonLpBranchingSolver.hpp", "file": "BonLpBranchingSolver.hpp", "brief": "LP-based strong branching solver using ECP cuts\n\nImplements strong branching by solving LP relaxations enhanced with\nExtended Cutting Plane (ECP) cuts, avoiding full NLP solves.\n\n**Algorithm:**\n1. markHotStart(): Extract LP from NLP, generate initial ECP cuts\n2. solveFromHotStart(): Re-solve LP with bound changes + new ECP cuts\n3. unmarkHotStart(): Clean up LP solver\n\n**Parameters:**\n- maxCuttingPlaneIterations_: Max ECP iterations per strong branch\n- abs_ecp_tol_, rel_ecp_tol_: Tolerances for ECP convergence\n- warm_start_mode_: Basis or Clone warm starting", "see": ["StrongBranchingSolver for base class", "EcpCuts for the cut generator used"], "has_pass2": false}, "src/Algorithms/Branching/BonChooseVariable.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonChooseVariable.hpp", "filename": "BonChooseVariable.hpp", "file": "BonChooseVariable.hpp", "brief": "Strong branching and pseudo-cost based variable selection for MINLP\n\nImplements branching variable selection for nonlinear branch-and-bound.\nCombines strong branching (solving LP/NLP relaxations) with pseudo-costs\n(estimates from historical branching information).\n\n**Key classes:**\n- BonChooseVariable: Main branching decision maker (extends OsiChooseVariable)\n- HotInfo: Stores strong branching results for a candidate\n\n**Branching decision process:**\n1. setupList(): Identify fractional variables, rank by pseudo-costs\n2. doStrongBranching(): Evaluate top candidates via LP/NLP solves\n3. chooseVariable(): Select best candidate based on objective change\n\n**Pseudo-cost computation:**\n- Estimates objective change per unit change in variable value\n- Updated after each branching decision using actual results\n- Used to avoid expensive strong branching after trust is established", "see": ["OsiChooseVariable for the base class interface", "BonPseudoCosts for pseudo-cost storage and update"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonEcpCuts.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonEcpCuts.hpp", "filename": "BonEcpCuts.hpp", "file": "BonEcpCuts.hpp", "brief": "Extended Cutting Plane (ECP) cut generator for MINLP\n\nGenerates OA cuts iteratively at LP solution points, refining the\nlinear approximation without requiring NLP solves at every iteration.\n\n**ECP algorithm:**\n1. Solve LP relaxation\n2. Find most violated nonlinear constraint at LP solution\n3. Add OA cut for that constraint\n4. Re-solve LP, repeat until tolerance met or max iterations\n\n**Parameters:**\n- numRounds_: Maximum ECP iterations\n- abs_violation_tol_: Stop when max violation < tolerance\n- rel_violation_tol_: Relative violation tolerance\n- beta_: Probability factor for skipping cuts (randomization)", "see": ["OaDecompositionBase for base class", "LpBranchingSolver for use in strong branching"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonOaNlpOptim.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOaNlpOptim.hpp", "filename": "BonOaNlpOptim.hpp", "file": "BonOaNlpOptim.hpp", "brief": "NLP-based outer approximation cut generator\n\nGenerates OA cuts by solving NLP relaxations at B&B nodes, rather than\nat integer feasible points (as in classical OA). More expensive per cut,\nbut can improve bounds at fractional nodes.\n\n**Algorithm:**\n1. At each B&B node (up to maxDepth_), solve NLP relaxation\n2. Generate linearization cuts at NLP solution\n3. Add cuts to strengthen LP relaxation\n\n**Parameters:**\n- maxDepth_: Maximum tree depth for NLP solves (default 10)\n- addOnlyViolated_: Only add cuts violated by LP solution\n- global_: Add cuts globally (valid throughout tree)\n- solves_per_level_: Control NLP solve frequency", "see": ["OaDecompositionBase for the classical OA algorithm", "CglCutGenerator for the cut generator interface"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonOaFeasChecker.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOaFeasChecker.hpp", "filename": "BonOaFeasChecker.hpp", "file": "BonOaFeasChecker.hpp", "brief": "OA-based feasibility checker for MILP solutions\n\nChecks if MILP solution is feasible for original MINLP by evaluating\nnonlinear constraints and generating OA/Benders cuts if infeasible.\n\n**Cut types (CutsTypes):**\n- OA: Standard outer approximation cuts (gradient-based)\n- Benders: Benders optimality/feasibility cuts\n\n**Cut discard policies (CutsPolicies):**\n- DetectCycles: Detect and handle cycling\n- KeepAll: Keep all generated cuts\n- TreatAsNormal: Standard cut management", "see": ["OaDecompositionBase for base class", "OACutGenerator2 for main OA cut generator"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonOaDecBase.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOaDecBase.hpp", "filename": "BonOaDecBase.hpp", "file": "BonOaDecBase.hpp", "brief": "Base class for Outer Approximation (OA) decomposition algorithms\n\nImplements the foundation for OA-based MINLP algorithms. OA iterates between\nsolving MILP subproblems and NLP subproblems, generating linear outer\napproximations of the nonlinear constraints.\n\n**OA algorithm outline:**\n1. Solve MILP relaxation → get integer solution x*\n2. Fix integers to x*, solve NLP → get nonlinear solution y*\n3. Generate linearization cuts at y* (gradient-based)\n4. Add cuts to MILP, repeat until convergence\n\n**Key components:**\n- solverManip: RAII helper to save/restore solver state\n- performOa(): Virtual method implementing specific OA variant\n- post_nlp_solve(): Handle NLP solution and update bounds\n\n**Parameters:**\n- global_: Add cuts globally (valid at all nodes)\n- addOnlyViolated_: Only add cuts violated by current solution\n- maxLocalSearch_: Limit on local search iterations", "see": ["OACutGenerator2 for classical OA implementation", "BonOaNlpOptim for NLP-based OA variant", "CglCutGenerator for the cut generator interface"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonOACutGenerator2.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOACutGenerator2.hpp", "filename": "BonOACutGenerator2.hpp", "file": "BonOACutGenerator2.hpp", "brief": "Classical Outer Approximation cut generator for MINLP\n\nImplements the standard OA algorithm of Duran & Grossmann (1986) for convex\nMINLP. Generates linearization cuts by Taylor expansion of nonlinear\nconstraints at NLP solutions.\n\n**Algorithm:**\n1. Solve initial NLP relaxation to get y0\n2. Generate OA cuts: g(y0) + ∇g(y0)ᵀ(x - y0) ≤ 0\n3. Solve MILP with cuts → integer solution x*\n4. Fix integers, solve NLP → new point y*\n5. Add cuts at y*, update bounds, repeat\n\n**Convergence:**\n- For convex problems: finite convergence to global optimum\n- For nonconvex: heuristic, may find local optima", "see": ["OaDecompositionBase for the base class", "SubMipSolver for MILP subproblem solving"], "has_pass2": false}, "src/Algorithms/Ampl/BonAmplSetup.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Ampl/BonAmplSetup.hpp", "filename": "BonAmplSetup.hpp", "file": "BonAmplSetup.hpp", "brief": "AMPL-specific Bonmin setup and initialization\n\nExtends BonminSetup to initialize from AMPL command-line arguments\nand .nl files. Handles reading options and model from AMPL.\n\n**Initialization methods:**\n- initialize(argv): Read from command line and files\n- initialize(argv, opt_content, nl_content): Read from strings (for Bcp)\n- fillOsiInterface(): Initialize existing interface from strings", "see": ["BonminSetup for algorithm configuration", "AmplInterface for the solver interface"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonOuterApprox.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonOuterApprox.hpp", "filename": "BonOuterApprox.hpp", "file": "BonOuterApprox.hpp", "brief": "Concrete outer approximation extractor for MINLP\n\nBuilds linear outer approximations of nonlinear constraints by\nlinearizing at a given point (typically NLP solution).\n\n**Linearization formula:**\nFor constraint g(x) <= 0 at point x*:\ng(x*) + nabla g(x*)^T (x - x*) <= 0\n\n**Coefficient cleaning (cleanNnz):**\nSmall coefficients are handled to avoid numerical issues:\n- >= tiny_: Keep as-is\n- < veryTiny_: Remove (absorb into RHS if possible)\n- Between: Try to absorb, else bump to tiny_", "see": ["TMINLP2OsiLP for abstract base class", "BonOaDecBase for OA algorithm driver"], "has_pass2": false}}}, "CHiPPS-ALPS": {"name": "CHiPPS-ALPS", "file_count": 21, "pass2_count": 0, "files": {"src/AlpsKnowledgeBrokerMPI.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgeBrokerMPI.h", "filename": "AlpsKnowledgeBrokerMPI.h", "file": "AlpsKnowledgeBrokerMPI.h", "brief": "MPI-based parallel implementation of ALPS knowledge broker\n\nAlpsKnowledgeBrokerMPI provides scalable parallel tree search using MPI.\nImplements hierarchical Master-Hub-Worker architecture.\n\n**Process hierarchy:**\n- Master (rank 0): Inter-cluster load balancing, termination detection\n- Hubs: Manage worker clusters, intra-cluster load balancing\n- Workers: Process subtrees, report status to hub\n\n**Core methods per process type:**\n- masterMain(): Balance hubs, check termination\n- hubMain(): Balance workers, report to master\n- workerMain(): Explore subtrees, request work\n\n**Load balancing:**\n- Inter-cluster: Master moves work between hubs\n- Intra-cluster: Hub moves work between workers\n- Work donation: Split subtrees when requested\n\n**Static initialization schemes:**\n- Root initialization: Master generates nodes, distributes\n- Spiral: Distribute initial work in spiral pattern\n\n**MPI communicators:**\n- MPI_COMM_WORLD: All processes\n- clusterComm_: Hub and its workers\n- hubComm_: Master and all hubs", "see": ["AlpsKnowledgeBroker for base class", "AlpsKnowledgeBrokerSerial for single-process version", "AlpsParams for parallel parameters (hubNum, etc.)"], "has_pass2": false}, "src/AlpsParameterBase.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsParameterBase.h", "filename": "AlpsParameterBase.h", "file": "AlpsParameterBase.h", "brief": "Generic parameter management infrastructure for ALPS\n\nBase classes for defining and parsing typed parameter sets.\nDerived from BCP_parameters.hpp design.\n\n**AlpsParameterT enum:**\n- AlpsBoolPar, AlpsIntPar, AlpsDoublePar, AlpsStringPar\n- AlpsStringArrayPar: Multiple values per key\n\n**AlpsParameter class:**\nIdentifies a single parameter by type and index within type.\n\n**AlpsParameterSet class:**\nContainer for typed parameter arrays with file parsing.\nSubclasses must implement:\n- createKeywordList(): Define keyword→parameter mappings\n- setDefaultEntries(): Set default values\n\n**Reading parameters:**\n- readFromFile(): Parse parameter file\n- readFromStream(): Parse from istream\n- readFromArglist(): Parse command-line args", "see": ["AlpsParams for ALPS-specific parameter set", "BcpsParams, BlisParams for higher-level parameters"], "has_pass2": false}, "src/Alps.h": {"path": "layer-3/CHiPPS-ALPS/src/Alps.h", "filename": "Alps.h", "file": "Alps.h", "brief": "Core definitions for ALPS parallel tree search framework\n\nCentral header defining enumerations, constants, and types for the\nAbstract Library for Parallel Search (ALPS).\n\n**Core enumerations:**\n- AlpsNodeStatus: Node states (Candidate, Evaluated, Pregnant, Branched, Fathomed)\n- AlpsSearchType: Search strategies (BestFirst, BreadthFirst, DepthFirst, Hybrid)\n- AlpsKnowledgeType: Knowledge categories (Model, Node, Solution, SubTree)\n- AlpsExitStatus: Termination status (Optimal, TimeLimit, NodeLimit, etc.)\n- AlpsPhase: Execution phase (Rampup, Search, Rampdown)\n\n**Key constants:**\n- ALPS_OBJ_MAX, ALPS_BND_MAX: Numerical bounds\n- ALPS_ZERO, ALPS_GEN_TOL: Numerical tolerances\n\n**Design philosophy:**\n- Subtree is the basic unit of work in parallel execution\n- Workers process subtrees autonomously\n- Hub brokers coordinate work distribution", "see": ["AlpsKnowledgeBroker for central search management", "AlpsTreeNode for node abstraction", "AlpsModel for problem definition"], "has_pass2": false}, "src/AlpsKnowledgePool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgePool.h", "filename": "AlpsKnowledgePool.h", "file": "AlpsKnowledgePool.h", "brief": "Abstract base class for knowledge storage pools\n\nAlpsKnowledgePool defines the API for all pool types in ALPS.\nKnowledge is stored with associated priority values for ordering.\n\n**Pool types (AlpsKnowledgePoolType):**\n- NodePool: Stores tree nodes awaiting processing\n- SolutionPool: Stores feasible solutions found\n- SubTreePool: Stores subtrees for parallel distribution\n\n**Core API:**\n- addKnowledge(kl, priority): Add with ordering priority\n- getKnowledge(): Peek at highest-priority item\n- popKnowledge(): Remove highest-priority item\n- getBestKnowledge(): Get best quality item\n- hasKnowledge(): Check if pool is non-empty", "see": ["AlpsNodePool for node storage", "AlpsSolutionPool for solution storage", "AlpsSubTreePool for subtree storage"], "has_pass2": false}, "src/AlpsModel.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsModel.h", "filename": "AlpsModel.h", "file": "AlpsModel.h", "brief": "Abstract base class for user problem data in ALPS tree search\n\nAlpsModel holds problem-specific data (constraints, objectives, variables).\nUsers inherit this class and implement virtual methods for their application.\n\n**Key virtual methods to implement:**\n- readInstance(): Read problem from file (master process only)\n- setupSelf(): Initialize model after data loaded/received\n- preprocess(): Preprocessing before search\n- createRoot(): Create the root tree node\n- encode()/decodeToSelf(): Serialize for parallel distribution\n\n**Parallel execution flow:**\n1. Master: readInstance() → setupSelf() → preprocess() → createRoot()\n2. Master: encode() model and send to workers\n3. Workers: decodeToSelf() → setupSelf() → begin processing nodes\n4. All: postprocess() after search completes", "see": ["AlpsKnowledge for serialization interface", "AlpsTreeNode for node processing", "AlpsKnowledgeBroker for search orchestration"], "has_pass2": false}, "src/AlpsEncoded.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsEncoded.h", "filename": "AlpsEncoded.h", "file": "AlpsEncoded.h", "brief": "Serialization buffer for ALPS knowledge objects\n\nAlpsEncoded is the binary buffer for packing/unpacking AlpsKnowledge\nobjects for network transmission in parallel search.\n\n**Buffer structure:**\n- type_: Integer identifying the knowledge type\n- size_: Current data size in bytes\n- representation_: Raw byte buffer\n- pos_: Current read position\n\n**Template methods for serialization:**\n- writeRep(T): Write single value\n- readRep(T): Read single value\n- writeRep(T*, len): Write array with length prefix\n- readRep(T*, len): Read array\n- writeRep/readRep for std::string, std::vector\n\n**Memory management:**\n- Default: 16KB initial allocation\n- make_fit(): Grows buffer as needed (4x growth)\n- clear(): Reset buffer to empty state\n\nBased on BCP_buffer and CoinEncoded designs.", "see": ["AlpsKnowledge::encode()/decode() for usage", "AlpsKnowledgeBroker::registerClass() for type registration"], "has_pass2": false}, "src/AlpsKnowledgeBrokerSerial.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgeBrokerSerial.h", "filename": "AlpsKnowledgeBrokerSerial.h", "file": "AlpsKnowledgeBrokerSerial.h", "brief": "Serial (single-process) implementation of ALPS knowledge broker\n\nAlpsKnowledgeBrokerSerial provides single-threaded tree search.\nUse this for debugging or when parallel execution is not needed.\n\n**Usage:**\n@code\nMyModel model;\nAlpsKnowledgeBrokerSerial broker(argc, argv, model);\nbroker.search(&model);\nbroker.printBestSolution();\n@endcode\n\n**Key methods:**\n- initializeSearch(): Read parameters and problem data\n- rootSearch(): Execute tree search from root\n- searchLog(): Print search statistics\n- printBestSolution(): Output best solution found\n\n**Differences from MPI version:**\n- Single subtree pool (no distribution)\n- No message passing or load balancing\n- All phases (rampup/search/rampdown) in one process", "see": ["AlpsKnowledgeBroker for base class", "AlpsKnowledgeBrokerMPI for parallel version"], "has_pass2": false}, "src/AlpsTreeNode.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsTreeNode.h", "filename": "AlpsTreeNode.h", "file": "AlpsTreeNode.h", "brief": "Search tree node abstraction for ALPS parallel tree search\n\nAlpsTreeNode represents a single node in the search tree. Users inherit\nthis class to implement application-specific node processing.\n\n**Node status lifecycle (AlpsNodeStatus):**\n- Candidate: Waiting in pool for processing\n- Evaluated: Processed, waiting for branching decision\n- Pregnant: Ready to branch, has child descriptions\n- Branched: Children created and added to pool\n- Fathomed: Pruned (by bound, infeasibility, or solution)\n\n**Key virtual methods:**\n- process(): Perform bounding operation (e.g., LP solve)\n- branch(): Create child nodes from pregnant node\n- createNewTreeNode(): Factory for child node creation\n\n**Node data:**\n- quality_: Node quality for selection (lower = better)\n- solEstimate_: Estimated solution quality\n- desc_: AlpsNodeDesc holding problem-specific data\n- explicit_: 1=full description, 0=relative/diff", "see": ["AlpsNodeDesc for node description", "AlpsSubTree for subtree management", "AlpsNodePool for node storage"], "has_pass2": false}, "src/AlpsSubTree.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSubTree.h", "filename": "AlpsSubTree.h", "file": "AlpsSubTree.h", "brief": "Subtree management for ALPS parallel tree search\n\nAlpsSubTree is the basic unit of work in parallel ALPS. Workers process\nentire subtrees autonomously, enabling scalable parallelism.\n\n**Key components:**\n- root_: Root node of this subtree\n- nodePool_: Leaf nodes awaiting processing\n- diveNodePool_: Nodes for diving exploration\n- activeNode_: Currently being processed\n\n**Core operations:**\n- exploreSubTree(): Process nodes up to limits\n- exploreUnitWork(): Process bounded amount of work\n- createChildren(): Branch and add children to pool\n- splitSubTree(): Split off portion for redistribution\n- rampUp(): Generate initial nodes for parallel start\n\n**Diving strategy:**\n- diveNodePool_ holds nodes for deep exploration\n- diveDepth_ tracks current dive depth\n- Helps find feasible solutions quickly\n\n**Dead node removal:**\n- removeDeadNodes(): Recursively remove fathomed branches\n- fathomAllNodes(): Clear entire subtree", "see": ["AlpsTreeNode for individual nodes", "AlpsNodePool for node storage", "AlpsSubTreePool for subtree collection"], "has_pass2": false}, "src/AlpsNodePool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsNodePool.h", "filename": "AlpsNodePool.h", "file": "AlpsNodePool.h", "brief": "Priority queue for tree nodes awaiting processing\n\nAlpsNodePool stores candidate nodes ordered by search strategy.\nUses AlpsPriorityQueue with AlpsSearchStrategy comparison.\n\n**Storage:**\n- candidateList_: Priority queue of AlpsTreeNode pointers\n- searchStrategy_: Comparison function for ordering\n\n**Key operations:**\n- addKnowledge(): Add node with quality as priority\n- getKnowledge()/popKnowledge(): Access/remove best node\n- setNodeSelection(): Change search strategy (re-heaps)\n- getBestNode(): Get node with best quality\n\n**Search strategies supported:**\n- BestFirst: Lowest quality value first\n- DepthFirst: Deepest node first\n- BreadthFirst: Shallowest node first\n- BestEstimate: Best estimated solution first", "see": ["AlpsKnowledgePool for base class", "AlpsPriorityQueue for heap implementation", "AlpsSearchStrategy for comparison classes"], "has_pass2": false}, "src/AlpsMessage.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsMessage.h", "filename": "AlpsMessage.h", "file": "AlpsMessage.h", "brief": "Log messages for ALPS search status and events\n\nDefines ALPS_Message enum for CoinMessageHandler logging.\nNOT the MPI message tags (see AlpsMessageTag.h).\n\n**Message categories:**\n- ALPS_DONATE_*: Work donation events\n- ALPS_LOADBAL_*: Load balancing status\n- ALPS_RAMPUP_*: Initial distribution phase\n- ALPS_TERM_*: Termination detection\n- ALPS_S_*: Serial-only messages\n- ALPS_T_*: Termination reasons\n\n**AlpsMessage class:**\nExtends CoinMessages with ALPS-specific message definitions.\nUsed with CoinMessageHandler for consistent logging.", "see": ["CoinMessageHandler for message output control", "AlpsParams::msgLevel for verbosity settings", "AlpsMessageTag.h for MPI message tags"], "has_pass2": false}, "src/AlpsKnowledge.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledge.h", "filename": "AlpsKnowledge.h", "file": "AlpsKnowledge.h", "brief": "Base class for all ALPS knowledge types (Model, Node, Solution, SubTree)\n\nAlpsKnowledge is the abstract base for all sharable data in parallel search.\nDerived classes must implement encode()/decode() for serialization.\n\n**Knowledge types (AlpsKnowledgeType):**\n- Model: Problem data shared at initialization\n- Node: Search tree nodes processed by workers\n- Solution: Feasible solutions found during search\n- SubTree: Collections of nodes for work distribution\n\n**Serialization for parallel execution:**\n- encode(): Pack object into AlpsEncoded buffer\n- decode(): Create new object from AlpsEncoded\n- decodeToSelf(): Unpack into existing object\n\nSimple contiguous classes can use default encode/decode.\nClasses with pointers or STL containers must override.", "see": ["AlpsEncoded for serialization buffer", "AlpsKnowledgeBroker for knowledge management", "AlpsModel, AlpsTreeNode, AlpsSolution for concrete types"], "has_pass2": false}, "src/AlpsSolution.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSolution.h", "filename": "AlpsSolution.h", "file": "AlpsSolution.h", "brief": "Base class for solutions found during ALPS tree search\n\nAlpsSolution is the abstract base class for feasible solutions.\nUsers inherit this class to store application-specific solution data.\n\n**Base class data:**\n- index_: Node index where solution was found\n- depth_: Tree depth where solution was found\n\n**Key virtual methods:**\n- print(): Output solution to stream\n- encode()/decodeToSelf(): Serialize for parallel sharing\n\nSolutions are stored in AlpsSolutionPool with their quality values.\nThe incumbent (best solution) is tracked and used for pruning.", "see": ["AlpsSolutionPool for solution storage", "AlpsKnowledge for serialization interface", "AlpsKnowledgeBroker::getBestKnowledge() for retrieving best solution"], "has_pass2": false}, "src/AlpsSolutionPool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSolutionPool.h", "filename": "AlpsSolutionPool.h", "file": "AlpsSolutionPool.h", "brief": "Storage for feasible solutions found during search\n\nAlpsSolutionPool stores solutions ordered by quality (priority).\nLower priority value = better solution (for minimization).\n\n**Storage:**\n- solutions_: multimap<double, AlpsSolution*> ordered by quality\n- maxNumSolutions_: Maximum solutions to retain\n\n**Key operations:**\n- addKnowledge(): Add solution, possibly discard worst if at limit\n- getBestKnowledge(): Get solution with lowest priority (best)\n- getAllKnowledges(): Get all solutions for output\n- clean(): Delete all solutions\n\n**AlpsSolutionInterface macro:**\nConvenience macro defining getNumSolutions, getBestSolution, etc.\nfor use in classes that contain a solution pool.", "see": ["AlpsSolution for solution base class", "AlpsKnowledgePool for base interface", "AlpsKnowledgeBroker::solPool_ for broker's solution pool"], "has_pass2": false}, "src/AlpsSubTreePool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSubTreePool.h", "filename": "AlpsSubTreePool.h", "file": "AlpsSubTreePool.h", "brief": "Pool of subtrees for work distribution in parallel search\n\nAlpsSubTreePool stores subtrees for load balancing between processes.\nKey component for parallel scalability in ALPS.\n\n**Storage:**\n- subTreeList_: Priority queue of AlpsSubTree pointers\n- Ordering by subtree quality for work distribution\n\n**Key operations:**\n- addKnowledge(): Add subtree from work donation\n- getKnowledge()/popKnowledge(): Get subtree for processing\n- setComparison(): Set subtree selection strategy\n- getBestQuality(): Get quality of best subtree\n\n**Parallel use:**\n- Serial: Single subtree in pool\n- Parallel: Multiple subtrees for load balancing\n- Subtrees split/merged for work sharing", "see": ["AlpsSubTree for subtree structure", "AlpsKnowledgePool for base interface", "AlpsKnowledgeBroker::subTreePool_ for broker's pool"], "has_pass2": false}, "src/AlpsKnowledgeBroker.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgeBroker.h", "filename": "AlpsKnowledgeBroker.h", "file": "AlpsKnowledgeBroker.h", "brief": "Central manager for parallel tree search knowledge and coordination\n\nAlpsKnowledgeBroker is the abstract base class for search coordination.\nIt manages knowledge pools, search strategy, and execution statistics.\n\n**Key responsibilities:**\n- Manage SubTreePool and SolutionPool\n- Register knowledge types for serialization\n- Track search statistics (nodes processed, branched, etc.)\n- Apply search strategy (node/tree selection)\n\n**Knowledge pool management:**\n- addKnowledge()/getKnowledge(): Store/retrieve knowledge\n- registerClass(): Register decode functions for parallel\n- decoderObject(): Get decoder for knowledge type\n\n**Search phases (AlpsPhase):**\n- Rampup: Initial node generation and distribution\n- Search: Main parallel exploration\n- Rampdown: Collect results and terminate\n\n**Implementations:**\n- AlpsKnowledgeBrokerSerial: Single-process execution\n- AlpsKnowledgeBrokerMPI: MPI-based parallel execution", "see": ["AlpsKnowledgeBrokerSerial for serial implementation", "AlpsKnowledgeBrokerMPI for parallel implementation", "AlpsSearchStrategy for node/tree selection"], "has_pass2": false}, "src/AlpsNodeDesc.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsNodeDesc.h", "filename": "AlpsNodeDesc.h", "file": "AlpsNodeDesc.h", "brief": "Node description holding subproblem-specific data\n\nAlpsNodeDesc holds problem data for a tree node separate from tree\nstructure data. Users inherit this class for application-specific data.\n\n**Design rationale:**\n- AlpsTreeNode: Tree structure (parent, children, status)\n- AlpsNodeDesc: Subproblem data (bounds, constraints, etc.)\n\nThis separation simplifies parallel communication: only the\ndescription needs to be encoded for node transfer.\n\n**Representation modes:**\n- Explicit: Full problem data stored\n- Relative: Differences from parent node", "see": ["AlpsTreeNode for tree structure", "AlpsKnowledge for serialization interface"], "has_pass2": false}, "src/AlpsTime.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsTime.h", "filename": "AlpsTime.h", "file": "AlpsTime.h", "brief": "Timer utilities for ALPS search (CPU and wall clock)\n\nProvides timing functions and AlpsTimer class for measuring execution time.\n\n**Free functions:**\n- AlpsCpuTime(): Alias for CoinCpuTime()\n- AlpsGetTimeOfDay(): Wall clock (MPI_Wtime if MPI, else CoinGetTimeOfDay)\n\n**AlpsTimer class:**\n- start()/stop(): Control timing interval\n- getCpuTime()/getWallClockTime(): Get elapsed time\n- getTime(): Returns CPU or wall based on clockType_\n- reachCpuLimit()/reachWallLimit(): Check time limits\n- setLimit(): Set time limit for checks\n\n**Clock types (AlpsClockType):**\n- AlpsClockTypeCpu: Process CPU time\n- AlpsClockTypeWallClock: Real elapsed time (default)", "see": ["AlpsParams::timeLimit for search time limit", "AlpsKnowledgeBroker timers for search statistics"], "has_pass2": false}, "src/AlpsParams.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsParams.h", "filename": "AlpsParams.h", "file": "AlpsParams.h", "brief": "Parameter set for ALPS tree search configuration\n\nAlpsParams extends AlpsParameterSet with ALPS-specific parameters.\nParameters control search strategy, limits, load balancing, and logging.\n\n**Boolean parameters (boolParams):**\n- deleteDeadNode: Remove fathomed nodes (default: true)\n- interClusterBalance: Master balances hubs (default: true)\n- intraClusterBalance: Hub balances workers (default: true)\n\n**Integer parameters (intParams):**\n- nodeLimit: Max nodes to process (default: INT_MAX)\n- solLimit: Max solutions to store (default: INT_MAX)\n- searchStrategy: 0=best, 1=estimate, 2=BFS, 3=DFS, 4=hybrid\n- msgLevel: Output verbosity (0=none, 1=summary, 2=moderate, 3=verbose)\n- hubNum: Number of hub processes\n- unitWorkNodes: Nodes per work unit\n\n**Double parameters (dblParams):**\n- timeLimit: Max search time in seconds\n- tolerance: Numerical tolerance (default: 1e-6)\n- donorThreshold/receiverThreshold: Load balancing thresholds", "see": ["AlpsParameterBase for parameter infrastructure", "AlpsModel::readParameters() for loading from file"], "has_pass2": false}, "src/AlpsSearchStrategyBase.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSearchStrategyBase.h", "filename": "AlpsSearchStrategyBase.h", "file": "AlpsSearchStrategyBase.h", "brief": "Template base class for search strategy comparison functions\n\nDefines the template interface for node and subtree comparison.\nUsed in priority queues to order items for selection.\n\n**AlpsSearchStrategy<T> template:**\n- compare(x, y): Return true if y preferred over x\n- operator(): Calls compare() for STL compatibility\n- weight_: Optional parameter for tuning\n- type_: Strategy identifier (BestFirst, DFS, etc.)\n\n**Virtual methods for advanced strategies:**\n- selectNextNode(): Custom node selection logic\n- createNewNodes(): Custom branching logic\n\n**AlpsCompare<T> wrapper:**\nAdapter holding strategy pointer for use with std::priority_queue.\nUsed in AlpsPriorityQueue.", "see": ["AlpsSearchStrategy.h for concrete implementations", "AlpsNodePool for node selection usage", "AlpsSubTreePool for subtree selection usage"], "has_pass2": false}, "src/AlpsSearchStrategy.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSearchStrategy.h", "filename": "AlpsSearchStrategy.h", "file": "AlpsSearchStrategy.h", "brief": "Node and subtree selection strategies for ALPS tree search\n\nDefines comparison classes for selecting which node or subtree to explore next.\ncompare(x,y) returns true if y should be processed before x.\n\n**Subtree selection (AlpsTreeSelection):**\n- AlpsTreeSelectionBest: Best quality subtree first\n- AlpsTreeSelectionBreadth: Shallowest root first\n- AlpsTreeSelectionDepth: Deepest root first\n- AlpsTreeSelectionEstimate: Best estimated solution first\n\n**Node selection (AlpsNodeSelection):**\n- AlpsNodeSelectionBest: Best bound first (minimize dual gap)\n- AlpsNodeSelectionBreadth: BFS (shallowest first)\n- AlpsNodeSelectionDepth: DFS (deepest first, finds solutions fast)\n- AlpsNodeSelectionEstimate: Best estimated solution first\n- AlpsNodeSelectionHybrid: Combination strategy\n\n**Hybrid strategy:**\nCombines best-first selection with diving for solution finding.\nselectNextNode() and createNewNodes() can be overridden.", "see": ["AlpsSearchStrategyBase for template base class", "AlpsKnowledgeBroker::setNodeSelection() for configuration", "AlpsSubTree::setNodeSelection() for per-subtree override"], "has_pass2": false}}}, "CHiPPS-BLIS": {"name": "CHiPPS-BLIS", "file_count": 22, "pass2_count": 0, "files": {"src/BlisHeurRound.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisHeurRound.h", "filename": "BlisHeurRound.h", "file": "BlisHeurRound.h", "brief": "Simple rounding heuristic for MILP\n\nAttempts to find an integer-feasible solution by rounding\nfractional variables from the LP solution.\n\n**Algorithm:**\n1. Start with LP relaxation solution\n2. Round each integer variable to nearest integer\n3. Check if resulting solution is feasible\n4. If not, try to fix infeasibilities\n\n**Matrix storage:**\nMaintains both column-major (matrix_) and row-major (matrixByRow_)\nrepresentations for efficient access during rounding.\n\n**seed_:** Random seed for tie-breaking decisions.", "see": ["BlisHeuristic for the base class interface", "BlisModel::callHeuristics() for heuristic invocation"], "has_pass2": false}, "src/BlisBranchStrategyPseudo.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyPseudo.h", "filename": "BlisBranchStrategyPseudo.h", "file": "BlisBranchStrategyPseudo.h", "brief": "Pseudo-cost branching strategy based on historical LP degradation\n\nPseudo-cost branching uses learned estimates of objective degradation\nto select branching variables without solving child LPs.\n\n**Pseudo-cost definition:**\nFor variable xⱼ with fractional value fⱼ = xⱼ* - floor(xⱼ*):\n- ψⱼ⁻ = average (ΔObj / fⱼ) over down branches\n- ψⱼ⁺ = average (ΔObj / (1-fⱼ)) over up branches\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ with value xⱼ*:\n   - Estimate down degradation: Δ⁻ = ψⱼ⁻ · fⱼ\n   - Estimate up degradation: Δ⁺ = ψⱼ⁺ · (1-fⱼ)\n   - Score = μ·min(Δ⁻, Δ⁺) + (1-μ)·max(Δ⁻, Δ⁺)\n2. Select variable with highest score\n\n**Initialization:**\nBefore enough observations, uses default pseudo-costs or\nfalls back to other criteria (objective coefficient, etc.)\n\n**Complexity:** O(n) where n = number of integer variables\nMuch faster than strong branching but less accurate.", "see": ["BlisPseudo for pseudo-cost data structure", "BlisBranchStrategyRel for reliability branching (hybrid approach)", "BlisBranchStrategyStrong for strong branching"], "has_pass2": false}, "src/BlisBranchStrategyRel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyRel.h", "filename": "BlisBranchStrategyRel.h", "file": "BlisBranchStrategyRel.h", "brief": "Reliability branching strategy (strong + pseudo-cost hybrid)\n\nReliability branching combines the accuracy of strong branching with\nthe efficiency of pseudo-costs. It uses strong branching until\npseudo-cost estimates become \"reliable\" (observed enough times).\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ:\n   - If pseudo-costs for xⱼ are reliable (count ≥ relibility_):\n     Use pseudo-cost estimate for score\n   - Else:\n     Perform strong branching (solve child LPs)\n     Update pseudo-costs with observed degradation\n2. Select variable with highest score\n\n**relibility_ parameter:**\nNumber of observations required before trusting pseudo-costs.\nHigher values → more strong branching (accurate but slow)\nLower values → more pseudo-cost usage (fast but less accurate)\n\n**Advantages:**\n- Best of both worlds: accurate early, fast later\n- Pseudo-costs improve as search progresses\n- Default strategy in many modern MILP solvers", "see": ["BlisBranchStrategyStrong for pure strong branching", "BlisBranchStrategyPseudo for pure pseudo-cost branching", "BlisPseudo for pseudo-cost data structure"], "has_pass2": false}, "src/BlisConstraint.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisConstraint.h", "filename": "BlisConstraint.h", "file": "BlisConstraint.h", "brief": "Constraint (cut) representation for BLIS branch-and-cut\n\nRepresents a linear constraint (row) that can be added to the LP.\nUsed for both original constraints and generated cutting planes.\n\n**Data members:**\n- size_, indices_, values_: Sparse row representation\n- Bounds inherited from BcpsConstraint (lbHard, ubHard, lbSoft, ubSoft)\n\n**Key methods:**\n- createOsiRowCut(): Convert to OsiRowCut for LP solver\n- violation(): Compute constraint violation for given LP solution\n- hashing(): Compute hash key for duplicate detection\n\n**Usage in branch-and-cut:**\n1. CglCutGenerator produces OsiRowCut objects\n2. Converted to BlisConstraint for storage in pool\n3. Applied to LP via createOsiRowCut() when needed", "see": ["BlisConGenerator for cut generation management", "BlisVariable for the dual (column) representation", "BcpsConstraintPool for constraint storage"], "has_pass2": false}, "src/BlisMessage.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisMessage.h", "filename": "BlisMessage.h", "file": "BlisMessage.h", "brief": "Log messages for BLIS MILP solver events\n\nDefines BLIS_Message enum for CoinMessageHandler logging.\nSimilar to CoinMessages but specialized for BLIS output.\n\n**Message types:**\n- BLIS_CUTOFF_INC: Cutoff improvement\n- BLIS_CUT_STAT_*: Cut generation statistics\n- BLIS_GAP_*: Optimality gap status\n- BLIS_HEUR_*: Heuristic events and statistics\n- BLIS_ROOT_*: Root node processing info", "see": ["CoinMessageHandler for message output control", "BlisParams for verbosity settings"], "has_pass2": false}, "src/BlisConGenerator.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisConGenerator.h", "filename": "BlisConGenerator.h", "file": "BlisConGenerator.h", "brief": "Interface between BLIS and CGL cut generators\n\nWraps a CglCutGenerator with configuration controlling when and how\ncutting planes are generated during branch-and-cut.\n\n**Strategy options (BlisCutStrategy):**\n- None: Cut generator disabled\n- Root: Generate cuts only at root node\n- Auto: BLIS decides based on effectiveness\n- Periodic: Generate every cutGenerationFrequency_ nodes\n\n**Trigger conditions:**\n- normal_: Call in standard cut generation loop\n- atSolution_: Call when integer solution found\n- whenInfeasible_: Call when LP becomes infeasible\n\n**Statistics tracked:**\n- numConsGenerated_/numConsUsed_: Cut effectiveness\n- time_: CPU time spent in generator\n- calls_/noConsCalls_: Call frequency tracking\n\n**Usage:**\n1. Create: BlisConGenerator(model, new CglGomory(), \"Gomory\")\n2. Register: model->addCutGenerator(generator)\n3. BLIS calls generateConstraints() based on strategy", "see": ["BlisConstraint for the constraint representation", "CglCutGenerator for the CGL base class", "BlisModel::addCutGenerator() for registration"], "has_pass2": false}, "src/BlisHelp.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisHelp.h", "filename": "BlisHelp.h", "file": "BlisHelp.h", "brief": "Utility functions for BLIS solver operations\n\nHelper functions for common BLIS operations including cut conversion,\nstrong branching, warm start serialization, and parallel cut detection.\n\n**Cut conversion:**\n- BlisOsiCutToConstraint(): Convert OsiRowCut to BlisConstraint\n\n**Strong branching:**\n- BlisStrongBranch(): Evaluate branching on a single variable\n  Returns degradation estimates for up/down branches\n\n**Warm start serialization:**\n- BlisEncodeWarmStart(): Pack CoinWarmStartBasis to AlpsEncoded\n- BlisDecodeWarmStart(): Unpack CoinWarmStartBasis from AlpsEncoded\n\n**Cut management:**\n- BlisHashingOsiRowCut(): Compute hash for duplicate detection\n- BlisParallelCutCut(): Check if two cuts are parallel\n- BlisParallelCutCon(): Check cut vs. constraint parallelism\n- BlisParallelConCon(): Check constraint vs. constraint parallelism", "see": ["BlisConstraint for constraint representation", "BlisNodeDesc for warm start storage", "BlisBranchStrategyStrong for strong branching usage"], "has_pass2": false}, "src/BlisBranchStrategyMaxInf.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyMaxInf.h", "filename": "BlisBranchStrategyMaxInf.h", "file": "BlisBranchStrategyMaxInf.h", "brief": "Maximum infeasibility branching strategy\n\nThe simplest branching variable selection: branch on the integer\nvariable that is furthest from being integer.\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ with value xⱼ*:\n   - Compute infeasibility: inf = min(xⱼ* - floor(xⱼ*), ceil(xⱼ*) - xⱼ*)\n   - This is the distance to nearest integer\n2. Select variable with maximum infeasibility\n\n**Rationale:**\nVariables far from integrality are \"more fractional\" and branching\non them may force larger changes to the LP solution.\n\n**Trade-offs:**\n- Very fast: O(n) scan of integer variables\n- No learning or LP solves required\n- Generally produces larger search trees than pseudo-cost or strong\n- Good for initial exploration or when speed matters more than tree size", "see": ["BlisBranchStrategyPseudo for pseudo-cost branching (learned estimates)", "BlisBranchStrategyStrong for strong branching (accurate but slow)"], "has_pass2": false}, "src/BlisHeuristic.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisHeuristic.h", "filename": "BlisHeuristic.h", "file": "BlisHeuristic.h", "brief": "Base class for primal heuristics in BLIS\n\nPrimal heuristics search for integer-feasible solutions during\nbranch-and-cut. Good solutions improve the cutoff for pruning.\n\n**Strategy options (BlisHeurStrategy):**\n- None: Heuristic disabled\n- Root: Run only at root node\n- Auto: BLIS decides based on effectiveness\n- Periodic: Run every heurCallFrequency_ nodes\n- BeforeRoot: Run before solving first LP (e.g., feasibility pump)\n\n**Key virtual method:**\n- searchSolution(): Try to find a solution improving objectiveValue\n  Returns true if solution found, fills newSolution array\n\n**Statistics tracked:**\n- numSolutions_: Solutions found by this heuristic\n- time_: CPU time spent\n- calls_/noSolsCalls_: Success rate tracking", "see": ["BlisHeurRound for simple rounding heuristic", "BlisModel::addHeuristic() for registration", "BlisModel::callHeuristics() for invocation"], "has_pass2": false}, "src/BlisBranchObjectInt.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchObjectInt.h", "filename": "BlisBranchObjectInt.h", "file": "BlisBranchObjectInt.h", "brief": "Branching object for integer variable dichotomy\n\nRepresents a two-way branch on an integer variable.\nCreated when branching decisions are made at a tree node.\n\n**Branch structure:**\nFor variable xⱼ with fractional value v*:\n- Down branch: down_[0] ≤ xⱼ ≤ down_[1] = floor(v*)\n- Up branch: up_[0] = ceil(v*) ≤ xⱼ ≤ up_[1]\n\n**Direction semantics:**\n- direction_ = -1: Down branch executed first\n- direction_ = +1: Up branch executed first\n\n**Key methods:**\n- branch(): Apply bounds for current arm, advance to next\n- getDown()/getUp(): Access bound arrays\n\n**Serialization:**\nencode()/decode() for MPI transmission of branch decisions.", "see": ["BlisObjectInt for the integer variable object", "BlisBranchStrategy* for variable selection", "BlisTreeNode::branch() for branching implementation"], "has_pass2": false}, "src/BlisBranchStrategyBilevel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyBilevel.h", "filename": "BlisBranchStrategyBilevel.h", "file": "BlisBranchStrategyBilevel.h", "brief": "Branching strategy for bilevel programming problems\n\nSpecialized branching for bilevel optimization problems where there\nis a leader (upper-level) and follower (lower-level) decision maker.\n\n**Bilevel programming:**\n- Leader optimizes over (x, y) where y solves follower's problem\n- min_x { F(x, y) : G(x, y) ≥ 0, y ∈ argmin_y { f(x, y) : g(x, y) ≥ 0 } }\n- MILP reformulation uses complementarity or indicator constraints\n\n**Branching approach:**\nTypically branches on complementarity-related variables or\nuses specialized selection for leader vs. follower variables.", "see": ["BlisBranchObjectBilevel for bilevel branching objects", "BlisBranchStrategyStrong for standard strong branching"], "has_pass2": false}, "src/BlisModel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisModel.h", "filename": "BlisModel.h", "file": "BlisModel.h", "brief": "MILP model class extending BiCePS for branch-and-cut\n\nBlisModel is the main model class for the BLIS MILP solver.\nExtends BcpsModel with LP solver, cut generators, heuristics, and branching.\n\n**Initialization workflow:**\n1. readInstance() or importModel(): Load problem data\n2. readParameters(): Configure solver behavior\n3. setupSelf(): Initialize LP solver, objects, strategies\n4. preprocess(): Apply presolve transformations\n5. createRoot(): Create initial search tree node\n\n**LP Solver integration:**\n- origLpSolver_: User-provided OsiSolverInterface\n- presolvedLpSolver_: After presolve transformations\n- lpSolver_: Active solver (presolved or original)\n\n**Cut generation:**\n- addCutGenerator(): Register CglCutGenerator or BlisConGenerator\n- cutStrategy_: When to generate (Root, Auto, Periodic)\n- constraintPool_: Store generated cuts\n\n**Branching:**\n- branchStrategy_: BcpsBranchStrategy for variable selection\n- objects_: BcpsObject array (integer variables, SOS, etc.)\n- priority_: Branching priorities per object\n\n**Heuristics:**\n- addHeuristic(): Register BlisHeuristic instances\n- heurStrategy_: When to call (Root, Auto, Periodic, BeforeRoot)\n\n**Solution management:**\n- incumbent_: Best integer solution found\n- cutoff_: Objective bound for pruning\n- storeSolution(): Record new solutions\n\n**Parallel support:**\n- encode()/decodeToSelf(): Serialize model for MPI\n- packSharedKnowledge(): Share pseudo-costs, cuts, variables", "see": ["BlisTreeNode for node processing using this model", "BlisBranchStrategy* for branching strategies", "BlisConGenerator for cut generation wrapper", "BlisHeuristic for heuristic interface"], "has_pass2": false}, "src/BlisVariable.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisVariable.h", "filename": "BlisVariable.h", "file": "BlisVariable.h", "brief": "Variable representation for column generation in BLIS\n\nExtends BcpsVariable with sparse column representation for LP.\nUsed when variables are generated dynamically (column generation).\n\n**Data members:**\n- objCoef_: Objective coefficient for this variable\n- size_, indices_, values_: Sparse column in constraint matrix\n\n**Usage:**\nIn standard MILP solving, variables are implicit in the LP solver.\nBlisVariable is used when variables need to be:\n- Communicated between processes (parallel B&C)\n- Generated dynamically (column generation/branch-and-price)\n- Stored in variable pools for sharing", "see": ["BlisConstraint for the dual (row) representation", "BcpsVariablePool for variable storage"], "has_pass2": false}, "src/Blis.h": {"path": "layer-3/CHiPPS-BLIS/src/Blis.h", "filename": "Blis.h", "file": "Blis.h", "brief": "Core enumerations and constants for BLIS (BiCePS Linear Integer Solver)\n\nBLIS is a concrete implementation of the CHiPPS framework for MILP.\nHierarchy: ALPS (parallel search) → BiCePS (branch-cut-price) → BLIS (MILP).\n\n**BlisLpStatus enum:**\nLP relaxation solve outcomes (Optimal, Infeasible, IterLim, etc.)\n\n**BlisReturnStatus enum:**\nNode processing outcomes (Ok, ErrLp, Feasible, Branch, etc.)\n\n**BlisCutStrategy enum:**\n- NotSet, None: No cut generation\n- Root: Generate cuts only at root node\n- Auto: Automatic frequency selection\n- Periodic: Generate every cutGenerationFrequency_ nodes\n\n**BlisHeurStrategy enum:**\nSame as cut strategy plus BeforeRoot (e.g., feasibility pump)\n\n**BlisBranchingStrategy enum:**\n- MaxInfeasibility: Branch on most fractional variable\n- PseudoCost: Use pseudo-costs from LP degradation\n- Reliability: Pseudo-costs with strong branching fallback\n- Strong: Full strong branching (solve child LPs)\n- Bilevel: For bilevel programming problems\n\n**BlisBranchingObjectType enum:**\nInteger variables, SOS constraints, bilevel variables", "see": ["BlisModel for the MILP model class", "BlisTreeNode for node processing", "BlisParams for parameter configuration"], "has_pass2": false}, "src/BlisPseudo.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisPseudo.h", "filename": "BlisPseudo.h", "file": "BlisPseudo.h", "brief": "Pseudo-cost data structure for branching variable selection\n\nTracks historical LP objective degradation when branching on a variable.\nUsed to estimate branching quality without solving child LPs.\n\n**Pseudo-cost formula:**\nFor variable xⱼ branched with fractional part f:\n- Up cost: ψ⁺ = Δobj⁺ / (1 - f), averaged over up branches\n- Down cost: ψ⁻ = Δobj⁻ / f, averaged over down branches\n\n**Score calculation:**\nscore_ = weight_ * min(downCost_, upCost_) + (1 - weight_) * max(...)\nDefault weight_ = 1.0 emphasizes minimum degradation.\n\n**Key members:**\n- upCost_/downCost_: Average per-unit objective change\n- upCount_/downCount_: Number of observations (for reliability)\n- weight_: Weighting in score formula [0, 1]\n\n**Usage:**\n- update(): Called after branching to incorporate new observation\n- getScore(): Returns estimated branching quality\n- Serializable via AlpsKnowledge for parallel sharing", "see": ["BlisBranchStrategyPseudo for pseudo-cost branching", "BlisBranchStrategyRel for reliability branching", "BlisObjectInt::pseudocost() which stores per-variable costs"], "has_pass2": false}, "src/BlisObjectInt.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisObjectInt.h", "filename": "BlisObjectInt.h", "file": "BlisObjectInt.h", "brief": "Integer variable object for branching decisions\n\nRepresents an integer-constrained variable in the MILP model.\nStores original bounds, breakeven point, and pseudo-costs.\n\n**Key attributes:**\n- columnIndex_: LP column index for this integer variable\n- originalLower_/Upper_: Bounds before any branching\n- breakEven_: Threshold for up vs. down preference (default 0.5)\n- pseudocost_: BlisPseudocost for branching decisions\n\n**Key methods:**\n- infeasibility(): Returns fractionality in [0.0, 0.5]\n- createBranchObject(): Creates BlisBranchObjectInt for branching\n- feasibleRegion(): Fixes variable to nearest integer\n- preferredNewFeasible(): Direction based on reduced cost\n\n**Pseudo-costs:**\nEach integer object maintains its own pseudo-cost history,\nenabling per-variable learning for branching decisions.", "see": ["BlisBranchObjectInt for the branching action", "BlisPseudocost for pseudo-cost data structure", "BlisBranchStrategyPseudo for pseudo-cost branching"], "has_pass2": false}, "src/BlisBranchStrategyStrong.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyStrong.h", "filename": "BlisBranchStrategyStrong.h", "file": "BlisBranchStrategyStrong.h", "brief": "Strong branching strategy for MILP variable selection\n\nStrong branching is the most accurate (but expensive) branching strategy.\nFor each candidate variable, it actually solves child LP relaxations\nto measure the objective degradation.\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ:\n   - Create down branch (xⱼ ≤ floor(xⱼ*))\n   - Create up branch (xⱼ ≥ ceil(xⱼ*))\n   - Solve both LPs (limited iterations)\n   - Record objective change ΔDown, ΔUp\n2. Score = μ·min(ΔDown, ΔUp) + (1-μ)·max(ΔDown, ΔUp)\n3. Select variable with highest score\n\n**BlisStrong struct:**\nStores results for one candidate: objective changes, infeasibility counts,\nsolver completion status for up/down branches.\n\n**Trade-offs:**\n- Produces smallest search trees (best variable selection)\n- Very expensive at each node (many LP solves)\n- Typically used with iteration limits per candidate LP", "see": ["BlisBranchStrategyRel for reliability branching (strong + pseudo-cost hybrid)", "BlisBranchStrategyPseudo for pure pseudo-cost branching"], "has_pass2": false}, "src/BlisParams.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisParams.h", "filename": "BlisParams.h", "file": "BlisParams.h", "brief": "BLIS-specific parameter set for MILP solver configuration\n\nExtends AlpsParameterSet with parameters controlling branching,\ncut generation, heuristics, and optimality tolerances.\n\n**Parameter categories:**\n\n**chrParams (bool):**\n- cutRampUp: Generate cuts during parallel ramp-up\n- presolve: Apply presolve transformations\n- shareConstraints/Variables: Parallel knowledge sharing\n- sharePseudocostRampUp/Search: Share branching data\n\n**intParams:**\n- branchStrategy: 0=MaxInf, 1=Pseudo, 2=Reliability, 3=Strong, 4=Bilevel\n- cutStrategy, cut*Strategy: Cut generator control (-2=root, -1=auto, 0=off)\n- heurStrategy, heur*Strategy: Heuristic control\n- strongCandSize: Candidates for strong branching\n- pseudoRelibility: Observations before trusting pseudo-costs\n\n**dblParams:**\n- cutoff: Upper bound for pruning\n- cutoffInc: Cutoff increment for fathoming\n- integerTol: Tolerance for integrality\n- optimalRelGap/AbsGap: Termination gaps\n- pseudoWeight: Score formula weighting", "see": ["AlpsParameterSet for base class", "BlisModel which holds this parameter set"], "has_pass2": false}, "src/BlisSolution.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisSolution.h", "filename": "BlisSolution.h", "file": "BlisSolution.h", "brief": "Integer solution representation for BLIS\n\nExtends BcpsSolution with BLIS-specific solution handling.\nStores variable values and objective for integer-feasible solutions.\n\n**Key methods:**\n- print(): Output solution, showing near-integer values as integers\n- encode()/decode(): Serialization for parallel solution sharing\n\n**Usage:**\nCreated by heuristics or when LP solution is integer-feasible.\nStored in AlpsSolutionPool, shared across processes in parallel.", "see": ["BcpsSolution for base class with size_, values_, quality_", "AlpsSolutionPool for solution storage", "BlisHeuristic::searchSolution() which returns these"], "has_pass2": false}, "src/BlisBranchObjectBilevel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchObjectBilevel.h", "filename": "BlisBranchObjectBilevel.h", "file": "BlisBranchObjectBilevel.h", "brief": "Branching object for bilevel programming problems\n\nSpecialized branching object for bilevel optimization where\nbranching may involve multiple variables (a \"branching set\").\n\n**branchingSet_:**\nA deque of variable indices that are affected by this branch.\nUnlike standard integer branching (single variable), bilevel\nproblems may require coordinated changes to multiple variables.", "see": ["BlisBranchStrategyBilevel for bilevel branching strategy", "BlisBranchObjectInt for standard integer branching"], "has_pass2": false}, "src/BlisNodeDesc.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisNodeDesc.h", "filename": "BlisNodeDesc.h", "file": "BlisNodeDesc.h", "brief": "Node description storing LP basis and branching info\n\nExtends BcpsNodeDesc with simplex warm start and pseudo-cost data.\nContains the state needed to reconstruct a search tree node.\n\n**Warm start data:**\n- basis_: CoinWarmStartBasis for LP hot-starting\n- Stores structural and artificial variable status (basic/nonbasic)\n\n**Branching history (for pseudo-cost updates):**\n- branchedDir_: Direction of branch that created this node (-1/+1)\n- branchedInd_: Object index that was branched on\n- branchedVal_: Value at branching point (used in pseudo-cost update)\n\n**Serialization:**\nencode()/decode() serialize basis and branching info for MPI.\nUses BlisEncodeWarmStart/BlisDecodeWarmStart helpers.", "see": ["BlisTreeNode for the node using this description", "CoinWarmStartBasis for LP basis representation", "BlisPseudocost::update() which uses branching history"], "has_pass2": false}, "src/BlisTreeNode.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisTreeNode.h", "filename": "BlisTreeNode.h", "file": "BlisTreeNode.h", "brief": "Concrete branch-and-bound tree node for MILP solving\n\nBlisTreeNode extends BcpsTreeNode with LP-based bounding and cut generation.\nImplements the main node processing loop for branch-and-cut.\n\n**Node processing (process()):**\n1. installSubProblem(): Set up LP with current bounds\n2. bound(): Solve LP relaxation\n3. callHeuristics(): Search for integer solutions\n4. generateConstraints(): Add cutting planes\n5. selectBranchObject(): Choose branching variable\n6. branch(): Create child nodes\n\n**Cut generation workflow:**\n- generateConstraints(): Call registered CglCutGenerators\n- getViolatedConstraints(): Check for violated cuts\n- applyConstraints(): Add cuts to LP and re-solve\n- parallel(): Remove duplicate/parallel cuts\n\n**Bound tightening:**\n- reducedCostFix(): Fix variables using reduced costs\n\n**Node descriptions:**\n- convertToExplicit(): Full bound/cut information\n- convertToRelative(): Delta from parent (space-efficient)\n\n**Branching:**\n- selectBranchObject(): Use model's BcpsBranchStrategy\n- branch(): Create children with modified bounds", "see": ["BlisModel for the model containing LP solver and generators", "BlisNodeDesc for node description structure", "BcpsBranchStrategy for branching variable selection"], "has_pass2": false}}}, "Couenne": {"name": "Couenne", "file_count": 104, "pass2_count": 0, "files": {"src/interfaces/BonCouenneInterface.hpp": {"path": "layer-3/Couenne/src/interfaces/BonCouenneInterface.hpp", "filename": "BonCouenneInterface.hpp", "file": "BonCouenneInterface.hpp", "brief": "Couenne's interface to Bonmin/AMPL\n\nExtends Bonmin's AmplInterface to read AMPL models and build\nCouenne's symbolic problem representation with convex relaxations.\n\n**Key methods:**\n- readAmplNlFile(): Read AMPL .nl file into Couenne structures\n- extractLinearRelaxation(): Build initial LP relaxation with OA cuts\n\n**Linear relaxation extraction:**\n1. Solve continuous NLP relaxation\n2. Generate linearization cuts at NLP optimum\n3. Load cuts into OsiSolverInterface for B&B", "see": ["CouenneProblem for the symbolic representation", "CouenneCutGenerator for cut generation"], "has_pass2": false}, "src/interfaces/CouenneUserInterface.hpp": {"path": "layer-3/Couenne/src/interfaces/CouenneUserInterface.hpp", "filename": "CouenneUserInterface.hpp", "file": "CouenneUserInterface.hpp", "brief": "Abstract base class for Couenne user interfaces\n\nDefines the interface contract for problem input sources.\nConcrete implementations read from AMPL, OSInstance, or custom formats.\n\n**Required methods (pure virtual):**\n- getCouenneProblem(): Return symbolic problem representation\n- getTMINLP(): Return Bonmin TMINLP interface for NLP solves\n\n**Optional hooks:**\n- setupJournals(): Configure output streams (default: stdout)\n- addBabPlugins(): Add cut generators, heuristics, bound tighteners\n- writeSolution(): Output final solution\n\n**Usage pattern:**\n```cpp\nCouenneAmplInterface interface(options, journalist);\nCouenneProblem* prob = interface.getCouenneProblem();\n// ... solve ...\ninterface.writeSolution(bab);\n```", "see": ["CouenneAmplInterface for AMPL .nl file input", "CouenneOSInterface for Optimization Services input"], "has_pass2": false}, "src/interfaces/CouenneTNLP.hpp": {"path": "layer-3/Couenne/src/interfaces/CouenneTNLP.hpp", "filename": "CouenneTNLP.hpp", "file": "CouenneTNLP.hpp", "brief": "Ipopt TNLP interface for Couenne problems\n\nWraps CouenneProblem as an Ipopt::TNLP for NLP solving.\nComputes derivatives using Couenne's symbolic expression DAG.\n\n**Purpose:**\nAllows Ipopt to solve the continuous relaxation or NLP\nsubproblems arising during Couenne's spatial B&B.\n\n**Derivative computation:**\n- Gradient: Via expression differentiation (gradient_ vector)\n- Jacobian: Via ExprJac symbolic Jacobian structure\n- Hessian: Via ExprHess symbolic Hessian of Lagrangian\n\n**Key methods (Ipopt callbacks):**\n- get_nlp_info(): Returns dimensions and sparsity info\n- get_bounds_info(): Variable/constraint bounds\n- eval_f(), eval_grad_f(): Objective and gradient\n- eval_g(), eval_jac_g(): Constraints and Jacobian\n- eval_h(): Hessian of Lagrangian\n- finalize_solution(): Store optimal solution\n\n**Hessian storage:**\noptHessian_ can store the optimal Hessian for later use\n(e.g., in distance computations for Feasibility Pump).", "see": ["CouenneProblem which provides the expression DAG", "ExprJac for Jacobian computation", "ExprHess for Hessian computation"], "has_pass2": false}, "src/interfaces/CouenneMINLPInterface.hpp": {"path": "layer-3/Couenne/src/interfaces/CouenneMINLPInterface.hpp", "filename": "CouenneMINLPInterface.hpp", "file": "CouenneMINLPInterface.hpp", "brief": "OsiSolverInterface for MINLP via Couenne expressions\n\nProvides an Osi-compatible interface for solving MINLP continuous\nrelaxations using NLP solvers (Ipopt or FilterSQP). Gradients and\nJacobians are computed from the expression DAG.\n\n**Solver enum:**\n- EIpopt: Interior-point via Ipopt\n- EFilterSQP: Sequential quadratic programming\n- EAll: Use multiple solvers for robustness\n\n**Key methods:**\n- initialSolve(): Solve initial continuous relaxation\n- resolve(): Re-solve after bound changes\n- getOuterApproximation(): Generate OA cuts at current point\n- extractLinearRelaxation(): Build LP relaxation from NLP\n\n**Outer approximation support:**\n- getOuterApproximation(): Generate linearizations at solution\n- getConstraintOuterApproximation(): Single constraint OA\n- getBendersCut(): Benders decomposition cut\n- getFeasibilityOuterApproximation(): Cuts for feasibility pump\n\n**Strong branching:**\nUses StrongBranchingSolver for approximate LP-based evaluation\nduring variable selection.", "see": ["CouenneProblem for the symbolic representation", "CouenneTNLP for the Ipopt TNLP adapter"], "has_pass2": false}, "src/disjunctive/CouenneDisjCuts.hpp": {"path": "layer-3/Couenne/src/disjunctive/CouenneDisjCuts.hpp", "filename": "CouenneDisjCuts.hpp", "file": "CouenneDisjCuts.hpp", "brief": "Disjunctive cut generator for nonconvex MINLP\n\nGenerates lift-and-project style disjunctive cuts by solving a\nCut-Generating Linear Program (CGLP) for each disjunction.\n\n**Algorithm:**\nFor a disjunction (P1 OR P2) where P1, P2 are linear systems:\n1. Create CGLP with variables for cut coefficients\n2. Add constraints ensuring cut valid for both P1 and P2\n3. Solve CGLP to find deepest cut separating current LP solution\n\n**Disjunction sources:**\n- Integer variable branching: x <= k OR x >= k+1\n- Bound disjunctions from strong branching candidates\n- Convexification-based disjunctions\n\n**Key methods:**\n- getDisjunctions(): Collect all disjunctions to consider\n- generateDisjCuts(): Solve CGLP for each disjunction\n- separateWithDisjunction(): Generate Couenne cuts on both sides\n\n**Parameters:**\n- initDisjPercentage_/Number_: How many disjunctions to use\n- depthLevelling_: Depth to reduce number of disjunctions\n- depthStopSeparate_: Depth to stop separation entirely\n- activeRows_/Cols_: Only include active constraints in CGLP", "see": ["CglLandP for related lift-and-project cuts", "CouenneCutGenerator for convexification cuts"], "has_pass2": false}, "src/readnl/CouenneAmplInterface.hpp": {"path": "layer-3/Couenne/src/readnl/CouenneAmplInterface.hpp", "filename": "CouenneAmplInterface.hpp", "file": "CouenneAmplInterface.hpp", "brief": "AMPL .nl file reader for Couenne\n\nReads optimization problems from AMPL Solver Library (ASL)\nformat and converts to Couenne's expression DAG representation.\n\n**Key methods:**\n- getCouenneProblem(): Parse .nl file → CouenneProblem\n- getTMINLP(): Wrap as Bonmin TMINLP for NLP solves\n- writeSolution(): Write .sol file back to AMPL\n\n**Internal conversion:**\n- readASLfg(): Read using ASL fg (function/gradient) reader\n- readnl(): Alternative .nl parsing\n- nl2e(): Convert ASL expr* to Couenne expression*\n\n**ASL integration:**\nUses the AMPL Solver Library (ASL) C structures to parse\nthe binary .nl format. The nl2e() method recursively converts\nASL expression trees to Couenne expression nodes.", "see": ["CouenneUserInterface base class", "CouenneProblem the target representation"], "has_pass2": false}, "src/expression/CouExpr.hpp": {"path": "layer-3/Couenne/src/expression/CouExpr.hpp", "filename": "CouExpr.hpp", "file": "CouExpr.hpp", "brief": "Expression container with operator overloading for algebraic construction\n\nProvides a user-friendly wrapper around the expression DAG with\noverloaded operators for building mathematical expressions in\nnatural algebraic notation.\n\n**CouExpr container:**\n- Wraps an expression* with value semantics (cloning on copy)\n- Allows algebraic expression construction: e1 + e2, sin(e), etc.\n\n**Supported operations:**\n- Arithmetic: +, -, *, /, %, ^ (power)\n- Trigonometric: sin, cos\n- Other: log, exp\n\n**Operand types:**\n- CouExpr & CouExpr → CouExpr\n- CouNumber & CouExpr → CouExpr\n- CouExpr & CouNumber → CouExpr\n\n**Example:**\n```cpp\nCouExpr x(exprVar), y(exprVar);\nCouExpr f = sin(x) + x*y + exp(y);\nexpression* e = f.Expression();\n```", "see": ["expression which is the underlying representation", "CouenneProblem for high-level model construction"], "has_pass2": false}, "src/expression/CouenneDomain.hpp": {"path": "layer-3/Couenne/src/expression/CouenneDomain.hpp", "filename": "CouenneDomain.hpp", "file": "CouenneDomain.hpp", "brief": "Point and bounding box with save/restore stack\n\nManages the current solution point and variable bounds during\nexpression evaluation and bound tightening. Provides LIFO\nsave/restore functionality for backtracking.\n\n**DomainPoint:**\n- x_[]: Current variable values\n- lb_[]: Lower bounds\n- ub_[]: Upper bounds\n- copied_: True if data is owned (must free on destruction)\n- isNlp_: True if point came from NLP solver (feasible)\n\n**Domain:**\n- point_: Current active point\n- domStack_: Stack of saved points for restore\n\n**push()/pop() pattern:**\n```cpp\ndomain.push(newPoint);  // Save current, use new\n// ... work with new bounds ...\ndomain.pop();           // Restore previous\n```\n\n**Usage:**\nDuring B&B, each node has different bounds. push() saves the\nparent's bounds before processing a child; pop() restores them\nwhen backtracking.", "see": ["CouenneProblem which owns the Domain", "expression::domain_ pointer in each expression"], "has_pass2": false}, "src/expression/CouenneExpression.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExpression.hpp", "filename": "CouenneExpression.hpp", "file": "CouenneExpression.hpp", "brief": "Base class for expression DAG nodes in Couenne\n\nDefines the abstract interface for all expression types (constants,\nvariables, operators). Each expression can evaluate itself, generate\nconvex relaxation cuts, and participate in bound propagation.\n\n**Expression hierarchy:**\n- exprConst: Constant values\n- exprVar/exprAux: Original and auxiliary variables\n- exprOp: Operators (unary, binary, n-ary)\n- exprCopy/exprClone: References to other expressions\n\n**Key virtual methods:**\n- operator()(): Evaluate expression at current point\n- generateCuts(): Generate OA-style convexification cuts for w = f(x)\n- impliedBound(): Backward bound propagation (w bounds → x bounds)\n- getBounds(): Forward bound propagation (x bounds → w bounds)\n- standardize(): Convert to auxiliary variable form\n\n**auxSign enum:**\nDefines how auxiliary w relates to its expression f(x):\n- AUX_LEQ: w <= f(x) (under-estimator)\n- AUX_EQ: w = f(x) (equality definition)\n- AUX_GEQ: w >= f(x) (over-estimator)", "see": ["CouenneProblem for problem container", "exprOp for operator implementations"], "has_pass2": false}, "src/expression/CouenneExprBound.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprBound.hpp", "filename": "CouenneExprBound.hpp", "file": "CouenneExprBound.hpp", "brief": "Expressions representing variable bounds (l_i, u_i)\n\nExpression nodes that evaluate to the current lower/upper bound\nof a variable. Used in parametric convexification to express\nbound-dependent cuts.\n\n**exprLowerBound:**\n- Evaluates to domain_->lb(varIndex_)\n- Prints as \"l_i\" where i is variable index\n- Derivative is 0 (bounds are parameters, not variables)\n\n**exprUpperBound:**\n- Evaluates to domain_->ub(varIndex_)\n- Prints as \"u_i\" where i is variable index\n- Derivative is 0\n\n**Usage example:**\nFor convexification cut on f(x) with x ∈ [l, u]:\nThe secant line passes through (l, f(l)) and (u, f(u)).\nThe cut coefficients depend on l and u, represented\nas exprLowerBound and exprUpperBound.\n\n**Note:**\nThese behave like constants for differentiation purposes\nbut their values change during B&B as bounds tighten.", "see": ["CouenneDomain which provides the bound values", "exprVar for variable expressions"], "has_pass2": false}, "src/expression/CouenneExprVar.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprVar.hpp", "filename": "CouenneExprVar.hpp", "file": "CouenneExprVar.hpp", "brief": "Variable expression class for original decision variables\n\nRepresents decision variables in the expression DAG. All problem\nvariables (original and auxiliary) derive from this class.\n\n**Key attributes:**\n- varIndex_: Index in the problem's variable vector\n- domain_: Pointer to current point and bounds\n\n**Methods:**\n- operator()(): Return current variable value from domain\n- lb()/ub(): Access current lower/upper bounds\n- differentiate(): Returns 1 if differentiating w.r.t. this variable\n- impliedBound(): Propagate bounds from auxiliary to original variables\n\n**Linearity:**\nOriginal variables have Linearity() = LINEAR (affine in themselves).", "see": ["exprAux for auxiliary variables w = f(x)", "Domain for point/bounds container"], "has_pass2": false}, "src/expression/CouennePrecisions.hpp": {"path": "layer-3/Couenne/src/expression/CouennePrecisions.hpp", "filename": "CouennePrecisions.hpp", "file": "CouennePrecisions.hpp", "brief": "Numerical tolerances and constants for Couenne\n\nCentral definition of numerical tolerances used throughout\nCouenne for comparisons, cut generation, and bound handling.\n\n**General tolerances:**\n- COUENNE_EPS (1e-7): General numerical tolerance\n- COUENNE_BOUND_PREC (1e-5): Looser tolerance for bounds\n- COUENNE_EPS_INT (1e-9): Integrality check tolerance\n- COUENNE_EPS_SIMPL (1e-20): Simplification zero threshold\n\n**Infinity and bounds:**\n- COUENNE_INFINITY (1e50): Represents ±∞ in bounds\n- MAX_BOUND (1e45): Practical bound maximum\n- Couenne_large_bound (9.999e12): LP unbounded threshold\n\n**Cut coefficients:**\n- COU_MAX_COEFF (1e9): Maximum cut coefficient\n- COU_MIN_COEFF (1e-9): Minimum cut coefficient\n(Ensures numerical stability in LP solver)\n\n**Utility macros:**\n- COUENNE_round(x): Round to nearest integer\n- COUENNE_sign(x): Return +1 or -1", "see": ["CouenneCutGenerator which uses these tolerances"], "has_pass2": false}, "src/expression/CouenneExprConst.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprConst.hpp", "filename": "CouenneExprConst.hpp", "file": "CouenneExprConst.hpp", "brief": "Constant expression node\n\nRepresents a constant value in the expression DAG.\nLeaf node with fixed value that never changes.\n\n**Properties:**\n- Type: CONST (leaf node)\n- Linearity: ZERO if |value| < ε, else CONSTANT\n- Bounds: [value, value] (trivial)\n- Differentiation: Always returns 0\n- Rank: 0 (lowest priority for branching)\n\n**isInteger():**\nChecks if the constant value is an integer (within tolerance).\nUsed for determining integrality of expressions.\n\n**Usage:**\n- Constant terms in objective/constraints\n- Right-hand sides of constraints\n- Exponents in power expressions", "see": ["expression base class", "exprVar for variable expressions"], "has_pass2": false}, "src/expression/CouenneExprIVar.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprIVar.hpp", "filename": "CouenneExprIVar.hpp", "file": "CouenneExprIVar.hpp", "brief": "Integer variable expression node\n\nRepresents an integer-restricted decision variable in the\nexpression DAG. Inherits from exprVar with additional\nintegrality information.\n\n**Integer markers:**\n- isInteger(): Always returns true (variable is integer-valued)\n- isDefinedInteger(): Always returns true (defined as integer)\n\n**Printed notation:**\nPrints as \"y_i\" where i is the variable index, distinguishing\nfrom continuous variables which print as \"x_i\".\n\n**Usage:**\nInteger variables require special handling in:\n- Branch-and-bound (must branch to enforce integrality)\n- Feasibility checking (must satisfy x ∈ ℤ)\n- Solution rounding in heuristics", "see": ["exprVar base class for continuous variables", "CouenneProblem::addVariable() for variable creation"], "has_pass2": false}, "src/expression/CouenneExprStore.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprStore.hpp", "filename": "CouenneExprStore.hpp", "file": "CouenneExprStore.hpp", "brief": "Expression that returns previously stored value\n\nReturns the Value() of the pointed-to expression rather than\nre-evaluating it. Used for efficiency when an expression has\nalready been evaluated and the value cached.\n\n**Key difference from exprCopy/exprClone:**\n- exprCopy: Calls (*copy_)() to evaluate\n- exprClone: Calls (*copy_)() to evaluate\n- exprStore: Calls copy_->Value() to get cached value\n\n**Usage pattern:**\nWhen an expression tree is evaluated top-down, intermediate\nresults can be stored in value_ fields. exprStore allows\nretrieval of these stored values without re-computation.\n\n**Destructor:**\nLike exprClone, sets copy_ = NULL to prevent deletion.\nDoes NOT own the pointed-to expression.", "see": ["exprCopy which owns and evaluates", "exprClone which references and evaluates"], "has_pass2": false}, "src/expression/CouenneTypes.hpp": {"path": "layer-3/Couenne/src/expression/CouenneTypes.hpp", "filename": "CouenneTypes.hpp", "file": "CouenneTypes.hpp", "brief": "Core type definitions and enumerations for Couenne\n\nCentral header defining enums, types, and small classes used\nthroughout the Couenne codebase.\n\n**Key enumerations:**\n- nodeType: CONST, VAR, UNARY, N_ARY, COPY, AUX, EMPTY\n- linearity_type: ZERO, CONSTANT, LINEAR, QUADRATIC, NONLINEAR\n- convexity: CONVEX, CONCAVE, AFFINE, NONCONVEX\n- expr_type: Codes for each expression class (COU_EXPRSUM, etc.)\n- monotonicity: INCREAS, DECREAS, NONMONOTONE\n\n**t_chg_bounds class:**\nTracks whether lower/upper bounds have been:\n- UNCHANGED: No modification\n- CHANGED: Modified during bound tightening\n- EXACT: Bound is tight (equality holds)\n\n**CouNumber:**\ntypedef double CouNumber - the main floating-point type.\n\n**unary_function:**\nFunction pointer type for unary operators: CouNumber → CouNumber", "see": ["expression which uses these types", "CouenneProblem which uses t_chg_bounds"], "has_pass2": false}, "src/expression/CouenneExprOp.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprOp.hpp", "filename": "CouenneExprOp.hpp", "file": "CouenneExprOp.hpp", "brief": "Base class for n-ary operators in expression DAG\n\nBase for all non-leaf operators (sum, mul, pow, div, etc.).\nProvides argument list management and common functionality.\n\n**Key attributes:**\n- arglist_: Array of pointers to child expressions\n- nargs_: Number of arguments\n\n**Derived operator classes:**\n- exprSum, exprSub: Addition and subtraction\n- exprMul, exprDiv: Multiplication and division\n- exprPow: Power function x^k\n- exprLog, exprExp: Logarithm and exponential\n- exprSin, exprCos: Trigonometric functions\n- exprMin, exprMax: Min/max functions\n- exprAbs: Absolute value\n\n**Methods:**\n- standardize(): Replace with auxiliary variable w = f(x)\n- clonearglist(): Deep copy argument list for cloning\n- rank(): Maximum rank of arguments + 1", "see": ["exprUnary for unary operators (single argument)", "CouenneProblem for expression tree container"], "has_pass2": false}, "src/expression/CouenneExprAux.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprAux.hpp", "filename": "CouenneExprAux.hpp", "file": "CouenneExprAux.hpp", "brief": "Auxiliary variable class for reformulated nonlinear expressions\n\nAuxiliary variables replace nonlinear terms during standardization,\nenabling generation of convex relaxations. If w = f(x), then cuts\nare generated for the relation between w and f(x).\n\n**Key attributes:**\n- image_: The expression f(x) this auxiliary represents\n- sign_: Relation type (AUX_LEQ: w <= f(x), AUX_EQ: w = f(x), AUX_GEQ)\n- rank_: Depth in expression DAG (used for branching priority)\n- multiplicity_: How many times this aux appears in reformulation\n\n**Bound expressions:**\n- lb_/ub_: Expressions computing bounds from child variable bounds\n\n**compExpr functor:**\nEnables set-based lookup of auxiliary variables, avoiding duplicates\nwhen the same expression appears multiple times.", "see": ["exprVar for base class", "CouenneProblem::standardize() for auxiliary creation"], "has_pass2": false}, "src/expression/CouenneExprCopy.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprCopy.hpp", "filename": "CouenneExprCopy.hpp", "file": "CouenneExprCopy.hpp", "brief": "Reference copy of an expression\n\nPoints to another expression and delegates all operations to it.\nUsed to share expressions without duplicating the entire subtree.\n\n**Key concept:**\nexprCopy owns its copy_ pointer and destroys it in destructor.\nThis is the \"owning\" copy - use exprClone for non-owning.\n\n**Delegation pattern:**\nAll methods (evaluate, differentiate, bounds, etc.) forward\nto the underlying copy_ expression.\n\n**Original():**\nReturns the ultimate underlying expression, chasing through\nany chain of copies to find the actual expr.\n\n**value_:**\nCaches the last computed value for use by exprStore expressions.\n\n**Important warning:**\nThis destructor deletes copy_, unlike exprClone and exprStore.\nBe careful about ownership when using expression copies.", "see": ["exprClone for non-owning copy (different destructor behavior)", "exprStore for stored value version", "expression base class"], "has_pass2": false}, "src/expression/CouenneExprClone.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprClone.hpp", "filename": "CouenneExprClone.hpp", "file": "CouenneExprClone.hpp", "brief": "Non-owning reference to another expression\n\nPoints to another expression but does NOT own it - the destructor\nsets copy_ = NULL to prevent exprCopy's destructor from deleting it.\n\n**Key difference from exprCopy:**\n- exprCopy: Owns copy_, deletes it in destructor\n- exprClone: Does NOT own copy_, just references it\n\n**Usage:**\nUse exprClone when you need multiple references to the same\nexpression without duplicating memory or ownership.\n\n**getOriginal():**\nUsed in constructor to skip through chains of clones/copies\nto find the actual underlying expression.\n\n**Evaluation:**\noperator() calls (*copy_)() which evaluates the pointed-to\nexpression and returns its result.", "see": ["exprCopy which owns its referenced expression", "exprStore for storing computed values"], "has_pass2": false}, "src/expression/CouenneExprUnary.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprUnary.hpp", "filename": "CouenneExprUnary.hpp", "file": "CouenneExprUnary.hpp", "brief": "Base class for univariate function expressions\n\nProvides common framework for all single-argument functions like\nsin, cos, log, exp, sqrt, etc. Derived classes override F() to\ndefine the actual function.\n\n**Key virtual methods for derived classes:**\n- F(): Return the unary_function pointer for evaluation\n- generateCuts(): Create convexification cuts for w = f(x)\n- impliedBound(): Backward propagation (w bounds → x bounds)\n\n**Convexification approach:**\nUnivariate functions use tangent cuts (supporting hyperplanes) on\nconvex regions and secant cuts on concave regions.", "see": ["exprLog, exprExp, exprSin, exprCos for concrete implementations", "exprOp for n-ary operators"], "has_pass2": false}, "src/branch/CouenneChooseVariable.hpp": {"path": "layer-3/Couenne/src/branch/CouenneChooseVariable.hpp", "filename": "CouenneChooseVariable.hpp", "file": "CouenneChooseVariable.hpp", "brief": "Variable selection for branching in global optimization\n\nExtends OsiChooseVariable to select branching variables based on\nnonconvexity-specific criteria. The goal is to choose variables\nthat most effectively reduce the relaxation gap.\n\n**setupList():**\nBuilds list of candidate branching objects, filtering by:\n- Infeasibility threshold (|w - f(x)| > tol)\n- Variable type and importance\n- Depth-based filtering\nReturns -1 if node is proven infeasible during setup.\n\n**feasibleSolution():**\nChecks if current solution satisfies all constraints including\nauxiliary variable definitions w = f(x) within tolerance.\n\n**Selection criteria:**\nUnlike MIP (which branches on integer violations), MINLP must\nalso consider nonconvex constraint violations.", "see": ["CouenneChooseStrong for strong branching variant", "CouenneObject for infeasibility computation"], "has_pass2": false}, "src/branch/CouenneThreeWayBranchObj.hpp": {"path": "layer-3/Couenne/src/branch/CouenneThreeWayBranchObj.hpp", "filename": "CouenneThreeWayBranchObj.hpp", "file": "CouenneThreeWayBranchObj.hpp", "brief": "Three-way spatial branching for continuous variables\n\nDivides a variable's domain into three parts instead of two,\nwhich can provide better convexification around the current\nLP solution point.\n\n**Three-way split:**\nGiven interval [l, u] and dividers lcrop_ and rcrop_:\n- Left branch:   [l, lcrop_]\n- Center branch: [lcrop_, rcrop_]\n- Right branch:  [rcrop_, u]\n\n**When useful:**\n- Current LP solution is interior to [l, u]\n- Better convexification needed around that point\n- Two-way split would create very unbalanced children\n\n**Branch order (firstBranch_):**\n- 0: left first\n- 1: center first (THREE_CENTER default)\n- 2: right first\n\nCenter-first often preferred since it contains the current point\nand may quickly find improving solutions.\n\n**Comparison to two-way:**\nThree-way creates more nodes but can reduce overall tree size\nby getting better bounds faster near the current solution.", "see": ["CouenneBranchingObject for standard two-way branching", "CouenneVarObject which can create three-way branches"], "has_pass2": false}, "src/branch/CouenneVarObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneVarObject.hpp", "filename": "CouenneVarObject.hpp", "file": "CouenneVarObject.hpp", "brief": "Variable-based branching object for MINLP\n\nBranching object that focuses on original problem variables rather\nthan auxiliary variables. Computes infeasibility by aggregating\nacross all auxiliaries that depend on this variable.\n\n**Infeasibility computation:**\nFor variable x, sum/min/max over all auxiliaries w = f(...,x,...)\nthe violation |w - f(...,x,...)|. This captures how much branching\non x could help close all related gaps.\n\n**Variable selection modes:**\n- OSI_SIMPLE: Use LP solution value directly\n- OSI_STRONG: Use strong branching estimate\n\n**Branch creation:**\nCan create either:\n- CouenneBranchingObject: standard two-way branch\n- CouenneThreeWayBranchObj: three-way spatial branch\n\n**isCuttable():**\nReturns whether we're on the \"bad\" side where cuts would help.\nIf not cuttable, branching is more likely to help.", "see": ["CouenneObject base class for auxiliary-based branching", "CouenneBranchingObject for branch execution"], "has_pass2": false}, "src/branch/CouenneComplBranchingObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneComplBranchingObject.hpp", "filename": "CouenneComplBranchingObject.hpp", "file": "CouenneComplBranchingObject.hpp", "brief": "Branching object for complementarity constraints\n\nHandles branching on complementarity conditions x₁·x₂ = 0\n(or ≤ 0, ≥ 0 variants) arising in MPEC problems.\n\n**Branching strategy:**\nFor x₁·x₂ = 0, creates two children:\n- Left child: x₁ = 0 (fix first variable)\n- Right child: x₂ = 0 (fix second variable)\n\n**Sign variants:**\n- sign_ = 0: Classical x₁·x₂ = 0\n- sign_ = -1: x₁·x₂ ≤ 0 (opposite signs allowed)\n- sign_ = +1: x₁·x₂ ≥ 0 (same signs required)\n\n**MPEC context:**\nMathematical Programs with Equilibrium Constraints often have\ncomplementarity conditions like x ⊥ (Ax - b) ≥ 0, meaning\nx ≥ 0, Ax - b ≥ 0, and x·(Ax - b) = 0.", "see": ["CouenneComplObject which creates these branching objects", "CouenneBranchingObject base class"], "has_pass2": false}, "src/branch/CouenneBranchingObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneBranchingObject.hpp", "filename": "CouenneBranchingObject.hpp", "file": "CouenneBranchingObject.hpp", "brief": "Spatial branching object for continuous and integer variables\n\nExecutes branching on a variable (which may be continuous) to\npartition the domain and tighten the convex relaxation.\n\n**Spatial branching:**\nUnlike MIP where branching only on integers, Couenne branches on\ncontinuous variables to reduce the gap between the convex relaxation\nand the original nonconvex function.\n\n**branch() method:**\n1. Restrict variable bounds based on branch direction\n2. Optionally run FBBT (doFBBT_) to propagate new bounds\n3. Optionally add convexification cuts (doConvCuts_)\n4. Return estimated objective change\n\n**Bound cropping:**\n- COUENNE_CROP: Minimum bound tightening threshold\n- COUENNE_LARGE_INTERVAL: Threshold for \"large\" variable domains\n- COUENNE_NEAR_BOUND: Threshold for being \"near\" a bound\n\n**Simulation mode:**\nsimulate_ flag indicates strong branching evaluation without\nactually committing the branch.", "see": ["CouenneObject for infeasibility computation", "CouenneChooseStrong for strong branching"], "has_pass2": false}, "src/branch/CouenneOrbitObj.hpp": {"path": "layer-3/Couenne/src/branch/CouenneOrbitObj.hpp", "filename": "CouenneOrbitObj.hpp", "file": "CouenneOrbitObj.hpp", "brief": "Orbital branching using symmetry detection (DISABLED)\n\nNOTE: This code is currently commented out/disabled.\n\nImplements orbital branching which exploits problem symmetry to\nreduce the search space. Uses nauty library to detect symmetries\nand organize variables into orbits.\n\n**Orbital branching concept:**\nIf variables {x1, x2, x3} are symmetric (permutable without\nchanging the problem), then branching on any one of them is\nequivalent. The orbit {x1, x2, x3} can be treated as a single\nbranching decision, pruning symmetric subtrees.\n\n**Node class:**\nStores variable metadata for symmetry detection:\n- index: variable index\n- coeff, lb, ub: bounds and coefficients\n- color: vertex coloring for nauty\n\n**Integration with nauty:**\n- Compute_Symmetry(): Calls nauty to find automorphisms\n- Print_Orbits(): Display detected orbits\n- ChangeBounds(): Update bounds respecting symmetry", "see": ["CouenneNauty for nauty integration", "CouenneObject base class"], "has_pass2": false}, "src/branch/CouenneVTObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneVTObject.hpp", "filename": "CouenneVTObject.hpp", "file": "CouenneVTObject.hpp", "brief": "Violation transfer branching for MINLP variables\n\nComputes variable infeasibility by aggregating violations from\nall auxiliary variables whose definitions depend on this variable.\n\n**Violation transfer concept:**\nFor variable x, measure infeasibility as:\nsum/min/max over all auxiliaries w where w = f(...,x,...):\n  |w - f(...,x,...)|\n\n**Rationale:**\nVariables appearing in many violated auxiliary definitions\nare good branching candidates since branching on them can\nsimultaneously reduce multiple auxiliary violations.\n\n**Comparison to CouenneVarObject:**\n- CouenneVarObject: Direct integrality/bound violation\n- CouenneVTObject: Aggregated auxiliary violations", "see": ["CouenneVarObject base class", "CouenneDepGraph for variable dependencies"], "has_pass2": false}, "src/branch/CouenneProjections.hpp": {"path": "layer-3/Couenne/src/branch/CouenneProjections.hpp", "filename": "CouenneProjections.hpp", "file": "CouenneProjections.hpp", "brief": "Point-to-segment projection utilities\n\nGeometric utilities for projecting points onto line segments,\nused in convexification cut generation and branching point selection.\n\n**project():**\nProjects point (x₀, y₀) onto the segment defined by:\n- Line: ax + by + c ≥ 0 (or ≤ 0 based on sign)\n- Bounds: x ∈ [lb, ub]\nReturns distance to segment, 0 if point satisfies constraint.\nOptionally returns projection coordinates (xp, yp).\n\n**projectSeg():**\nProjects point (x₀, y₀) onto segment between (x₁,y₁) and (x₂,y₂).\nUsed when segment is defined by endpoints rather than coefficients.\n\n**Usage in branching:**\nHelps compute optimal branching points by finding closest\nfeasible points to current LP solution.", "see": ["CouenneBranchingObject for branching point selection"], "has_pass2": false}, "src/branch/CouenneComplObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneComplObject.hpp", "filename": "CouenneComplObject.hpp", "file": "CouenneComplObject.hpp", "brief": "Branching object for complementarity constraints\n\nHandles branching on complementarity conditions x₁ · x₂ ≤ 0,\nx₁ · x₂ ≥ 0, or x₁ · x₂ = 0. Common in equilibrium problems,\nKKT conditions, and MPECs (Mathematical Programs with\nEquilibrium Constraints).\n\n**Complementarity branching:**\nFor x₁ · x₂ = 0, branch on either:\n- x₁ = 0 (one child node)\n- x₂ = 0 (other child node)\n\n**Sign handling (sign_):**\n- sign_ = 0: Classical x₁ · x₂ = 0\n- sign_ = +1: x₁ · x₂ ≤ 0\n- sign_ = -1: x₁ · x₂ ≥ 0\n\n**Infeasibility:**\nMeasures how much the complementarity is violated\nto determine branching priority.", "see": ["CouenneObject base class", "CouenneComplBranchingObject for the branching execution"], "has_pass2": false}, "src/branch/CouenneNauty.hpp": {"path": "layer-3/Couenne/src/branch/CouenneNauty.hpp", "filename": "CouenneNauty.hpp", "file": "CouenneNauty.hpp", "brief": "Interface to nauty library for symmetry detection\n\nWraps the nauty graph automorphism library to detect symmetries\nin MINLP problems. Symmetry information enables orbital branching\nand isomorphism pruning to reduce the search space.\n\n**Nauty integration:**\nThe problem is represented as a graph where:\n- Nodes: Variables and constraints\n- Edges: Connections based on problem structure\nNauty computes the automorphism group of this graph.\n\n**Key methods:**\n- addElement(): Add edge to graph\n- computeAuto(): Compute automorphism group\n- getOrbits(): Get variable orbits (symmetry equivalence classes)\n- getGroupSize(): Size of symmetry group\n- getNumGenerators(): Number of group generators\n\n**Coloring (VarStatus):**\n- FIX_AT_ZERO, FIX_AT_ONE, FREE: Variable status for partitioning\n- color_node(): Assign color to node for refined symmetry\n\n**Performance tracking:**\n- nautyCalls_: Number of calls to nauty\n- nautyTime_: Time spent in nauty computations", "see": ["CouenneOrbitObj for orbital branching objects", "CouenneOrbitBranchingObj for orbital branch execution"], "has_pass2": false}, "src/branch/CouenneObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneObject.hpp", "filename": "CouenneObject.hpp", "file": "CouenneObject.hpp", "brief": "Branching object for auxiliary variables w = f(x)\n\nDefines branching for auxiliary variables based on their infeasibility\n|w - f(x)|. Creates branches to restore feasibility of the relation.\n\n**Infeasibility measure:**\nFor w = f(x), infeasibility = |current_w - f(current_x)|\n\n**Branching strategies (brSelStrat):**\n- MID_INTERVAL: Branch at interval midpoint\n- MIN_AREA: Minimize area between convex relaxation and function\n- BALANCED: Balance improvement on both branches\n- LP_CENTRAL: Use LP solution as branching point\n- LP_CLAMPED: LP solution clamped to safe region\n\n**Branching modes:**\n- TWO_LEFT/RIGHT/RAND: Two-way branching\n- THREE_LEFT/CENTER/RIGHT/RAND: Three-way branching\n\n**Pseudocost estimation:**\n- downEstimate_/upEstimate_: Predicted improvement for each branch\n- pseudocostMult: Method for computing estimates", "see": ["CouenneBranchingObject for the actual branch creation", "CouenneChooseVariable for variable selection"], "has_pass2": false}, "src/branch/CouenneOrbitBranchingObj.hpp": {"path": "layer-3/Couenne/src/branch/CouenneOrbitBranchingObj.hpp", "filename": "CouenneOrbitBranchingObj.hpp", "file": "CouenneOrbitBranchingObj.hpp", "brief": "Orbital branching object using symmetry\n\nSpatial branching object that exploits problem symmetry via\norbital branching to prune symmetric subtrees.\n\n**Orbital branching:**\nWhen symmetry is detected (via nauty), branching on one variable\nin an orbit implicitly covers all symmetric variables in that orbit.\nThis avoids exploring symmetric portions of the search tree.\n\n**Key features:**\n- boundBranch(): Returns true if only bound changes (no cuts)\n- simulate_: Flag for simulating branch without execution\n- Integrates with CouenneNauty symmetry detection\n\n**Branch execution:**\nbranch() applies bound changes and optionally generates\nconvexification cuts, with symmetry-aware pruning.", "see": ["CouenneNauty for symmetry detection", "CouenneOrbitObj which creates these branching objects", "CouenneBranchingObject base class"], "has_pass2": false}, "src/branch/CouenneSOSObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneSOSObject.hpp", "filename": "CouenneSOSObject.hpp", "file": "CouenneSOSObject.hpp", "brief": "Special Ordered Set (SOS) branching for Couenne\n\nExtends OsiSOS to include Couenne-specific functionality like\nbound tightening and convexification cut generation at branching.\n\n**SOS Types:**\n- SOS Type 1: At most one variable can be nonzero\n- SOS Type 2: At most two adjacent variables can be nonzero\n\n**CouenneSOSObject:**\nWraps OsiSOS with:\n- problem_: Link to CouenneProblem for bound tightening\n- reference_: Associated auxiliary variable\n- doFBBT_: Enable FBBT at branching\n- doConvCuts_: Add convexification cuts at branching\n\n**CouenneSOSBranchingObject:**\nExecutes the SOS branching, dividing variables into sets\nwhere the SOS constraint can be separately enforced.\n\n**TODO in code:**\nNotes extension to handle Σxᵢ ≤ k constraints with small k\nusing SOS-like branching instead of individual variable branching.", "see": ["OsiSOS base class", "CbcSOS for CBC's SOS implementation"], "has_pass2": false}, "src/branch/CouenneChooseStrong.hpp": {"path": "layer-3/Couenne/src/branch/CouenneChooseStrong.hpp", "filename": "CouenneChooseStrong.hpp", "file": "CouenneChooseStrong.hpp", "brief": "Strong branching for global MINLP optimization\n\nExtends Bonmin's strong branching to handle nonconvex constraints\nby evaluating actual LP bound improvement from branching.\n\n**doStrongBranching():**\nFor each candidate variable:\n1. Tentatively branch both ways (simulateBranch)\n2. Re-solve LP relaxations\n3. Record bound improvement\n4. Select best candidate\n\nReturn codes:\n- -1: Both branches infeasible\n- 0: All inspected, nothing fixable\n- 1: Some variables can be fixed\n- 2: Returning early with one fixable\n- 3: Time limit reached\n\n**Selection criteria (estimateProduct_):**\n- false: Convex combination of min/max estimates (classic)\n- true: Product (1e-6 + min) * max (reliability-like)\n\n**Pseudocost updates (pseudoUpdateLP_):**\nIf true, updates pseudocosts using distance between LP point\nand resulting branch LP solutions.", "see": ["CouenneChooseVariable for simpler selection", "CouenneBranchingObject for branch execution"], "has_pass2": false}, "src/convex/CouenneCutGenerator.hpp": {"path": "layer-3/Couenne/src/convex/CouenneCutGenerator.hpp", "filename": "CouenneCutGenerator.hpp", "file": "CouenneCutGenerator.hpp", "brief": "Main convexification cut generator for global MINLP optimization\n\nGenerates linear outer approximation cuts to build convex relaxations\nof nonconvex MINLPs. Works with the symbolic CouenneProblem representation.\n\n**Cut generation (generateCuts):**\n1. For each auxiliary variable w = f(x), generate linearization cuts\n2. Cuts are based on expression type (convex envelope, secants, tangents)\n3. Uses current LP solution as linearization point\n\n**Convexification types (conv_type):**\n- Current-point linearization (most common)\n- Multi-point sampling for tighter relaxations\n\n**Cut types generated:**\n- Tangent cuts for convex functions\n- Secant cuts for concave functions\n- McCormick envelope cuts for bilinear terms\n- Specialized cuts for sin, cos, exp, log, etc.\n\n**Helper methods:**\n- addEnvelope(): Generate convex/concave envelope for univariate function\n- addSegment(): Add secant line between two points\n- addTangent(): Add tangent at given point\n- createCut(): Build and validate OsiRowCut", "see": ["CouenneProblem for the symbolic problem representation", "expression::generateCuts() for per-operator cut generation"], "has_pass2": false}, "src/bound_tightening/CouenneFixPoint.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneFixPoint.hpp", "filename": "CouenneFixPoint.hpp", "file": "CouenneFixPoint.hpp", "brief": "Fixpoint-based bound tightening via constraint propagation\n\nImplements Feasibility-Based Bound Tightening (FBBT) using fixpoint\niteration. Propagates bounds through expression DAG until no further\ntightening is possible.\n\n**Algorithm:**\n1. For each constraint g(x) <= 0, propagate bounds forward (x → g)\n2. Propagate bounds backward (g bounds → x bounds) via implied bounds\n3. Repeat until fixpoint (no bound changes) or iteration limit\n\n**Extended model option (extendedModel_):**\nCreates additional LP rows for tighter propagation.\n\n**Performance tracking:**\n- nTightened_: Count of bounds tightened\n- CPUtime_: Time spent in FBBT\n- perfIndicator_: Detailed performance metrics", "see": ["CouenneProblem::boundTightening() for main bound tightening entry", "expression::impliedBound() for backward propagation"], "has_pass2": false}, "src/bound_tightening/CouenneBTPerfIndicator.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneBTPerfIndicator.hpp", "filename": "CouenneBTPerfIndicator.hpp", "file": "CouenneBTPerfIndicator.hpp", "brief": "Performance metrics for bound tightening\n\nTracks effectiveness of bound tightening methods (FBBT, OBBT, etc.)\nby measuring how much bounds are reduced and how many variables are fixed.\n\n**Metrics tracked:**\n- nFixed_: Number of variables fixed (lb == ub)\n- boundRatio_: Average bound width shrinkage ratio\n- shrunkInf_: Bounds that became finite from infinite\n- shrunkDoubleInf_: [-inf,inf] that became [a,inf] or [-inf,b]\n- nProvedInfeas_: Number of infeasibility proofs\n\n**Usage:**\n1. Call setOldBounds() before bound tightening\n2. Run bound tightening\n3. Call update() with new bounds\n4. Metrics accumulated for end-of-run summary\n\n**Timing:**\n- totalTime_: CPU time spent in this bound tightener\n- nRuns_: Number of invocations", "see": ["CouenneFixPoint which uses this for FBBT stats", "CouenneAggrProbing which uses this for OBBT stats"], "has_pass2": false}, "src/bound_tightening/CouenneMultiVarProbe.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneMultiVarProbe.hpp", "filename": "CouenneMultiVarProbe.hpp", "file": "CouenneMultiVarProbe.hpp", "brief": "Multi-variable probing for bound tightening\n\nExtension of single-variable probing that considers multiple\nvariables simultaneously for bound tightening. Can detect\nimplications that single-variable probing would miss.\n\n**Multi-variable probing:**\nInstead of fixing one variable at a time, considers combinations\nof variable settings to derive stronger implied bounds.\n\n**Example:**\nSingle-variable probing on x1 and x2 separately might not tighten bounds,\nbut probing x1=lb AND x2=ub together might prove infeasibility or\nderive new bounds.\n\n**Parameters:**\n- maxTime_: Maximum time for probing\n- numCols_: Number of columns to consider", "see": ["CouenneAggrProbing for single-variable OBBT", "CouenneFixPoint for FBBT"], "has_pass2": false}, "src/bound_tightening/CouenneSparseBndVec.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneSparseBndVec.hpp", "filename": "CouenneSparseBndVec.hpp", "file": "CouenneSparseBndVec.hpp", "brief": "Sparse vector with O(1) initialization for bound tracking\n\nEfficient sparse+dense hybrid data structure for tracking which\nvariables had bounds tightened. Avoids O(n) initialization cost.\n\n**Data structure (Briggs-Torczon):**\n- dInd_[0..n_-1]: Dense list of assigned indices\n- sInd_[i]: Position of index i in dense list (garbage if unset)\n- data_[i]: Value at index i (garbage if unset)\n\n**Key operations:**\n- operator[](i): O(1) access, auto-initializes new entries\n- reset(): O(1) clear (just set n_ = 0)\n- nElements(): Number of assigned entries\n- indices()/data(): Access for iteration\n\n**Why useful in FBBT:**\nIn bound tightening, typically k << n variables have bounds\ntightened. This structure allows O(k) iteration over tightened\nbounds without O(n) initialization/cleanup overhead.\n\n**Note:**\nValgrind will complain about uninitialized reads - this is expected\ndue to the lazy initialization design.", "see": ["CouenneFixPoint which uses this for bound change tracking"], "has_pass2": false}, "src/bound_tightening/CouenneInfeasCut.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneInfeasCut.hpp", "filename": "CouenneInfeasCut.hpp", "file": "CouenneInfeasCut.hpp", "brief": "Signal infeasibility via fictitious cut\n\nMechanism for bound tighteners to signal detected infeasibility\nto the node solver without early termination.\n\n**WipeMakeInfeas():**\nAdds a fictitious cut 1 ≤ x₀ ≤ -1 to the cut set. This\nimpossible constraint signals to subsequent components that\nthe node is infeasible.\n\n**isWiped():**\nChecks whether the cut set contains the infeasibility signal.\nCut generators should check this before expensive operations.\n\n**Why needed:**\nIn Cgl's cut generator framework, generators cannot directly\nprune nodes. This sentinel cut mechanism allows bound tighteners\nto signal infeasibility discovered during FBBT/OBBT.", "see": ["CouenneFixPoint which may detect infeasibility", "CouenneAggrProbing which may detect infeasibility"], "has_pass2": false}, "src/bound_tightening/CouenneAggrProbing.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneAggrProbing.hpp", "filename": "CouenneAggrProbing.hpp", "file": "CouenneAggrProbing.hpp", "brief": "Aggressive probing for bound tightening\n\nImplements Optimality-Based Bound Tightening (OBBT) through aggressive\nprobing. Temporarily fixes a variable bound and solves the resulting\nsubproblem to determine if a tighter bound is achievable.\n\n**Algorithm for probing variable x_i:**\n1. Set objective to minimize (or maximize) x_i\n2. Solve resulting NLP/MINLP subproblem\n3. Optimal value gives new bound on x_i\n4. Restore original objective\n\n**Designed for parallelization:**\nEach variable can be probed independently, making this suitable\nfor parallel execution. User can probe specific variables without\nusing full cut generator interface.\n\n**Parameters:**\n- maxTime_: Maximum time per variable probe\n- maxNodes_: Maximum B&B nodes per probe\n- maxFailedSteps_: Stop after consecutive failures\n- restoreCutoff_: Whether to restore cutoff after probing\n\n**Expense:**\nVery expensive (solves NLP per bound per variable), so use sparingly\nor in parallel environments.", "see": ["CouenneFixPoint for cheaper FBBT alternative", "CouenneMultiVarProbe for multi-variable probing"], "has_pass2": false}, "src/main/CouenneOSInterface.hpp": {"path": "layer-3/Couenne/src/main/CouenneOSInterface.hpp", "filename": "CouenneOSInterface.hpp", "file": "CouenneOSInterface.hpp", "brief": "Optimization Services (OS) interface for Couenne\n\nReads optimization problems from OSInstance format (COIN-OR\nOptimization Services) and converts to Couenne representation.\n\n**OSInstance format:**\nXML-based representation for optimization problems defined by\nthe Optimization Services project. Supports linear, quadratic,\nand nonlinear constraints with expression trees.\n\n**Key methods:**\n- getCouenneProblem(): Convert OSInstance → CouenneProblem\n- getTMINLP(): Wrap as Bonmin TMINLP\n- writeSolution(): Output in OS solution format\n\n**Usage:**\nAlternative to AMPL interface for problems defined via\nOptimization Services XML or programmatic APIs.", "see": ["CouenneUserInterface base class", "CouenneAmplInterface alternative AMPL input"], "has_pass2": false}, "src/main/CouenneBab.hpp": {"path": "layer-3/Couenne/src/main/CouenneBab.hpp", "filename": "CouenneBab.hpp", "file": "CouenneBab.hpp", "brief": "Main Branch-and-Bound driver for Couenne\n\nExtends Bonmin::Bab to add Couenne-specific functionality\nfor spatial branch-and-bound on nonconvex MINLPs.\n\n**Inheritance:**\nCouenneBab → Bonmin::Bab → CbcModel\n\n**Key additions over Bonmin::Bab:**\n- Stores CouenneProblem pointer for access to expression DAG\n- Overrides bestSolution() and bestObj() for proper handling\n- bestBound() returns min of parent bound and best objective\n\n**Usage:**\nCalled from BonCouenneSetup after problem setup.\nThe branchAndBound() method runs the full spatial B&B algorithm,\nusing Couenne's convexification, bound tightening, and\nbranching strategies.\n\n**Solution retrieval:**\n- bestSolution(): Optimal variable values (or best known)\n- bestObj(): Objective value at best solution\n- bestBound(): Lower bound on optimal value", "see": ["BonCouenneSetup which configures and launches this", "Bonmin::Bab base class", "CouenneProblem for the problem representation"], "has_pass2": false}, "src/main/BonCouenneSetup.hpp": {"path": "layer-3/Couenne/src/main/BonCouenneSetup.hpp", "filename": "BonCouenneSetup.hpp", "file": "BonCouenneSetup.hpp", "brief": "Main setup class for Couenne global optimizer\n\nExtends Bonmin's setup to configure Couenne's global optimization\ncomponents including cut generators, bound tightening, and heuristics.\n\n**Initialization (InitializeCouenne):**\n1. Read AMPL model and options\n2. Create CouenneProblem representation\n3. Standardize problem (create auxiliary variables)\n4. Register cut generators (convexification, bound tightening)\n5. Configure branching and heuristics\n\n**Key components configured:**\n- CouenneCutGenerator: Convexification cuts\n- CouenneFixPoint: FBBT bound tightening\n- CouenneFeasPump: Feasibility pump heuristic\n- CouenneChooseVariable: Branching variable selection\n\n**SmartAsl:**\nReference-counted wrapper for ASL pointer (AMPL Solver Library).", "see": ["BonminSetup for base class", "CouenneProblem for the problem representation"], "has_pass2": false}, "src/main/BonCouenneInfo.hpp": {"path": "layer-3/Couenne/src/main/BonCouenneInfo.hpp", "filename": "BonCouenneInfo.hpp", "file": "BonCouenneInfo.hpp", "brief": "Information passing between B&B components\n\nExtends Bonmin's BabInfo with Couenne-specific information,\nparticularly storage of NLP solutions found during search.\n\n**NlpSolution class:**\nReference-counted storage for NLP solutions:\n- n_: Number of variables\n- sol_[]: Variable values\n- objVal_: Objective function value\n\n**CouenneInfo:**\n- nlpSols_: List of all NLP solutions found\n- addSolution(): Record a new NLP solution\n- NlpSolutions(): Access stored solutions\n\n**Usage:**\nWhen Couenne finds feasible NLP solutions (from heuristics\nor at B&B nodes), they are stored here for incumbent tracking\nand warm-starting purposes.", "see": ["Bonmin::BabInfo base class", "CouenneFeasPump which generates NLP solutions"], "has_pass2": false}, "src/problem/CouenneJournalist.hpp": {"path": "layer-3/Couenne/src/problem/CouenneJournalist.hpp", "filename": "CouenneJournalist.hpp", "file": "CouenneJournalist.hpp", "brief": "Couenne-specific logging categories using Ipopt's Journalist\n\nExtends Ipopt's Journalist logging facility with Couenne-specific\ncategories for selective debug output.\n\n**Log categories defined:**\n- J_BRANCHING: Branch-and-bound variable selection\n- J_BOUNDTIGHTENING: FBBT and OBBT progress\n- J_CONVEXIFYING: Cut generation and convexification\n- J_PROBLEM: Problem construction and reformulation\n- J_NLPHEURISTIC: NLP solve heuristics\n- J_DISJCUTS: Disjunctive cut generation (CGLP)\n- J_REFORMULATE: Expression reformulation\n- J_COUENNE: General Couenne messages\n\n**Usage:**\n```cpp\njnlst->Printf(J_SUMMARY, J_BRANCHING,\n              \"Selected var %d with score %.2f\\n\", idx, score);\n```\n\n**Verbosity levels (from Ipopt):**\n- J_NONE, J_ERROR, J_WARNING, J_SUMMARY, J_DETAILED, ...", "see": ["Ipopt::Journalist for full logging API"], "has_pass2": false}, "src/problem/CouenneSolverInterface.hpp": {"path": "layer-3/Couenne/src/problem/CouenneSolverInterface.hpp", "filename": "CouenneSolverInterface.hpp", "file": "CouenneSolverInterface.hpp", "brief": "OsiSolverInterface wrapper with bound tightening integration\n\nTemplate class wrapping any OsiSolverInterface (typically OsiClpSolverInterface)\nwith Couenne-specific enhancements for spatial B&B.\n\n**Key features:**\n1. Applies FBBT before resolve() to tighten bounds\n2. Uses expression-based isInteger() for auxiliary variables\n3. Integrates NLP solutions into branching decisions\n\n**Bound tightening integration:**\n- tightenBounds(): Apply FBBT before LP solve\n- tightenBoundsCLP(): Clp-specific variable bound analysis\n- tightenBoundsCLP_Light(): Lightweight version for hot starts\n\n**Infeasibility tracking:**\n- knowInfeasible_: Detected during bound tightening\n- knowDualInfeasible_: Continuous relaxation is unbounded\n- isProvenPrimalInfeasible(): May detect before LP solve\n\n**Template usage:**\nCouenneSolverInterface<OsiClpSolverInterface> wraps Clp\nwith Couenne's bound tightening infrastructure.", "see": ["CouenneCutGenerator for the convexification engine", "CouenneFixPoint for FBBT implementation"], "has_pass2": false}, "src/problem/CouenneRecordBestSol.hpp": {"path": "layer-3/Couenne/src/problem/CouenneRecordBestSol.hpp", "filename": "CouenneRecordBestSol.hpp", "file": "CouenneRecordBestSol.hpp", "brief": "Best solution recording with feasibility tracking\n\nMaintains the best feasible solution found during optimization,\nwith detailed violation tracking for debugging and reporting.\n\n**Initial domain info:**\n- cardInitDom: Problem dimension\n- initIsInt[]: Integer variable flags\n- listInt: Indices of integer variables\n- initDomLb[], initDomUb[]: Original bounds (for feasibility check)\n\n**Best solution state:**\n- hasSol: True if any solution has been recorded\n- sol[]: Current best solution vector\n- val: Objective value of best solution\n- maxViol: Maximum violation (bound, integrality, or constraint)\n\n**Modified solution workspace:**\n- modSol[], modSolVal, modSolMaxViol: Temporary storage for\n  checkNLP2 and update operations before committing\n\n**update() methods:**\n- update(sol, card, val, viol): Update if val < current val\n- update(): Commit modSol if modSolVal < current val\n\n**compareAndSave():**\nCompare two solutions, keep better one with finite value (<1e49).\nReturns: -1 if both infinite, 0 if solA saved, 1 if solB saved.", "see": ["CouenneProblem which owns the CouenneRecordBestSol", "checkNLP2 which uses modSol for temporary evaluation"], "has_pass2": false}, "src/problem/CouenneProblem.hpp": {"path": "layer-3/Couenne/src/problem/CouenneProblem.hpp", "filename": "CouenneProblem.hpp", "file": "CouenneProblem.hpp", "brief": "Central MINLP problem representation with expression DAG\n\nThe heart of Couenne's global optimization approach. Represents MINLPs\nsymbolically as expression trees, enabling automatic convexification,\nbound propagation, and reformulation.\n\n**Key data structures:**\n- variables_: Original, auxiliary, and defined variables\n- objectives_/constraints_: Symbolic expressions\n- graph_: Dependency graph for evaluation ordering\n- auxSet_: Set of auxiliary variables w = f(x) for linearization\n\n**Bound tightening methods:**\n- FBBT (doFBBT_): Feasibility-based bound tightening via constraint propagation\n- OBBT (doOBBT_): Optimality-based bound tightening via LP solves\n- RCBT (doRCBT_): Reduced-cost bound tightening\n- ABT (doABT_): Aggressive bound tightening via domain partitioning\n\n**Reformulation (standardize()):**\nConverts nonlinear expressions into auxiliary variable definitions\nw = f(x), enabling generation of convex relaxations for each operator.\n\n**Symmetry handling:**\nUses nauty library for graph automorphism detection and orbital branching.", "see": ["expression for the expression base class", "CouenneCutGenerator for convexification cut generation"], "has_pass2": false}, "src/problem/CouenneProblemElem.hpp": {"path": "layer-3/Couenne/src/problem/CouenneProblemElem.hpp", "filename": "CouenneProblemElem.hpp", "file": "CouenneProblemElem.hpp", "brief": "Constraint and objective classes for Couenne problems\n\nDefines the building blocks for optimization problems:\nconstraints with expression bounds and objective functions.\n\n**CouenneConstraint:**\nRepresents lb_ <= body_ <= ub_ where all three are expressions:\n- body_: The constraint expression (e.g., exp(x1+x2))\n- lb_: Lower bound expression (defaults to 0 or -INFINITY)\n- ub_: Upper bound expression (defaults to 0 or +INFINITY)\n\n**Default bound handling:**\n- Both NULL → equality constraint (lb = ub = 0)\n- lb NULL, ub set → lb = -INFINITY (one-sided upper)\n- ub NULL, lb set → ub = +INFINITY (one-sided lower)\n\n**CouenneObjective:**\nMinimization objective wrapper. Maximization problems are\nconverted by negating the objective during problem input.\n\n**standardize():**\nDecomposes complex expressions into auxiliary variable\ndefinitions for reformulation-linearization.", "see": ["CouenneProblem which owns constraints and objectives", "expression for the expression tree classes"], "has_pass2": false}, "src/problem/CouenneGlobalCutOff.hpp": {"path": "layer-3/Couenne/src/problem/CouenneGlobalCutOff.hpp", "filename": "CouenneGlobalCutOff.hpp", "file": "CouenneGlobalCutOff.hpp", "brief": "Global best solution and cutoff value storage\n\nManages the incumbent (best known feasible solution) and its\nobjective value across the entire B&B search.\n\n**Role in B&B:**\n- Provides cutoff value for pruning nodes\n- Stores best solution found so far\n- Updated when heuristics or B&B find improvements\n\n**Members:**\n- cutoff_: Objective value of best solution (upper bound for min)\n- sol_: Variable values of best solution\n- valid_: Whether stored solution corresponds to cutoff\n\n**Thread safety note:**\nIn parallel environments, this would need synchronization.\nCurrently assumes sequential access.\n\n**setCutOff():**\nUpdates cutoff and optionally stores the solution vector.\nRequires CouenneProblem pointer for size information.", "see": ["CouenneProblem which owns the GlobalCutOff", "CouenneFeasPump which updates cutoff when finding solutions"], "has_pass2": false}, "src/standardize/CouenneLQelems.hpp": {"path": "layer-3/Couenne/src/standardize/CouenneLQelems.hpp", "filename": "CouenneLQelems.hpp", "file": "CouenneLQelems.hpp", "brief": "Linear and quadratic term storage for standardization\n\nData structures for collecting linear and quadratic terms during\nexpression standardization (conversion to auxiliary variable form).\n\n**quadElem:**\nSingle quadratic term c·xᵢ·xⱼ:\n- varI_, varJ_: Variable pointers\n- coeff_: Coefficient c\n\n**LinMap:**\nSparse map of linear terms: index → coefficient\n- insert(index, coe): Add/accumulate coefficient\n- Auto-removes zero entries\n\n**QuadMap:**\nSparse map of quadratic terms: (i,j) → coefficient\n- insert(indI, indJ, coe): Add/accumulate coefficient\n- Auto-removes zero entries\n\n**Usage:**\nDuring standardization, expressions like x² + 2xy + 3x + 4\nare decomposed into LinMap (3x + 4 constant) and QuadMap\n(x² + 2xy). These are then converted to auxiliary form.", "see": ["CouenneProblem::standardize() which uses these structures"], "has_pass2": false}, "src/heuristics/CouenneFeasPump.hpp": {"path": "layer-3/Couenne/src/heuristics/CouenneFeasPump.hpp", "filename": "CouenneFeasPump.hpp", "file": "CouenneFeasPump.hpp", "brief": "Feasibility Pump heuristic for nonconvex MINLP\n\nAlternates between NLP and MILP solves to find feasible solutions:\n- MILP phase: Find integer point closest to NLP solution\n- NLP phase: Find NLP-feasible point closest to integer solution\n\n**Algorithm:**\n1. Solve NLP relaxation → get continuous solution nSol\n2. Solve MILP minimizing distance to nSol → get integer iSol\n3. Solve NLP minimizing distance to iSol → get new nSol\n4. If nSol = iSol, found feasible; else repeat\n\n**Distance weights (fadingCoeff):**\n- multDistNLP_/MILP_: Weight on distance from target\n- multHessNLP_/MILP_: Weight on Hessian (smoothing)\n- multObjFNLP_/MILP_: Weight on original objective\n\n**Tabu management (fpTabuMgtPolicy):**\n- FP_TABU_POOL: Avoid previously visited solutions\n- FP_TABU_PERTURB: Random perturbation on cycling\n- FP_TABU_CUT: Add cuts to forbid solution\n\n**SCIP integration:**\nCan use SCIP instead of Cbc for MILP phase (COUENNE_HAS_SCIP).", "see": ["CouenneTNLP for the NLP interface", "CouenneFPpool for solution pool management"], "has_pass2": false}, "src/heuristics/CouenneIterativeRounding.hpp": {"path": "layer-3/Couenne/src/heuristics/CouenneIterativeRounding.hpp", "filename": "CouenneIterativeRounding.hpp", "file": "CouenneIterativeRounding.hpp", "brief": "Iterative rounding heuristic for nonconvex MINLP\n\nAlternates between MILP and NLP solves to find feasible solutions:\n1. Solve MILP relaxation to get integer assignment\n2. Fix integers, solve NLP for continuous variables\n3. Add cuts, iterate\n\n**Two modes:**\n- F-IR (Feasibility): Find first feasible solution using interior-point warmstart\n- I-IR (Improvement): Improve existing solution using local branching\n\n**Key parameters:**\n- maxRoundingIter_: Max MILP-NLP iterations per call\n- maxFirPoints_: Number of starting points in F-IR\n- omega_: Factor for log-barrier parameter (interior point)\n- baseLbRhs_: Local branching neighborhood size\n\n**Aggressiveness levels:**\n- 0: Light (5 iterations, 60s)\n- 1: Medium (10 iterations, 120s)\n- 2: Heavy (20 iterations, 300s)\n\n**CPLEX integration:**\nIf CPLEX available, uses it for MILP; otherwise uses CBC heuristics.", "see": ["CouenneFeasPump for the specialized feasibility pump", "CbcHeuristic base class"], "has_pass2": false}, "src/heuristics/BonInitHeuristic.hpp": {"path": "layer-3/Couenne/src/heuristics/BonInitHeuristic.hpp", "filename": "BonInitHeuristic.hpp", "file": "BonInitHeuristic.hpp", "brief": "Heuristic to inject initial NLP solution into Cbc\n\nCommunicates the initial NLP solution (computed before B&B starts)\nto Cbc as a known feasible solution.\n\n**Purpose:**\nWhen Couenne solves the root node NLP and finds a feasible MINLP\nsolution, this heuristic stores it so Cbc can use it as the\ninitial incumbent.\n\n**Usage:**\n```cpp\n// After initial NLP solve\nInitHeuristic heur(objValue, solution, problem);\ncbcModel.addHeuristic(&heur);\n```\n\n**solution() method:**\nReturns the stored initial solution on first call. Subsequent\ncalls return 0 (no new solution) since the initial solution\nhas already been provided.", "see": ["CouenneBab which adds this heuristic", "NlpSolveHeuristic for NLP solves during B&B"], "has_pass2": false}, "src/heuristics/CouenneFPpool.hpp": {"path": "layer-3/Couenne/src/heuristics/CouenneFPpool.hpp", "filename": "CouenneFPpool.hpp", "file": "CouenneFPpool.hpp", "brief": "Solution pool and tabu list for Feasibility Pump\n\nManages collections of solutions found during FP iterations,\nproviding restart points and avoiding revisiting similar solutions.\n\n**CouenneFPsolution:**\nWrapper for a solution with cached infeasibility metrics:\n- nNLinf_: Count of nonlinear infeasibilities\n- nIinf_: Count of integer infeasibilities\n- maxNLinf_/maxIinf_: Maximum violations\n- objVal_: Objective function value\n\n**Comparison modes (what_to_compare):**\n- SUM_NINF: Compare by sum of infeasibility counts\n- SUM_INF: Compare by sum of infeasibility magnitudes\n- OBJVAL: Compare by objective value\n- ALL_VARS: Compare all variable values (for tabu)\n- INTEGER_VARS: Compare only integer variables (for tabu)\n\n**CouenneFPpool:**\nSet of solutions with custom comparator. Used to:\n- Store good MILP solutions for NLP restarts\n- Implement tabu list to avoid cycling\n- Find closest solution in pool (findClosestAndReplace)", "see": ["CouenneFeasPump which uses this pool", "CouenneIterativeRounding for another heuristic"], "has_pass2": false}, "src/heuristics/BonNlpHeuristic.hpp": {"path": "layer-3/Couenne/src/heuristics/BonNlpHeuristic.hpp", "filename": "BonNlpHeuristic.hpp", "file": "BonNlpHeuristic.hpp", "brief": "NLP heuristic for near-integer B&B nodes\n\nCalls NLP solver when CouenneObjects are nearly satisfied to\nfind MINLP feasible solutions during branch-and-bound.\n\n**Triggering conditions:**\n- All CouenneObjects within maxNlpInf_ tolerance\n- Integer variables are rounded\n- No violated SOS constraints\n\n**Key parameters:**\n- maxNlpInf_ (default 1e-5): Max auxiliary violation to trigger\n- numberSolvePerLevel_: NLP calls allowed per tree level\n\n**Algorithm:**\n1. Check if current LP solution is nearly MINLP-feasible\n2. Round integer variables to nearest integer\n3. Call Ipopt to find local NLP solution\n4. If feasible, return as candidate incumbent\n\n**Comparison to FeasibilityPump:**\n- FeasPump: Alternates MILP/NLP from any point\n- NlpHeuristic: Only triggers when already near-feasible", "see": ["CouenneFeasPump for feasibility pump heuristic", "InitHeuristic for initial solution injection"], "has_pass2": false}, "src/util/CouenneSparseMatrix.hpp": {"path": "layer-3/Couenne/src/util/CouenneSparseMatrix.hpp", "filename": "CouenneSparseMatrix.hpp", "file": "CouenneSparseMatrix.hpp", "brief": "Sparse matrix storage for Feasibility Pump distance\n\nSimple COO (coordinate) format sparse matrix used to store\nthe Hessian of the Lagrangian at optimum for later use in\nmodified distance computations.\n\n**Storage format:**\nCOO (Coordinate list):\n- val_[k]: Value of k-th nonzero\n- row_[k]: Row index of k-th nonzero\n- col_[k]: Column index of k-th nonzero\n- num_: Total number of nonzeros\n\n**Usage in Feasibility Pump:**\nThe Hessian from an NLP solve can be saved and used to modify\nthe distance metric in subsequent MILP solves, biasing the\nsearch toward regions where the NLP objective improves.\n\n**Note:** This is a simple storage class, not a full sparse\nmatrix implementation. For operations, use CoinPackedMatrix.", "see": ["CouenneTNLP::optHessian_ which stores this", "CouenneFeasPump which uses modified distances"], "has_pass2": false}, "src/util/CouenneFunTriplets.hpp": {"path": "layer-3/Couenne/src/util/CouenneFunTriplets.hpp", "filename": "CouenneFunTriplets.hpp", "file": "CouenneFunTriplets.hpp", "brief": "Function triplets: f(x), f'(x), f''(x), and (f')⁻¹\n\nBundles a univariate function with its first two derivatives and\nthe inverse of the first derivative. Used in convexification to\ncompute tangent points and envelope parameters.\n\n**Abstract base: funtriplet**\n- F(x): Main function value\n- Fp(x): First derivative f'(x)\n- Fpp(x): Second derivative f''(x)\n- FpInv(x): Inverse of first derivative (f')⁻¹(x)\n\n**simpletriplet:**\nStores function pointers for each operation. Suitable for\nstandard functions like exp, log, sin, cos.\n\n**powertriplet:**\nSpecialized for xᵏ with analytical derivatives:\n- F(x) = xᵏ\n- Fp(x) = k·xᵏ⁻¹\n- Fpp(x) = k(k-1)·xᵏ⁻²\n- FpInv(y) = (y/k)^(1/(k-1))\n\n**kpowertriplet:**\nExtension for c·xᵏ with scalar multiplier.", "see": ["CouenneExprPow which uses these for convexification"], "has_pass2": false}, "src/util/CouenneRootQ.hpp": {"path": "layer-3/Couenne/src/util/CouenneRootQ.hpp", "filename": "CouenneRootQ.hpp", "file": "CouenneRootQ.hpp", "brief": "Root finding for Q^k(x) polynomials in power convexification\n\nFinds roots of the polynomial Q^k(x) = Σᵢ₌₁^{2k} i·x^{i-1} used in\nconvexification of odd powers. Based on Liberti & Pantelides (2003).\n\n**Mathematical background:**\nFor odd power expressions w = x^k, the convex/concave envelope\nrequires finding specific points where tangent lines from the\nboundary touch the curve. The Q^k polynomial characterizes these.\n\n**Usage:**\n- rootQ(k): Computes root for exponent 2k+1\n- Qroot class: Caches computed roots in static map for efficiency\n\n**Implementation:**\n- Odd k: Computed via Newton's method, cached in Qmap\n- Even k (2,4,6,8,10): Hardcoded analytical values\n  - k=2: -(√2 - 1) ≈ -0.414\n  - k=4: ≈ -0.560\n  - k=6: ≈ -0.641\n  - etc.", "see": ["CouenneExprOddPow which uses these roots", "CouenneExprSignPow for signed power convexification"], "has_pass2": false}, "src/expression/operators/CouenneExprMin.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMin.hpp", "filename": "CouenneExprMin.hpp", "file": "CouenneExprMin.hpp", "brief": "N-ary minimum operator w = min(x1, x2, ..., xn)\n\nRepresents the minimum of multiple expressions. The min function is\ncontinuous but nonsmooth (non-differentiable at kinks where arguments\nare equal).\n\n**Convexification challenge:**\nmin is neither convex nor concave. Standard approach:\n- Introduce auxiliary variables: w = min(x1,...,xn)\n- Add constraints: w <= xi for all i (overestimators)\n- Underestimator requires disjunction: w >= x_k for some k\n\n**Bound propagation:**\n- lb(w) = min(lb(x1), ..., lb(xn))\n- ub(w) = min(ub(x1), ..., ub(xn))\n\n**Implementation note:**\nUses copy/store pattern with 2n arguments: copies for evaluation,\nstores for caching evaluated values.", "see": ["exprMax for the dual operation", "CouenneExprAbs for another nonsmooth operator"], "has_pass2": false}, "src/expression/operators/CouenneExprLog.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprLog.hpp", "filename": "CouenneExprLog.hpp", "file": "CouenneExprLog.hpp", "brief": "Natural logarithm expression w = log(x)\n\nRepresents the natural logarithm function. This is a concave function\non its domain (x > 0), so convexification uses tangent cuts for\noverestimation and secant cuts for underestimation.\n\n**Convexification for w = log(x) on [l,u]:**\n- Overestimator (tangent): w <= log(x0) + (x - x0)/x0 at any x0 in (l,u)\n- Underestimator (secant): w >= log(l) + (log(u) - log(l))/(u - l) * (x - l)\n\n**Implied bounds:**\n- Given w in [wl, wu]: x in [exp(wl), exp(wu)]\n- Given x in [xl, xu] with xl > 0: w in [log(xl), log(xu)]\n\n**Bijective property:**\nLog is bijective with inverse exp(), enabling tighter bound propagation.", "see": ["CouenneExprExp for the inverse function", "CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprSignPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSignPow.hpp", "filename": "CouenneExprSignPow.hpp", "file": "CouenneExprSignPow.hpp", "brief": "Signed power function w = x * |x|^(k-1) = sign(x) * |x|^k\n\nGeneralizes power to handle negative arguments while preserving\nsign, creating a bijective function for any real k.\n\n**Definition:**\nsignpow(x, k) = x * |x|^(k-1) = sign(x) * |x|^k\n- For x >= 0: same as x^k\n- For x < 0: -|x|^k (preserves sign)\n\n**Key property:**\nUnlike standard power x^k which is undefined for x < 0 when k is\nnon-integer, signpow is defined and continuous for all x.\n\n**Convexification:**\n- For k > 1: convex for x > 0, concave for x < 0\n- For 0 < k < 1: concave for x > 0, convex for x < 0\n- Point x = 0 is an inflection point\n\n**Bijectivity:**\nInverse is sign(y) * |y|^(1/k), enabling exact reverse propagation.", "see": ["exprPow for standard power (handles only x >= 0 for non-integer k)", "exprOddPow for integer odd powers"], "has_pass2": false}, "src/expression/operators/CouenneExprMultiLin.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMultiLin.hpp", "filename": "CouenneExprMultiLin.hpp", "file": "CouenneExprMultiLin.hpp", "brief": "Multilinear product w = x1 * x2 * ... * xn\n\nProduct of n expressions. Generalizes bilinear (n=2) and trilinear (n=3)\nto arbitrary number of factors.\n\n**Standardization approaches:**\n1. Recursive bilinear: w = ((x1*x2)*x3)*... using nested aux variables\n2. Trilinear grouping: group into triplets when n >= 3\n3. Direct multilinear relaxation (more complex cuts)\n\n**Convexification complexity:**\n- Bilinear (n=2): 4 McCormick cuts\n- Trilinear (n=3): 8 Meyer-Floudas cuts\n- n-linear: 2^n cuts (exponential in n)\n\nFor large n, recursive bilinear decomposition is preferred despite\nintroducing auxiliary variables.\n\n**Bound propagation:**\nProduct of intervals requires considering all 2^n corner combinations\nto find the true bounds. Simplified by interval arithmetic.\n\n**Implied bounds (impliedBoundMul):**\nFrom w = x1*x2*...*xn and bounds on w, derive bounds on factors.\nMore complex than bilinear case.\n\n**Branching:**\nbalancedMul() selects branching point to balance the relaxation\nimprovement across all factors.", "see": ["exprMul for bilinear terms (base class)", "exprTrilinear for direct trilinear relaxation"], "has_pass2": false}, "src/expression/operators/CouenneExprSum.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSum.hpp", "filename": "CouenneExprSum.hpp", "file": "CouenneExprSum.hpp", "brief": "N-ary sum expression w = sum(f_i(x))\n\nRepresents sum of multiple expressions. Sums are linear in their\narguments if all arguments are linear.\n\n**Implied bounds for w = a0 + sum(a_i * x_i):**\nGiven bounds w in [l,u] and coefficients partitioned by sign:\n- I1: indices where a_i > 0\n- I2: indices where a_i < 0\n\nFor i in I1:\n- x_i >= (l - a0 - sum_{j!=i,j in I1}(a_j*u_j) - sum_{j in I2}(a_j*l_j)) / a_i\n- x_i <= (u - a0 - sum_{j!=i,j in I1}(a_j*l_j) - sum_{j in I2}(a_j*u_j)) / a_i\n\n**Quadratic detection:**\ncreateQuadratic() scans sum terms for products and creates exprQuad\nif enough quadratic terms exist for alpha-convexification.\n\n**Linearity:**\nReturns maximum linearity level among all arguments.", "see": ["CouenneExprGroup for constant + linear + nonlinear terms", "CouenneExprQuad for quadratic expressions"], "has_pass2": false}, "src/expression/operators/CouenneExprOddPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprOddPow.hpp", "filename": "CouenneExprOddPow.hpp", "file": "CouenneExprOddPow.hpp", "brief": "Odd integer power w = x^k where k is odd\n\nHandles x^1, x^3, x^5, etc. These have special structure:\n- Bijective (one-to-one) over all reals\n- Pass through origin with same sign as x\n- No symmetry about y-axis (unlike even powers)\n\n**Convexity structure:**\n- x^3: convex for x > 0, concave for x < 0 (S-shaped)\n- x^5, x^7, ...: similar S-shape, steeper for higher k\n- Inflection point at x = 0\n\n**Bound propagation:**\nSince bijective: lb(w) = lb(x)^k, ub(w) = ub(x)^k (sign preserved)\n\n**Implied bounds (inverse):**\nFrom w = x^k: x = sign(w) * |w|^(1/k)\n- lb(x) = sign(lb(w)) * |lb(w)|^(1/k)\n- ub(x) = sign(ub(w)) * |ub(w)|^(1/k)\n\n**Convexification:**\nFor w = x^k on [a, b]:\n- If both same sign: secant overestimator, tangent underestimators (or vice versa)\n- If spans zero: use separate relaxations for positive and negative parts", "see": ["exprEvenPow for even powers (x^2, x^4, ...)", "exprSignPow for non-integer signed powers"], "has_pass2": false}, "src/expression/operators/CouenneExprPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprPow.hpp", "filename": "CouenneExprPow.hpp", "file": "CouenneExprPow.hpp", "brief": "Power expression w = x^k with convexification\n\nHandles power functions with constant exponents. Special cases\n(odd/even integer powers) have specialized implementations.\n\n**Signed power (issignpower_):**\nsignpower(x,k) = sign(x) * |x|^k - useful for odd fractional powers\nof potentially negative bases.\n\n**Convexification depends on exponent k:**\n- k > 1: Convex on x > 0, tangent cuts below, secant above\n- 0 < k < 1: Concave on x > 0, secant cuts below, tangent above\n- k < 0: Convex on x > 0 (inverse), requires x > 0\n\n**Key functions:**\n- safe_pow(): Handles negative bases with odd/even exponent check\n- addPowEnvelope(): Generate envelope cuts for power functions\n- powNewton(): Find optimal tangent point using Newton's method", "see": ["CouenneExprEvenPow for x^(2k) - always convex, symmetric", "CouenneExprOddPow for x^(2k+1) - S-shaped, inflection at 0", "CouenneExprSignPow for signed power variant"], "has_pass2": false}, "src/expression/operators/CouenneExprCeil.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprCeil.hpp", "filename": "CouenneExprCeil.hpp", "file": "CouenneExprCeil.hpp", "brief": "Ceiling function w = ceil(x)\n\nReturns the smallest integer not less than x. Piecewise constant\nfunction with jump discontinuities at each integer.\n\n**Convexification challenge:**\nceil(x) is neither convex nor concave nor continuous. The function\nis constant on intervals (n-1, n] with value n.\n\n**Bound propagation:**\n- lb(w) = ceil(lb(x))\n- ub(w) = ceil(ub(x))\n\n**Integrality:**\nResult is always integer-valued. Can be used in MINLP constraints\nthat round up quantities.\n\n**Relationship to floor:**\nceil(x) = -floor(-x), so implementations can share code.", "see": ["exprFloor for the complementary floor function"], "has_pass2": false}, "src/expression/operators/CouenneExprMax.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMax.hpp", "filename": "CouenneExprMax.hpp", "file": "CouenneExprMax.hpp", "brief": "N-ary maximum operator w = max(x1, x2, ..., xn)\n\nRepresents the maximum of multiple expressions. The max function is\nconvex (pointwise supremum of linear functions) but nonsmooth.\n\n**Convexification:**\nmax is convex, so underestimators are straightforward:\n- w >= xi for all i (linear underestimators)\n\nOverestimators require disjunction:\n- w <= x_k for some k (one of the arguments achieves the max)\n\n**Bound propagation:**\n- lb(w) = max(lb(x1), ..., lb(xn))\n- ub(w) = max(ub(x1), ..., ub(xn))\n\n**Subgradient at nondifferentiable points:**\nAt kinks where multiple arguments are equal to max, any convex\ncombination of their gradients is a valid subgradient.", "see": ["exprMin for the dual operation", "exprAbs which can be expressed as max(x, -x)"], "has_pass2": false}, "src/expression/operators/CouenneExprIf.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprIf.hpp", "filename": "CouenneExprIf.hpp", "file": "CouenneExprIf.hpp", "brief": "Conditional (if-then-else) expression operator\n\nImplements the AMPL ifnl() operator:\n  if (condition) then expr1 else expr2\n\n**Status:** This is a placeholder/stub class. Full implementation\nwould require handling disjunctive convexification.\n\n**Convexification challenge:**\nConditional expressions create disjunctive feasible regions.\nStandard approach would use big-M formulations or disjunctive\nprogramming techniques to handle the branching logic.\n\n**Usage in AMPL:**\nModels with if-then-else create nonsmooth, potentially\ndiscontinuous functions that are difficult to convexify.", "see": ["CouenneDisjCuts for disjunctive cut generation", "exprOp base class"], "has_pass2": false}, "src/expression/operators/CouenneExprBinProd.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprBinProd.hpp", "filename": "CouenneExprBinProd.hpp", "file": "CouenneExprBinProd.hpp", "brief": "Product of binary variables (specialized multilinear)\n\nRepresents products of binary (0-1) variables: w = x₁ * x₂ * ... * xₙ\nwhere each xᵢ ∈ {0, 1}. Inherits from exprMul but exploits the\nbinary domain for tighter cuts.\n\n**Key simplification:**\nFor binary variables, w = 1 iff all xᵢ = 1, otherwise w = 0.\nThis is equivalent to logical AND.\n\n**Linearization (standard form):**\nFor n binaries, the product can be linearized exactly with:\n- w ≤ xᵢ for all i (w = 0 if any xᵢ = 0)\n- w ≥ Σxᵢ - (n-1) (w = 1 if all xᵢ = 1)\n\n**Bounds:**\n- Lower bound: 0 always (product of non-negative)\n- Upper bound: 1 always (product of binaries ≤ 1)\n\n**isCuttable():**\nReturns false - binary products are fully linearizable,\nno convexification cuts beyond the standard form needed.", "see": ["exprMul which this specializes", "CouenneExprMultiLin for general multilinear products"], "has_pass2": false}, "src/expression/operators/CouenneExprExp.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprExp.hpp", "filename": "CouenneExprExp.hpp", "file": "CouenneExprExp.hpp", "brief": "Exponential expression w = exp(x)\n\nRepresents the exponential function e^x. This is a convex function\nover all of R, so convexification uses secant cuts for overestimation\nand tangent cuts for underestimation.\n\n**Convexification for w = exp(x) on [l,u]:**\n- Underestimator (tangent): w >= exp(x0) + exp(x0)*(x - x0) at any x0 in [l,u]\n- Overestimator (secant): w <= exp(l) + (exp(u) - exp(l))/(u - l) * (x - l)\n\n**Implied bounds:**\n- Given w in [wl, wu] with wl > 0: x in [log(wl), log(wu)]\n- Given x in [xl, xu]: w in [exp(xl), exp(xu)]\n\n**Bijective property:**\nExp is bijective with inverse log(), enabling tighter bound propagation.\n\n**Convexity:** Convex everywhere, simplifying global optimization.", "see": ["CouenneExprLog for the inverse function", "CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprFloor.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprFloor.hpp", "filename": "CouenneExprFloor.hpp", "file": "CouenneExprFloor.hpp", "brief": "Floor function w = floor(x)\n\nReturns the largest integer not exceeding x. Piecewise constant\nfunction with jump discontinuities at each integer.\n\n**Convexification challenge:**\nfloor(x) is neither convex nor concave nor continuous. The function\nis constant on intervals [n, n+1) with value n.\n\n**Bound propagation:**\n- lb(w) = floor(lb(x))\n- ub(w) = floor(ub(x))\n\n**Integrality:**\nResult is always integer-valued, even when argument is continuous.\nThis can introduce implicit integrality constraints.\n\n**Not cuttable:**\nNo convex relaxation exists for piecewise constant functions.\nMust rely on branching to handle.", "see": ["exprCeil for the complementary ceiling function"], "has_pass2": false}, "src/expression/operators/CouenneExprNorm.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprNorm.hpp", "filename": "CouenneExprNorm.hpp", "file": "CouenneExprNorm.hpp", "brief": "p-norm expression: ||f(x)||_p\n\nRepresents the ℓₚ norm: ||f(x)||_p = (Σᵢ |fᵢ(x)|^p)^(1/p)\n\n**Status:** This is a placeholder/stub class.\n\n**Special cases:**\n- p = 1: Sum of absolute values (piecewise linear)\n- p = 2: Euclidean norm (convex, smooth except at origin)\n- p = ∞: Maximum absolute value (convex, nonsmooth)\n\n**Convexity:**\nAll ℓₚ norms with p ≥ 1 are convex functions.\nThis makes overestimation straightforward (tangent planes),\nbut underestimation requires handling the non-negativity\nand norm structure.\n\n**Common usage:**\n- Distance constraints: ||x - a||₂ ≤ r\n- Regularization: minimize f(x) + λ||x||₁", "see": ["exprAbs for absolute value (used in p=1 norm)", "exprOp base class"], "has_pass2": false}, "src/expression/operators/CouenneExprDiv.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprDiv.hpp", "filename": "CouenneExprDiv.hpp", "file": "CouenneExprDiv.hpp", "brief": "Division expression w = x / y\n\nRepresents division of two expressions. Division is reformulated as\nmultiplication: w = x/y becomes w*y = x, using bilinear convexification.\n\n**Standardization:**\nDivision x/y is converted to x * (1/y), where 1/y is handled by exprInv.\nThis allows reuse of McCormick envelope cuts for the product.\n\n**Linearity check:**\nIf denominator is constant, expression inherits linearity of numerator.\nOtherwise, expression is nonlinear.\n\n**Bound safety:**\n- is_boundbox_regular(): Checks if bounds are suitable for convexification\n- SAFE_COEFFICIENT (1e9): Maximum coefficient for OsiRowCut\n- BR_NEXT_ZERO: Safety margin near zero denominator\n\n**Not cuttable:**\nLike products, division is concave on both sides and cannot be\nfurther linearized - only bound tightening helps.", "see": ["CouenneExprMul for product convexification", "CouenneExprInv for 1/x handling"], "has_pass2": false}, "src/expression/operators/CouenneExprSin.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSin.hpp", "filename": "CouenneExprSin.hpp", "file": "CouenneExprSin.hpp", "brief": "Sine expression w = sin(x)\n\nRepresents the sine function. Sine is challenging for global optimization\ndue to its oscillatory nature - alternates between convex and concave\nregions every pi interval.\n\n**Convexification challenges:**\n- Convex on [2k*pi - pi, 2k*pi] for integer k\n- Concave on [2k*pi, 2k*pi + pi]\n- Multiple local optima on unbounded domains\n- Special handling needed when domain spans multiple periods\n\n**Implied bounds:**\n- w in [-1, 1] always\n- Given w bounds, x bounds via arcsin with period handling\n- trigImpliedBound(): Generalized procedure for sin/cos\n\n**Branching:**\ntrigSelBranch(): Selects branching points based on convexity regions\nand current LP solution position relative to sin curve.\n\n**Not cuttable:** Due to alternating convexity, cutting planes\nare less effective than branching.", "see": ["CouenneExprCos for cosine (uses same procedures)", "CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprOpp.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprOpp.hpp", "filename": "CouenneExprOpp.hpp", "file": "CouenneExprOpp.hpp", "brief": "Negation operator w = -x\n\nLinear operator that returns the opposite of an expression.\nPreserves linearity of the argument.\n\n**Convexification:**\n- Affine function: requires no convexification\n- Convex relaxation is exact\n- Linearity() returns same as argument's Linearity()\n\n**Bound propagation:**\n- lb(w) = -ub(x)\n- ub(w) = -lb(x)\nNote: bounds are swapped!\n\n**Implied bounds:**\nFrom w = -x: if w >= a then x <= -a, if w <= b then x >= -b\n\n**Integrality preservation:**\nIf x is integer, -x is also integer.\n\n**Simplification:**\n-(-x) simplifies to x (double negation elimination)", "see": ["exprSub for subtraction x - y = x + (-y)"], "has_pass2": false}, "src/expression/operators/CouenneExprEvenPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprEvenPow.hpp", "filename": "CouenneExprEvenPow.hpp", "file": "CouenneExprEvenPow.hpp", "brief": "Even integer power w = x^k where k is even\n\nHandles x^2, x^4, x^6, etc. These have special structure:\n- Always non-negative: w >= 0\n- Symmetric about y-axis: f(x) = f(-x)\n- NOT bijective: x and -x give same w\n\n**Convexity:**\nEven powers are convex everywhere (sum of squares is convex).\nThis greatly simplifies the convex relaxation.\n\n**Bound propagation:**\n- lb(w) = 0 if [lb(x), ub(x)] contains 0\n- lb(w) = min(lb(x)^k, ub(x)^k) otherwise\n- ub(w) = max(lb(x)^k, ub(x)^k)\n\n**Implied bounds (non-bijective!):**\nFrom w = x^k where k even:\n- |x| <= w^(1/k), so -w^(1/k) <= x <= w^(1/k)\n- Cannot determine sign of x from w alone\n\n**Convexification:**\nSince convex: tangent line underestimators everywhere.\nSecant line is the overestimator.\n\n**Special case x^2:**\nMost common even power, often handled separately for efficiency.", "see": ["exprOddPow for odd powers (bijective, S-shaped)", "exprQuad for sum of squares"], "has_pass2": false}, "src/expression/operators/CouenneExprAbs.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprAbs.hpp", "filename": "CouenneExprAbs.hpp", "file": "CouenneExprAbs.hpp", "brief": "Absolute value expression w = |x|\n\nRepresents the absolute value function. This is a convex function\nwith a non-differentiable point at x = 0.\n\n**Convexification for w = |x| on [l,u]:**\n- If l >= 0: w = x (identity)\n- If u <= 0: w = -x (negation)\n- If l < 0 < u: convex envelope is\n  - w >= x (right branch)\n  - w >= -x (left branch)\n  - w <= ((u+l)*x + u*(-l) - l*u) / (u - l) (secant from (l,-l) to (u,u))\n\n**Implied bounds:**\n- w >= 0 always\n- Given w in [0, wu]: x in [-wu, wu] ∩ [l, u]\n- Given x in [l, u]: w in [min(|l|,|u|)?, max(|l|,|u|)]\n\n**Integer preservation:**\nIf argument is integer, |x| is also integer.", "see": ["CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprMul.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMul.hpp", "filename": "CouenneExprMul.hpp", "file": "CouenneExprMul.hpp", "brief": "N-ary multiplication expression with McCormick convexification\n\nRepresents products of the form w = x1 * x2 * ... * xn.\nDuring standardization, n-ary products are decomposed into\nbinary products: w1 = x1*x2, w2 = w1*x3, etc.\n\n**McCormick envelope for bilinear w = x*y:**\nGiven bounds [xl,xu] and [yl,yu], adds 4 linear constraints:\n- w >= xl*y + x*yl - xl*yl (underestimator 1)\n- w >= xu*y + x*yu - xu*yu (underestimator 2)\n- w <= xl*y + x*yu - xl*yu (overestimator 1)\n- w <= xu*y + x*yl - xu*yl (overestimator 2)\n\n**Key functions:**\n- unifiedProdCuts(): Generate McCormick cuts for products/divisions\n- upperEnvHull(): Improved cuts using convex hull computation\n- computeMulBrDist(): Distance to convexification for branching", "see": ["CouenneExprDiv for division (uses same convexification)", "CouenneExprBinProd for binary product specialization"], "has_pass2": false}, "src/expression/operators/CouenneExprCos.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprCos.hpp", "filename": "CouenneExprCos.hpp", "file": "CouenneExprCos.hpp", "brief": "Cosine function w = cos(x)\n\nBounded periodic function in [-1, 1]. Convexification follows the\nsame approach as exprSin with a phase shift.\n\n**Convexification (depends on bound interval):**\n- If interval < π: tangent line is overestimator on convex part,\n  underestimator on concave part\n- Secant line connects interval endpoints\n- Newton iteration (trigNewton) finds optimal tangent points\n\n**Implied bounds:**\nSince cos is bounded, w ∈ [-1, 1] always. Uses trigImpliedBound()\nfor reverse propagation when w bounds are known.\n\n**Derivative:**\nd(cos(x))/dx = -sin(x), used for gradient computations.\n\n**Period handling:**\nIf bound interval spans multiple periods, relaxation becomes\ntrivial: -1 <= w <= 1.", "see": ["exprSin for detailed convexification algorithm", "CouenneExprBCos for bound computation"], "has_pass2": false}, "src/expression/operators/CouenneExprInv.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprInv.hpp", "filename": "CouenneExprInv.hpp", "file": "CouenneExprInv.hpp", "brief": "Reciprocal function w = 1/x\n\nHyperbolic function that is convex on (0, ∞) and (-∞, 0) separately.\nSingular at x = 0, so domain handling is critical.\n\n**Convexification by domain:**\n- x > 0: 1/x is convex, use secant overestimator and tangent underestimators\n- x < 0: 1/x is convex, same approach\n- If x can cross zero: must handle carefully, possible branching\n\n**Tangent line at x = a:**\nw = 1/a - (x - a)/a² = 2/a - x/a²\n\n**Secant between (a, 1/a) and (b, 1/b):**\nw = 1/a + (1/b - 1/a)/(b - a) * (x - a) = 1/a - (x - a)/(ab)\n\n**Properties:**\n- Bijective: inverse of 1/x is 1/x (self-inverse)\n- Used in division: x/y reformulated as x * (1/y)\n- Derivatives: d(1/x)/dx = -1/x², d²(1/x)/dx² = 2/x³", "see": ["exprDiv which uses exprInv internally", "exprMul for the multiplication component"], "has_pass2": false}, "src/expression/operators/CouenneExprQuad.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprQuad.hpp", "filename": "CouenneExprQuad.hpp", "file": "CouenneExprQuad.hpp", "brief": "Quadratic expression with alpha-convexification\n\nRepresents expressions of the form:\n  w = a0 + a'x + x'Qx + sum(nonlinear_terms)\n\nUses alpha-convexification (Adjiman-Floudas, LaGO/Nowak-Vigerske)\nto create convex/concave envelopes based on eigenvalue analysis.\n\n**Alpha-convexification algorithm:**\n1. Scale Q by variable bounds: A = Diag(u-l) * Q * Diag(u-l)\n2. Compute min/max eigenvalues lambda_min, lambda_max\n3. Under-estimator: x'Qx + sum(lambda_min_i * (x_i - l_i)(u_i - x_i))\n4. Over-estimator: x'Qx + sum(lambda_max_i * (x_i - l_i)(u_i - x_i))\n\n**Data structures:**\n- matrix_: Sparse Q matrix as vector of (variable, column) pairs\n- eigen_: Cached eigenvalues/eigenvectors for convexification\n- bounds_: Cached bounds to detect when recomputation needed\n\n**Key methods:**\n- alphaConvexify(): Compute eigenvalues for convexification\n- quadCuts(): Generate OA cuts from alpha-convexification", "see": ["CouenneExprGroup for linear + nonlinear base class", "CouenneExprMul for bilinear terms (McCormick)"], "has_pass2": false}, "src/expression/operators/CouenneExprTrilinear.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprTrilinear.hpp", "filename": "CouenneExprTrilinear.hpp", "file": "CouenneExprTrilinear.hpp", "brief": "Trilinear product expression w = x*y*z\n\nSpecialized handling for products of exactly three terms.\nTrilinear terms are common in pooling problems and arise\nfrom reformulation of bilinear products with parameters.\n\n**Standardization options:**\nA trilinear term x*y*z can be reformulated as:\n1. w1 = x*y, w = w1*z (two bilinear terms)\n2. Direct trilinear relaxation\n\n**Convexification:**\nUses direct trilinear relaxation when available, which can\nprovide tighter bounds than cascaded bilinear products.\nGenerates 8 linear constraints forming a polyhedral relaxation.\n\n**Implied bounds:**\nGiven w in [wl,wu] and bounds on x,y,z, tightens bounds\nby considering all 8 corners of the 3D box.", "see": ["CouenneExprMul for general n-ary products", "CouenneExprBinProd for binary products"], "has_pass2": false}, "src/expression/operators/CouenneExprSub.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSub.hpp", "filename": "CouenneExprSub.hpp", "file": "CouenneExprSub.hpp", "brief": "Binary subtraction operator w = x - y\n\nLinear operator for computing the difference of two expressions.\nLinearity is the maximum of the two arguments' linearity.\n\n**Convexification:**\n- Affine in both arguments: no convexification needed\n- Linearity() = max(Linearity(x), Linearity(y))\n\n**Bound propagation:**\n- lb(w) = lb(x) - ub(y)\n- ub(w) = ub(x) - lb(y)\n\n**Implied bounds:**\nFrom w = x - y:\n- x >= lb(w) + lb(y)\n- x <= ub(w) + ub(y)\n- y >= lb(x) - ub(w)\n- y <= ub(x) - lb(w)\n\n**Standardization:**\nMay be rewritten as x + (-1)*y using exprSum and exprOpp\nduring problem transformation.", "see": ["exprSum for addition", "exprOpp for negation"], "has_pass2": false}, "src/expression/operators/CouenneExprPWLinear.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprPWLinear.hpp", "filename": "CouenneExprPWLinear.hpp", "file": "CouenneExprPWLinear.hpp", "brief": "Piecewise linear function expression\n\nRepresents piecewise linear (PWL) functions defined by breakpoints\nand slopes or by point sequences.\n\n**Status:** This is a placeholder/stub class.\n\n**Mathematical form:**\nf(x) = aᵢ * x + bᵢ  for  xᵢ ≤ x < xᵢ₊₁\n\n**Convexification:**\n- If convex (slopes increasing): trivial envelope\n- If concave (slopes decreasing): trivial envelope\n- If neither: requires disjunctive handling\n\n**Standard MIP formulation:**\nUses binary variables to select active segment:\n- λᵢ ∈ [0,1], Σλᵢ = 1, at most 2 adjacent λᵢ positive (SOS2)\n- x = Σλᵢxᵢ, y = Σλᵢyᵢ\n\n**Usage:**\nCommon in approximating nonlinear functions or modeling\ncosts with quantity discounts.", "see": ["CouenneExprAbs for a simple PWL function (V-shape)", "exprOp base class"], "has_pass2": false}, "src/expression/operators/CouenneExprGroup.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprGroup.hpp", "filename": "CouenneExprGroup.hpp", "file": "CouenneExprGroup.hpp", "brief": "Mixed expression: constant + linear + nonlinear terms\n\nRepresents expressions of the form:\n  w = c0 + sum(a_i * x_i) + sum(f_j(x))\nwhere c0 is constant, a_i*x_i are linear terms, and f_j are nonlinear.\n\n**Data structures:**\n- c0_: Constant term\n- lcoeff_: Vector of (variable, coefficient) pairs for linear part\n- Inherits arglist_ from exprSum for nonlinear terms\n\n**Factory pattern:**\ngenExprGroup() checks parameters and returns:\n- exprConst if only constant\n- exprVar if single variable with coefficient 1\n- exprGroup otherwise\n\n**Evaluation:**\noperator() = c0 + sum(coefficient * variable_value) + sum(nonlinear)\n\n**Base for exprQuad:**\nexprQuad extends this to add x'Qx quadratic terms.", "see": ["CouenneExprSum for pure sum of expressions", "CouenneExprQuad for quadratic extension"], "has_pass2": false}, "src/expression/partial/CouenneExprJac.hpp": {"path": "layer-3/Couenne/src/expression/partial/CouenneExprJac.hpp", "filename": "CouenneExprJac.hpp", "file": "CouenneExprJac.hpp", "brief": "Symbolic Jacobian of constraints via expression DAG\n\nStores the constraint Jacobian as a sparse matrix of expression\npointers. Each nonzero ∂g_i/∂x_j is an expression node that can\nbe evaluated at any point.\n\n**Sparse structure:**\n- nnz_: Number of structural nonzeros\n- iRow_[], jCol_[]: COO format indices\n- expr_[k]: Expression for the k-th nonzero entry\n\n**Construction:**\nBuilt from CouenneProblem by differentiating each constraint\nbody expression with respect to each variable it depends on.\n\n**Usage:**\nCalled by Ipopt's eval_jac_g() callback to provide the\nconstraint Jacobian. Expression-based representation allows\nexact derivatives without finite differences.", "see": ["ExprHess for the Lagrangian Hessian", "CouenneTNLP which uses ExprJac for NLP solves"], "has_pass2": false}, "src/expression/partial/CouenneExprHess.hpp": {"path": "layer-3/Couenne/src/expression/partial/CouenneExprHess.hpp", "filename": "CouenneExprHess.hpp", "file": "CouenneExprHess.hpp", "brief": "Symbolic Hessian of Lagrangian via expression DAG\n\nStores the Hessian of the Lagrangian:\n∇²L(x,λ) = ∇²f(x) + Σᵢ λᵢ ∇²gᵢ(x)\n\n**Sparse structure:**\n- nnz_: Number of structural nonzeros (lower triangle)\n- iRow_[], jCol_[]: COO format indices\n- numL_[k]: How many λᵢ contribute to position k\n- lamI_[k][]: Indices of contributing λᵢ\n- expr_[k][j]: Expression for ∂²gⱼ/∂x_iRow[k]∂x_jCol[k]\n\n**Evaluation:**\nFor each nonzero position (i,j), sum over all constraints:\nH[i,j] = expr_obj[i,j] + Σₖ λ[lamI[k]] * expr[k][...]\n\n**Feasibility pump extension:**\nCan be extended with gg' term for feasibility pump objectives\nby augmenting the objective with ||x - x̄||² + gradient terms.", "see": ["ExprJac for the constraint Jacobian", "CouenneTNLP which uses ExprHess for NLP solves"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBCos.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBCos.hpp", "filename": "CouenneExprBCos.hpp", "file": "CouenneExprBCos.hpp", "brief": "Bound operators for cosine expressions\n\nComputes tight bounds for cos(x) given bounds [l, u] on x.\nUsed by FBBT (feasibility-based bound tightening) to propagate\nbounds through trigonometric expressions.\n\n**exprLBCos (lower bound):**\n- Returns -1 if interval spans full period (u - l >= 2π)\n- Returns -1 if interval contains π + 2kπ (minimum of cos)\n- Otherwise returns min(cos(l), cos(u))\n\n**exprUBCos (upper bound):**\n- Returns +1 if interval spans full period\n- Returns +1 if interval contains 2kπ (maximum of cos)\n- Otherwise returns max(cos(l), cos(u))\n\n**Period handling:**\nUses floor(l/2π - offset) < floor(u/2π - offset) to detect\nif critical points lie within [l, u].", "see": ["CouenneExprCos for the cosine expression itself", "CouenneExprBSin for sine bound computation"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBDiv.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBDiv.hpp", "filename": "CouenneExprBDiv.hpp", "file": "CouenneExprBDiv.hpp", "brief": "Bound operators for division expressions\n\nComputes tight bounds for n/d given bounds [n, N] on numerator\nand [d, D] on denominator. Handles singularity at d = 0.\n\n**safeDiv():**\nUtility for safe division avoiding NaN:\n- Returns 0 if numerator is ~0\n- Returns ±∞ if denominator is ~0 (sign-aware)\n- Otherwise returns a/b\n\n**exprLBDiv (lower bound):**\nCase analysis on signs:\n- d > 0: positive denominator → lb = n/D or n/d\n- d ≤ 0, D > 0: crosses zero → -∞ (unbounded)\n- d ≤ 0, D ≤ 0: negative denominator → lb = N/D or N/d\n\n**exprUBDiv (upper bound):**\nSymmetric case analysis:\n- d > 0: ub = N/D or N/d\n- d ≤ 0, D > 0: crosses zero → +∞\n- d ≤ 0, D ≤ 0: ub = n/D or n/d", "see": ["CouenneExprDiv for the division expression", "CouenneExprBMul for multiplication bounds"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBQuad.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBQuad.hpp", "filename": "CouenneExprBQuad.hpp", "file": "CouenneExprBQuad.hpp", "brief": "Bound expressions for quadratic forms\n\nExpression classes that compute bounds on quadratic forms\n∑ qᵢⱼ·xᵢ·xⱼ + ∑ bᵢ·xᵢ + c given variable bounds.\n\n**exprLBQuad:**\nReturns lower bound on the quadratic form by calling\ncomputeQBound(-1) on the referenced exprQuad.\n\n**exprUBQuad:**\nReturns upper bound on the quadratic form by calling\ncomputeQBound(+1) on the referenced exprQuad.\n\n**Bound computation (in exprQuad):**\nUses eigenvalue-based approach when Q is indefinite:\n- Decompose Q = ∑ λᵢ·vᵢ·vᵢᵀ\n- Bound each eigenterm using variable bounds\n- Sum contributions accounting for sign of λᵢ\n\n**Usage in FBBT:**\nThese expressions enable bound tightening on quadratic\nconstraints without explicitly enumerating all bilinear terms.", "see": ["CouenneExprQuad which implements computeQBound()", "CouenneExprBMul for bilinear bound operators"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBMul.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBMul.hpp", "filename": "CouenneExprBMul.hpp", "file": "CouenneExprBMul.hpp", "brief": "Bound operators for bilinear product expressions\n\nComputes tight bounds for x * y given bounds [n, N] on x\nand [d, D] on y. Used by FBBT for bound propagation.\n\n**safeProd():**\nUtility for safe multiplication avoiding overflow:\n- Handles ±∞ * 0 → 0\n- Handles ±∞ * finite → ±∞ (sign-aware)\n- Otherwise returns a * b\n\n**exprLBMul (lower bound):**\nFor bounds [n, N] × [d, D]:\n- d ≥ 0, n ≥ 0: lb = n * d\n- d ≥ 0, n < 0: lb = n * D\n- d < 0, N > 0: lb = min(N * d, n * D)\n- d < 0, N ≤ 0, D > 0: lb = n * D\n- d < 0, N ≤ 0, D ≤ 0: lb = N * D\n\n**exprUBMul (upper bound):**\nSymmetric case analysis yielding max of corner products.\n\n**Note:** The four corners {n, N} × {d, D} contain the\nextrema for bilinear functions over boxes.", "see": ["CouenneExprMul for the multiplication expression", "CouenneExprBDiv for division bounds"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBSin.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBSin.hpp", "filename": "CouenneExprBSin.hpp", "file": "CouenneExprBSin.hpp", "brief": "Bound operators for sine expressions\n\nComputes tight bounds for sin(x) given bounds [l, u] on x.\nUsed by FBBT (feasibility-based bound tightening) to propagate\nbounds through trigonometric expressions.\n\n**exprLBSin (lower bound):**\n- Returns -1 if interval spans full period (u - l >= 2π)\n- Returns -1 if interval contains 3π/2 + 2kπ (minimum of sin)\n- Otherwise returns min(sin(l), sin(u))\n\n**exprUBSin (upper bound):**\n- Returns +1 if interval spans full period\n- Returns +1 if interval contains π/2 + 2kπ (maximum of sin)\n- Otherwise returns max(sin(l), sin(u))\n\n**Period handling:**\nUses floor(l/2π - offset) < floor(u/2π - offset) to detect\nif critical points lie within [l, u]:\n- offset = 0.75 for min (3π/2)\n- offset = 0.25 for max (π/2)", "see": ["CouenneExprSin for the sine expression itself", "CouenneExprBCos for cosine bound computation"], "has_pass2": false}, "src/bound_tightening/twoImpliedBT/CouenneTwoImplied.hpp": {"path": "layer-3/Couenne/src/bound_tightening/twoImpliedBT/CouenneTwoImplied.hpp", "filename": "CouenneTwoImplied.hpp", "file": "CouenneTwoImplied.hpp", "brief": "Bound tightening from pairs of linear constraints\n\nDerives implied bounds by combining two linear inequalities via\nconvex combinations. Standard FBBT uses single constraints; this\nextends to pairs for tighter bounds.\n\n**Algorithm:**\nGiven constraints h and k, form convex combination with α ∈ [0,1]:\n  ℓ' ≤ Σᵢ bᵢ(α) xᵢ ≤ u'\n\nFor each variable xᵢ, the implied bounds are piecewise rational\nfunctions of α. Find optimal α to maximize lower bounds / minimize\nupper bounds.\n\n**Key insight:**\nDivide [0,1] into intervals at breakpoints cᵢ where coefficients\nchange sign. On each interval, functions are differentiable and\noptima can be found analytically.\n\n**Example:**\nFor x + y ≥ 2 and x - y ≥ 1 with x ∈ [0,4], y ∈ [0,1]:\n- Single-constraint FBBT: x ≥ 1\n- Two-implied (sum): x ≥ 1.5", "see": ["CouenneFixPoint for standard single-constraint FBBT", "CouenneAggrProbing for OBBT"], "has_pass2": false}, "src/problem/depGraph/CouenneDepGraph.hpp": {"path": "layer-3/Couenne/src/problem/depGraph/CouenneDepGraph.hpp", "filename": "CouenneDepGraph.hpp", "file": "CouenneDepGraph.hpp", "brief": "Dependency graph between auxiliary and original variables\n\nTracks which variables depend on which others, enabling:\n- Correct evaluation order (topological sort)\n- Cycle detection (would indicate model error)\n- Dependency analysis for bound propagation\n\n**DepNode:**\nA vertex in the dependency graph:\n- index_: Variable index\n- depList_: Forward star (variables this one depends on)\n- order_: Evaluation order from topological sort\n- color_: For DFS cycle detection (white/gray/black)\n\n**DepGraph:**\nThe full dependency DAG:\n- vertices_: Set of DepNodes\n- createOrder(): Topological sort for evaluation order\n- checkCycles(): Detect circular dependencies\n- depends(w, x): Check if w depends on x\n\n**Usage:**\nWhen w = f(x, y), node for w has edges to nodes for x and y.\nEvaluation must process x, y before w. Bound propagation\nfollows the reverse order.\n\n**replaceIndex():**\nUsed when redundant auxiliary w := x is found and w is\nreplaced by x throughout.", "see": ["exprAux which uses this for evaluation ordering", "CouenneProblem which owns the dependency graph"], "has_pass2": false}, "src/cut/crossconv/CouenneCrossConv.hpp": {"path": "layer-3/Couenne/src/cut/crossconv/CouenneCrossConv.hpp", "filename": "CouenneCrossConv.hpp", "file": "CouenneCrossConv.hpp", "brief": "Cuts from redundant relationships between auxiliary variables\n\nIdentifies and exploits algebraic relationships between auxiliary\nvariables that arise from the reformulation. These give valid\nequalities or inequalities that strengthen the relaxation.\n\n**Example relationships (commented out but informative):**\n\n1. **SumLogAuxRel**: x3 = log(x1), x4 = log(x2), x5 = x1*x2\n   Implies: x3 + x4 = log(x5), giving a valid cut.\n\n2. **MultiProdRel**: If x_k = x_i*x_j, x_l = x_i*x_p, x_q = x_k*x_p, x_r = x_l*x_j\n   Then x_q = x_r (both equal x_i*x_j*x_p).\n\n3. **BiProdDivRel**: Division chains that should be consistent.\n\n4. **PowRel**: If x_j = x_i^α and x_p = x_i^β, then x_p = x_j^(β/α).\n\n**How it works:**\n1. Analyze auxiliary variable definitions\n2. Detect patterns that imply redundant relationships\n3. Generate cuts enforcing these relationships\n\nThese cuts are \"free\" in the sense that they come from algebraic\nidentities, not from convexification of specific expressions.", "see": ["CouenneCutGenerator for standard convexification cuts", "CouenneDisjCuts for disjunctive cuts"], "has_pass2": false}, "src/cut/sdpcuts/CouennePSDcon.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/CouennePSDcon.hpp", "filename": "CouennePSDcon.hpp", "file": "CouennePSDcon.hpp", "brief": "Positive semidefinite constraint X ⪰ 0\n\nRepresents a constraint that a matrix of expressions must be\npositive semidefinite (all eigenvalues non-negative).\n\n**Mathematical meaning:**\nX ⪰ 0 is equivalent to:\n- All eigenvalues of X are non-negative\n- v'Xv >= 0 for all vectors v\n- X can be written as X = A'A for some matrix A\n\n**Common usage:**\n- Product matrices: X_ij = x_i * x_j must be PSD\n- Covariance matrices in statistics\n- Second-order cone constraints (via PSD embedding)\n\n**Handling in Couenne:**\nPSD constraints are not directly enforceable in MILP/NLP.\nInstead, they are enforced via:\n1. SDP cuts (see CouenneSdpCuts)\n2. Eigenvalue constraints (expensive)\n3. Principal minor constraints", "see": ["CouenneSdpCuts for cut generation from PSD constraints", "CouenneExprMatrix for matrix representation"], "has_pass2": false}, "src/cut/sdpcuts/CouenneSdpCuts.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/CouenneSdpCuts.hpp", "filename": "CouenneSdpCuts.hpp", "file": "CouenneSdpCuts.hpp", "brief": "SDP-based cutting planes using matrix positive semidefiniteness\n\nGenerates cuts exploiting that product matrices X = (x_ij) where x_ij = x_i*x_j\nmust be positive semidefinite. These cuts strengthen the LP relaxation beyond\nwhat McCormick envelopes provide.\n\n**Theory:**\nFor X ⪰ 0 (positive semidefinite), we have a'Xa >= 0 for any vector a.\nIf current LP solution X* violates this (a'X*a < 0), then a'Xa >= 0 is a\nvalid cut. The most violated cut corresponds to eigenvector of minimum\nnegative eigenvalue of X*.\n\n**Algorithm (Qualizza-Belotti-Margot):**\n1. Build matrix X* from current LP solution (auxiliary products)\n2. Compute eigenvalue decomposition of X*\n3. For negative eigenvalues, generate cuts a'Xa >= 0\n4. Sparsify cuts for numerical stability\n\n**Parameters:**\n- numEigVec_: Number of eigenvectors to use (default: all)\n- onlyNegEV_: Only use negative eigenvalues (default: yes)\n- useSparsity_: Sparsify eigenvalues (default: no)\n- fillMissingTerms_: Add fictitious aux vars for denser minors\n\n**References:**\n- Sherali-Fraticelli: Original RLT-based SDP strengthening\n- Qualizza, Belotti, Margot: Efficient implementation used here", "see": ["CouennePSDcon for explicit PSD constraints", "CouenneExprMatrix for sparse matrix representation"], "has_pass2": false}, "src/cut/sdpcuts/dsyevx_wrapper.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/dsyevx_wrapper.hpp", "filename": "dsyevx_wrapper.hpp", "file": "dsyevx_wrapper.hpp", "brief": "LAPACK dsyevx wrapper for symmetric eigenvalue computation\n\nC++ interface to LAPACK's dsyevx routine for computing selected\neigenvalues and eigenvectors of a real symmetric matrix.\n\n**dsyevx_interface parameters:**\n- n: Matrix dimension\n- A: Input symmetric matrix (upper or lower triangle)\n- m: [out] Number of eigenvalues found\n- w: [out] Eigenvalues in ascending order\n- z: [out] Eigenvectors (column-wise)\n- tolerance: Relative accuracy for eigenvalues\n- lb_ev, ub_ev: Eigenvalue range to compute (RANGE='V')\n- firstidx, lastidx: Index range to compute (RANGE='I')\n\n**Usage in Couenne:**\nUsed by CouenneSdpCuts to compute negative eigenvalues of the\nlifted matrix X - xxᵀ, where violations indicate cuts.\n\n**Algorithm:**\nDSYEVX uses bisection followed by inverse iteration for\nselected eigenvalues, more efficient than full diagonalization.", "see": ["CouenneSdpCuts which uses this for semidefinite cuts"], "has_pass2": false}, "src/cut/sdpcuts/CouenneMatrix.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/CouenneMatrix.hpp", "filename": "CouenneMatrix.hpp", "file": "CouenneMatrix.hpp", "brief": "Sparse matrix and vector of expressions\n\nData structures for representing matrices of Couenne expressions,\nused primarily for SDP cuts and PSD constraints.\n\n**Classes:**\n- CouenneScalar: Single indexed expression element\n- CouenneSparseVector: Sparse vector of expressions (ordered set)\n- CouenneExprMatrix: Sparse matrix with row and column major storage\n\n**Storage:**\nUses std::set with custom comparators for O(log n) lookup.\nBoth row-major and column-major views maintained for efficient\nmatrix-vector and matrix-matrix operations.\n\n**Usage in SDP cuts:**\nX = (x_ij) where x_ij is the auxiliary for product x_i * x_j.\nvarIndices_ stores the original variables x_i used in products.\n\n**Operations:**\n- Vector dot product: v1 * v2\n- Matrix-vector product: M * v\n- Matrix-matrix product: M1 * M2\n- Threshold multiplication (early termination)", "see": ["CouenneSdpCuts which uses these for cut generation", "CouennePSDcon for PSD constraint using this matrix"], "has_pass2": false}, "src/cut/ellipcuts/CouenneEllipCuts.hpp": {"path": "layer-3/Couenne/src/cut/ellipcuts/CouenneEllipCuts.hpp", "filename": "CouenneEllipCuts.hpp", "file": "CouenneEllipCuts.hpp", "brief": "Ellipsoidal cuts for nonconvex quadratic constraints\n\nPlaceholder for ellipsoidal cutting planes derived from\nthe geometry of quadratic constraints.\n\n**Concept (not yet implemented):**\nFor a quadratic constraint xᵀQx + bᵀx ≤ c, the feasible\nregion forms an ellipsoid (if Q ≻ 0). Ellipsoidal cuts\nexploit this structure for tighter relaxations.\n\n@note Currently empty - functionality may be in development\n      or handled elsewhere (e.g., CouenneSdpCuts).", "see": ["CouenneSdpCuts for SDP-based cuts on quadratics", "CouenneExprQuad for quadratic expression handling"], "has_pass2": false}, "src/heuristics/cons_rowcuts.h": {"path": "layer-3/Couenne/src/heuristics/cons_rowcuts.h", "filename": "cons_rowcuts.h", "file": "cons_rowcuts.h", "brief": "constraint handler for rowcuts constraints\n        enables separation of convexification cuts during SCIP solution procedure", "author": "Timo Berthold\n@license This file is licensed under the Eclipse Public License (EPL)", "has_pass2": false}}}, "Dip": {"name": "Dip", "file_count": 25, "pass2_count": 1, "files": {"Dip/src/DecompApp.h": {"path": "layer-3/Dip/Dip/src/DecompApp.h", "filename": "DecompApp.h", "file": "DecompApp.h", "brief": "User application interface - derive to define your decomposition\n\nDecompApp is the main user-facing class. Derive from it to define:\n- Model decomposition (core vs relaxed constraints)\n- Subproblem solvers\n- Problem-specific heuristics\n\n**Model Decomposition:**\n- m_modelCore: Core constraints A'' (linking/complicating constraints)\n- m_modelRelax: Map of relaxed models by block index\n- m_modelRelaxNest: Nested relaxations for multi-level decomposition\n\n**Key Virtual Methods:**\n- createModels(): Define core/relax structure (REQUIRED)\n- solveRelaxed(): Custom subproblem solver (optional)\n- generateInitVars(): Initial columns for Phase 1\n- APPisUserFeasible(): Application-specific feasibility check\n- APPheuristics(): Primal heuristics for incumbent solutions\n\n**Automatic Blocking:**\nCan use hMETIS (COIN_HAS_METIS) for automatic constraint partitioning\ninto blocks for parallel column generation.\n\n**Typical Usage:**\n```cpp\nclass MyApp : public DecompApp {\n   void createModels() {\n      // Define m_modelCore with linking constraints\n      // Define m_modelRelax[b] for each block b\n   }\n};\n```", "see": ["DecompModel.h for model container classes", "DecompAlgo.h for algorithm that uses this app", "DecompParam.h for configuration parameters"], "return": "True, if x is feasible; otherwise, false.", "has_pass2": false}, "Dip/src/DecompCutPool.h": {"path": "layer-3/Dip/Dip/src/DecompCutPool.h", "filename": "DecompCutPool.h", "file": "DecompCutPool.h", "brief": "Pool of generated cuts (DecompCut) for cut management\n\nDecompCutPool manages cuts waiting to enter the master problem.\nInherits from std::vector<DecompWaitingRow> for storage.\n\n**Key Functions:**\n- reExpand(): Regenerate row coefficients when columns change\n- createRowReform(): Transform x-space cut to lambda-space\n- setViolations(): Calculate cut violations vs current solution\n\n**Cut Selection:**\n- is_greater_thanD comparator sorts by violation\n- Most violated cuts enter master first\n- Duplicate cuts filtered via hash\n\n**Reformulation:**\nCuts in x-space must be reformulated to lambda-space for\nDantzig-Wolfe master: a'x >= b becomes a'(sum_s s*lambda_s) >= b\ncreateRowReform() handles this transformation.\n\n**Validity Flag:**\n- m_rowsAreValid: Track if cuts need re-expansion\n- Set false when column set changes\n- reExpand() regenerates coefficients", "see": ["DecompCut.h for cut representation", "DecompWaitingRow.h for pool entry wrapper", "DecompAlgoCGL.h for CGL cut generators"], "has_pass2": false}, "Dip/src/DecompAlgoCGL.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoCGL.h", "filename": "DecompAlgoCGL.h", "file": "DecompAlgoCGL.h", "brief": "Interface to CGL (COIN-OR Cut Generation Library)\n\nDecompAlgoCGL wraps CGL cut generators for use in DIP algorithms.\nProvides automatic generation of standard MIP cuts.\n\n**Available CGL Generators:**\n- CglClique: Clique cuts from conflict graph\n- CglOddHole: Odd hole inequalities\n- CglFlowCover: Flow cover cuts\n- CglKnapsackCover: Knapsack cover cuts\n- CglMixedIntegerRounding2: MIR cuts\n- CglGomory: Gomory mixed-integer cuts\n\n**Usage:**\n- Created by DecompAlgo during initialization\n- Called during PHASE_CUT to generate cuts\n- Returns cuts via DecompCutOsi wrapper\n\n**Configuration:**\n- Generators enabled/disabled via DecompParam\n- Per-generator parameters can be set\n- Different strategies for PC vs C algorithms\n\n**Integration:**\nCGL generates cuts in x-space. For Price-and-Cut, these are\nreformulated to lambda-space before adding to master.", "see": ["DecompCutPool.h for cut storage", "DecompParam.h for CGL configuration", "CglCutGenerator (CGL) for base generator interface"], "has_pass2": false}, "Dip/src/DecompCut.h": {"path": "layer-3/Dip/Dip/src/DecompCut.h", "filename": "DecompCut.h", "file": "DecompCut.h", "brief": "Cut representation in original x-space\n\nDecompCut represents a cutting plane (valid inequality) that can be\nadded to strengthen the formulation. Cuts are defined in original\nx-space and expanded to the master problem.\n\n**Key Data Members:**\n- m_lb, m_ub: Row bounds (lb <= ax <= ub)\n- m_violation: How much current solution violates this cut\n- m_effCnt: Effectiveness counter for cut management\n- m_strHash: Hash string for duplicate detection\n\n**User-Defined Cuts:**\nUsers can derive from DecompCut and override expandCutToRow()\nto provide compact storage. Example: TSP subtour elimination\nstores only the customer set S, then expandCutToRow() generates\nthe full row sum_{i,j in S} x_ij >= |S| - 1.\n\n**Cut Management:**\n- Cuts with low effectiveness (m_effCnt) may be removed\n- Duplicate cuts detected via hash string\n- DecompCutPool manages active cuts\n\n**CGL Integration:**\nStandard cuts can come from CGL generators. User cuts\nsupplement or replace CGL cuts via DecompApp::generateCuts().", "see": ["DecompCutPool.h for cut pool management", "DecompAlgoCGL.h for CGL integration"], "has_pass2": false}, "Dip/src/DecompConstraintSet.h": {"path": "layer-3/Dip/Dip/src/DecompConstraintSet.h", "filename": "DecompConstraintSet.h", "file": "DecompConstraintSet.h", "brief": "Storage for constraint matrix and bounds (A, b, l, u)\n\nDecompConstraintSet stores the full specification of a constraint set\nused in decomposition: matrix, row/column bounds, integrality markers.\n\n**Matrix Storage:**\n- M: CoinPackedMatrix (row-major or column-major)\n- rowLB, rowUB: Row bounds (rowLB <= Ax <= rowUB)\n- colLB, colUB: Variable bounds\n- rowSense, rowRhs: Alternative row format ('L', 'G', 'E', 'R')\n\n**Variable Information:**\n- integerVars: Indices of integer variables\n- integerMark: 'I' for integer, 'C' for continuous\n- colNames, rowNames: Human-readable names\n\n**Block Structure:**\n- activeColumns: Which columns appear in this block\n- activeColumnsS: Set version for O(1) lookup\n- masterOnlyCols: Columns that only appear in master\n\n**Sparse Representation:**\nFor blocks with few active columns:\n- m_isSparse: Enable sparse mode\n- m_origToSparse, m_sparseToOrig: Column mappings\nReduces memory for blocks with disjoint variable sets.", "see": ["DecompModel.h for wrapper with metadata", "DecompApp.h where constraint sets are defined"], "has_pass2": false}, "Dip/src/AlpsDecompTreeNode.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompTreeNode.h", "filename": "AlpsDecompTreeNode.h", "file": "AlpsDecompTreeNode.h", "brief": "ALPS tree node for DIP branch-and-bound\n\nAlpsDecompTreeNode represents a node in the B&B tree, deriving from\nAlpsTreeNode to integrate with ALPS's tree management.\n\n**Key Responsibilities:**\n- Store branching decisions (bound changes from parent)\n- Implement process() to solve node via DecompAlgo\n- Implement branch() to create child nodes\n- Check for new incumbents\n\n**Branching Storage:**\n- downBranchLB_, downBranchUB_: Bounds for down branch\n- upBranchLB_, upBranchUB_: Bounds for up branch\nStored as (var_index, bound_value) pairs.\n\n**Key Methods:**\n- process(): Main bounding - calls DecompAlgo::processNode()\n- branch(): Create children with new bounds\n- checkIncumbent(): Test if solution improves best known\n- createNewTreeNode(): Factory for child nodes\n\n**Node Processing Flow:**\n1. ALPS selects node from tree\n2. Calls process() which invokes DecompAlgo\n3. DecompAlgo returns bound and status\n4. If fractional, branch() creates children", "see": ["AlpsDecompModel.h for parent model", "DecompAlgo.h for node processing logic", "AlpsTreeNode (ALPS) for base class"], "has_pass2": false}, "Dip/src/AlpsDecompModel.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompModel.h", "filename": "AlpsDecompModel.h", "file": "AlpsDecompModel.h", "brief": "ALPS integration - model class bridging DIP and ALPS B&B\n\nAlpsDecompModel derives from AlpsModel to integrate DIP's decomposition\nalgorithms with ALPS's branch-and-bound framework.\n\n**Key Responsibilities:**\n- Hold pointer to active DecompAlgo\n- Create root node (AlpsDecompTreeNode)\n- Manage solution pool and incumbent\n- Handle ALPS callbacks and parameters\n\n**ALPS Hierarchy:**\nAlpsKnowledge -> AlpsModel -> AlpsDecompModel\n\n**Key Methods:**\n- createRoot(): Create initial tree node\n- setupSelf(): Initialize from parameters\n- registerKnowledge(): Register solution types\n\n**Data Flow:**\n1. User creates DecompApp and DecompAlgo\n2. AlpsDecompModel wraps the algorithm\n3. ALPS drives B&B, calling process() on nodes\n4. Nodes delegate to DecompAlgo for bounding", "see": ["AlpsDecompTreeNode.h for B&B tree nodes", "DecompAlgo.h for the algorithm being integrated", "AlpsModel (ALPS) for base class interface"], "has_pass2": false}, "Dip/src/DecompVar.h": {"path": "layer-3/Dip/Dip/src/DecompVar.h", "filename": "DecompVar.h", "file": "DecompVar.h", "brief": "Column generation variable (lambda) representation\n\nDecompVar represents a column in the Dantzig-Wolfe reformulation.\nEach lambda_s corresponds to an extreme point s of a subproblem\npolyhedron: conv{x : A'x >= b', x integer}.\n\n**Key Data Members:**\n- m_s: CoinPackedVector storing the x-space representation\n- m_origCost: c's (original objective contribution)\n- m_redCost: (c - u*A'')s - alpha (reduced cost for pricing)\n- m_blockId: Which block generated this column\n- m_effCnt: Effectiveness counter for column management\n- m_strHash: Hash for duplicate detection\n\n**Column Interpretation:**\nIn master problem: lambda_s in {0,1}\nConvexity constraint: sum_s lambda_s = 1 (per block)\nOriginal solution: x = sum_s s * lambda_s\n\n**Reduced Cost Calculation:**\nrc(s) = c's - u'(A''s) - alpha_block\nwhere u = dual of linking constraints,\nalpha_block = dual of convexity constraint for block", "see": ["DecompVarPool.h for column pool management", "DecompAlgoPC.h for pricing/column generation"], "has_pass2": false}, "Dip/src/DecompWaitingCol.h": {"path": "layer-3/Dip/Dip/src/DecompWaitingCol.h", "filename": "DecompWaitingCol.h", "file": "DecompWaitingCol.h", "brief": "Wrapper pairing DecompVar with its master column coefficients\n\nDecompWaitingCol bundles a DecompVar (the x-space representation)\nwith its expanded column (A''s) for the master problem.\n\n**Data Members:**\n- m_var: The variable in x-space (extreme point s)\n- m_col: CoinPackedVector of (A''s) master coefficients\n\n**Purpose:**\nWhen pricing generates a new column s, we need both:\n1. The x-space representation for solution reconstruction\n2. The master coefficients for LP updates\nThis class keeps them paired.\n\n**Lifecycle:**\n- Created during pricing when subproblem returns solution\n- Stored in DecompVarPool\n- m_col regenerated via reExpand() when master changes\n- Eventually transferred to master LP", "see": ["DecompVar.h for the variable itself", "DecompVarPool.h for column pool management"], "has_pass2": false}, "Dip/src/DecompStats.h": {"path": "layer-3/Dip/Dip/src/DecompStats.h", "filename": "DecompStats.h", "file": "DecompStats.h", "brief": "Statistics tracking for DIP algorithm performance\n\nProvides classes for recording and reporting algorithm statistics.\n\n**DecompObjBound:**\nRecords bound history at checkpoints:\n- phase, cutPass, pricePass: Algorithm state\n- timeStamp: Wall clock from start\n- thisBound/thisBoundUB: Current node bounds\n- bestBound: Global lower bound\n- thisBoundIP: Best integer solution\n\n**DecompStats:**\nAggregate statistics for the algorithm:\n- objHistoryBound: Vector of DecompObjBound snapshots\n- Various timers for profiling subsystems\n- Counts of cuts/columns generated\n\n**Usage:**\n- DecompAlgo creates and updates stats during execution\n- Bound history enables convergence analysis\n- Timers help identify bottlenecks", "see": ["DecompAlgo.h where stats are collected", "UtilTimer.h for timer implementation"], "has_pass2": false}, "Dip/src/DecompWaitingRow.h": {"path": "layer-3/Dip/Dip/src/DecompWaitingRow.h", "filename": "DecompWaitingRow.h", "file": "DecompWaitingRow.h", "brief": "Wrapper pairing DecompCut with its row coefficients\n\nDecompWaitingRow bundles a DecompCut with both its x-space row\nand reformulated lambda-space row for the master problem.\n\n**Data Members:**\n- m_cut: The cut object with bounds and metadata\n- m_row: CoinPackedVector in original x-space\n- m_rowReform: CoinPackedVector in lambda-space for master\n\n**Reformulation:**\nOriginal cut: a'x >= b\nIn master: sum_s (a's)lambda_s >= b\nm_rowReform stores coefficients (a's) for each lambda variable.\n\n**Lifecycle:**\n- Created during cut generation\n- Stored in DecompCutPool\n- m_rowReform computed via createRowReform()\n- Eventually added to master LP", "see": ["DecompCut.h for the cut itself", "DecompCutPool.h for cut pool management"], "has_pass2": false}, "Dip/src/DecompParam.h": {"path": "layer-3/Dip/Dip/src/DecompParam.h", "filename": "DecompParam.h", "file": "DecompParam.h", "brief": "Comprehensive parameter class for DIP algorithm configuration\n\nDecompParam contains 100+ parameters controlling all aspects of DIP\ndecomposition algorithms. Parameters read from file via UtilParameters.\n\n**Logging Parameters:**\n- LogLevel: Verbosity (0=silent to 5=debug)\n- LogDumpModel: When to dump LP models (0=never, 2=always)\n- LogObjHistory: Print objective progress\n\n**Algorithm Limits:**\n- TotalCutItersLimit, TotalPriceItersLimit: Iteration caps\n- RoundCutItersLimit, RoundPriceItersLimit: Per-round limits\n- TimeLimit: Wall clock limit\n- NodeLimit: Max B&B nodes\n\n**Subproblem Control:**\n- SubProbTimeLimitExact: Time for exact subproblem solves\n- SubProbTimeLimitHeur: Time for heuristic solves\n- SubProbNumSolLimit: Max solutions per subproblem\n- SubProbParallelType: OpenMP scheduling strategy\n\n**Dual Stabilization:**\n- DualStab: Enable Wentges stabilization\n- DualStabAlpha: Smoothing parameter [0,1]\n\n**Branching:**\n- BranchEnforceInSubProb: Branch on x in subproblems\n- BranchEnforceInMaster: Branch on lambda in master\n\n**Strategy:**\n- PCStrategy: Balance pricing vs cutting\n- CutCglStrategy: Which CGL cuts to use", "see": ["DecompApp.h where m_param is stored", "UtilParameters for parameter file parsing"], "has_pass2": false}, "Dip/src/DecompAlgo.h": {"path": "layer-3/Dip/Dip/src/DecompAlgo.h", "filename": "DecompAlgo.h", "file": "DecompAlgo.h", "brief": "Base class for all DIP decomposition algorithms\n\nDecompAlgo is the algorithmic engine that orchestrates:\n- Master problem management (LP relaxation)\n- Subproblem solving (pricing/column generation)\n- Cut generation and management\n- Phase transitions and convergence\n\n**Key Data Members:**\n- m_masterSI: Master LP solver interface\n- m_app: Pointer to user's DecompApp\n- m_modelCore/m_modelRelax: Problem decomposition\n- m_vars/m_cuts: Generated columns and cuts\n- m_xhat: Current LP solution in original x-space\n\n**Algorithm Phases:**\n- PHASE_PRICE1: Feasibility with artificial variables\n- PHASE_PRICE2: Optimizing with generated columns\n- PHASE_CUT: Adding violated inequalities\n\n**Virtual Methods for Subclasses:**\n- createMasterProblem(): Build initial restricted master\n- processNode(): Main node processing loop\n- generateVars(): Column generation (pricing)\n- generateCuts(): Cut separation\n- getMasterDualSolution(): Dual values for pricing\n\n**Derived Classes:**\n- DecompAlgoPC: Price-and-Cut (Dantzig-Wolfe)\n- DecompAlgoC: Cutting plane only\n- DecompAlgoRC: Relax-and-Cut (Lagrangian)", "algorithm": "Price-and-Cut Hybrid (generateVars + generateCuts):\n  Interleave column generation and cut separation:\n  while (improving):\n    1. generateVars(): Solve pricing, add columns\n    2. generateCuts(): Separate cuts on current x̂\n    3. Update master LP, reoptimize\n  Cuts added to master affect dual space and pricing.", "math": "L(u) = u'b + min_x {(c - A'u)'x : x ∈ X}\n  Solve Lagrangian dual: max_u L(u) via subgradient or bundle methods.\n  - Subgradient: u^{t+1} = u^t + step * (b - Ax^t)\n  - L(u) provides lower bound; project x to get upper bound.", "ref": ["Dantzig, G.B. and Wolfe, P. (1960). \"Decomposition principle for\n       linear programs\". Operations Research 8(1):101-111.", "Barnhart, C. et al. (1998). \"Branch-and-price: Column generation\n       for solving huge integer programs\". Operations Research 46(3).", "Held, M. and Karp, R.M. (1971). \"The traveling-salesman problem\n       and minimum spanning trees: Part II\". Math Programming 1(1):6-25.", "Desaulniers, G. et al. (2005). \"Column Generation\". Springer."], "complexity": "Column generation: O(#iterations × pricing_cost)\n  Pricing cost depends on subproblem structure (often NP-hard but small)\n  Master LP: O(m × n × simplex_iterations) where n grows with columns", "see": ["DecompAlgoPC.h for Dantzig-Wolfe implementation", "DecompAlgoC.h for cutting plane method", "DecompAlgoRC.h for Lagrangian relaxation", "DecompApp.h for user model definition"], "has_pass2": true}, "Dip/src/DecompModel.h": {"path": "layer-3/Dip/Dip/src/DecompModel.h", "filename": "DecompModel.h", "file": "DecompModel.h", "brief": "Wrapper classes for constraint sets used in decomposition\n\nProvides model containers that wrap DecompConstraintSet with\nmetadata and optional solver interfaces.\n\n**DecompModel:**\nBasic wrapper for a constraint set (A'x >= b' or A''x >= b'')\n- m_model: Pointer to the constraint set\n- m_modelName: Human-readable name for debugging\n- m_blockId: Which block this belongs to (-1 for core)\n\n**DecompSubModel:**\nExtended model with OsiSolverInterface for subproblem solving\n- m_osi: Solver interface (Clp, Cpx, Grb, etc.)\n- solveAsMIP(): Solve subproblem as MIP\n- Used when user doesn't provide custom solveRelaxed()\n\n**Usage in Decomposition:**\n- Core model (A''): Linking constraints kept in master\n- Relax models (A'): Block constraints for subproblems\n- Each block b has m_modelRelax[b] in DecompApp", "see": ["DecompConstraintSet.h for underlying matrix storage", "DecompApp.h for model decomposition setup"], "has_pass2": false}, "Dip/src/AlpsDecompSolution.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompSolution.h", "filename": "AlpsDecompSolution.h", "file": "AlpsDecompSolution.h", "brief": "ALPS solution wrapper for DIP incumbent solutions\n\nAlpsDecompSolution derives from AlpsSolution to integrate DIP\nsolutions with ALPS's solution management and reporting.\n\n**Data Members:**\n- m_size: Number of variables\n- m_values: Solution vector\n- m_quality: Objective value\n- m_app: Pointer to DecompApp for printing\n\n**ALPS Integration:**\n- Stored in ALPS solution pool\n- Used for incumbent tracking\n- Supports encode/decode for parallelism\n\n**Lifecycle:**\n- Created when DIP finds integer-feasible solution\n- Passed to ALPS via registerKnowledge()\n- Best solution reported at termination", "see": ["DecompSolution.h for base solution class", "AlpsDecompModel.h for model integration", "AlpsSolution (ALPS) for base class"], "has_pass2": false}, "Dip/src/AlpsDecompNodeDesc.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompNodeDesc.h", "filename": "AlpsDecompNodeDesc.h", "file": "AlpsDecompNodeDesc.h", "brief": "ALPS node descriptor storing branching bounds\n\nAlpsDecompNodeDesc derives from AlpsNodeDesc to store the\ndescription of a tree node for serialization and reconstruction.\n\n**Storage:**\nDIP doesn't use full differencing (delta encoding), so node\ndescriptions primarily store bound changes from branching:\n- Variable lower/upper bound modifications\n- Warm start basis (optional)\n\n**Key Methods:**\n- encode(): Serialize to AlpsEncoded for transmission\n- decode(): Reconstruct from encoded form\n\n**ALPS Integration:**\n- Each AlpsDecompTreeNode has an AlpsDecompNodeDesc\n- Used for checkpointing and parallel distribution\n- Compact representation for tree storage", "see": ["AlpsDecompTreeNode.h for the tree node using this", "AlpsNodeDesc (ALPS) for base class interface"], "has_pass2": false}, "Dip/src/DecompAlgoRC.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoRC.h", "filename": "DecompAlgoRC.h", "file": "DecompAlgoRC.h", "brief": "Relax-and-Cut algorithm (Lagrangian relaxation with cuts)\n\nDecompAlgoRC implements Lagrangian relaxation:\n- Dualize complicating constraints with multipliers u\n- Solve Lagrangian subproblem: min (c - u'A'')x s.t. A'x >= b'\n- Update multipliers via subgradient optimization\n- Add cuts to improve bounds\n\n**Key Data Members:**\n- m_u: Lagrangian multiplier vector\n- m_step: Subgradient step size\n- m_LB/m_UB: Current Lagrangian bounds\n- m_shatVar: Solution from subproblem\n\n**Subgradient Update:**\nu_{k+1} = max(0, u_k + step * (b'' - A''x_k))\nStep size adjusted based on bound improvement.\n\n**When to Use:**\n- Very large problems where LP is expensive\n- Natural decomposition into easy subproblems\n- When approximate bounds suffice\n\n**Key Differences from PC:**\n- No LP solver needed for master (just subgradient)\n- Faster per-iteration but slower convergence\n- Bounds may not be as tight as LP-based methods", "see": ["DecompAlgo.h for base class", "DecompAlgoPC.h for LP-based alternative"], "has_pass2": false}, "Dip/src/Decomp.h": {"path": "layer-3/Dip/Dip/src/Decomp.h", "filename": "Decomp.h", "file": "Decomp.h", "brief": "Central header with enums, constants, and solver interfaces for DIP\n\nThis is the foundational include for DIP (Decomposition for Integer\nProgramming). It provides all enums, constants, and conditional solver\ninterface includes.\n\n**Algorithm Types (DecompAlgoType):**\n- CUT: Cutting plane method only\n- PRICE_AND_CUT: Dantzig-Wolfe column generation with cuts\n- RELAX_AND_CUT: Lagrangian relaxation with cuts\n- VOL_AND_CUT: Volume algorithm variant\n\n**Algorithm Phases (DecompPhase):**\n- PHASE_PRICE1: Restricted master with artificial columns\n- PHASE_PRICE2: Normal pricing after Phase 1 feasibility\n- PHASE_CUT: Cut generation phase\n- PHASE_DONE: Node processing complete\n\n**Solver Support:**\nConditionally includes OSI interfaces for:\n- Clp (DIP_HAS_CLP): Default open-source LP\n- CPLEX (DIP_HAS_CPX): IBM commercial solver\n- Gurobi (DIP_HAS_GRB): Commercial solver\n- Xpress (DIP_HAS_XPR): FICO commercial solver\n- CBC (DIP_HAS_CBC): COIN-OR MIP solver\n- SYMPHONY (DIP_HAS_SYMPHONY): Parallel MIP solver\n\n**Key Constants:**\n- DecompBigNum: Large value for infinity (1e21)\n- DecompEpsilon: Numerical tolerance (1e-6)\n- DecompZero: Near-zero threshold (1e-14)", "see": ["DecompAlgo.h for base algorithm class", "DecompApp.h for user application interface", "DecompParam.h for algorithm parameters"], "has_pass2": false}, "Dip/src/DecompMemPool.h": {"path": "layer-3/Dip/Dip/src/DecompMemPool.h", "filename": "DecompMemPool.h", "file": "DecompMemPool.h", "brief": "Pre-allocated memory arrays for performance\n\nDecompMemPool provides reusable scratch arrays to avoid repeated\nallocations during the inner loops of the algorithm.\n\n**Pre-allocated Arrays:**\n- dblArrNCoreCols: Double array sized for core columns\n- dblArrNCoreRows: Double array sized for core rows\n\n**Usage:**\nRather than allocating temporary arrays in hot paths like\nreduced cost calculation or constraint evaluation, use these\npre-allocated buffers for better cache performance.\n\n**Memory Management:**\n- allocateMemory(): Size arrays based on problem dimensions\n- Destructor frees all allocated memory\n- Single allocation at algorithm start", "see": ["DecompAlgo.h where memory pool is used"], "has_pass2": false}, "Dip/src/DecompAlgoPC.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoPC.h", "filename": "DecompAlgoPC.h", "file": "DecompAlgoPC.h", "brief": "Price-and-Cut algorithm (Dantzig-Wolfe decomposition with cuts)\n\nDecompAlgoPC implements the most powerful DIP algorithm combining:\n- Column generation (pricing subproblems)\n- Cut generation (violated inequalities)\n- Branch-and-bound integration via ALPS\n\n**Dantzig-Wolfe Reformulation:**\nOriginal: min c'x s.t. A''x >= b'', A'x >= b', x integer\nReformulated: min sum_s (c's_s)lambda_s s.t. sum_s (A''s_s)lambda_s >= b''\nwhere s_s are extreme points of conv{x: A'x >= b', x integer}\n\n**Dual Stabilization:**\n- m_dual: Stabilized dual used for pricing\n- m_dualRM: Raw dual from restricted master\n- m_dualST: Smoothed dual (Wentges smoothing)\nPrevents oscillation and improves convergence using:\n  dual_stab = alpha * dual_RM + (1-alpha) * dual_prev\n\n**Key Overrides:**\n- createMasterProblem(): Build restricted master with convexity rows\n- getMasterDualSolution(): Return stabilized duals for pricing\n- phaseUpdate(): Manage PRICE1->PRICE2->CUT transitions\n\n**Phases:**\n- PHASE_PRICE1: Generate initial columns, drive out artificials\n- PHASE_PRICE2: Normal pricing until no negative reduced cost\n- PHASE_CUT: Add cuts, then return to pricing if cuts found", "see": ["DecompAlgo.h for base class interface", "DecompVar.h for column representation", "DecompCut.h for cut representation"], "has_pass2": false}, "Dip/src/DecompSolution.h": {"path": "layer-3/Dip/Dip/src/DecompSolution.h", "filename": "DecompSolution.h", "file": "DecompSolution.h", "brief": "Solution storage for feasible/incumbent solutions\n\nDecompSolution stores primal solutions found during decomposition\n(from heuristics, subproblems, or node processing).\n\n**Key Data Members:**\n- m_size: Number of variables (columns)\n- m_values: Solution vector x*\n- m_quality: Objective value c'x* (for incumbent tracking)\n\n**Solution Sources:**\n- Subproblem solutions (during pricing)\n- User heuristics (DecompApp::APPheuristics)\n- Rounding/diving heuristics\n- Integer LP solutions\n\n**Output Methods:**\n- print(): Display solution with specified precision\n- printMIPLIB(): Output in MIPLIB solution format\n\n**Integration:**\nSolutions are passed to ALPS via AlpsDecompModel for\nincumbent tracking and solution reporting.", "see": ["DecompApp.h for heuristic callbacks", "AlpsDecompModel.h for ALPS integration"], "has_pass2": false}, "Dip/src/DecompSolverResult.h": {"path": "layer-3/Dip/Dip/src/DecompSolverResult.h", "filename": "DecompSolverResult.h", "file": "DecompSolverResult.h", "brief": "Container for subproblem/LP solver results\n\nDecompSolverResult captures all outputs from solving a subproblem\nor LP relaxation, including status, bounds, and solutions.\n\n**Status Information:**\n- m_solStatus: Solver-specific status code\n- m_isOptimal: True if proven optimal\n- m_isUnbounded: True if unbounded detected\n- m_isCutoff: True if cutoff by bound\n\n**Bounds:**\n- m_objLB: Lower bound on optimal value\n- m_objUB: Upper bound / incumbent value\n\n**Solutions:**\n- m_nSolutions: Number of solutions found\n- m_solution: Vector of solution vectors (for solution pools)\n\n**Usage:**\nReturned by subproblem solvers to DecompAlgo. Used for:\n- Generating new columns (m_solution contains extreme points)\n- Checking feasibility (m_isOptimal, m_solStatus)\n- Updating bounds (m_objLB, m_objUB)", "see": ["DecompSubModel.h for subproblem solving", "DecompAlgo.h for result processing"], "has_pass2": false}, "Dip/src/DecompVarPool.h": {"path": "layer-3/Dip/Dip/src/DecompVarPool.h", "filename": "DecompVarPool.h", "file": "DecompVarPool.h", "brief": "Pool of generated columns (DecompVar) for column generation\n\nDecompVarPool manages columns waiting to enter the master problem.\nInherits from std::vector<DecompWaitingCol> for storage.\n\n**Key Functions:**\n- isDuplicate(): Check if column already exists (via hash)\n- isParallel(): Check angular distance between columns\n- setReducedCosts(): Update reduced costs with new duals\n- reExpand(): Regenerate column coefficients after master changes\n\n**Column Selection:**\n- is_less_thanD comparator sorts by reduced cost\n- Most negative reduced cost columns enter master first\n- Parallel/duplicate columns filtered out\n\n**Validity Flag:**\n- m_colsAreValid: Track if columns need re-expansion\n- Set false when master constraint set changes\n- reExpand() regenerates coefficients\n\n**Integration with DecompAlgo:**\n- Subproblem solutions become DecompWaitingCol entries\n- Pool sorted, filtered, best columns added to master\n- Columns may be kept for future iterations", "see": ["DecompVar.h for column representation", "DecompWaitingCol.h for pool entry wrapper", "DecompAlgoPC.h for column generation driver"], "has_pass2": false}, "Dip/src/DecompAlgoC.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoC.h", "filename": "DecompAlgoC.h", "file": "DecompAlgoC.h", "brief": "Cutting Plane Method algorithm (no column generation)\n\nDecompAlgoC implements classic cutting plane method:\n- Solve LP relaxation\n- Find violated cuts\n- Add cuts and resolve\n- Repeat until integer or no cuts found\n\n**When to Use:**\n- Problems where LP relaxation is strong\n- No natural block structure for decomposition\n- Simpler than PC when pricing isn't needed\n\n**Algorithm Flow:**\n1. Solve initial LP relaxation\n2. Start in PHASE_CUT (no pricing phases)\n3. Generate cuts via CGL or user methods\n4. Re-solve LP with new cuts\n5. Branch if no cuts found and not integer\n\n**Key Differences from DecompAlgoPC:**\n- No pricing/column generation\n- No Phase 1/Phase 2 distinction\n- Master problem contains original variables (not lambdas)\n- Simpler master structure, faster per-iteration\n\n**Cut Generation:**\nUses same cut pool and CGL integration as PC variant.", "see": ["DecompAlgo.h for base class", "DecompAlgoPC.h for full Price-and-Cut", "DecompAlgoCGL.h for CGL cut integration"], "has_pass2": false}, "Dip/src/dippy/DippyDecompAlgo.h": {"path": "layer-3/Dip/Dip/src/dippy/DippyDecompAlgo.h", "filename": "DippyDecompAlgo.h", "param": ["utilParam parameter class", "pProb a DipProblem python object"], "has_pass2": false}}}, "DisCO": {"name": "DisCO", "file_count": 24, "pass2_count": 0, "files": {"src/DcoHeuristic.hpp": {"path": "layer-3/DisCO/src/DcoHeuristic.hpp", "filename": "DcoHeuristic.hpp", "file": "DcoHeuristic.hpp", "brief": "Abstract base class for primal heuristics\n\nDcoHeuristic provides the interface for heuristics that search\nfor feasible solutions during branch-and-bound.\n\n**Heuristic Types (DcoHeurType):**\n- DcoHeurTypeRounding: Round fractional solution\n\n**Strategies (DcoHeurStrategy):**\n- None: Disabled\n- Root: Only at root node\n- Auto: System decides\n- Periodic: Every N nodes\n- BeforeRoot: Before first LP solve\n\n**Statistics (DcoHeurStats):**\n- numCalls_: Times heuristic invoked\n- numNoSolCalls_: Calls with no solution found\n- time_: CPU time consumed\n- numSolutions_: Solutions found\n\n**Pure Virtual:**\n- searchSolution(): Find and return feasible solution or NULL", "see": ["DcoHeurRounding.hpp for simple rounding heuristic", "DcoTreeNode.hpp::callHeuristics() for invocation", "DcoModel.hpp::heuristics_ for storage"], "has_pass2": false}, "src/DcoModel.hpp": {"path": "layer-3/DisCO/src/DcoModel.hpp", "filename": "DcoModel.hpp", "file": "DcoModel.hpp", "brief": "Main model class for Mixed-Integer Conic Optimization\n\nDcoModel derives from BcpsModel and represents the master MISOCP problem.\nIt manages problem data, solver interfaces, and algorithm configuration.\n\n**Two Solving Modes:**\n- Direct conic: Uses OsiConicSolverInterface (CPLEX, Mosek)\n- OA (Outer Approximation): Uses OsiSolverInterface with linear cuts\n\n**Problem Structure:**\n- matrix_: Linear constraint matrix (CoinPackedMatrix)\n- coneStart_, coneMembers_, coneType_: Conic constraints\n- integerCols_: Indices of integer variables\n- relaxedCols_, relaxedRows_: Objects with relaxed integrality/cone\n\n**Algorithm Components:**\n- branchStrategy_: Variable selection (MaxInf, Pseudo, Strong, Reliability)\n- conGenerators_: Map of constraint generators (MILP + conic cuts)\n- heuristics_: Primal heuristics (rounding)\n\n**Key Virtual Methods:**\n- readInstance(): Load MPS or CBF files\n- setupSelf(): Initialize solver, cuts, heuristics\n- createRoot(): Create root DcoTreeNode\n- feasibleSolution(): Check integer and cone feasibility", "see": ["DcoTreeNode.hpp for tree node processing", "DcoConGenerator.hpp for cut generation interface", "DcoParams.hpp for configuration parameters"], "has_pass2": false}, "src/DcoNodeDesc.hpp": {"path": "layer-3/DisCO/src/DcoNodeDesc.hpp", "filename": "DcoNodeDesc.hpp", "file": "DcoNodeDesc.hpp", "brief": "Tree node description storing branching and warm-start data\n\nDcoNodeDesc stores the information that distinguishes a tree node\nfrom its parent, enabling node reconstruction and pseudocost updates.\n\n**Inheritance:** DcoNodeDesc -> BcpsNodeDesc -> AlpsNodeDesc\n\n**Branching Data (for pseudocost updates):**\n- branchedDir_: Up or down branch direction\n- branchedInd_: Index of variable branched on\n- branchedVal_: Value at which branching occurred\n\n**Warm Start:**\n- basis_: CoinWarmStartBasis for LP warm-starting\n\n**From BcpsNodeDesc (inherited):**\n- vars_: Variable bound modifications (BcpsObjectListMod)\n- cons_: Constraint modifications (BcpsObjectListMod)\n\n**Serialization:**\nencode()/decode() for parallel tree distribution.", "see": ["DcoTreeNode.hpp for the node using this descriptor", "DcoBranchObject.hpp for branching decisions", "CoinWarmStartBasis (CoinUtils) for LP basis"], "has_pass2": false}, "src/DcoConstraint.hpp": {"path": "layer-3/DisCO/src/DcoConstraint.hpp", "filename": "DcoConstraint.hpp", "file": "DcoConstraint.hpp", "brief": "Abstract base class for DisCO constraints\n\nDcoConstraint is the base class for all constraints in DisCO,\nsupporting both linear and conic constraint types.\n\n**Inheritance:** DcoConstraint -> BcpsConstraint -> BcpsObject -> AlpsKnowledge\n\n**Constraint Hierarchy:**\n- DcoConstraint (ABC)\n  - DcoLinearConstraint: Linear row with coefficients\n  - DcoConicConstraint: Lorentz or rotated Lorentz cone\n\n**Type Field (DcoConstraintType):**\n- Core: Original problem constraints\n- MILP cuts: Clique, Gomory, MIR, etc.\n- Conic cuts: IPM, OA approximations\n\n**Key Methods:**\n- createOsiRowCut(): Convert to OsiRowCut (returns NULL for conic)\n- constraintType(): Get/set the constraint source type", "see": ["DcoLinearConstraint.hpp for linear constraints", "DcoConicConstraint.hpp for conic constraints", "Dco.hpp for DcoConstraintType enum"], "has_pass2": false}, "src/DcoConicConstraint.hpp": {"path": "layer-3/DisCO/src/DcoConicConstraint.hpp", "filename": "DcoConicConstraint.hpp", "file": "DcoConicConstraint.hpp", "brief": "Second-order cone (Lorentz cone) constraint representation\n\nDcoConicConstraint represents conic constraints of two types:\n- Lorentz cone: ||x_1...x_{n-1}|| <= x_n\n- Rotated Lorentz cone: 2*x_1*x_2 >= ||x_3...x_n||^2\n\n**Data Members:**\n- coneType_: DcoLorentzCone or DcoRotatedLorentzCone\n- coneSize_: Number of variables in cone\n- members_: Variable indices forming the cone\n- supports_: Linear constraints approximating the cone (OA)\n- activeSupports_: Which supports are binding\n\n**OA (Outer Approximation):**\nWhen using linear solvers, conic constraints are approximated by\nlinear supporting hyperplanes. The supports_ array stores these\ncuts, and activeSupports_ tracks which are currently tight.\n\n**Feasibility:**\ninfeasibility() returns the cone violation at the current solution,\nused to decide if more OA cuts are needed.", "see": ["DcoConstraint.hpp for base class", "DcoLinearConstraint.hpp for linear supports", "Dco.hpp for DcoLorentzConeType enum"], "has_pass2": false}, "src/DcoVariable.hpp": {"path": "layer-3/DisCO/src/DcoVariable.hpp", "filename": "DcoVariable.hpp", "file": "DcoVariable.hpp", "brief": "Variable representation for MISOCP\n\nDcoVariable represents a decision variable in the optimization problem,\nstoring bounds, integrality type, and branching information.\n\n**Inheritance:** DcoVariable -> BcpsVariable -> BcpsObject -> AlpsKnowledge\n\n**BcpsObject Fields (inherited):**\n- lbHard_/ubHard_: Original bounds\n- lbSoft_/ubSoft_: Current bounds (may be tighter from branching)\n- intType_: Integer ('I'), binary ('B'), or continuous ('C')\n- objectIndex_: Position in model's variable array\n\n**Key Methods:**\n- infeasibility(): Returns integrality violation (for integer variables)\n- createBranchObject(): Create DcoBranchObject for branching on this\n- clone(): Deep copy for node differencing\n\n**Storage:**\nVariables are stored in BcpsModel::variables_ (inherited by DcoModel).\nInteger variable indices are tracked in DcoModel::integerCols_.", "see": ["DcoBranchObject.hpp for branching on variables", "DcoModel.hpp for variable storage", "BcpsObject (BCPS) for base class fields"], "has_pass2": false}, "src/DcoSolution.hpp": {"path": "layer-3/DisCO/src/DcoSolution.hpp", "filename": "DcoSolution.hpp", "file": "DcoSolution.hpp", "brief": "Incumbent solution for MISOCP\n\nDcoSolution stores a feasible solution found during branch-and-bound,\nincluding the variable values and objective quality.\n\n**Inheritance:** DcoSolution -> BcpsSolution -> AlpsSolution -> AlpsKnowledge\n\n**From BcpsSolution (inherited):**\n- size_: Number of variables\n- values_: Solution vector\n- quality_: Objective value\n\n**Selection Methods:**\n- selectNonzeros(): Return solution with only non-zero entries\n- selectFractional(): Return entries with fractional values\n\n**Solution Sources:**\n- DcoSolutionTypeBounding: From node relaxation\n- DcoSolutionTypeHeuristic: From primal heuristics\n- DcoSolutionTypeStrong: From strong branching", "see": ["DcoModel.hpp::feasibleSolution() for feasibility checking", "DcoHeuristic.hpp for primal heuristics", "BcpsSolution (BCPS) for base class"], "has_pass2": false}, "src/DcoLinearConGenerator.hpp": {"path": "layer-3/DisCO/src/DcoLinearConGenerator.hpp", "filename": "DcoLinearConGenerator.hpp", "file": "DcoLinearConGenerator.hpp", "brief": "Wrapper for CGL linear cut generators\n\nDcoLinearConGenerator wraps CglCutGenerator objects to integrate\nstandard MILP cutting planes into DisCO's branch-and-cut.\n\n**Available CGL Generators:**\n- CglClique: Clique cuts\n- CglOddHole: Odd hole cuts\n- CglFlowCover: Flow cover cuts\n- CglKnapsackCover: Knapsack cover cuts\n- CglMixedIntegerRounding: MIR cuts\n- CglGomory: Gomory cuts\n- CglProbing: Probing cuts\n- CglTwomir: Two-MIR cuts\n\n**generateConstraints() Flow:**\n1. Call CglCutGenerator::generateCuts()\n2. Convert OsiCuts to DcoLinearConstraint objects\n3. Add to BcpsConstraintPool\n4. Update statistics", "see": ["DcoConGenerator.hpp for base class", "DcoConicConGenerator.hpp for conic cuts", "CglCutGenerator (CGL) for interface"], "has_pass2": false}, "src/DcoPresolve.hpp": {"path": "layer-3/DisCO/src/DcoPresolve.hpp", "filename": "DcoPresolve.hpp", "file": "DcoPresolve.hpp", "brief": "Presolve/preprocessing for MISOCP problems\n\nDcoPresolve extends OsiPresolve to handle conic problems,\napplying reductions before branch-and-bound.\n\n**Supported Operations:**\n- Bound tightening (improve_bounds)\n- Redundant row/column removal (from OsiPresolve)\n- Problem scaling and reformulation\n\n**Two Modes (compile-time):**\n- OA mode (__OA__): Uses OsiSolverInterface\n- Direct mode: Uses OsiConicSolverInterface\n\n**Usage:**\n1. DcoPresolve(origModel) - Initialize with original model\n2. presolve() - Apply reductions, create presolvedModel_\n3. Solve presolved problem\n4. postsolve() - Restore solution to original space", "see": ["DcoModel.hpp::preprocess() for integration", "OsiPresolve (Osi) for base class", "DcoParams.hpp::presolve for enable/disable"], "has_pass2": false}, "src/DcoMessage.hpp": {"path": "layer-3/DisCO/src/DcoMessage.hpp", "filename": "DcoMessage.hpp", "file": "DcoMessage.hpp", "brief": "Message codes and handler for DisCO logging\n\nDcoMessage extends CoinMessages to provide structured logging\nwith configurable verbosity and debug levels.\n\n**Message Categories (DISCO_Message):**\n- Gap/Cutoff: DISCO_CUTOFF_INC, DISCO_GAP_YES/NO\n- Cut stats: DISCO_CUT_STATS_FINAL, DISCO_CUT_GENERATED\n- Node logging: DISCO_NODE_LOG, DISCO_NODE_BRANCH\n- Input: DISCO_READ_NOINTS, DISCO_READ_NOCONES, DISCO_READ_MPSERROR\n- Solver: DISCO_SOLVER_STATUS, DISCO_SOLVER_FAILED\n- Heuristics: DISCO_HEUR_SOL_FOUND, DISCO_HEUR_STATS_FINAL\n- Branching: DISCO_PSEUDO_REPORT, DISCO_STRONG_REPORT\n\n**Debug Levels (DISCO_Debug_Level):**\n- DISCO_DLOG_BRANCH: Branching decisions\n- DISCO_DLOG_CUT: Cut generation\n- DISCO_DLOG_PROCESS: Node processing\n- DISCO_DLOG_GRUMPY: Visualization output", "see": ["DcoModel.hpp::dcoMessageHandler_ for message handling", "CoinMessages (CoinUtils) for base class"], "has_pass2": false}, "src/DcoCbfIO.hpp": {"path": "layer-3/DisCO/src/DcoCbfIO.hpp", "filename": "DcoCbfIO.hpp", "file": "DcoCbfIO.hpp", "brief": "CBF (Conic Benchmark Format) file reader for DisCO\n\nDcoCbfIO reads and writes conic problems in the standard CBF format,\nwhich is a text-based format for conic optimization problems.\n\n**CONES Enum (CBF domain types):**\n- FREE_RANGE: Free variables (F)\n- POSITIVE_ORT: Nonnegative orthant (L+)\n- NEGATIVE_ORT: Nonpositive orthant (L-)\n- FIXPOINT_ZERO: Fixed to zero (L=)\n- QUAD_CONE: Lorentz/quadratic cone (Q): ||x_{2:n}|| <= x_1\n- RQUAD_CONE: Rotated quadratic cone (QR): 2*x_1*x_2 >= ||x_{3:n}||^2\n\n**File Structure (CBF format):**\n- VER: Version number\n- OBJSENSE: MIN or MAX\n- VAR: Number of variables and domains\n- INT: Integer variables\n- CON: Number of constraints and domains\n- OBJACOORD: Objective coefficients\n- ACOORD: Constraint matrix in coordinate format\n- BCOORD: Constraint bounds/fixed terms\n\n**Key Methods:**\n- readCbf(): Parse CBF file into internal structures\n- writeCbf(): Output problem in CBF format\n- getProblem(): Convert to standard form (colLB, colUB, rowLB, rowUB, matrix, cones)", "see": ["DcoModel.hpp::readInstance() for usage", "http://cblib.zib.de/ for CBF format specification"], "has_pass2": false}, "src/DcoBranchStrategyStrong.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyStrong.hpp", "filename": "DcoBranchStrategyStrong.hpp", "file": "DcoBranchStrategyStrong.hpp", "brief": "Strong branching variable selection strategy\n\nStrong branching evaluates candidate variables by actually solving\nthe LP/conic relaxation for both branches before deciding.\n\n**Algorithm:**\n1. Select top N candidates (strongCandSize parameter)\n2. For each candidate, temporarily apply up/down bounds\n3. Solve relaxation and record objective change\n4. Score = product rule (down_change * up_change)\n5. Choose variable with best score\n\n**Scoring (updateScore):**\nUses product scoring: score = down_degradation * up_degradation\nThis favors variables where both branches improve the bound.\n\n**Trade-offs:**\n- Pro: Best branching decisions, smaller trees\n- Con: Expensive, many LP/conic solves per node", "see": ["DcoBranchStrategyPseudo.hpp for pseudocost (cheaper)", "DcoBranchStrategyRel.hpp for reliability (hybrid)", "DcoParams.hpp::strongCandSize for candidate count"], "has_pass2": false}, "src/DcoTreeNode.hpp": {"path": "layer-3/DisCO/src/DcoTreeNode.hpp", "filename": "DcoTreeNode.hpp", "file": "DcoTreeNode.hpp", "brief": "Branch-and-bound tree node for MISOCP\n\nDcoTreeNode inherits BcpsTreeNode->AlpsTreeNode and implements\nthe core branch-and-cut loop for conic optimization.\n\n**Inheritance:** DcoTreeNode -> BcpsTreeNode -> AlpsTreeNode\n\n**Node Status (inherited from ALPS):**\n- Candidate: Fresh, unprocessed\n- Evaluated: Processed, may need more cuts\n- Pregnant: Ready to branch\n- Branched: Children created\n- Fathomed/Discarded: Pruned\n\n**Processing Flow:**\n1. process(): Main entry point called by ALPS\n2. installSubProblem(): Load bounds from node descriptor\n3. boundingLoop(): Iterate solve->cuts until done\n4. bound(): Solve conic/LP relaxation\n5. generateConstraints(): Add MILP and conic cuts\n6. branchConstrainOrPrice(): Decide next action\n7. branch(): Create child nodes\n\n**BcpStats Structure:**\nTracks cuts, bounds, and objective improvement per node.", "see": ["DcoNodeDesc.hpp for node description storage", "DcoModel.hpp for model context", "DcoBranchObject.hpp for branching decisions"], "has_pass2": false}, "src/DcoLinearConstraint.hpp": {"path": "layer-3/DisCO/src/DcoLinearConstraint.hpp", "filename": "DcoLinearConstraint.hpp", "file": "DcoLinearConstraint.hpp", "brief": "Linear constraint (row) representation\n\nDcoLinearConstraint stores a linear constraint in sparse form:\nlb <= sum(values[i] * x[indices[i]]) <= ub\n\n**Data Members:**\n- size_: Number of nonzero coefficients\n- indices_: Variable indices with nonzero coefficients\n- values_: Coefficient values\n- Bounds inherited from DcoConstraint/BcpsObject\n\n**Usage:**\n- Core constraints from the original problem\n- MILP cutting planes (Gomory, MIR, etc.)\n- OA supports approximating conic constraints\n\n**Conversion:**\n- createOsiRowCut(): Convert to OsiRowCut for solver\n\n**Feasibility:**\n- infeasibility(): Returns constraint violation", "see": ["DcoConstraint.hpp for base class", "DcoConicConstraint.hpp::supports_ uses these", "OsiRowCut (Osi) for solver representation"], "has_pass2": false}, "src/DcoConicConGenerator.hpp": {"path": "layer-3/DisCO/src/DcoConicConGenerator.hpp", "filename": "DcoConicConGenerator.hpp", "file": "DcoConicConGenerator.hpp", "brief": "Wrapper for conic outer approximation cut generators\n\nDcoConicConGenerator wraps CglConicCutGenerator to generate linear\ncuts that approximate conic constraints (Outer Approximation).\n\n**Conic Cut Types (DcoConicCutType):**\n- IPM: Interior point method based\n- IPMInt: Integer-aware IPM cuts\n- OA: Standard outer approximation\n- MIR: Conic MIR cuts\n- GD1: Gradient descent cuts\n\n**OA Algorithm:**\nFor a Lorentz cone ||x|| <= t, generate linear supports:\n- At current solution point x*, generate tangent hyperplane\n- a'x <= b where (a,b) defines supporting hyperplane\n\n**generateConstraints() Flow:**\n1. Call CglConicCutGenerator::generateCuts()\n2. Convert OsiConicCuts to DcoLinearConstraint objects\n3. Add to BcpsConstraintPool", "see": ["DcoConGenerator.hpp for base class", "DcoLinearConGenerator.hpp for MILP cuts", "DcoConicConstraint.hpp for conic constraint storage"], "has_pass2": false}, "src/DcoConGenerator.hpp": {"path": "layer-3/DisCO/src/DcoConGenerator.hpp", "filename": "DcoConGenerator.hpp", "file": "DcoConGenerator.hpp", "brief": "Abstract base class for constraint/cut generators\n\nDcoConGenerator provides the interface for generating cutting planes\n(constraints) during the branch-and-cut algorithm.\n\n**Generator Hierarchy:**\n- DcoConGenerator (ABC)\n  - DcoLinearConGenerator: Wraps CglCutGenerator (MILP cuts)\n  - DcoConicConGenerator: Wraps CglConicCutGenerator (OA cuts)\n\n**Statistics (DcoConGeneratorStats):**\n- numConsGenerated_: Total cuts generated\n- numConsUsed_: Cuts that were actually added\n- time_: CPU time consumed\n- numCalls_: Times generator was invoked\n- numNoConsCalls_: Calls producing no cuts\n\n**Control Parameters:**\n- strategy_: None, Root, Auto, Periodic\n- frequency_: How often to call (1 = every node)\n\n**Pure Virtual:**\n- generateConstraints(): Populate BcpsConstraintPool", "see": ["DcoLinearConGenerator.hpp for MILP cuts", "DcoConicConGenerator.hpp for conic OA cuts", "DcoModel.hpp::conGenerators_ for storage"], "has_pass2": false}, "src/DcoParams.hpp": {"path": "layer-3/DisCO/src/DcoParams.hpp", "filename": "DcoParams.hpp", "file": "DcoParams.hpp", "brief": "DisCO algorithm parameters and configuration\n\nDcoParams extends AlpsParameterSet to provide MISOCP-specific\nparameters for controlling the branch-and-cut algorithm.\n\n**Parameter Categories:**\n\n**Boolean (chrParams):**\n- cutRampUp, presolve, shareConstraints/Variables\n- sharePseudocostRampUp/Search\n\n**Integer (intParams):**\n- branchStrategy: 0=MaxInf, 1=Pseudo, 2=Reliability, 3=Strong, 4=Bilevel\n- Cut strategies: cutCliqueStrategy, cutGomoryStrategy, etc.\n- Cut frequencies: cutCliqueFreq, cutGomoryFreq, etc.\n- Conic cuts: cutIpmStrategy, cutOaStrategy\n- heurStrategy, heurRoundStrategy\n- lookAhead, pseudoReliability, strongCandSize\n\n**Double (dblParams):**\n- cutFactor, cutoff, objTol, integerTol, coneTol\n- optimalRelGap, optimalAbsGap\n- pseudoWeight, tailOff\n- OA parameters: cutOaBeta, cutOaSlack1/2\n\n**Serialization:**\n- pack()/unpack(): Encode for parallel distribution", "see": ["DcoModel.hpp for parameter usage", "AlpsParameterSet (ALPS) for base class"], "has_pass2": false}, "src/DcoHeurRounding.hpp": {"path": "layer-3/DisCO/src/DcoHeurRounding.hpp", "filename": "DcoHeurRounding.hpp", "file": "DcoHeurRounding.hpp", "brief": "Simple rounding heuristic for finding feasible solutions\n\nDcoHeurRounding implements a simple rounding heuristic based on\nAchterberg's dissertation to find integer feasible solutions.\n\n**Algorithm (searchSolution):**\n1. Take fractional LP/conic solution\n2. Round each integer variable to nearest integer\n3. Check feasibility (constraints and cones)\n4. Return solution if feasible, NULL otherwise\n\n**Bound Fixing (bound_fix):**\nUses constraint structure to determine safe rounding directions:\n- down_fix[i]: Variable i can be safely rounded down\n- up_fix[i]: Variable i can be safely rounded up\n\n**searchSolution2():**\nAlternative rounding strategy considering constraint types.", "see": ["DcoHeuristic.hpp for base class", "DcoModel.hpp::feasibleSolution() for feasibility checking", "DcoSolution.hpp for solution storage"], "has_pass2": false}, "src/DcoBranchStrategyPseudo.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyPseudo.hpp", "filename": "DcoBranchStrategyPseudo.hpp", "file": "DcoBranchStrategyPseudo.hpp", "brief": "Pseudocost branching variable selection\n\nPseudocost branching uses historical data about variable branching\nperformance to estimate which variable will improve bounds most.\n\n**Pseudocost Definitions (from Achterberg):**\n- f_j^+ = ceil(x_j) - x_j (fractional up)\n- f_j^- = x_j - floor(x_j) (fractional down)\n- phi_j^- = avg(Delta_j^- / f_j^-) over all down branches on j\n- phi_j^+ = avg(Delta_j^+ / f_j^+) over all up branches on j\n\n**Scoring Formula:**\nscore = (1-u)*min(phi^-, phi^+) + u*max(phi^-, phi^+)\nwhere u = 1/6 (score_factor_)\n\n**Statistics Tracked:**\n- down_num_/up_num_: Observation counts per variable\n- down_derivative_/up_derivative_: phi values", "see": ["DcoBranchStrategyStrong.hpp for initial pseudocost estimation", "DcoBranchStrategyRel.hpp for reliability branching (hybrid)", "DcoParams.hpp::pseudoWeight for score factor"], "has_pass2": false}, "src/Dco.hpp": {"path": "layer-3/DisCO/src/Dco.hpp", "filename": "Dco.hpp", "file": "Dco.hpp", "brief": "Central header defining DisCO enums, types, and constants\n\nThis header defines all enumeration types used throughout DisCO\n(Discrete Conic Optimization), a Mixed-Integer Second-Order Cone\nProgramming (MISOCP) solver.\n\n**Constraint Types (DcoConstraintType):**\n- Core constraints from the original problem\n- MILP cuts: Clique, FlowCover, Gomory, Knapsack, MIR, OddHole, Probe, TwoMIR\n- Conic cuts: IPM, IPMint, OA (Outer Approximation), CMIR, GD1\n\n**Cone Types (DcoLorentzConeType):**\n- DcoLorentzCone: Standard ||x|| <= t\n- DcoRotatedLorentzCone: 2*x1*x2 >= ||x_rest||^2\n\n**Cut Strategies (DcoCutStrategy):**\n- None, Root only, Auto, Periodic\n\n**Branching Strategies (DcoBranchingStrategy):**\n- MaxInfeasibility, PseudoCost, Reliability, Strong, Bilevel", "see": ["DcoModel.hpp for the main model class", "DcoConstraint.hpp for constraint hierarchy", "DcoConicConstraint.hpp for conic constraint handling"], "has_pass2": false}, "src/DcoBranchStrategyMaxInf.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyMaxInf.hpp", "filename": "DcoBranchStrategyMaxInf.hpp", "file": "DcoBranchStrategyMaxInf.hpp", "brief": "Maximum infeasibility branching strategy\n\nMaximum infeasibility branching selects the variable with the\nlargest integrality violation (closest to 0.5 fractional part).\n\n**Algorithm:**\nFor each fractional integer variable x_j with value v:\n- Compute fractionality = min(v - floor(v), ceil(v) - v)\n- Select variable with maximum fractionality\n\n**Branching Flow:**\n1. createCandBranchObjects(): Check all integer variables\n2. Create DcoBranchObject for each fractional variable\n3. betterBranchObject(): Compare by infeasibility score\n4. bestBranchObject_ set by BcpsBranchStrategy\n\n**Trade-offs:**\n- Pro: Simple, fast, no historical data needed\n- Con: Often poor branching decisions, larger trees", "see": ["DcoBranchStrategyPseudo.hpp for pseudocost (smarter)", "DcoBranchStrategyStrong.hpp for strong (best but slow)", "DcoVariable.hpp::infeasibility() for violation calculation"], "has_pass2": false}, "src/DcoSubTree.hpp": {"path": "layer-3/DisCO/src/DcoSubTree.hpp", "filename": "DcoSubTree.hpp", "file": "DcoSubTree.hpp", "brief": "Subtree representation for parallel search\n\nDcoSubTree extends BcpsSubTree to represent a portion of the\nbranch-and-bound tree for parallel distribution.\n\n**Inheritance:** DcoSubTree -> BcpsSubTree -> AlpsSubTree\n\n**Usage in Parallel:**\n- Subtrees are work units distributed to workers\n- Contains subset of tree nodes for processing\n- Supports encode/decode for network transmission\n\nNote: Currently minimal implementation - most logic in base class.", "see": ["DcoTreeNode.hpp for tree node implementation", "BcpsSubTree (BCPS) for base class", "AlpsSubTree (ALPS) for parallel infrastructure"], "has_pass2": false}, "src/DcoConfig.hpp": {"path": "layer-3/DisCO/src/DcoConfig.hpp", "filename": "DcoConfig.hpp", "file": "DcoConfig.hpp", "brief": "Build configuration header for DisCO\n\nStandard COIN-OR configuration pattern that selects appropriate\nconfig files based on build system and context.\n\n**Configuration Selection:**\n- With autotools (HAVE_CONFIG_H defined):\n  - Library build (DISCO_BUILD): uses config.h\n  - Client code: uses config_dco.h\n- Without autotools (e.g., Visual Studio):\n  - Library build: uses config_default.h\n  - Client code: uses config_dco_default.h", "see": ["DcoModel.hpp for main solver interface"], "has_pass2": false}, "src/DcoBranchObject.hpp": {"path": "layer-3/DisCO/src/DcoBranchObject.hpp", "filename": "DcoBranchObject.hpp", "file": "DcoBranchObject.hpp", "brief": "Branching decision for integer variable dichotomy\n\nDcoBranchObject stores the information needed to create two child\nnodes by branching on a fractional integer variable.\n\n**Branching Dichotomy:**\nFor variable x_i with fractional value v:\n- Down branch: x_i <= floor(v) (ubDownBranch_)\n- Up branch: x_i >= ceil(v) (lbUpBranch_)\n\n**Inherited from BcpsBranchObject:**\n- objectIndex_: Variable index being branched\n- score_: Branching score (from strategy)\n- value_: Current fractional value\n- direction_: Which branch to explore next\n\n**Key Methods:**\n- numBranches(): Returns 2 (binary branching)\n- branch(): Apply bound change to solver, return new bound", "see": ["DcoVariable.hpp::createBranchObject()", "DcoTreeNode.hpp::branch() for child creation", "DcoBranchStrategyStrong.hpp for scoring"], "has_pass2": false}}}, "SYMPHONY": {"name": "SYMPHONY", "file_count": 39, "pass2_count": 0, "files": {"include/sym_prep.h": {"path": "layer-3/SYMPHONY/include/sym_prep.h", "filename": "sym_prep.h", "file": "sym_prep.h", "brief": "MIP preprocessing (presolve) for SYMPHONY\n\nPreprocessing reduces problem size before B&C by fixing variables,\nremoving redundant constraints, and tightening bounds.\n\n**PREPdesc structure:**\n- mip: Current (preprocessed) problem\n- orig_mip: Original problem for solution recovery\n- stats: Preprocessing statistics\n- params: Preprocessing parameters\n- sr: Single-row relaxation data\n- impl_*: Implication data for logical fixing\n\n**Modification types:**\n- FIX_BINARY, FIX_OTHER: Variable fixing\n- IMPROVE_UB, IMPROVE_LB: Bound tightening\n- IMPROVE_COEF: Coefficient strengthening\n- FIX_AGGREGATE: Variable aggregation\n\n**prep_stats tracks:**\n- rows_deleted: Redundant constraints removed\n- vars_fixed: Variables fixed to bounds\n- coeffs_changed: Strengthened coefficients\n- bounds_tightened: Improved variable bounds\n- vars_integerized: Continuous→integer conversions\n\n**Single-row relaxation (SRdesc):**\n- Bounds derivation from single constraints\n- Used for probing and bound tightening\n\n**Main functions:**\n- sym_presolve(): Entry point for preprocessing\n- prep_basic(): Main preprocessing loop\n- prep_improve_variable(): Bound tightening\n- prep_check_redundancy(): Detect redundant rows\n- prep_delete_duplicate_rows_cols(): Remove duplicates\n- prep_substitute_cols(): Variable substitution\n\n**Return codes:**\n- PREP_UNMODIFIED: No changes made\n- PREP_MODIFIED: Problem reduced\n- PREP_INFEAS: Infeasibility detected\n- PREP_SOLVED: Optimal found during presolve", "see": ["sym_prep_params.h for preprocessing parameters", "sym_types.h for ROWinfo, COLinfo structures"], "has_pass2": false}, "include/sym_cp_params.h": {"path": "layer-3/SYMPHONY/include/sym_cp_params.h", "filename": "sym_cp_params.h", "file": "sym_cp_params.h", "brief": "Cut pool process parameters\n\nParameters for managing the global cut pool storage.\n\n**cp_params structure:**\n- verbosity: Output level\n- warm_start: Load cuts from file at start\n- warm_start_file_name: Saved cut pool file\n- logging: Save cut pool during solve\n- log_file_name: Cut pool log file\n\n**Size management:**\n- block_size: Allocation granularity\n- max_size: Maximum memory for cuts\n- max_number_of_cuts: Hard limit on cut count\n\n**Quality control:**\n- cuts_to_check: How many cuts to check for violations\n- delete_which: Deletion strategy (quality/touches)\n- touches_until_deletion: Inactivity threshold\n- min_to_delete: Minimum cuts to remove at once\n- check_which: Which cuts to check (level/touches)", "see": ["sym_cp.h for cut pool process", "sym_constants.h for CHECK_* and DELETE_* values"], "has_pass2": false}, "include/sym_macros.h": {"path": "layer-3/SYMPHONY/include/sym_macros.h", "filename": "sym_macros.h", "file": "sym_macros.h", "brief": "Utility macros for SYMPHONY\n\nCommon macros for memory management, parameter parsing,\nPVM communication, and utility operations.\n\n**Random number generation:**\n- SRANDOM(seed): Seed generator (srand/srandom)\n- RANDOM(): Get random number (rand/random)\n\n**Memory allocation:**\n- REMALLOC(ptr, type, old, new, block): Realloc with free first\n- REALLOC(ptr, type, old, new, block): Standard realloc\n- FREE(p): Safe free with NULL check\n\n**PVM communication (parallel mode):**\n- READ_INT_DESC(): Read integer array descriptor\n- READ_CHAR_ARRAY_WITH_SIZE(): Read sized char array\n- READ_STR_LIST(): Read string list\n\n**Parameter file parsing:**\n- READ_INT_PAR(par): Parse integer parameter\n- READ_DBL_PAR(par): Parse double parameter\n- READ_STR_PAR(par): Parse string parameter\n- READ_STRINT_PAR(): Parse string-to-int mapping\n- READPAR_ERROR(x): Report parse error\n\n**Data copying:**\n- COPY_DBL_ARRAY_DESC(): Copy double array descriptor\n- COPY_ARRAY_DESC(): Copy integer array descriptor\n- COPY_STAT(): Copy status array\n\n**User function handling:**\n- CALL_USER_FUNCTION(f): Check USER_ERROR return\n- CALL_WRAPPER_FUNCTION(f): Check wrapper return\n\n**Standard utilities:**\n- PRINT(verb, thresh, args): Conditional print\n- MIN(a,b), MAX(a,b): Comparisons\n- isset(a,i), setbit(a,i): Bit array operations\n\n**OpenMP atomics:**\n- OPENMP_ATOMIC_WRITE: Thread-safe write\n- OPENMP_ATOMIC_UPDATE: Thread-safe update", "see": ["sym_proto.h for PROTO macro"], "has_pass2": false}, "include/sym_messages.h": {"path": "layer-3/SYMPHONY/include/sym_messages.h", "filename": "sym_messages.h", "file": "sym_messages.h", "brief": "PVM message tags for parallel SYMPHONY\n\nDefines message types for inter-process communication in parallel mode.\nUsed with PVM (Parallel Virtual Machine) for distributed solving.\n\n**Message numbering scheme:**\n- 1xx: General messages (lifecycle, bounds)\n- 2xx: Master process messages (data requests)\n- 3xx: Tree manager ↔ LP messages\n- 4xx: LP process outgoing messages\n- 5xx: Cut/solution pool messages\n- 6xx: Cut packing messages\n- 7xx: Column packing messages\n\n**Lifecycle messages (1xx):**\n- YOU_CAN_DIE (100): Permission to terminate\n- I_AM_DEAD (101): Termination acknowledgment\n- UPPER_BOUND (103): New incumbent found\n- WRITE_LOG_FILE (105): Checkpoint request\n\n**Data request messages (2xx):**\n- REQUEST_FOR_LP_DATA/LP_DATA: LP worker setup\n- REQUEST_FOR_CG_DATA/CG_DATA: Cut generator setup\n- REQUEST_FOR_CP_DATA/CP_DATA: Cut pool setup\n- TM_DATA (210): Tree manager startup\n\n**Tree manager ↔ LP (3xx):**\n- LP__NODE_DESCRIPTION (300): Node state to TM\n- LP__BRANCHING_INFO (301): Branching decision\n- LP__IS_FREE (302): Ready for new node\n- LP__ACTIVE_NODE_DATA (306): Node assignment\n- LP__DIVING_INFO (307): Dive/backtrack instruction\n\n**Solution messages (4xx):**\n- FEASIBLE_SOLUTION_NONZEROS (410): Sparse solution\n- LP_SOLUTION_NONZEROS (420): LP solution for cuts\n\n**Pool messages (5xx-6xx):**\n- POOL_YOU_ARE_USELESS (501): Pool shutdown\n- PACKED_CUT (600): Single cut\n- PACKED_CUTS_TO_CP (601): Cuts to pool\n- NO_MORE_CUTS (605): End of cut stream", "see": ["sym_proccomm.h for communication functions"], "has_pass2": false}, "include/sym_pack_array.h": {"path": "layer-3/SYMPHONY/include/sym_pack_array.h", "filename": "sym_pack_array.h", "file": "sym_pack_array.h", "brief": "Array serialization for PVM communication\n\nPack/unpack functions for SYMPHONY data structures in parallel mode.\n\n**Array packing:**\n- pack_array_desc(): Serialize array_desc (int list)\n- unpack_array_desc(): Deserialize array_desc\n- pack_double_array_desc(): Serialize with double stats\n- unpack_double_array_desc(): Deserialize double array\n\n**Basis packing:**\n- pack_basis(): Serialize basis_desc for warm start\n- unpack_basis(): Deserialize basis_desc", "see": ["sym_proccomm.h for communication functions", "sym_types.h for array_desc, basis_desc"], "has_pass2": false}, "include/sym_master_u.h": {"path": "layer-3/SYMPHONY/include/sym_master_u.h", "filename": "sym_master_u.h", "file": "sym_master_u.h", "brief": "User callbacks for master process\n\nDefines callbacks for customizing problem setup and solution handling.\nReturn USER_DEFAULT to use built-in behavior for standard MIP solving.\n\n**Initialization callbacks:**\n- user_usage(): Print custom command-line help\n- user_initialize(): Allocate user data structure\n- user_free_master(): Clean up user data\n- user_readparams(): Read custom parameters\n- user_io(): Custom problem input\n\n**Problem setup:**\n- user_initialize_root_node(): Define base problem\n  - Set base variables, base cuts, column gen strategy\n  - This is the main hook for problem definition\n- user_start_heurs(): Run initial heuristics for bounds\n- user_init_draw_graph(): Setup visualization\n\n**Solution handling:**\n- user_receive_feasible_solution(): Process new incumbent\n- user_display_solution(): Custom solution output\n- user_send_feas_sol(): Known feasible solution for warm start\n\n**Data distribution (parallel mode):**\n- user_send_lp_data(): Send data to LP workers\n- user_send_cg_data(): Send data to cut generators\n- user_send_cp_data(): Send data to cut pools\n\n**Warm start support:**\n- user_ws_update_cuts(): Update cuts for modified problem\n- user_process_own_messages(): Custom message handling", "see": ["sym_master.h for master process", "sym_lp_u.h for LP process callbacks"], "has_pass2": false}, "include/sym_master.h": {"path": "layer-3/SYMPHONY/include/sym_master.h", "filename": "sym_master.h", "file": "sym_master.h", "brief": "Master process for SYMPHONY's parallel branch-and-cut\n\nThe master process coordinates the overall solve, managing problem\ndata, solution bounds, and communication with worker processes.\n\n**sym_environment structure:**\n- mip: Current MIP description (possibly presolved)\n- orig_mip: Original problem before presolve\n- prep_mip: Presolved problem description\n- par: Solver parameters\n- ub/lb: Best upper/lower bounds\n- best_sol: Best feasible solution found\n- warm_start: Warm start data for re-solving\n- tm: Tree manager (when compiled in)\n- cp: Cut pools (when compiled in)\n\n**User callback functions (_u suffix):**\n- initialize_u(): User initialization\n- io_u(): Custom problem I/O\n- start_heurs_u(): Run initial heuristics\n- display_solution_u(): Custom solution display\n- initialize_root_node_u(): Set up root node\n\n**Warm start operations:**\n- create_copy_warm_start(): Clone warm start data\n- get_lb_for_new_rhs(): Sensitivity analysis\n- get_ub_for_new_rhs(): Upper bound for modified RHS", "see": ["symphony.h for the public API", "sym_tm.h for tree manager details"], "has_pass2": false}, "include/sym_dg.h": {"path": "layer-3/SYMPHONY/include/sym_dg.h", "filename": "sym_dg.h", "file": "sym_dg.h", "brief": "Draw Graph (DG) process for SYMPHONY visualization\n\nInteractive graphical display of solutions and search progress.\nCommunicates with Tcl/Tk-based GUI via pipes.\n\n**dg_prob structure:**\n- windows: Array of display windows\n- par: Visualization parameters\n- master: Master process tid\n\n**Window management:**\n- win_desc: Window display settings (size, fonts, scale)\n- dg_graph: Graph data (nodes, edges)\n- buf_fifo: Message buffer queue\n\n**dg_node:**\n- node_id, posx, posy: Identifier and position\n- radius, label, weight: Display properties\n- dash: Line pattern for node border\n\n**dg_edge:**\n- edge_id, tail, head: Identifier and endpoints\n- weight, dash: Display properties\n\n**Main functions:**\n- init_dgwin(): Create new window\n- display_graph_on_canvas(): Render graph\n- copy_window_structure(): Clone window\n- find_node(), find_edge(): Lookup elements\n- compress_graph(): Remove deleted elements\n\n**Communication:**\n- spprint(): Printf to pipe\n- start_child(): Launch GUI process\n- wait_for_you_can_die(): Graceful shutdown", "see": ["sym_dg_params.h for visualization parameters", "sym_dg_u.h for user callbacks"], "has_pass2": false}, "include/symphony_api.h": {"path": "layer-3/SYMPHONY/include/symphony_api.h", "filename": "symphony_api.h", "file": "symphony_api.h", "brief": "Internal API header (legacy, includes sym_master.h)\n\nThis header provides the internal API declarations using the\nPROTO() macro for K&R C compatibility. Modern code should use\nsymphony.h which provides the same functions with standard\nANSI C prototypes.\n\n**Included headers:**\n- sym_proto.h: PROTO() macro definition\n- sym_master.h: Master process data structures\n- sym_messages.h: Message type definitions\n\n@note Prefer using symphony.h for new code", "see": ["symphony.h for the modern public API"], "has_pass2": false}, "include/sym_proccomm.h": {"path": "layer-3/SYMPHONY/include/sym_proccomm.h", "filename": "sym_proccomm.h", "file": "sym_proccomm.h", "brief": "Process communication abstraction for parallel SYMPHONY\n\nWrapper functions for PVM (Parallel Virtual Machine) communication.\nProvides portable inter-process communication in distributed mode.\n\n**PVM integration:**\n- __PVM__ flag enables PVM3 library\n- DataInPlace = PvmDataRaw for efficient packing\n- PROCESS_OK = PvmOk for status checks\n\n**Process management:**\n- register_process(): Join PVM\n- spawn(): Start remote processes\n- pstat(): Check process status\n- kill_proc(): Terminate process\n- comm_exit(): Leave PVM\n\n**Message sending:**\n- init_send(): Initialize send buffer\n- send_char/int/dbl/float_array(): Pack arrays\n- send_str(): Pack string\n- send_msg(): Send to one recipient\n- msend_msg(): Multicast to multiple recipients\n\n**Message receiving:**\n- receive_msg(): Blocking receive\n- treceive_msg(): Timed receive\n- nreceive_msg(): Non-blocking receive\n- receive_char/int/dbl/float_array(): Unpack arrays\n- receive_str(): Unpack string\n\n**Buffer management:**\n- bufinfo(): Get buffer metadata\n- freebuf(): Release buffer\n- setsbuf(), setrbuf(): Set active buffers", "see": ["sym_messages.h for message tags", "sym_tm.h for tree manager using communication"], "has_pass2": false}, "include/sym_dg_params.h": {"path": "layer-3/SYMPHONY/include/sym_dg_params.h", "filename": "sym_dg_params.h", "file": "sym_dg_params.h", "brief": "Draw Graph parameters and message constants\n\nParameters for visualization and GUI communication constants.\n\n**dg_params structure:**\n- canvas_width/height: Drawing area size\n- viewable_width/height: Visible window size\n- disp_nodelabels/nodeweights/edgeweights: Show labels\n- node_radius: Default node size\n- scale_factor: Zoom level\n- *_font: Font specifications\n- interactive_mode: Enable user interaction\n- mouse_tracking: Track mouse position\n\n**Message protocols (CTOI_* = Client to Intermediary):**\n- CTOI_INITIALIZE_WINDOW: Create new window\n- CTOI_SET_GRAPH: Load graph data\n- CTOI_DRAW_GRAPH: Render current graph\n- CTOI_MODIFY_GRAPH: Update graph elements\n- CTOI_WAIT_FOR_CLICK_*: User interaction\n\n**Messages (ITOC_* = Intermediary to Client):**\n- ITOC_CLICK_HAPPENED: User clicked\n- ITOC_WINDOW_*: Window status responses\n\n**Modification types (MODIFY_*):**\n- ADD_NODES, DELETE_NODES: Node operations\n- ADD_EDGES, DELETE_EDGES: Edge operations\n- CHANGE_WEIGHTS_*: Update labels\n- CHANGE_DASH_*: Update line styles", "see": ["sym_dg.h for draw graph process"], "has_pass2": false}, "include/sym_cg_params.h": {"path": "layer-3/SYMPHONY/include/sym_cg_params.h", "filename": "sym_cg_params.h", "file": "sym_cg_params.h", "brief": "Cut generator process parameters\n\nMinimal parameter structure for the cut generator process.\nMost cut generation parameters are in sym_lp_params.h (cgl_params).\n\n**cg_params:**\n- verbosity: Output level for cut generation\n- do_findcuts: Enable/disable user cut generation", "see": ["sym_cg.h for cut generator process", "sym_lp_params.h for CGL cut parameters"], "has_pass2": false}, "include/sym_types.h": {"path": "layer-3/SYMPHONY/include/sym_types.h", "filename": "sym_types.h", "file": "sym_types.h", "brief": "Core data structures for SYMPHONY's branch-and-cut\n\nDefines the fundamental data structures used throughout SYMPHONY\nfor representing problems, solutions, tree nodes, and cuts.\n\n**Problem representation:**\n- MIPdesc: Complete MIP in CSC format (matbeg, matind, matval)\n- MIPinfo: Problem statistics (var types, row types, density)\n\n**Tree node structures:**\n- bc_node: B&C tree node with bounds, solution, children\n- node_desc: Node description (basis, cuts, variables)\n- branch_obj: Branching decision (variable, children, bounds)\n- branch_desc: Single branch description (sense, rhs)\n\n**Cut structures:**\n- cut_data: Cut coefficients and metadata\n- row_data: Cut with effectiveness tracking\n- waiting_row: Pending cut with violation info\n\n**Solution structures:**\n- lp_sol: LP solution (sparse xind/xval format)\n- sp_solution: Solution pool entry\n- warm_start_desc: Complete warm start state\n\n**Statistics:**\n- problem_stat: Tree size, depth, diving stats\n- lp_stat_desc: LP calls, cuts generated by type\n- node_times: Timing breakdown per node\n\n**Presolve support:**\n- COLinfo: Column statistics and implications\n- ROWinfo: Row bounds, types, redundancy\n- IMPvar/IMPlist: Variable implication lists", "see": ["sym_master.h for sym_environment", "sym_tm.h for tm_prob"], "has_pass2": false}, "include/sym_lp_params.h": {"path": "layer-3/SYMPHONY/include/sym_lp_params.h", "filename": "sym_lp_params.h", "file": "sym_lp_params.h", "brief": "LP solver process parameters for SYMPHONY\n\nParameters controlling LP relaxation solving, cut generation,\nbranching, and primal heuristics in each B&C node.\n\n**cgl_params (CGL cut generation):**\n- generate_cgl_*_cuts: Enable specific cut families\n- generate_cgl_*_cuts_freq: How often to generate (1=every node)\n- *_max_depth: Tree depth limit for each cut type\n- use_chain_strategy: Smart cut generation chain\n\n**lp_params structure sections:**\n\n**Matrix growth control:**\n- max_non_dual_feas_to_add_*: Limits on violated cuts added\n- mat_row/col_compress_*: When to compress matrix\n\n**Tailing off detection:**\n- tailoff_gap_backsteps/frac: Detect stalling LP progress\n- tailoff_obj_backsteps/frac: Objective improvement threshold\n\n**Branching control:**\n- strong_branching_cand_num_*: Candidates for strong branching\n- rel_br_threshold: Reliability branching parameters\n- use_sos_branching: Special ordered sets support\n\n**Primal heuristics:**\n- fp_*: Feasibility pump parameters\n- fr_*: Feasibility-based restricted search\n- rs_*: RINS (Relaxation Induced Neighborhood Search)\n- lb_*: Local branching\n- ds_*: Diving heuristics (fractional, guided, etc.)", "see": ["sym_lp.h for LP process using these parameters", "sym_cg.h for cut generation"], "has_pass2": false}, "include/sym_pack_cut.h": {"path": "layer-3/SYMPHONY/include/sym_pack_cut.h", "filename": "sym_pack_cut.h", "file": "sym_pack_cut.h", "brief": "Cut serialization for PVM communication\n\nPack/unpack functions for cut_data in parallel mode.\n\n**Cut packing:**\n- pack_cut(): Serialize cut_data for transmission\n- unpack_cut(): Deserialize received cut_data", "see": ["sym_proccomm.h for communication functions", "sym_types.h for cut_data structure", "sym_cp.h for cut pool using these functions"], "has_pass2": false}, "include/sym_proto.h": {"path": "layer-3/SYMPHONY/include/sym_proto.h", "filename": "sym_proto.h", "file": "sym_proto.h", "brief": "Function prototype and path length macros\n\nDefines the PROTO macro for ANSI C function prototypes\nand standard path/line length constants.\n\n**Length constants:**\n- MAX_FILE_NAME_LENGTH (255): File path buffer size\n- MACH_NAME_LENGTH (255): Machine name buffer size\n- MAX_LINE_LENGTH (255): Line buffer size\n\n**PROTO macro:**\n- PROTO(x) expands to x for ANSI C prototypes\n- Allows: `int foo PROTO((int a, int b));`\n- Legacy support for K&R C compilers (commented out)", "see": ["sym_types.h for data types"], "has_pass2": false}, "include/sym_constants.h": {"path": "layer-3/SYMPHONY/include/sym_constants.h", "filename": "sym_constants.h", "file": "sym_constants.h", "brief": "Core constants and status codes for SYMPHONY\n\nDefines all numeric constants used throughout SYMPHONY including\nerror codes, status values, algorithm modes, and return codes.\n\n**Error codes (process_chain):**\n- ERROR__NO_BRANCHING_CANDIDATE, ERROR__ILLEGAL_RETURN_CODE\n- ERROR__NUMERICAL_INSTABILITY, ERROR__COMM_ERROR\n\n**Problem types:**\n- ZERO_ONE_PROBLEM, INTEGER_PROBLEM, MIXED_INTEGER_PROBLEM\n\n**Node status (NODE_STATUS__*):**\n- CANDIDATE, BRANCHED_ON, HELD, ROOT, PRUNED\n- TIME_LIMIT, ITERATION_LIMIT, WARM_STARTED\n\n**LP solver status:**\n- LP_OPTIMAL, LP_D_INFEASIBLE, LP_D_UNBOUNDED\n- LP_D_ITLIM, LP_D_OBJLIM, LP_TIME_LIMIT\n\n**Basis status:**\n- VAR_AT_LB, VAR_BASIC, VAR_AT_UB, VAR_FREE, VAR_FIXED\n\n**Cut source:**\n- INTERNAL_CUT_POOL, EXTERNAL_CUT_POOL\n- INTERNAL_CUT_GEN, EXTERNAL_CUT_GEN\n\n**CGL generators (CGL_*_GENERATOR):**\n- PROBING, KNAPSACK, CLIQUE, GOMORY, TWOMIR, FLOWCOVER, MIR\n\n**Branching actions:**\n- DO_BRANCH, DO_NOT_BRANCH, DO_NOT_BRANCH__FATHOMED\n- PRUNE_THIS_CHILD, RETURN_THIS_CHILD, KEEP_THIS_CHILD\n\n**Variable status flags:**\n- NOT_FIXED, TEMP_FIXED_TO_LB, PERM_FIXED_TO_LB\n- BASE_VARIABLE, VARIABLE_BRANCHED_ON, NOT_REMOVABLE\n\n**Column generation strategies:**\n- FATHOM__DO_NOT_GENERATE_COLS__DISCARD\n- FATHOM__GENERATE_COLS__RESOLVE\n- BEFORE_BRANCH__GENERATE_COLS__RESOLVE\n\n**VBC visualization (VBC_*):**\n- INTERIOR_NODE, PRUNED, ACTIVE_NODE, CAND_NODE, FEAS_SOL_FOUND\n\n**Diving heuristics:**\n- VLENGTH_DIVING, GUIDED_DIVING, CROSSOVER_DIVING\n- EUC_DIVING, RANK_DIVING, FRAC_DIVING\n\n**Presolve return codes:**\n- PREP_UNMODIFIED, PREP_MODIFIED, PREP_INFEAS, PREP_SOLVED", "see": ["sym_types.h for data structures using these constants", "sym_lp.h for LP process using these codes"], "has_pass2": false}, "include/sym_lp.h": {"path": "layer-3/SYMPHONY/include/sym_lp.h", "filename": "sym_lp.h", "file": "sym_lp.h", "brief": "LP solver process for SYMPHONY's branch-and-cut\n\nThe LP process solves LP relaxations at each B&C node, manages\ncuts, and performs branching decisions.\n\n**lp_prob structure:**\n- lp_data: Underlying LP solver interface (LPdata)\n- desc: Current node description\n- waiting_rows: Cuts ready to be added\n- slack_cuts: Cuts that became slack\n- pcost_down/up: Pseudo-costs for branching\n- bdesc: Branch descriptions along path from root\n\n**Main processing loop (process_chain):**\n1. Solve LP relaxation\n2. Check feasibility, update bounds\n3. Generate cuts (CGL cuts + user cuts)\n4. Add best cuts, resolve LP\n5. Check tail-off, decide to branch\n6. Select branching variable (strong branching)\n7. Create children, send to tree manager\n\n**Cut generation (generate_cgl_cuts_new):**\nIntegrates CGL cut generators: Gomory, knapsack, clique,\nprobing, MIR, flow cover, lift-and-project, etc.\n\n**Branching functions:**\n- select_branching_object(): Variable selection\n- strong_branch(): Evaluate branching by LP re-solve\n- branch(): Create child nodes\n\n**User callbacks (_u functions):**\n- is_feasible_u(): Check integer feasibility\n- select_candidates_u(): Custom branching candidates\n- generate_cuts_in_lp_u(): User-defined cuts", "see": ["sym_tm.h for tree manager", "sym_cg.h for cut generator process", "sym_lp_solver.h for LP solver interface"], "has_pass2": false}, "include/sym_cp_u.h": {"path": "layer-3/SYMPHONY/include/sym_cp_u.h", "filename": "sym_cp_u.h", "file": "sym_cp_u.h", "brief": "User callbacks for cut pool process\n\nDefines callbacks for custom cut checking in the cut pool.\nCut pool stores cuts for reuse across B&C nodes.\n\n**User callbacks:**\n- user_receive_cp_data(): Receive problem-specific data\n- user_free_cp(): Clean up user data\n- user_receive_lp_solution_cp(): Custom solution handling\n\n**Cut checking protocol (three-phase):**\n1. user_prepare_to_check_cuts(): Initialize with LP solution\n2. user_check_cut(): Check single cut for violation\n   - Sets is_violated flag\n   - Optionally computes quality score\n3. user_finished_checking_cuts(): Cleanup after checking\n\n**Default behavior:**\nReturns USER_DEFAULT for built-in EXPLICIT_ROW handling.\nUser cuts need custom checking implementation.", "see": ["sym_cp.h for cut pool process", "sym_cg_u.h for cut generation callbacks"], "has_pass2": false}, "include/sym_master_params.h": {"path": "layer-3/SYMPHONY/include/sym_master_params.h", "filename": "sym_master_params.h", "file": "sym_master_params.h", "brief": "Master process parameters aggregating all component params\n\nTop-level parameter structure containing parameters for all\nSYMPHONY processes: master, TM, LP, CG, CP, DG, and preprocessing.\n\n**params structure contains:**\n- cp_par: Cut pool parameters\n- cg_par: Cut generator parameters\n- lp_par: LP solver parameters\n- tm_par: Tree manager parameters\n- dg_par: Draw graph parameters (visualization)\n- prep_par: Preprocessing parameters\n\n**Execution control:**\n- warm_start: Resume from saved state\n- verbosity: Global output level\n- random_seed: For reproducibility\n- do_branch_and_cut: Enable B&C algorithm\n- do_draw_graph: Enable visualization\n- use_permanent_cut_pools: Persistent cut storage\n\n**Input/output:**\n- infile: Problem file (MPS/LP/GMPL)\n- file_type: MPS_FORMAT, LP_FORMAT, GMPL_FORMAT\n- datafile: GMPL data file if needed\n- test_dir: Directory for test problems\n- obj_offset: Constant added to objective\n\n**Multi-criteria optimization:**\n- multi_criteria: Enable bi-objective mode\n- mc_search_order: Search strategy\n- mc_compare_solution_tolerance: Pareto comparison\n- mc_warm_start: Reuse solutions across objectives\n\n**Parallel configuration:**\n- tm_exe, dg_exe: Process executables\n- tm_machine, dg_machine: Machine assignments\n- pvm_trace: PVM debugging", "see": ["sym_master.h for master process", "symphony.h for API using these parameters"], "has_pass2": false}, "include/sym_timemeas.h": {"path": "layer-3/SYMPHONY/include/sym_timemeas.h", "filename": "sym_timemeas.h", "file": "sym_timemeas.h", "brief": "Time measurement utilities for SYMPHONY\n\nPortable timing functions and timeval manipulation macros.\nUses Windows time on MSVC, sys/time.h otherwise.\n\n**Timing functions:**\n- start_time(): Start timer\n- used_time(): CPU time since last call\n- wall_clock(): Wall-clock elapsed time\n\n**timeval manipulation macros:**\n- TVCLEAR(tv): Zero a timeval\n- TVISSET(tv): Check if timeval is non-zero\n- TVXLTY(x,y): Compare x < y\n- TVXADDY(z,x,y): z = x + y\n- TVXSUBY(z,x,y): z = x - y\n- TVTODBL(tv): Convert to double seconds\n- DBLTOTV(d,tv): Convert double to timeval\n\n**VBC output macros:**\n- PRINT_TIME(tm, f): Print HH:MM:SS:MS format\n- PRINT_TIME2(tm, f): Print decimal seconds", "see": ["sym_win32_time.h for Windows implementation"], "has_pass2": false}, "include/sym_cg.h": {"path": "layer-3/SYMPHONY/include/sym_cg.h", "filename": "sym_cg.h", "file": "sym_cg.h", "brief": "Cut Generator process for SYMPHONY\n\nThe Cut Generator (CG) receives LP solutions and generates\nviolated cutting planes. Can run as separate process or\ncompiled into LP process (SYM_COMPILE_IN_CG).\n\n**cg_prob structure:**\n- cur_sol: Current LP solution to separate\n- par: Cut generator parameters\n- ub: Current upper bound for validity checking\n- cuts_to_add: Generated cuts pending addition\n\n**Workflow:**\n1. Receive LP solution from LP process\n2. Call user's find_cuts_u() callback\n3. User calls cg_add_explicit_cut() or cg_add_user_cut()\n4. Send cuts back to LP process\n\n**Cut creation:**\n- create_explicit_cut(): Build cut from explicit coefficients\n- cg_add_explicit_cut(): Add explicit cut to list\n- cg_add_user_cut(): Add user-defined packed cut\n\n**User callbacks:**\n- find_cuts_u(): Main separation routine\n- check_validity_of_cut_u(): Verify cut correctness\n- receive_cg_data_u(): Receive problem-specific data", "see": ["sym_lp.h for LP process which calls CG", "sym_cp.h for cut pool storage", "sym_cg_u.h for user callback declarations"], "has_pass2": false}, "include/sym_tm_params.h": {"path": "layer-3/SYMPHONY/include/sym_tm_params.h", "filename": "sym_tm_params.h", "file": "sym_tm_params.h", "brief": "Tree manager parameters for SYMPHONY\n\nParameters controlling the branch-and-cut search tree exploration.\n\n**Process configuration:**\n- lp_exe, cg_exe, cp_exe: Worker executables\n- lp/cg/cp_mach_num: Number of machines per type\n- lp/cg/cp_machs: Machine name arrays\n- max_active_nodes: Parallelism limit\n- max_cp_num: Maximum cut pools\n\n**Node selection (node_selection_rule):**\n- LOWEST_LP_FIRST: Best-bound search\n- DEPTH_FIRST_SEARCH: Deep diving\n- BREADTH_FIRST_SEARCH: Level-by-level\n- BEST_FIRST_SEARCH: Estimate-based\n\n**Diving strategy:**\n- unconditional_dive_frac: Always dive this fraction\n- diving_strategy: How to choose dive vs backtrack\n- diving_k, diving_threshold: Diving parameters\n\n**Termination criteria:**\n- time_limit: Maximum solve time\n- gap_limit: Optimality tolerance\n- node_limit: Maximum nodes to explore\n- find_first_feasible: Stop at first solution\n\n**Logging and warm start:**\n- logging, logging_interval: Progress logging\n- warm_start: Resume from saved state\n- warm_start_node_limit: Nodes to load\n- tree_log_file_name, cut_log_file_name: State files\n- vbc_emulation: Visualization output\n\n**Column generation:**\n- colgen_strat[2]: Strategy for two phases\n- price_in_root: Column generation at root\n\n**Solution pool:**\n- max_sp_size: Maximum solutions to keep", "see": ["sym_tm.h for tree manager process", "sym_constants.h for selection rule values"], "has_pass2": false}, "include/sym_primal_heuristics.h": {"path": "layer-3/SYMPHONY/include/sym_primal_heuristics.h", "filename": "sym_primal_heuristics.h", "file": "sym_primal_heuristics.h", "brief": "Primal heuristics for finding feasible solutions\n\nCollection of heuristics to find feasible MIP solutions quickly.\nCalled during B&C to improve incumbent and provide bounds.\n\n**Feasibility Pump (FP):**\n- Alternates between LP and rounding to find feasible point\n- FPdata: State for pumping iterations\n- FPvars: Variable info (binary, integer, auxiliary)\n- fp_round(): Round LP solution\n- fp_solve_lp(): Solve pumping LP\n- Adds auxiliary vars (x+, x-) for distance minimization\n\n**Rounding heuristics:**\n- round_solution(): Simple rounding of LP relaxation\n- shift_solution(): Shift rounding with constraint repair\n\n**Local search:**\n- local_search(): Neighborhood search from current solution\n- apply_local_search(): Wrapper with gap checking\n\n**Diving heuristics (ds_*):**\n- Probe fixing decisions along promising directions\n- ds_fix_vars(): Choose variables to fix\n- ds_get_frac_vars(): Identify fractional variables\n- Types: fractional, guided, crossover, euclidean, rank\n\n**Restricted search (fr_*):**\n- Solve reduced MIP near current solution\n- restricted_search(): Main entry point\n- fr_force_feasible(): Fix subset of variables\n\n**Local branching (lb_*):**\n- Add neighborhood constraint around incumbent\n- lbranching_search(): Solve constrained sub-MIP\n\n**Solution pool (sp_*):**\n- sp_add_solution(): Store new solution\n- sp_delete_solution(): Remove from pool\n- sp_is_solution_in_sp(): Check for duplicates", "see": ["sym_lp_params.h for heuristic parameters (fp_*, fr_*, rs_*, lb_*, ds_*)", "sym_lp.h for LP process calling heuristics"], "has_pass2": false}, "include/sym_prep_params.h": {"path": "layer-3/SYMPHONY/include/sym_prep_params.h", "filename": "sym_prep_params.h", "file": "sym_prep_params.h", "brief": "Preprocessing parameters for SYMPHONY\n\nParameters controlling MIP presolve operations.\n\n**prep_params structure:**\n- level: Preprocessing aggressiveness (0=off)\n- dive_level: Bound propagation depth\n- impl_dive_level: Implication chain depth\n- impl_limit: Max implications to explore\n- do_probe: Enable probing\n- verbosity: Output detail level\n- reduce_mip: Enable full problem reduction\n\n**Probing control:**\n- probe_verbosity: Probing output level\n- probe_level: Probing aggressiveness\n\n**Single-row relaxation:**\n- do_single_row_rlx: Enable SR bounds\n- single_row_rlx_ratio: Sparsity threshold\n- max_sr_cnt: Max rows to analyze\n\n**Aggregated row relaxation:**\n- do_aggregate_row_rlx: Combine rows for bounds\n- max_aggr_row_ratio: Density limit\n- max_aggr_row_cnt: Max aggregations\n\n**Resource limits:**\n- iteration_limit: Max preprocessing passes\n- time_limit: Preprocessing time budget\n- etol: Numerical tolerance\n\n**Output:**\n- display_stats: Show preprocessing summary\n- write_mps/write_lp: Save preprocessed problem", "see": ["sym_prep.h for preprocessing functions"], "has_pass2": false}, "include/symphony.h": {"path": "layer-3/SYMPHONY/include/symphony.h", "filename": "symphony.h", "file": "symphony.h", "brief": "Main public API for SYMPHONY MILP solver\n\nSYMPHONY is a parallel branch-cut-price framework for solving\nMixed Integer Linear Programs (MILPs). Supports both shared-memory\nand distributed-memory (MPI) parallelism.\n\n**Core API workflow:**\n```c\nsym_environment *env = sym_open_environment();\nsym_read_mps(env, \"problem.mps\");  // or sym_explicit_load_problem()\nsym_solve(env);\nsym_get_col_solution(env, solution);\nsym_close_environment(env);\n```\n\n**Problem input methods:**\n- sym_read_mps(): Read MPS format\n- sym_read_lp(): Read LP format\n- sym_read_gmpl(): Read GMPL/AMPL format\n- sym_explicit_load_problem(): Load from arrays\n\n**Solve methods:**\n- sym_solve(): Standard solve\n- sym_warm_solve(): Warm start from previous solution\n- sym_mc_solve(): Multi-criteria (bicriteria) solve\n\n**Return codes:**\n- TM_OPTIMAL_SOLUTION_FOUND (227): Optimal found\n- TM_TIME_LIMIT_EXCEEDED (228): Time limit\n- TM_NODE_LIMIT_EXCEEDED (229): Node limit\n- TM_FEASIBLE_SOLUTION_FOUND (235): Feasible but not proven optimal\n\n**Key parameters (via sym_set_int_param):**\n- \"node_selection_rule\": LOWEST_LP_FIRST, DEPTH_FIRST_SEARCH, etc.\n- \"time_limit\": Maximum solve time in seconds\n- \"gap_limit\": Relative MIP gap tolerance", "see": ["sym_master.h for internal master process structure", "sym_tm.h for tree manager internals"], "has_pass2": false}, "include/sym_cg_u.h": {"path": "layer-3/SYMPHONY/include/sym_cg_u.h", "filename": "sym_cg_u.h", "file": "sym_cg_u.h", "brief": "User callbacks for cut generator process\n\nDefines callbacks for custom cut generation in SYMPHONY.\nUsers implement problem-specific separation routines here.\n\n**Cut addition helpers (non-user):**\n- cg_add_explicit_cut(): Add cut with explicit coefficients\n- cg_add_user_cut(): Add packed user-defined cut\n- cg_send_cut(): Send cut to LP (internal)\n\n**User callbacks:**\n- user_receive_cg_data(): Receive problem-specific data\n- user_free_cg(): Clean up user data\n- user_find_cuts(): Main separation routine\n  - Called with LP solution (indices, values, objval)\n  - Populates cuts array via cg_add_* helpers\n- user_receive_lp_solution_cg(): Custom solution handling\n- user_check_validity_of_cut(): Debug validation (ifdef)\n\n**Example user_find_cuts implementation:**\n```c\nint user_find_cuts(void *user, int varnum, ...) {\n    // Check for violated inequalities\n    for (each inequality class) {\n        if (is_violated(values, ...)) {\n            cg_add_explicit_cut(nz, ind, val, rhs, 0.0,\n                                'L', TRUE, num_cuts, ...);\n        }\n    }\n    return USER_SUCCESS;\n}\n```", "see": ["sym_cg.h for cut generator process", "sym_lp_u.h for LP-side cut callbacks"], "has_pass2": false}, "include/sym_lp_solver.h": {"path": "layer-3/SYMPHONY/include/sym_lp_solver.h", "filename": "sym_lp_solver.h", "file": "sym_lp_solver.h", "brief": "LP solver abstraction layer for SYMPHONY\n\nProvides uniform interface to multiple LP solvers via OSI\n(Open Solver Interface) or native APIs.\n\n**Supported solvers (via compile flags):**\n- __OSI_CLP__: COIN-OR CLP (default, open source)\n- __OSI_CPLEX__: IBM CPLEX\n- __OSI_XPRESS__: FICO Xpress\n- __OSI_GLPK__: GNU Linear Programming Kit\n- __CPLEX__: Native CPLEX API\n- __OSL__: IBM OSL (legacy)\n\n**LPdata structure:**\n- si: OsiSolverInterface pointer (OSI mode)\n- lp: Native solver object (CPLEX/OSL mode)\n- n, m, nz: Columns, rows, nonzeros\n- x, dj, dualsol, slacks: Solution vectors\n- vars: Variable descriptors with bounds/status\n- mip: Original problem description\n- cgl: CGL cut generation parameters\n\n**Core LP operations:**\n- open/close_lp_solver(): Initialize/cleanup\n- load_lp_prob(): Load problem into solver\n- initial_lp_solve(): First solve from scratch\n- dual_simplex(): Resolve after modifications\n- solve_hotstart(): Quick re-solve with basis\n\n**Matrix modification:**\n- add_rows(), add_cols(): Extend problem\n- delete_rows(), delete_cols(): Remove elements\n- change_bounds(), change_rhs(): Modify constraints\n\n**Solution access:**\n- get_x(), get_dj_pi(), get_slacks(): Retrieve solutions\n- get_basis(), load_basis(): Basis manipulation\n\n**CGL cut generation:**\n- generate_cgl_cuts(): Call CGL generators\n- Supports: Gomory, MIR, Probing, Knapsack, Clique, etc.", "see": ["sym_lp.h for LP process using this interface", "CoinUtils for OSI documentation"], "has_pass2": false}, "include/SymConfig.h": {"path": "layer-3/SYMPHONY/include/SymConfig.h", "filename": "SymConfig.h", "file": "SymConfig.h", "brief": "Build configuration for SYMPHONY\n\nManages platform-specific configuration and symbol visibility.\n\n**Configuration modes:**\n- HAVE_CONFIG_H: Uses autoconf-generated config.h\n- Otherwise: Uses config_default.h / config_sym_default.h\n\n**Symbol visibility:**\n- SYMPHONYLIB_EXPORT: DLL export on Windows\n- __visibility__(\"default\"): GCC visibility for -fvisibility=hidden\n\n**Include hierarchy:**\n- SYMPHONYLIB_BUILD + HAVE_CONFIG_H → config.h\n- Client + HAVE_CONFIG_H → config_sym.h\n- SYMPHONYLIB_BUILD no autoconf → config_default.h\n- Client no autoconf → config_sym_default.h", "see": ["config_default.h for package dependencies", "config_sym_default.h for public API config"], "has_pass2": false}, "include/sym_tm.h": {"path": "layer-3/SYMPHONY/include/sym_tm.h", "filename": "sym_tm.h", "file": "sym_tm.h", "brief": "Tree Manager for SYMPHONY's parallel B&C\n\nThe Tree Manager (TM) maintains the branch-and-cut search tree,\ndispatches nodes to LP workers, and coordinates cut generation.\n\n**tm_prob structure:**\n- rootnode: Root of the B&C tree\n- active_nodes: Nodes currently being processed\n- samephase_cand: Nodes ready for processing in current phase\n- nextphase_cand: Nodes for next phase (two-phase algorithm)\n- lp/cg/cp: Process sets for LP solvers, cut generators, cut pools\n- pcost_down/pcost_up: Pseudo-costs for branching decisions\n- cuts: Global cut list\n\n**Node selection (del_best_node):**\n- LOWEST_LP_FIRST: Best-bound search\n- DEPTH_FIRST_SEARCH: Dive to find feasible solutions\n- BREADTH_FIRST_SEARCH: Level-by-level\n- BEST_FIRST_SEARCH: Estimate-based\n\n**Key operations:**\n- tm_initialize(): Set up tree from root description\n- solve(): Main tree search loop\n- generate_children(): Create child nodes after branching\n- shall_we_dive(): Decide diving vs backtracking\n- install_new_ub(): Update incumbent and prune\n\n**Two-phase algorithm:**\nPhase 1 explores with fewer cuts, Phase 2 adds more cuts\nto nodes marked for reconsideration.", "see": ["sym_master.h for master process", "sym_lp.h for LP solver process"], "has_pass2": false}, "include/sym_qsort.h": {"path": "layer-3/SYMPHONY/include/sym_qsort.h", "filename": "sym_qsort.h", "file": "sym_qsort.h", "brief": "Quicksort variants and utility functions\n\nSpecialized quicksort implementations for common data patterns.\n\n**Quicksort variants:**\n- qsort_i(): Sort integer array\n- qsort_id(): Sort int array, permute double array\n- qsort_ic(): Sort int array, permute char array\n- qsort_ii(): Sort int array, permute second int array\n- qsort_di(): Sort double array, permute int array\n\n**Utility functions:**\n- sym_gcd(): Greatest common divisor\n- d_gap(): Compute optimality gap percentage", "see": ["sym_lp.h for LP process using sort functions"], "has_pass2": false}, "include/sym_dg_u.h": {"path": "layer-3/SYMPHONY/include/sym_dg_u.h", "filename": "sym_dg_u.h", "file": "sym_dg_u.h", "brief": "User callbacks for Draw Graph visualization\n\nDefines callbacks for custom visualization behavior.\n\n**User callbacks:**\n- user_initialize_dg(): Global DG initialization\n- user_free_dg(): Global DG cleanup\n- user_dg_init_window(): Per-window setup\n- user_dg_free_window(): Per-window cleanup\n- user_dg_process_message(): Handle custom messages\n- user_interpret_text(): Process text input", "see": ["sym_dg.h for draw graph process", "sym_dg_params.h for message constants"], "has_pass2": false}, "include/sym_lp_u.h": {"path": "layer-3/SYMPHONY/include/sym_lp_u.h", "filename": "sym_lp_u.h", "file": "sym_lp_u.h", "brief": "User callbacks for LP solver process\n\nDefines callbacks that users implement to customize LP solving.\nReturn USER_DEFAULT to use built-in behavior.\n\n**Data transfer:**\n- user_receive_lp_data(): Receive problem-specific data\n- user_free_lp(): Clean up user data\n\n**Feasibility checking:**\n- user_is_feasible(): Custom integrality check\n- user_send_feasible_solution(): Pack solution for master\n- user_display_lp_solution(): Custom solution display\n\n**Branching callbacks:**\n- user_shall_we_branch(): Decide whether to branch\n- user_select_candidates(): Choose branching candidates\n- user_compare_candidates(): Rank two candidates\n- user_select_child(): Choose child node to explore\n\n**Cut handling:**\n- user_unpack_cuts(): Convert packed cuts to rows\n- user_generate_cuts_in_lp(): Generate cuts in LP process\n- user_same_cuts(): Compare two cuts for equality\n\n**Column generation:**\n- user_generate_column(): Create new variables dynamically\n- user_create_subproblem(): Build restricted LP\n\n**Built-in selection rules:**\n- branch_close_to_half(): Fractional variables near 0.5\n- branch_close_to_half_and_expensive(): Combine with objective\n- branch_close_to_one_and_cheap(): Variables near 1 with low cost", "see": ["sym_lp.h for LP process", "sym_cg_u.h for cut generator callbacks"], "has_pass2": false}, "include/config_default.h": {"path": "layer-3/SYMPHONY/include/config_default.h", "filename": "config_default.h", "file": "config_default.h", "brief": "Default build configuration for SYMPHONY library\n\nDefines package dependencies when not using autoconf.\nEdit this file to enable/disable optional solver backends.\n\n**Required packages (always enabled):**\n- COIN_HAS_CGL: CGL cut generators\n- COIN_HAS_CLP: CLP linear solver\n- COIN_HAS_COINUTILS: CoinUtils foundation\n- COIN_HAS_OSI: OSI solver interface\n\n**Optional packages (commented by default):**\n- COIN_HAS_CBC: Cbc MIP solver\n- COIN_HAS_VOL: Volume algorithm\n- COIN_HAS_CPX: CPLEX\n- COIN_HAS_GLPK: GLPK\n- COIN_HAS_MSK: Mosek\n- COIN_HAS_XPR: Xpress\n- COIN_DEBUG: Runtime sanity checks", "see": ["SymConfig.h for configuration selection"], "has_pass2": false}, "include/sym_cp.h": {"path": "layer-3/SYMPHONY/include/sym_cp.h", "filename": "sym_cp.h", "file": "sym_cp.h", "brief": "Cut Pool process for SYMPHONY\n\nThe Cut Pool (CP) stores generated cuts for potential reuse\nacross multiple B&C nodes. Manages cut quality and deduplication.\n\n**cut_pool structure:**\n- cuts: Array of cp_cut_data with quality tracking\n- size: Total memory used by cuts\n- cut_num: Number of cuts in pool\n- cur_sol: Solution being checked for violations\n\n**cp_cut_data extends cut_data with:**\n- touches: How many times cut was useful\n- level: Tree level where cut was generated\n- quality: Cut quality score for prioritization\n\n**Cut management:**\n- delete_ineffective_cuts(): Remove rarely-used cuts\n- delete_duplicate_cuts(): Remove identical cuts\n- order_cuts_by_quality(): Sort for violation checking\n\n**Check phases (CHECK_ONE_CUT pattern):**\n1. PREPARE_TO_CHECK_CUTS: Initialize checking\n2. CHECK_ONE_CUT: Evaluate single cut\n3. FINISH_TO_CHECK_CUTS: Finalize checking\n\n**User callbacks:**\n- check_cut_u(): Check if cut is violated\n- check_cuts_u(): Batch cut checking", "see": ["sym_cg.h for cut generation", "sym_lp.h for cut addition to LP"], "has_pass2": false}, "include/decomp/sp_params.h": {"path": "layer-3/SYMPHONY/include/decomp/sp_params.h", "filename": "sp_params.h", "file": "sp_params.h", "brief": "Solution/column pool parameters for decomposition\n\nParameters controlling column storage and management.\n\n**sp_params structure:**\n- verbosity: Output level\n- etol: Numerical tolerance\n- block_size: Allocation granularity\n- max_size: Maximum pool memory\n- max_number_of_sols: Hard limit on columns\n- min_to_delete: Minimum batch deletion size\n- touches_until_deletion: Inactivity threshold\n- compress_num/ratio: When to compress pool\n\n**Check strategies (check_which):**\n- CHECK_ALL_COLS: Check everything\n- CHECK_COL_LEVEL: By tree depth\n- CHECK_COL_TOUCHES: By activity\n- CHECK_COL_LEVEL_AND_TOUCHES: Combined\n\n**Delete strategies (delete_which):**\n- DELETE_DUPLICATE_COLS: Only duplicates\n- DELETE_DUPLICATE_AND_INEFFECTIVE_COLS: Also inactive", "see": ["decomp.h for column generation using pool"], "has_pass2": false}, "include/decomp/decomp_lp.h": {"path": "layer-3/SYMPHONY/include/decomp/decomp_lp.h", "filename": "decomp_lp.h", "file": "decomp_lp.h", "brief": "LP interface for decomposition master problem\n\nLoad/unload functions for the restricted master LP.\n\n**Functions:**\n- load_decomp_lp(): Initialize master LP with initial columns\n- unload_decomp_lp(): Cleanup master LP", "see": ["decomp.h for column generation algorithm", "sym_lp_solver.h for LPdata structure"], "has_pass2": false}, "include/decomp/decomp.h": {"path": "layer-3/SYMPHONY/include/decomp/decomp.h", "filename": "decomp.h", "file": "decomp.h", "brief": "Decomposition-based column generation for SYMPHONY\n\nImplements column generation via Dantzig-Wolfe decomposition\nfor problems with special structure.\n\n**Core functions:**\n- decomp(): Main decomposition algorithm\n- create_initial_lp(): Build restricted master problem\n- generate_new_cols(): Solve pricing subproblem\n- generate_cuts(): Cut generation in master\n- add_dcmp_cols(): Add columns to master LP\n\n**Column management:**\n- receive_cols(): Receive columns from subproblems\n- get_cols_from_pool(): Retrieve from column pool\n- free_dcmp_col_set(): Cleanup column set\n\n**User callbacks:**\n- user_generate_new_cols(): Custom pricing routine\n- user_unpack_col(): Expand packed column\n- user_pack_col(): Compress column for storage\n- user_check_col(): Verify column validity\n- user_set_rhs(): Define subproblem RHS\n- user_send_to_sol_pool(): Store promising columns", "see": ["decomp_types.h for col_data, dcmp_col_set", "decomp_lp.h for LP interface", "sp_params.h for column pool parameters"], "has_pass2": false}, "include/decomp/decomp_types.h": {"path": "layer-3/SYMPHONY/include/decomp/decomp_types.h", "filename": "decomp_types.h", "file": "decomp_types.h", "brief": "Data structures for decomposition column generation\n\nTypes for representing columns in Dantzig-Wolfe decomposition.\n\n**col_data (packed column):**\n- size: Size of coef array\n- coef: Packed column data\n- level: Tree level where generated\n- touches: Inactivity counter for deletion\n\n**dcmp_col_set (column batch):**\n- lb, ub, obj: Column bounds and costs\n- matbeg, matind, matval: CSC format matrix\n- num_cols, nzcnt: Dimensions\n- bd_type, ubnd: Bound type info", "see": ["decomp.h for column generation functions"], "has_pass2": false}}}, "Smi": {"name": "Smi", "file_count": 9, "pass2_count": 0, "files": {"Smi/src/SmiSmpsIO.hpp": {"path": "layer-3/Smi/Smi/src/SmiSmpsIO.hpp", "filename": "SmiSmpsIO.hpp", "file": "SmiSmpsIO.hpp", "brief": "SMPS (Stochastic MPS) file format reader and writer\n\nSmiSmpsIO extends CoinMpsIO to read/write the industry-standard SMPS\nformat for stochastic programming problems.\n\n**SMPS File Format (3 files):**\n- {name}.core: Core LP in MPS format (deterministic base problem)\n- {name}.time: Stage assignments (PERIODS section maps rows/cols to stages)\n- {name}.stoch: Stochastic data (SCENARIOS or INDEP sections)\n\n**SmiSectionType Enum:**\n- SMI_TIME_SECTION: PERIODS in time file\n- SMI_SCENARIOS_SECTION: SCENARIOS in stoch file\n- SMI_INDEPENDENT_SECTION: INDEP in stoch file\n\n**SmiSmpsType Enum:**\n- SMI_SC_CARD: SC card (scenario definition)\n- SMI_BL_CARD: BL card (branch location)\n- SMI_SMPS_COMBINE_ADD/REPLACE: Data combination rules\n\n**SmiSmpsCardReader Class:**\nExtends CoinMpsCardReader for SMPS-specific parsing:\n- periodName(): Stage name from time file\n- scenarioNew/Anc(): Scenario branching info\n- getProb(): Scenario probability\n\n**Key Methods:**\n- readTimeFile(): Parse stage assignments\n- readStochFile(): Parse stochastic data\n- writeSmps(): Output SMPS files", "see": ["SmiScnModel.hpp::readSmps() for high-level interface", "SmiScnData.hpp for SmiCoreData created from core file", "CoinMpsIO (CoinUtils) for MPS parsing base"], "param": ["filename The filename.", "extension The file extension.", "strictFormat Whether a strict format should be used or not.", "filename The filename.", "extension The file extension.", "strictFormat Whether a strict format should be used or not.", "filename The filename.", "extension The file extension.", "strictFormat Whether a strict format should be used or not."], "has_pass2": false}, "Smi/src/SmiMessage.hpp": {"path": "layer-3/Smi/Smi/src/SmiMessage.hpp", "filename": "SmiMessage.hpp", "file": "SmiMessage.hpp", "brief": "Message handler and codes for Smi logging\n\nSmiMessage extends CoinMessages to provide structured logging\nfor the stochastic programming interface.\n\n**SMI_Message Enum:**\n- SMI_SCENARIO_FINISHED: Scenario processing complete\n- SMI_DUMMY_END: End marker\n\n**SmiMessage Class:**\nCoinMessages-based message handler:\n- Constructor sets up message text templates\n- Supports multiple languages (default us_en)", "see": ["SmiScnModel.hpp for message handler usage", "CoinMessages, CoinMessageHandler (CoinUtils) for base classes"], "has_pass2": false}, "Smi/src/SmiScenarioTree.hpp": {"path": "layer-3/Smi/Smi/src/SmiScenarioTree.hpp", "filename": "SmiScenarioTree.hpp", "file": "SmiScenarioTree.hpp", "brief": "Scenario tree data structure for stochastic programming\n\nSmiScenarioTree is a template class for storing and navigating scenario\ntrees. Each root-to-leaf path represents one complete scenario.\n\n**SmiTreeNode<T> Template:**\nGeneric tree node with navigation:\n- parent_, child_, sibling_: Tree structure pointers\n- depth_: Distance from root (root = 0)\n- scen_: Scenario index for leaf nodes\n- child_labels_: Map for label-based child lookup\n\n**SmiScenarioTree<T> Template:**\nTree container with:\n- root_: Root node pointer\n- leaf_: Vector of leaf node pointers (one per scenario)\n- node_data: All node data in tree traversal order\n- scen_data: Temporary buffer for scenario path extraction\n\n**Key Operations:**\n- addPathtoLeaf(): Add new scenario path from branch point\n- find(scenario, stage): Get node at specific (scenario, stage)\n- find(labels): Get node by following label sequence\n- getScenario(s): Get vector of data along scenario s path\n\n**Tree Structure:**\n```\n      root (stage 0)\n     /    \\\n  node    node (stage 1)\n  / \\      |\nleaf leaf leaf (scenarios 0,1,2)\n```", "see": ["SmiScnModel.hpp for scenario model using this tree", "SmiScnData.hpp for SmiNodeData stored in tree nodes"], "has_pass2": false}, "Smi/src/SmiDiscreteDistribution.hpp": {"path": "layer-3/Smi/Smi/src/SmiDiscreteDistribution.hpp", "filename": "SmiDiscreteDistribution.hpp", "file": "SmiDiscreteDistribution.hpp", "brief": "Discrete probability distributions for stochastic programming\n\nClasses for representing discrete random variables and their distributions,\nused to generate scenario trees from independent stochastic elements.\n\n**SmiDiscreteDistribution Class:**\nContainer for multiple discrete random variables:\n- smiDiscrete_: Vector of SmiDiscreteRV objects\n- core_: Link to core problem data\n- combineRule_: How to merge events with core\n\n**SmiDiscreteRV Class:**\nSingle discrete random variable at a specific stage:\n- events_: Vector of SmiDiscreteEvent outcomes\n- stg_: Stage index for this RV\n- prob_: Total probability across events\n- addEvent(): Add outcome with LP data and probability\n\n**SmiDiscreteEvent Class:**\nOne possible outcome (realization) of a random variable:\n- Inherits SmiLinearData: matrix, bounds, objective changes\n- prob_: Probability of this event\n\n**Usage Pattern:**\n```cpp\nSmiDiscreteDistribution dist(core, rule);\nSmiDiscreteRV* rv = new SmiDiscreteRV(stage);\nrv->addEvent(matrix, clo, cup, obj, rlo, rup, 0.5);  // 50% chance\nrv->addEvent(..., 0.5);  // 50% chance\ndist.addDiscreteRV(rv);\nsmiModel.processDiscreteDistributionIntoScenarios(&dist);\n```", "see": ["SmiScnModel.hpp::processDiscreteDistributionIntoScenarios()", "SmiLinearData.hpp for LP data container", "SmiCoreCombineRule.hpp for combination rules"], "has_pass2": false}, "Smi/src/SmiScnModel.hpp": {"path": "layer-3/Smi/Smi/src/SmiScnModel.hpp", "filename": "SmiScnModel.hpp", "file": "SmiScnModel.hpp", "brief": "Main scenario model class for stochastic linear/quadratic programming\n\nSmiScnModel is the central class for representing and solving scenario-based\nstochastic programs. It builds deterministic equivalent (DE) formulations\nfrom scenario trees.\n\n**Typical Usage:**\n```cpp\nSmiScnModel smi;\nsmi.readSmps(\"problem\");              // Read SMPS files\nsmi.setOsiSolverHandle(new OsiClpSolverInterface());\nOsiSolverInterface* osi = smi.loadOsiSolverData();  // Build DE\nosi->initialSolve();                  // Solve extensive form\n```\n\n**Key Methods:**\n- readSmps(): Parse SMPS files (core + time + stoch)\n- generateScenario(): Add scenario with probability and branching info\n- loadOsiSolverData(): Build deterministic equivalent LP\n- getColSolution(scenario): Extract solution for specific scenario\n\n**Solution Values:**\n- solveWS(): Wait-and-See solution (perfect information)\n- solveEV(): Expected Value solution (average scenario)\n- solveEEV(): Expected result of using EV solution\n\n**SmiScnNode Class:**\nRepresents a node in the scenario tree with:\n- Stage index, probability, parent link\n- Column/row offsets into DE matrix\n- Link to SmiNodeData for LP data", "see": ["SmiScenarioTree.hpp for tree data structure", "SmiScnData.hpp for core/node data classes", "SmiSmpsIO.hpp for SMPS file I/O", "SmiCoreCombineRule.hpp for replace/add rules"], "param": ["name The name for the model and the written files", "winFileExtensions optional; false by default, so extensions will be [core, time, stoch]. With this parameter set to true, it will be [cor, tim, sto].", "strictFormat optional, true by default. Set to false if SMPS files should be written in free format."], "return": "-1 in case of no existing SMI model, otherwise 0", "has_pass2": false}, "Smi/src/SmiQuadratic.hpp": {"path": "layer-3/Smi/Smi/src/SmiQuadratic.hpp", "filename": "SmiQuadratic.hpp", "file": "SmiQuadratic.hpp", "brief": "Quadratic objective data for stochastic quadratic programming\n\nClasses for storing quadratic objective terms in stochastic QP problems.\nThe quadratic objective is: (1/2) x' Q x + c' x\n\n**SmiQuadraticData Class:**\nBase class storing Q matrix in compressed sparse column format:\n- _n: Number of columns\n- _starts: Column start indices (size n+1)\n- _indx: Row indices of nonzeros\n- _els: Values of nonzeros\n- _coff: Column offset for stage-based indexing\n- _hasData: Flag for valid data\n\n**CSC Format:**\nFor column j, nonzeros are at positions [starts[j], starts[j+1]):\n- Row indices: indx[starts[j]] ... indx[starts[j+1]-1]\n- Values: els[starts[j]] ... els[starts[j+1]-1]\n\n**SmiQuadraticDataDC Class:**\n\"Data Container\" subclass that owns its memory:\n- Constructor allocates arrays\n- Destructor frees arrays", "see": ["SmiScnData.hpp::SmiNodeData::addQuadraticObjective()", "SmiScnModel.hpp::loadQuadraticSolverData()", "ClpModel (Clp) for QP solving"], "has_pass2": false}, "Smi/src/SmiCoreCombineRule.hpp": {"path": "layer-3/Smi/Smi/src/SmiCoreCombineRule.hpp", "filename": "SmiCoreCombineRule.hpp", "file": "SmiCoreCombineRule.hpp", "brief": "Rules for combining core and stochastic data in SMPS\n\nIn SMPS, stochastic data is a \"diff\" from the core problem. This file\ndefines how to apply that diff (combine core + stochastic → scenario).\n\n**SmiCoreCombineRule (Abstract Base):**\nVirtual interface for combination rules:\n- Process(d1, offset, cpv): Modify dense array d1 using sparse cpv\n- Process(cpv1, cpv2): Combine two sparse vectors\n\n**SmiCoreCombineReplace (Singleton):**\nDefault SMPS rule - stochastic values REPLACE core values:\n- scenario[i] = stochastic[i] if defined, else core[i]\n- Use case: Changing a coefficient value entirely\n\n**SmiCoreCombineAdd (Singleton):**\nAlternative rule - stochastic values ADD to core values:\n- scenario[i] = core[i] + stochastic[i]\n- Use case: Perturbations, additive uncertainty\n\n**Singleton Pattern:**\nBoth concrete rules use singleton pattern for efficiency:\n- SmiCoreCombineReplace::Instance()\n- SmiCoreCombineAdd::Instance()\n\n**Usage:**\nRules are passed to generateScenario() or readSmps() to control\nhow scenario data is constructed from core + stochastic inputs.", "see": ["SmiScnModel.hpp::generateScenario() for usage", "SmiSmpsIO.hpp::readStochFile() for SMPS parsing", "SmiScnData.hpp::SmiNodeData for node-level application"], "has_pass2": false}, "Smi/src/SmiScnData.hpp": {"path": "layer-3/Smi/Smi/src/SmiScnData.hpp", "filename": "SmiScnData.hpp", "file": "SmiScnData.hpp", "brief": "Core and node data classes for stochastic models\n\nContains SmiCoreData (deterministic base LP) and SmiNodeData (scenario\nmodifications) that together define stochastic linear programs.\n\n**SmiCoreData Class:**\nStores the deterministic \"core\" LP problem with stage structure:\n- nrow_, ncol_: Total rows/columns in core problem\n- nstag_: Number of stages (periods)\n- stageRowPtr_[t], stageColPtr_[t]: Row/col offsets for stage t\n- nRowInStage_[t], nColInStage_[t]: Counts per stage\n- nodes_[t]: Core SmiNodeData for each stage\n- Dense bound arrays: cdrlo_, cdrup_, cdclo_, cdcup_, cdobj_\n\n**SmiNodeData Class:**\nStores LP modifications for a tree node (scenario/stage):\n- Sparse delta data: matrix rows, bounds, objective\n- combineRule_: How to merge with core (replace/add)\n- Stage membership and core linkage\n- Dense row map for efficient access\n\n**SMPS Data Flow:**\n1. Core MPS → SmiCoreData (base LP with stage assignments)\n2. Stoch file → SmiNodeData (per-scenario modifications)\n3. Combine with rule → Full scenario LP data\n\n**Type Aliases:**\n- SmiCoreIndex, SmiScenarioIndex, SmiStageIndex: int aliases", "see": ["SmiCoreCombineRule.hpp for combination rules", "SmiScnModel.hpp for scenario model using these classes", "SmiSmpsIO.hpp for SMPS file parsing"], "has_pass2": false}, "Smi/src/SmiLinearData.hpp": {"path": "layer-3/Smi/Smi/src/SmiLinearData.hpp", "filename": "SmiLinearData.hpp", "file": "SmiLinearData.hpp", "brief": "Container for linear programming data (matrix + bounds + objective)\n\nSmiLinearData bundles all LP data needed to define a linear subproblem\nor modification in stochastic programming.\n\n**Data Members:**\n- matrix_: Constraint matrix (CoinPackedMatrix)\n- dclo_, dcup_: Column (variable) bounds\n- drlo_, drup_: Row (constraint) bounds\n- dobj_: Objective coefficients\n\n**Constructors:**\n- Default: Empty data\n- Copy: From another SmiLinearData\n- From components: matrix + 5 sparse vectors\n- From OSI: Extract from OsiSolverInterface\n\n**Accessors:**\n- getMatrix(), getColLower(), etc.: Const references\n- getMutableMatrix(), etc.: Non-const for modification\n\n**Usage:**\nUsed as base class for SmiDiscreteEvent and as a data container\nfor passing LP modifications around the Smi system.", "see": ["SmiDiscreteDistribution.hpp::SmiDiscreteEvent for subclass", "SmiScnData.hpp::SmiNodeData for related node storage", "CoinPackedVector, CoinPackedMatrix (CoinUtils) for sparse data"], "has_pass2": false}}}, "oBB": {"name": "oBB", "file_count": 5, "pass2_count": 0, "files": {"nlopt/nlopt.hpp": {"path": "layer-3/oBB/nlopt/nlopt.hpp", "filename": "nlopt.hpp", "file": "nlopt.hpp", "brief": "C++ wrapper around NLopt C API - bundled with oBB\n\nProvides an object-oriented C++ interface to NLopt with RAII,\nexception handling, and std::vector support.\n\n**nlopt::opt Class:**\nMain optimizer class wrapping nlopt_opt:\n- Constructor: opt(algorithm, n) creates n-dimensional optimizer\n- set_min/max_objective(): Set objective (func or vfunc)\n- set_lower/upper_bounds(): Variable bounds\n- add_inequality/equality_constraint(): Add constraints\n- set_ftol_rel/abs(), set_xtol_rel/abs(): Tolerances\n- optimize(x, opt_f): Run optimization, returns nlopt::result\n\n**Function Types:**\n- nlopt::func: C-style callback (same as nlopt_func)\n- nlopt::vfunc: C++ style with std::vector<double> args\n\n**Exception Types:**\n- nlopt::roundoff_limited: Roundoff prevented progress\n- nlopt::forced_stop: Optimization was force-stopped\n- std::bad_alloc, std::invalid_argument: Standard exceptions\n\n**Usage Example:**\n```cpp\nnlopt::opt opt(nlopt::LD_SLSQP, 2);\nopt.set_min_objective(myFunc, NULL);\nopt.set_lower_bounds(lb);\nstd::vector<double> x = {1.0, 1.0};\ndouble minf;\nnlopt::result res = opt.optimize(x, minf);\n```", "see": ["nlopt.h for C API", "oBB for global optimization using this for local steps"], "has_pass2": false}, "slsqp/slsqp.h": {"path": "layer-3/oBB/slsqp/slsqp.h", "filename": "slsqp.h", "file": "slsqp.h", "brief": "SLSQP (Sequential Least Squares Programming) algorithm - bundled with oBB\n\nSLSQP is a gradient-based algorithm for constrained nonlinear optimization\nusing sequential quadratic programming with BFGS updates.\n\n**Algorithm (Kraft, 1988):**\n1. Solve QP subproblem: min (1/2)d'Bd + g'd s.t. linearized constraints\n2. Line search along direction d\n3. Update Hessian approximation B using BFGS\n4. Repeat until convergence\n\n**nlopt_slsqp() Parameters:**\n- n: Number of variables\n- f, f_data: Objective function (with gradient)\n- m, fc: Inequality constraints (m constraints, c(x) <= 0)\n- p, h: Equality constraints (p constraints, h(x) = 0)\n- lb, ub: Variable bounds\n- x: Initial point (modified in place)\n- minf: Output - optimal objective value\n- stop: Stopping criteria (nlopt_stopping struct)\n\n**Characteristics:**\n- Local, derivative-based optimization\n- Handles nonlinear equality and inequality constraints\n- Efficient for smooth, medium-scale problems\n- Used by oBB for local refinement in global optimization", "see": ["nlopt.h for nlopt_constraint structure", "nlopt-util.h for nlopt_stopping structure", "oBB for global optimization framework"], "has_pass2": false}, "nlopt/nlopt.h": {"path": "layer-3/oBB/nlopt/nlopt.h", "filename": "nlopt.h", "file": "nlopt.h", "brief": "NLopt (Nonlinear Optimization) C API - bundled with oBB\n\nNLopt is a library for nonlinear optimization providing a common\ninterface to many algorithms. This is a subset bundled with oBB.\n\n**nlopt_algorithm Enum (in this subset):**\n- NLOPT_LD_SLSQP: Sequential Least Squares Programming (local, derivative)\n\n**nlopt_result Enum:**\n- NLOPT_SUCCESS: Generic success\n- NLOPT_STOPVAL_REACHED, FTOL_REACHED, XTOL_REACHED: Convergence\n- NLOPT_MAXEVAL_REACHED, MAXTIME_REACHED: Limits hit\n- NLOPT_FAILURE, INVALID_ARGS, OUT_OF_MEMORY: Errors\n\n**Object-Oriented API:**\n- nlopt_create(algorithm, n): Create optimizer for n dimensions\n- nlopt_set_min/max_objective(): Set objective function\n- nlopt_set_lower/upper_bounds(): Set variable bounds\n- nlopt_add_inequality/equality_constraint(): Add constraints\n- nlopt_set_ftol_rel/abs(), xtol_rel/abs(): Set tolerances\n- nlopt_optimize(opt, x, &f): Run optimization\n- nlopt_destroy(opt): Clean up\n\n**Function Types:**\n- nlopt_func: double f(n, x, grad, data) - objective/constraint\n- nlopt_mfunc: void f(m, result, n, x, grad, data) - vector constraint", "see": ["nlopt.hpp for C++ wrapper", "slsqp.h for SLSQP implementation", "https://nlopt.readthedocs.io/ for full NLopt documentation"], "has_pass2": false}, "nlopt/nlopt-util.h": {"path": "layer-3/oBB/nlopt/nlopt-util.h", "filename": "nlopt-util.h", "file": "nlopt-util.h", "brief": "NLopt utility functions and stopping criteria - bundled with oBB\n\nInternal utilities for NLopt algorithms including floating-point\nhelpers, timing, stopping criteria, and constraint handling.\n\n**Floating-Point Helpers:**\n- nlopt_isinf(), nlopt_isfinite(), nlopt_istiny(), nlopt_isnan()\n\n**Timing:**\n- nlopt_seconds(): Current time in seconds\n- nlopt_time_seed(): Seed based on current time\n\n**nlopt_stopping Struct:**\nStopping criteria state:\n- n: Problem dimension\n- minf_max: Stop if f <= minf_max\n- ftol_rel, ftol_abs: Function tolerance (relative/absolute)\n- xtol_rel, xtol_abs: Variable tolerance\n- nevals, maxeval: Evaluation count/limit\n- maxtime, start: Time limit and start time\n- force_stop: External stop signal\n\n**Stopping Tests:**\n- nlopt_stop_f(): Check function value convergence\n- nlopt_stop_x(), nlopt_stop_dx(): Check variable convergence\n- nlopt_stop_evals(), nlopt_stop_time(): Check limits\n\n**nlopt_constraint Struct:**\nConstraint representation (f <= 0 or f = 0):\n- m: Constraint dimension\n- f/mf: Scalar or vector constraint function\n- tol: Feasibility tolerance", "see": ["nlopt.h for public API", "nlopt-internal.h for optimizer structure"], "has_pass2": false}, "nlopt/nlopt-internal.h": {"path": "layer-3/oBB/nlopt/nlopt-internal.h", "filename": "nlopt-internal.h", "file": "nlopt-internal.h", "brief": "NLopt internal optimizer structure - bundled with oBB\n\nInternal header defining the nlopt_opt_s structure (opaque nlopt_opt).\nNot part of public API - used by algorithm implementations.\n\n**struct nlopt_opt_s (nlopt_opt):**\nComplete optimizer state:\n- algorithm, n: Algorithm type and dimension (immutable)\n- f, f_data: Objective function and user data\n- pre: Optional preconditioner\n- maximize: Nonzero if maximizing\n- lb, ub: Variable bounds (length n)\n- m, fc: Inequality constraints (m constraints)\n- p, h: Equality constraints (p constraints)\n- Stopping criteria: stopval, ftol_rel, ftol_abs, xtol_rel, xtol_abs, maxeval, maxtime\n- force_stop: External stop flag\n- local_opt: Nested local optimizer (for hybrid algorithms)\n- dx: Initial step sizes for derivative-free algorithms\n- work: Algorithm-specific workspace\n- errmsg: Error message buffer\n\n**Global Defaults (deprecated):**\n- nlopt_local_search_alg_deriv/nonderiv\n- nlopt_local_search_maxeval\n- nlopt_stochastic_population", "see": ["nlopt.h for public API", "nlopt-util.h for stopping criteria helpers"], "has_pass2": false}}}}}, "layer-4": {"name": "layer-4", "library_count": 8, "libraries": {"Creme": {"name": "Creme", "file_count": 11, "pass2_count": 0, "files": {"Creme/src/Base/sparse.h": {"path": "layer-4/Creme/Creme/src/Base/sparse.h", "filename": "sparse.h", "file": "Base/sparse.h", "brief": "Sparse LP data structures for Creme solver\n\nCore data structure (sparseLP) for randomized LP solving.\nStores constraint matrix in sparse row format with bounds.", "see": ["linopt.h for one-opt local search", "rtr.h for randomized rounding"], "has_pass2": false}, "Creme/src/Base/chooseblock.h": {"path": "layer-4/Creme/Creme/src/Base/chooseblock.h", "filename": "chooseblock.h", "file": "Base/chooseblock.h", "brief": "Constraint block selection for Creme RTR algorithm\n\nchoose_block(): Select a block of constraints for randomized\nrounding based on violation and satisfaction status.", "see": ["rtr.h for randomized rounding algorithm", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/rtr.h": {"path": "layer-4/Creme/Creme/src/Base/rtr.h", "filename": "rtr.h", "file": "Base/rtr.h", "brief": "Randomized rounding algorithm for Creme LP solver\n\nImplements rtr() to find maximal feasible subsystems (MFS) using\nrandomized iterative rounding. USE_RTR/USE_LOCSRCH mode selection.", "see": ["locsrch.h for local search alternative", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/linopt.h": {"path": "layer-4/Creme/Creme/src/Base/linopt.h", "filename": "linopt.h", "file": "Base/linopt.h", "brief": "One-variable local optimization for Creme\n\nImplements one_opt() for single-variable neighborhood search.", "see": ["sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/move.h": {"path": "layer-4/Creme/Creme/src/Base/move.h", "filename": "move.h", "file": "Base/move.h", "brief": "Variable move operation for Creme local search\n\nmove(): Perform a single variable update in local search,\nadjusting primal values and updating constraint satisfaction.", "see": ["locsrch.h for local search algorithm", "linopt.h for one-opt improvement"], "has_pass2": false}, "Creme/src/Base/lpio.h": {"path": "layer-4/Creme/Creme/src/Base/lpio.h", "filename": "lpio.h", "file": "Base/lpio.h", "brief": "LP file I/O functions for Creme solver\n\nread_problem(): Parse LP problem from file into sparseLP structure.\nprintLP(): Display LP problem contents.\nclearLP(): Deallocate LP memory.", "see": ["sparse.h for sparseLP data structure"], "has_pass2": false}, "Creme/src/Base/misc.h": {"path": "layer-4/Creme/Creme/src/Base/misc.h", "filename": "misc.h", "file": "Base/misc.h", "brief": "Miscellaneous utilities for Creme solver\n\nMemory allocation helpers (reallocate_double/int), bzip2 file I/O,\nmatrix transpose creation, and vector norm computation.", "see": ["lpio.h for LP file reading", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/isfeas.h": {"path": "layer-4/Creme/Creme/src/Base/isfeas.h", "filename": "isfeas.h", "file": "Base/isfeas.h", "brief": "Feasibility checking for Creme LP solutions\n\nisFeas(): Verify if a point satisfies LP constraints.\nReturns number of satisfied constraints and updates satisfaction flags.", "see": ["locsrch.h for local search that uses feasibility checks", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/locsrch.h": {"path": "layer-4/Creme/Creme/src/Base/locsrch.h", "filename": "locsrch.h", "file": "Base/locsrch.h", "brief": "Local search algorithm for Creme LP solver\n\nImplements locsrch() with dvar (delta variable) and frontier structures.", "see": ["rtr.h for randomized rounding alternative", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/init.h": {"path": "layer-4/Creme/Creme/src/Base/init.h", "filename": "init.h", "file": "Base/init.h", "brief": "Solution initialization for Creme randomized LP solver\n\ninit_x(): Initialize primal variable values.\ninit_sat(): Initialize constraint satisfaction tracking (b-Ax, sat flags).\ncalc_lhs(): Compute left-hand side of constraints (with SIMD pragmas).", "see": ["rtr.h for randomized rounding algorithm", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/cmdline.h": {"path": "layer-4/Creme/Creme/src/Base/cmdline.h", "filename": "cmdline.h", "file": "Base/cmdline.h", "brief": "Command-line argument parsing for Creme solver\n\ntpar struct: Defines command-line options (short/long flags, types, defaults).\nreadargs(): Parse argc/argv against option definitions.\nprint_help(): Display usage information.", "see": ["sparse.h for LP data structure"], "has_pass2": false}}}, "GAMSlinks": {"name": "GAMSlinks", "file_count": 19, "pass2_count": 0, "files": {"src/scip/GamsScip.hpp": {"path": "layer-4/GAMSlinks/src/scip/GamsScip.hpp", "filename": "GamsScip.hpp", "file": "scip/GamsScip.hpp", "brief": "GAMS interface to SCIP solver\n\nConnects GAMS to SCIP for constraint integer programming.\nSupports LP, MIP, MINLP, and general constraint optimization.", "see": ["reader_gmo.h for GAMS model reader plugin", "event_solvetrace.h for solve tracing"], "has_pass2": false}, "src/bonmin/GamsBonmin.hpp": {"path": "layer-4/GAMSlinks/src/bonmin/GamsBonmin.hpp", "filename": "GamsBonmin.hpp", "file": "bonmin/GamsBonmin.hpp", "brief": "GAMS interface to Bonmin MINLP solver\n\nConnects GAMS to COIN-OR Bonmin for mixed-integer nonlinear programming.", "see": ["GamsMINLP.hpp for NLP subproblem representation"], "has_pass2": false}, "src/bonmin/GamsMINLP.hpp": {"path": "layer-4/GAMSlinks/src/bonmin/GamsMINLP.hpp", "filename": "GamsMINLP.hpp", "file": "bonmin/GamsMINLP.hpp", "brief": "Bonmin TMINLP implementation for GAMS models\n\nGamsMINLP: Implements Bonmin::TMINLP using GMO for MINLP evaluation.\nHandles variable types, SOS constraints, and branching priorities.", "see": ["GamsBonmin.hpp for Bonmin driver", "GamsCouenne.hpp for Couenne MINLP solver"], "has_pass2": false}, "src/osi/GamsOsiHelper.hpp": {"path": "layer-4/GAMSlinks/src/osi/GamsOsiHelper.hpp", "filename": "GamsOsiHelper.hpp", "file": "osi/GamsOsiHelper.hpp", "brief": "GAMS-OSI data transfer utilities\n\nHelper functions for loading LP problems from GMO to OSI,\nstoring solutions from OSI to GMO, and writing problem files.", "see": ["GamsOsi.hpp for OSI-based solver driver"], "return": "true on success, false on failure", "has_pass2": false}, "src/osi/GamsMessageHandler.hpp": {"path": "layer-4/GAMSlinks/src/osi/GamsMessageHandler.hpp", "filename": "GamsMessageHandler.hpp", "file": "osi/GamsMessageHandler.hpp", "brief": "CoinMessageHandler implementation for GAMS output\n\nRoutes COIN-OR messages through GAMS environment (gev) logging.\nThread-safe via shared mutex for concurrent solver access.", "see": ["GamsOsi.hpp for OSI solver integration", "GamsOsiHelper.hpp for problem loading utilities"], "has_pass2": false}, "src/osi/GamsOsi.hpp": {"path": "layer-4/GAMSlinks/src/osi/GamsOsi.hpp", "filename": "GamsOsi.hpp", "file": "osi/GamsOsi.hpp", "brief": "GAMS interface to OSI-compatible solvers\n\nConnects GAMS to commercial solvers (CPLEX, Gurobi, Mosek, Xpress)\nvia the COIN-OR Open Solver Interface.", "see": ["GamsOsiHelper.hpp, GamsMessageHandler.hpp"], "has_pass2": false}, "src/ipopt/GamsNLP.hpp": {"path": "layer-4/GAMSlinks/src/ipopt/GamsNLP.hpp", "filename": "GamsNLP.hpp", "file": "ipopt/GamsNLP.hpp", "brief": "Ipopt TNLP implementation for GAMS models\n\nGamsNLP: Implements Ipopt::TNLP using GMO (GAMS Modeling Object)\nfor objective/constraint evaluation, Jacobian sparsity, Hessian.", "see": ["GamsIpopt.hpp for solver driver", "GamsJournal.hpp for output handling"], "has_pass2": false}, "src/ipopt/GamsIpopt.hpp": {"path": "layer-4/GAMSlinks/src/ipopt/GamsIpopt.hpp", "filename": "GamsIpopt.hpp", "file": "ipopt/GamsIpopt.hpp", "brief": "GAMS interface to Ipopt NLP solver\n\nConnects GAMS modeling environment to COIN-OR Ipopt.\n\n**GamsIpopt Class:**\n- ipopt: SmartPtr to IpoptApplication\n- nlp: GamsNLP problem representation\n- Warmstart support for sequential solves", "see": ["GamsNLP.hpp for NLP problem representation", "GamsJournal.hpp for message handling"], "has_pass2": false}, "src/ipopt/GamsJournal.hpp": {"path": "layer-4/GAMSlinks/src/ipopt/GamsJournal.hpp", "filename": "GamsJournal.hpp", "file": "ipopt/GamsJournal.hpp", "brief": "Ipopt Journal implementation for GAMS output\n\nRoutes Ipopt logging through GAMS environment (gev) routines.\nSupports configurable status file output levels.", "see": ["GamsIpopt.hpp for main GAMS-Ipopt interface", "GamsNLP.hpp for NLP problem formulation"], "has_pass2": false}, "src/couenne/GamsCouenne.hpp": {"path": "layer-4/GAMSlinks/src/couenne/GamsCouenne.hpp", "filename": "GamsCouenne.hpp", "file": "couenne/GamsCouenne.hpp", "brief": "GAMS interface to Couenne global optimizer\n\nConnects GAMS to COIN-OR Couenne for global optimization of\nnon-convex MINLP problems via spatial branch-and-bound.", "has_pass2": false}, "src/utils/GamsOptionsSpecWriter.hpp": {"path": "layer-4/GAMSlinks/src/utils/GamsOptionsSpecWriter.hpp", "filename": "GamsOptionsSpecWriter.hpp", "file": "GamsOptionsSpecWriter.hpp", "author": "Stefan Vigerske", "has_pass2": false}, "src/soplex/GamsSoPlex.hpp": {"path": "layer-4/GAMSlinks/src/soplex/GamsSoPlex.hpp", "filename": "GamsSoPlex.hpp", "file": "soplex/GamsSoPlex.hpp", "brief": "GAMS interface to SoPlex LP solver\n\nConnects GAMS modeling environment to the ZIB SoPlex\nsimplex-based LP solver.", "has_pass2": false}, "src/cbc/GamsCbcHeurSolveTrace.hpp": {"path": "layer-4/GAMSlinks/src/cbc/GamsCbcHeurSolveTrace.hpp", "filename": "GamsCbcHeurSolveTrace.hpp", "file": "cbc/GamsCbcHeurSolveTrace.hpp", "brief": "CBC heuristic for GAMS solve trace reporting\n\nCbcHeuristic implementation that reports bound progress\nto GAMS solve trace data structure for performance analysis.", "see": ["GamsCbc.hpp for main GAMS-CBC interface"], "has_pass2": false}, "src/cbc/GamsCbc.hpp": {"path": "layer-4/GAMSlinks/src/cbc/GamsCbc.hpp", "filename": "GamsCbc.hpp", "file": "cbc/GamsCbc.hpp", "brief": "GAMS interface to CBC MIP solver\n\nConnects GAMS modeling environment to COIN-OR CBC.\n\n**GamsCbc Class:**\n- gmo/gev: GAMS modeling object and environment handles\n- setupProblem()/setupParameters(): Configure CBC from GAMS\n- Solve trace support for performance analysis", "see": ["GamsCbcHeurSolveTrace.hpp for MIP heuristic callback"], "has_pass2": false}, "src/scip/reader_gmo.h": {"path": "layer-4/GAMSlinks/src/scip/reader_gmo.h", "filename": "reader_gmo.h", "file": "reader_gmo.h", "brief": "GMO file reader", "author": "Stefan Vigerske", "has_pass2": false}, "src/scip/event_solvetrace.h": {"path": "layer-4/GAMSlinks/src/scip/event_solvetrace.h", "filename": "event_solvetrace.h", "file": "event_solvetrace.h", "brief": "event handler to write GAMS solve trace file", "author": "Stefan Vigerske", "has_pass2": false}, "src/amplsolver/convert_nl.h": {"path": "layer-4/GAMSlinks/src/amplsolver/convert_nl.h", "filename": "convert_nl.h", "return": "buf", "has_pass2": false}, "src/utils/GamsLicensing.h": {"path": "layer-4/GAMSlinks/src/utils/GamsLicensing.h", "filename": "GamsLicensing.h", "return": "True if a GAMS/CPLEX license is available, false otherwise", "has_pass2": false}, "src/utils/GamsSolveTrace.h": {"path": "layer-4/GAMSlinks/src/utils/GamsSolveTrace.h", "filename": "GamsSolveTrace.h", "return": "0, if successful; nonzero, if failure", "has_pass2": false}}}, "Gravity": {"name": "Gravity", "file_count": 27, "pass2_count": 1, "files": {"thirdparty/Ipopt/include/coin/AmplTNLP.hpp": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/AmplTNLP.hpp", "filename": "AmplTNLP.hpp", "return": "a pointer to a char* with the name of the stub", "has_pass2": false}, "thirdparty/Ipopt/include/coin/IpIpoptApplication.hpp": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/IpIpoptApplication.hpp", "filename": "IpIpoptApplication.hpp", "return": "Returns whether non-ipopt exceptions were rethrown before.", "has_pass2": false}, "include/gravity/param.h": {"path": "layer-4/Gravity/include/gravity/param.h", "filename": "param.h", "file": "gravity/param.h", "brief": "Parameters (data constants) indexed over sets\n\nParameters hold fixed data values used in optimization models.\n\n**param_ Base Class:**\n- _name: Parameter name\n- _id: Unique identifier\n- _indices: Index set this parameter is defined over\n- _intype: Internal storage type (double_, integer_, etc.)\n\n**Complex Number Support:**\n- _is_conjugate: Complex conjugate flag\n- _is_sqrmag: Magnitude squared\n- _is_angle: Phase angle\n- _real, _imag: Real/imaginary part pointers\n- _mag, _ang: Magnitude/angle pointers\n\n**Ipopt Integration:**\n- _l_dual, _u_dual: Dual values for bound constraints\n- _off: On/off flags per instance\n\n**param<type> Template Class:**\n- _val: Vector of values\n- _range: Min/max bounds\n- Indexed access via operator()\n\n**Usage:**\n```cpp\nparam<> cost(\"cost\");\ncost.in(Nodes);                    // Index over Nodes\ncost.set_val(node_costs);          // Set values\ncost(\"node1\");                     // Access by key\n```", "see": ["gravity/var.h for variables (params with bounds)", "gravity/constant.h for scalar constants"], "has_pass2": false}, "include/gravity/types.h": {"path": "layer-4/Gravity/include/gravity/types.h", "filename": "types.h", "file": "gravity/types.h", "brief": "Core type definitions and index set infrastructure for Gravity\n\nFoundational enums and classes for the Gravity modeling language.\n\n**Convexity Tracking:**\n- Convexity: linear_, convex_, concave_, undet_\n- Sign: neg_, non_pos_, zero_, non_neg_, pos_, unknown_\n- Enables automatic convexity detection in expressions\n\n**Function Classification:**\n- FType: const_, lin_, quad_, pol_, nlin_ (constraint complexity)\n- MType: lin_m, quad_m, pol_m, nlin_m (model type)\n- OperatorType: All mathematical operations (+, -, *, /, ^, sin, cos, etc.)\n\n**Index Set System (indices class):**\n- Defines sets over which variables/constraints are declared\n- Supports: ordered pairs, time expansion, matrix indexing\n- Operations: union, combine, exclude, filter\n- Used for: \"for all i in Nodes, j in Arcs\" style modeling\n\n**Space Types:**\n- R: Real numbers (R^n)\n- R_p: Positive reals\n- C: Complex numbers", "see": ["gravity/var.h for variable declarations over indices", "gravity/constraint.h for constraints over index sets", "gravity/model.h for combining all components"], "return": "index set of added indices.", "has_pass2": false}, "include/gravity/GurobiProgram.h": {"path": "layer-4/Gravity/include/gravity/GurobiProgram.h", "filename": "GurobiProgram.h", "file": "gravity/GurobiProgram.h", "brief": "Gurobi solver interface for LP/MIP/QP problems\n\nAdapts Gravity models to the Gurobi C++ API.\n\n**GurobiProgram Class:**\n- grb_env: Gurobi environment\n- grb_mod: Gurobi model\n- _grb_vars: Mapping from Gravity vars to GRBVar\n- _model: Pointer to Gravity Model\n\n**Model Building:**\n- prepare_model(): Initial conversion from Gravity\n- fill_in_grb_vmap(): Create GRBVar for each variable\n- create_grb_constraints(): Add constraints (linear/quadratic)\n- set_grb_objective(): Set objective function\n\n**Solving:**\n- solve(relax, mipgap): Optimize with optional LP relaxation\n- relax_model(): Remove integrality constraints\n- update_solution(): Copy solution back to Gravity model\n\n**Supported Problem Types:**\n- LP: Linear programming\n- MIP: Mixed-integer programming\n- QP: Quadratic programming (convex)\n- MIQP: Mixed-integer QP", "see": ["gravity/solver.h for unified solver dispatch", "gravity/model.h for Gravity model definition"], "has_pass2": false}, "include/gravity/poly.h": {"path": "layer-4/Gravity/include/gravity/poly.h", "filename": "poly.h", "file": "gravity/poly.h", "brief": "Polynomial term representations (linear, quadratic, polynomial)\n\nTerm classes for building polynomial expressions.\n\n**lterm (Linear Term):**\n- _coef: Coefficient (constant or param)\n- _p: Variable/parameter pointer\n- _sign: True if positive, false if negative\n- Represents: ±coef × p\n\n**qterm (Quadratic Term):**\n- _coef: Coefficient\n- _p1, _p2: Two variable/parameter pointers\n- _sign: Sign flag\n- Represents: ±coef × p1 × p2\n\n**pterm (Polynomial Term):**\n- _coef: Coefficient\n- _l: List of (param, power) pairs\n- Represents: coef × ∏(pᵢ^kᵢ)\n\n**Usage in Functions:**\n```cpp\nfunc.add_lterm(coef, x);      // coef*x\nfunc.add_qterm(coef, x, y);   // coef*x*y\n```", "see": ["gravity/func.h for functions using these terms", "gravity/expr.h for expression tree nodes"], "has_pass2": false}, "include/gravity/utils.h": {"path": "layer-4/Gravity/include/gravity/utils.h", "filename": "utils.h", "file": "gravity/utils.h", "brief": "Utility functions for timing, printing, and sign operations\n\nGeneral utilities used throughout the Gravity library.\n\n**Timing:**\n- get_wall_time(): Wall clock time in seconds\n- get_cpu_time(): CPU time in seconds\n\n**Printing:**\n- clean_print(): Format value with sign for display\n- RESET macro: Terminal color reset code\n\n**Parallelism:**\n- bounds(parts, mem): Split memory/indices into chunks\n  for parallel processing\n\n**Sign Arithmetic:**\n- reverse(Sign): Negate sign\n- sign_add(s1, s2): Result sign of sum\n- sign_product(s1, s2): Result sign of product\n\n**Time Indexing:**\n- time(p1, p2): Create time-indexed set [p1, p2]\n- time(idx1, ...): Variadic time index construction\n\n**Complex Number Support:**\n- Comparison operators for Cpx type", "see": ["gravity/types.h for Sign enum"], "has_pass2": false}, "include/gravity/constraint.h": {"path": "layer-4/Gravity/include/gravity/constraint.h", "filename": "constraint.h", "file": "gravity/constraint.h", "brief": "Constraint classes with type, duals, and lazy evaluation\n\nConstraints are functions with a constraint type (<=, >=, ==) and bounds.\n\n**Constraint_ Base Class:**\n- _id: Unique constraint identifier\n- _ctype: ConstraintType (leq, geq, eq)\n- _dual: Lagrange multipliers at KKT point\n- _jac_cstr_idx: First index in Jacobian nonzeros\n\n**Lazy Constraint Support:**\n- _all_lazy: Flag if all instances are lazy\n- _lazy: Per-instance lazy flags\n- _violated: Tracks which instances are violated\n- Used for cutting plane methods\n\n**On/Off Constraints:**\n- _onCoef, _offCoef: Coefficient vectors\n- For indicator constraints or big-M reformulations\n\n**Constraint<type> Template:**\n- Inherits from Constraint_ and func<type>\n- Combines function expression with constraint metadata\n\n**Creating Constraints:**\n```cpp\nConstraint<> c(\"flow_balance\", eq);\n// Or via operator overloading:\nauto c = (sum(x) <= 10);  // Creates leq constraint\nauto c = (y == 5);         // Creates eq constraint\n```", "see": ["gravity/func.h for expression functionality", "gravity/model.h for adding constraints to models"], "return": "True if line search successfully solved\n     The function assumes that the current value stored in vars is the outer point.\n     Interior and outer point classification depends on constraint type (\\geq 0 or \\leq 0) as input by ctype", "has_pass2": false}, "include/gravity/Auxiliary.h": {"path": "layer-4/Gravity/include/gravity/Auxiliary.h", "filename": "Auxiliary.h", "file": "gravity/Auxiliary.h", "brief": "Base class for auxiliary network objects (generators, loads)\n\nProvides a base class for objects attached to network nodes.\n\n**aux Class:**\n- _active: Whether this auxiliary is active in the model\n- _name: Identifier string\n- _phases: Set of electrical phases (for power systems)\n\n**Phase Support:**\n- set_phases(str): Parse phase string (e.g., \"1,2,3\")\n- has_phase(ph): Check if auxiliary has given phase\n\n**Usage in Power Systems:**\n- Generators attached to buses\n- Loads at network nodes\n- Capacitors, transformers, etc.", "see": ["gravity/Net.h for network container", "gravity/Node.h for bus/node representation"], "has_pass2": false}, "include/gravity/expr.h": {"path": "layer-4/Gravity/include/gravity/expr.h", "filename": "expr.h", "file": "gravity/expr.h", "brief": "Expression tree nodes for unary and binary operations\n\nExpressions are the building blocks for functions and constraints.\n\n**expr<type> Base Class:**\n- _coef: Coefficient multiplying expression\n- _all_convexity: Convexity type (linear, convex, concave, undet)\n- _all_sign: Sign type (pos, neg, zero, unknown)\n- _range: (min, max) value bounds\n- _to_str: String representation\n\n**uexpr<type> (Unary Expression):**\n- _otype: Operator type (cos_, sin_, exp_, log_, sqrt_, abs_, etc.)\n- _son: Child expression/constant\n- Used for: sin(x), cos(x), exp(x), log(x), sqrt(x), abs(x)\n\n**bexpr<type> (Binary Expression):**\n- _otype: Operator type (plus_, minus_, product_, div_, power_)\n- _lson, _rson: Left and right children\n- Used for: x+y, x-y, x*y, x/y, x^n\n\n**Expression Tree:**\n- Trees are built via operator overloading\n- Convexity propagates through operations\n- Sign analysis for bound tightening", "see": ["gravity/func.h for complete function class", "gravity/types.h for OperatorType enum"], "has_pass2": false}, "include/gravity/Net.h": {"path": "layer-4/Gravity/include/gravity/Net.h", "filename": "Net.h", "file": "gravity/Net.h", "brief": "Graph/network data structure for power systems and network optimization\n\nRepresents graphs with nodes, arcs, and cycles for network-based models.\n\n**Net Class:**\n- nodes: Vector of Node pointers\n- arcs: All arcs (existing + potential)\n- _exist_arcs: Only existing arcs\n- conting_arcs: Contingency arcs (for N-1 analysis)\n\n**Indexing Structures:**\n- nodeID: Map from node name to Node*\n- arcID: Map from (src_name, dest_name) to Arc*\n- arcMap: Map from arc name to Arc*\n\n**Node Pairs (for SDP relaxations):**\n- _node_pairs: Bus pairs for standard formulation\n- _node_pairs_chord: Bus pairs in chordal completion\n\n**Cycle Basis (for Kirchhoff constraints):**\n- cycle_basis: Vector of Path* forming basis\n- horton_net: Subnetwork for Horton algorithm\n\n**Tree Decomposition:**\n- _bags: Cliques from chordal extension\n- Used for SDP rank-1 constraints\n\n**Graph Algorithms:**\n- clone(), clone_undirected(): Copy graphs\n- Cycle detection, shortest paths\n- Chordal completion", "see": ["gravity/Node.h for node data", "gravity/Arc.h for arc/edge data", "gravity/Path.h for paths/cycles"], "return": "the id of the node removed", "has_pass2": false}, "include/gravity/IpoptProgram.h": {"path": "layer-4/Gravity/include/gravity/IpoptProgram.h", "filename": "IpoptProgram.h", "file": "gravity/IpoptProgram.h", "brief": "Ipopt solver interface implementing TNLP callbacks\n\nAdapts Gravity models to Ipopt's TNLP (Templated NLP) interface.\n\n**IpoptProgram<type> Class:**\n- Inherits from Ipopt::TNLP and Program<type>\n- _model: Pointer to Gravity Model\n\n**Required TNLP Callbacks:**\n- get_nlp_info(): Return problem dimensions (n, m, nnz_jac, nnz_hess)\n- get_bounds_info(): Variable and constraint bounds\n- get_starting_point(): Initial x, z_L, z_U, lambda\n- eval_f(): Objective function value\n- eval_grad_f(): Objective gradient\n- eval_g(): Constraint values\n- eval_jac_g(): Jacobian values and structure\n- eval_h(): Hessian of Lagrangian\n- finalize_solution(): Copy solution back to model\n\n**Sparsity Pattern:**\n- First call to eval_jac_g/eval_h: return structure (iRow, jCol)\n- Subsequent calls: return values only\n- Gravity tracks via _first_call_jac, _first_call_hess\n\n**Solution Recovery:**\n- finalize_solution() copies x values to model variables\n- Retrieves dual values (lambda) for constraints\n- Retrieves bound multipliers (z_L, z_U)", "algorithm": "Interior Point Method via Ipopt", "see": ["gravity/solver.h for unified solver interface", "layer-2/Ipopt/ for Ipopt implementation details"], "has_pass2": true}, "include/gravity/rapidcsv.h": {"path": "layer-4/Gravity/include/gravity/rapidcsv.h", "filename": "rapidcsv.h", "brief": "Datastructure holding parameters controlling how invalid numbers (including\n           empty strings) should be handled.", "param": ["pHasDefaultConverter  specifies if conversion of non-numerical strings shall be\n                               converted to a default numerical value, instead of causing\n                               an exception to be thrown (default).", "pDefaultFloat         floating-point default value to represent invalid numbers.", "pDefaultInteger       integer default value to represent invalid numbers.", "pConverterParams      specifies how conversion of non-numerical values to\n                               numerical datatype shall be handled.", "pVal                  numerical value", "pStr                  output string", "pVal                  numerical value", "pStr                  output string", "pVal                  string", "pStr                  string", "pVal                  string", "pStr                  string", "pColumnNameIdx        specifies the zero-based row index of the column labels, setting\n                               it to -1 prevents column lookup by label name, and gives access\n                               to all rows as document data.", "pRowNameIdx           specifies the zero-based column index of the row labels, setting\n                               it to -1 prevents row lookup by label name, and gives access\n                               to all columns as document data.", "pSeparator            specifies the column separator (default ',').", "pTrim                 specifies whether to trim leading and trailing spaces from\n                               cells read.", "pHasCR                specifies whether a new document (i.e. not an existing document read)\n                               should use CR/LF instead of only LF (default is to use standard\n                               behavior of underlying platforms - CR/LF for Win, and LF for others).", "pPath                 specifies the path of an existing CSV-file to populate the Document\n                               data with.", "pLabelParams          specifies which row and column should be treated as labels.", "pSeparatorParams      specifies which field and row separators should be used.", "pConverterParams      specifies how invalid numbers (including empty strings) should be\n                               handled.", "pStream               specifies an input stream to read CSV data from.", "pLabelParams          specifies which row and column should be treated as labels.", "pSeparatorParams      specifies which field and row separators should be used.", "pConverterParams      specifies how invalid numbers (including empty strings) should be\n                               handled.", "pDocument             specifies the Document instance to copy.", "pPath                 specifies the path of an existing CSV-file to populate the Document\n                               data with.", "pPath                 optionally specifies the path where the CSV-file will be created\n                               (if not specified, the original path provided when creating or\n                               loading the Document data will be used).", "pStream               specifies an output stream to write the data to.", "pColumnIdx            zero-based column index.", "pColumnName           column label name.", "pColumnIdx            zero-based column index.", "pColumn               vector of column data.", "pColumnName           column label name.", "pColumn               vector of column data.", "pColumnIdx            zero-based column index.", "pColumnName           column label name.", "pRowIdx               zero-based row index.", "pRowName              row label name.", "pRowIdx               zero-based row index.", "pRow                  vector of row data.", "pRowName              row label name.", "pRow                  vector of row data.", "pRowIdx               zero-based row index.", "pRowName              row label name.", "pColumnIdx            zero-based column index.", "pRowIdx               zero-based row index.", "pColumnName           column label name.", "pRowName              row label name.", "pColumnName           column label name.", "pRowIdx               zero-based row index.", "pColumnIdx            zero-based column index.", "pRowName              row label name.", "pRowIdx               zero-based row index.", "pColumnIdx            zero-based column index.", "pCell                 cell data.", "pColumnName           column label name.", "pRowName              row label name.", "pCell                 cell data.", "pColumnIdx            zero-based column index.", "pColumnIdx            zero-based column index.", "pColumnName           column name.", "pRowIdx               zero-based column index.", "pRowIdx               zero-based row index.", "pRowName              row name."], "has_pass2": false}, "include/gravity/func.h": {"path": "layer-4/Gravity/include/gravity/func.h", "filename": "func.h", "file": "gravity/func.h", "brief": "Expression functions with automatic differentiation and convexity tracking\n\nThe func class represents mathematical expressions with symbolic analysis.\n\n**func_ Base Class:**\n- _ftype: Function type (const_, lin_, quad_, pol_, nlin_)\n- _return_type: Numeric type (double_, integer_, binary_, complex_)\n- _all_convexity: Convexity of all instances (linear_, convex_, concave_, undet_)\n- _all_sign: Sign of all instances (pos_, neg_, zero_, unknown_)\n\n**Expression Components:**\n- _cst: Constant term\n- _lterms: Linear terms (map<string, lterm>)\n- _qterms: Quadratic terms (map<string, qterm>)\n- _pterms: Polynomial terms (map<string, pterm>)\n- _vars: Variables appearing in expression\n- _params: Parameters appearing in expression\n\n**Derivative Information:**\n- Symbolic differentiation via get_stored_derivative()\n- _hess_link: Sparsity pattern of Hessian\n- _nnz_j, _nnz_h: Jacobian/Hessian nonzeros\n\n**Operator Overloading:**\n- Arithmetic: +, -, *, /, ^\n- Transcendental: sin, cos, exp, log, sqrt\n- Comparison: <, <=, ==, >=, > (for constraints)\n\n**Convexity Rules:**\n- sum(linear) → linear\n- x² → convex, -x² → concave\n- exp(linear) → convex, log(linear) → concave\n- Composition rules applied automatically", "see": ["gravity/expr.h for expression tree nodes", "gravity/constraint.h for constraints built from functions"], "return": "a vector of monomials of degree d using the variables in the current function", "has_pass2": false}, "include/gravity/constant.h": {"path": "layer-4/Gravity/include/gravity/constant.h", "filename": "constant.h", "file": "gravity/constant.h", "brief": "Scalar and vector constants with type tracking\n\nBase constant class and typed constant templates for numeric values.\n\n**constant_ Base Class:**\n- _type: CType enum (binary_c, integer_c, double_c, complex_c, etc.)\n- _is_transposed: Transpose flag for matrix operations\n- _is_vector: True if vector/matrix\n- _dim[2]: Dimensions (rows, cols)\n- _polar: Complex polar representation flag\n\n**CType Classification:**\n- Scalar types: binary_c, short_c, integer_c, float_c, double_c, long_c\n- Special types: par_c (param), var_c (variable), func_c (function)\n- Expression types: uexp_c (unary), bexp_c (binary)\n\n**constant<type> Template:**\n- Holds single typed value\n- Implicit conversion from numeric types\n- String conversion with precision\n\n**Helper Functions:**\n- to_string_with_precision(): Format numbers\n- unit<type>(): Multiplicative identity (1)\n- zero<type>(): Additive identity (0)\n\n**Complex Support:**\n- Cpx typedef for complex<double>\n- Polar/rectangular representation", "see": ["gravity/param.h for indexed parameters", "gravity/var.h for optimization variables"], "return": "a string with the specified precision.", "has_pass2": false}, "include/gravity/CplexProgram.h": {"path": "layer-4/Gravity/include/gravity/CplexProgram.h", "filename": "CplexProgram.h", "file": "gravity/CplexProgram.h", "brief": "IBM CPLEX solver interface for LP/MIP/QP problems\n\nAdapts Gravity models to the IBM ILOG CPLEX C++ API.\n\n**CplexProgram Class:**\n- _cplex_env: IloEnv (CPLEX environment)\n- _cplex_model: IloModel (CPLEX model)\n- _cplex_vars: Mapping from Gravity vars to IloNumVarArray\n\n**Model Building:**\n- prepare_model(): Initial conversion from Gravity\n- fill_in_cplex_vars(): Create IloNumVar for each variable\n- create_cplex_constraints(): Add constraints\n- set_cplex_objective(): Set objective function\n\n**Solving:**\n- solve(relax, mipgap): Optimize with optional LP relaxation\n- warm_start(): Initialize from current solution\n- relax_model(): Remove integrality constraints\n\n**Callback Support:**\n- _cplex_contextmask: Context for callback location\n- create_callback(): Set up user callbacks", "see": ["gravity/solver.h for unified solver dispatch", "gravity/model.h for Gravity model definition"], "has_pass2": false}, "include/gravity/Arc.h": {"path": "layer-4/Gravity/include/gravity/Arc.h", "filename": "Arc.h", "file": "gravity/Arc.h", "brief": "Directed arc/edge for network optimization models\n\nRepresents an arc (edge) connecting two nodes in a graph.\n\n**Arc Class:**\n- _id: Numeric identifier\n- _name: String name\n- _src, _dest: Source and destination Node pointers\n- _weight: Arc weight/cost\n- _len: Arc length\n\n**Power System Attributes:**\n- _is_transformer: True if arc represents a transformer\n- _phases: Set of phases (for 3-phase systems)\n- _parallel: True if parallel arc exists\n\n**Planning Attributes:**\n- _active: Whether arc is active in model\n- _expansion: True if potential expansion arc\n- _imaginary: Imaginary arc for algorithm use\n\n**Graph Algorithm Support:**\n- in_cycle: True if arc is in a cycle\n- horton_path: Pointer to Horton path through this arc\n- _intersection: Common neighbors of src and dest\n- _intersection_clique: For clique tree algorithms\n\n**Methods:**\n- neighbour(Node* n): Get other endpoint\n- clone(): Deep copy arc", "see": ["gravity/Node.h for node data", "gravity/Path.h for paths/cycles", "gravity/Net.h for graph container"], "has_pass2": false}, "include/gravity/MosekProgram.h": {"path": "layer-4/Gravity/include/gravity/MosekProgram.h", "filename": "MosekProgram.h", "file": "gravity/MosekProgram.h", "brief": "Mosek Fusion API interface for conic optimization\n\nAdapts Gravity models to Mosek's Fusion API for conic programming.\n\n**MosekProgram Class:**\n- _mosek_model: Mosek Fusion Model\n- _mosek_vars: Vector of Mosek Variable::t handles\n\n**Conic Capabilities:**\n- Second-order cone (SOCP)\n- Semidefinite programming (SDP)\n- Exponential cone\n\n**Expression Building:**\n- form_Fx(qterms): Build quadratic matrix form F*x\n- create_lin_expr(lterms, cst): Linear expression construction\n\n**Model Building:**\n- fill_in_mosek_vars(): Create Mosek variables\n- create_mosek_constraints(): Add conic constraints\n- set_mosek_objective(): Set objective\n\n@note Mosek Fusion is designed specifically for conic optimization,\n      making it ideal for SOCP and SDP problems in power systems.", "see": ["gravity/solver.h for unified solver dispatch"], "has_pass2": false}, "include/gravity/solver.h": {"path": "layer-4/Gravity/include/gravity/solver.h", "filename": "solver.h", "file": "gravity/solver.h", "brief": "Unified solver interface for multiple optimization backends\n\nDispatches Gravity models to various solver implementations.\n\n**Supported Solvers (via SolverType enum):**\n- ipopt: Interior point NLP (nonlinear)\n- gurobi: Commercial LP/QP/MIP\n- cplex: Commercial LP/QP/MIP\n- bonmin: Open-source MINLP (NLP-based B&B)\n- mosek: Commercial conic/SDP\n- clp: COIN-OR simplex LP\n- highs: Open-source LP/MIP\n- sdpa: SDP solver\n\n**solver<type> Template Class:**\n- _model: Pointer to Gravity Model\n- _prog: Solver-specific program adapter\n- _stype: Which solver to use\n- _tol: Solver tolerance (default 1e-6)\n\n**Solver Options:**\n- set_option(string, string): String options\n- set_option(string, int): Integer options\n- set_option(string, double): Numeric options\n\n**Usage:**\n```cpp\nModel<> m;\n// ... build model ...\nsolver<> s(m, ipopt);\ns.run();\n```\n\n**Conditional Compilation:**\n- USE_IPOPT, USE_GUROBI, USE_CPLEX, etc.\n- SolverNotAvailable() throws if solver not compiled in", "see": ["gravity/model.h for model building", "gravity/IpoptProgram.h for Ipopt adapter", "gravity/GurobiProgram.h for Gurobi adapter"], "has_pass2": false}, "include/gravity/var.h": {"path": "layer-4/Gravity/include/gravity/var.h", "filename": "var.h", "file": "gravity/var.h", "brief": "Decision variables with bounds for optimization models\n\nVariables are parameters with lower and upper bounds that the solver optimizes.\n\n**var<type> Template Class:**\n- Inherits from param<type> (shares indexing, values storage)\n- Adds _lb, _ub as func<type> (bounds can be expressions)\n- Supports: double, int, bool (binary), complex<double>\n\n**Bound Types:**\n- Unbounded: var(\"x\") - uses numeric_limits\n- Non-negative: var(\"x\", non_neg_)\n- Non-positive: var(\"x\", non_pos_)\n- Bounded: var(\"x\", lb, ub)\n- Indexed bounds: var(\"x\", lb_param, ub_param) over indices\n\n**Lifted Variables:**\n- _lift flag for McCormick/RLT linearization\n- _original_vars: pointers to variables being lifted\n- _lift_lb, _lift_ub: whether lifted var needs bounding functions\n\n**Mosek/SDP Support:**\n- _in_q_cone: variable in quadratic cone\n- _psd: positive semidefinite matrix variable\n\n**Usage Example:**\n```cpp\nvar<> x(\"x\", 0, 1);           // x in [0,1]\nvar<int> y(\"y\", 0, 10);       // integer y in [0,10]\nvar<bool> z(\"z\");             // binary z\nx.in(Nodes);                   // x indexed over Nodes\n```", "see": ["gravity/param.h for base parameter class", "gravity/model.h for adding variables to models"], "return": "index set of added indices.", "has_pass2": false}, "include/gravity/model.h": {"path": "layer-4/Gravity/include/gravity/model.h", "filename": "model.h", "file": "gravity/model.h", "brief": "Main optimization model class combining variables, constraints, objective\n\nThe Model class is the central container for building optimization problems.\n\n**Model<type> Template Class:**\n- type: Numeric type (default double)\n- _vars: Map of all variables\n- _cons: Map of all constraints\n- _obj: Objective function\n- _objt: Objective type (minimize/maximize)\n\n**Building a Model:**\n```cpp\nModel<> m(\"MyModel\");\nvar<> x(\"x\", 0, 10);              // Variable x in [0,10]\nm.add(x.in(Nodes));                // Add indexed variable\nm.add(sum(x) <= 100);              // Add constraint\nm.min(sum(cost*x));                // Set objective\n```\n\n**Constraint Handling:**\n- Indexed constraints: one func<> generates multiple rows\n- Lazy constraints via _lazy vector\n- Parallel constraint evaluation (compute_constrs)\n- Parallel Jacobian computation (compute_jac)\n\n**Derivative Computation:**\n- Automatic symbolic differentiation\n- Jacobian sparsity pattern (_jac_cstr_idx)\n- Hessian sparsity pattern (_hess_link)\n\n**Model Types (MType):**\n- lin_m: Linear (LP/MIP)\n- quad_m: Quadratic (QP/MIQP)\n- pol_m: Polynomial\n- nlin_m: General nonlinear (NLP/MINLP)\n\n**I/O Support:**\n- MPS format (via CoinUtils)\n- NL format (AMPL, via mp library)", "see": ["gravity/solver.h for solving models", "gravity/constraint.h for constraint types"], "return": "the linearized constraint\n         @note This function will add constraints linking the lifted variables to the original ones, if a variable's partition is greater than 1, it will also add the disjunctive constraints corresponding to the partitionning of the variables.", "has_pass2": false}, "include/gravity/Path.h": {"path": "layer-4/Gravity/include/gravity/Path.h", "filename": "Path.h", "file": "gravity/Path.h", "brief": "Graph path/cycle representation for network algorithms\n\nRepresents a sequence of nodes forming a path or cycle in a network.\n\n**Path Class:**\n- nodes: List of Node pointers in sequence\n\n**Methods:**\n- source_dest(n1, n2): Check if path connects n1 to n2\n- length(): Number of edges in path\n- cycle(): True if path is a closed cycle\n- clone(): Deep copy path\n- to_str(): String representation\n\n**Usage in Gravity:**\n- cycle_basis in Net: Fundamental cycles for Kirchhoff constraints\n- horton_path in Arc: Horton algorithm shortest paths\n- Power flow loop constraints", "see": ["gravity/Net.h for graph container", "gravity/Arc.h for arc/edge data"], "has_pass2": false}, "include/gravity/Node.h": {"path": "layer-4/Gravity/include/gravity/Node.h", "filename": "Node.h", "file": "gravity/Node.h", "brief": "Graph node/vertex for network optimization models\n\nRepresents a node in network graphs (power systems, transportation, etc.).\n\n**Node Class:**\n- _name: Unique string identifier\n- _id: Numeric ID within container\n- _type_name: \"Nodes\" (for indexing)\n- _active: Whether node is active in model\n\n**Connectivity:**\n- branches: Vector of incident Arc pointers\n- degree(): Number of incident arcs\n- addArc(), removeArc(): Modify adjacency\n\n**Graph Algorithms:**\n- explored: BFS/DFS traversal flag\n- cycle: True if node is in a cycle\n- predecessor: Parent in BFS tree\n- distance: Distance from source in BFS\n- fill_in: Edges needed to make neighbors a clique\n\n**Power System Extensions:**\n- _phases: Set of phases (for 3-phase systems)", "see": ["gravity/Arc.h for edge/arc data", "gravity/Net.h for graph container"], "has_pass2": false}, "include/gravity/HiGHSProgram.h": {"path": "layer-4/Gravity/include/gravity/HiGHSProgram.h", "filename": "HiGHSProgram.h", "file": "gravity/HiGHSProgram.h", "brief": "HiGHS solver interface for LP/MIP problems\n\nAdapts Gravity models to the HiGHS open-source LP/MIP solver.\n\n**HiGHSProgram Class:**\n- Highs_mod: HighsModel (HiGHS model container)\n- Highs_inst: Highs solver instance\n\n**Model Building:**\n- prepare_model(): Initial conversion from Gravity\n- fill_in_var_map(): Create variable mappings\n- create_constraints(): Add linear constraints\n- set_objective(): Set linear objective\n\n**Solving:**\n- solve(relax, mipgap): Optimize with optional LP relaxation\n- update_solution(): Copy solution back to Gravity model\n- relax_model(): Remove integrality constraints\n\n**Supported Problem Types:**\n- LP: Linear programming\n- MIP: Mixed-integer programming\n\n@note HiGHS is an open-source solver that excels at large-scale LP\n      and MIP problems with state-of-the-art performance.", "see": ["gravity/solver.h for unified solver dispatch"], "has_pass2": false}, "thirdparty/mp/presolve.h": {"path": "layer-4/Gravity/thirdparty/mp/presolve.h", "filename": "presolve.h", "file": "presolve.h\n  Implementation of value presolver", "has_pass2": false}, "thirdparty/Ipopt/include/coin/PardisoLoader.h": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/PardisoLoader.h", "filename": "PardisoLoader.h", "param": ["libname The name under which the Pardiso lib can be found, or NULL to use a default name (libpardiso.SHAREDLIBEXT).", "msgbuf A buffer where we can store a failure message. Assumed to be NOT NULL!", "msglen Length of the message buffer."], "return": "Zero on success, nonzero on failure.", "has_pass2": false}, "thirdparty/Ipopt/include/coin/HSLLoader.h": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/HSLLoader.h", "filename": "HSLLoader.h", "see": ["LSL_isMA27available", "LSL_isMA28available", "LSL_isMA57available", "LSL_isMA77available", "LSL_isMA86available", "LSL_isMA97available", "LSL_isMC19available"], "param": ["libname The name under which the HSL lib can be found, or NULL to use a default name (libhsl.SHAREDLIBEXT).", "msgbuf A buffer where we can store a failure message. Assumed to be NOT NULL!", "msglen Length of the message buffer."], "return": "Zero on success, nonzero on failure.", "has_pass2": false}}}, "HiGHS": {"name": "HiGHS", "file_count": 124, "pass2_count": 5, "files": {"highs/Highs.h": {"path": "layer-4/HiGHS/highs/Highs.h", "filename": "Highs.h", "file": "Highs.h", "brief": "Main HiGHS solver class - high-performance LP/MIP/QP optimization\n\nHiGHS (High-performance Interior point and Gradient descent Solvers) is\nan open-source solver for LP, MIP, and convex QP problems.\n\n**Highs Class (Main API):**\nPrimary interface for model input, solving, and solution retrieval:\n- passModel(): Load LP/QP/MIP from HighsModel, HighsLp, or raw arrays\n- run(): Solve the incumbent model\n- getSolution(), getBasis(): Retrieve solution and basis\n- getModelStatus(): Check optimization result\n\n**Solving Capabilities:**\n- LP: Dual/primal simplex (HEkk) or interior point (IPX)\n- MIP: Branch-and-cut with presolve, cuts, and heuristics\n- QP: Convex quadratic programming via interior point or active set\n\n**Model Modification:**\n- addCol/addRow, deleteCols/deleteRows: Incremental model building\n- changeColBounds, changeRowBounds, changeColCost: Hot-start friendly\n- changeCoeff: Modify individual matrix coefficients\n\n**Basis Operations:**\n- getBasisInverseRow/Col: Access B^{-1} for advanced use\n- getBasisSolve/getBasisTransposeSolve: Solve B*x=b or B'*x=b\n- getReducedRow/Col: Compute B^{-1}*A columns\n\n**Options and Info:**\n- setOptionValue/getOptionValue: Configure solver behavior\n- getInfo: Retrieve solve statistics (iterations, time, etc.)", "algorithm": "MIP Solving:\n  - Presolve: probing, clique detection, coefficient strengthening\n  - Cuts: Gomory, MIR, clique, zero-half, knapsack cover\n  - Branching: reliability branching with pseudocost initialization\n  - Heuristics: RINS, local search, rounding, feasibility pump", "complexity": "LP: O(m·n·iterations) simplex, O(n³) per IPM iteration\n  MIP: exponential worst-case, highly structure-dependent", "ref": ["Huangfu, Q. and Hall, J.A.J. (2018). \"Parallelizing the dual revised\n  simplex method\". Math. Prog. Computation 10:119-142."], "see": ["lp_data/HighsLp.h for LP data structure", "lp_data/HighsOptions.h for configuration options", "simplex/HEkk.h for simplex implementation", "ipm/ipx/lp_solver.h for interior point solver", "mip/HighsMipSolver.h for MIP solver"], "has_pass2": true}, "check/Avgas.h": {"path": "layer-4/HiGHS/check/Avgas.h", "filename": "Avgas.h", "file": "simplex/Avgas.h", "brief": "Utilities for tests with AVGAS", "has_pass2": false}, "check/SpecialLps.h": {"path": "layer-4/HiGHS/check/SpecialLps.h", "filename": "SpecialLps.h", "file": "simplex/SpeciaLps.h", "brief": "Utilities for tests with special LPs", "has_pass2": false}, "highs/interfaces/highs_c_api.h": {"path": "layer-4/HiGHS/highs/interfaces/highs_c_api.h", "filename": "highs_c_api.h", "file": "interfaces/highs_c_api.h", "brief": "Pure C interface to HiGHS solver\n\nProvides C-compatible API for LP/MIP/QP solving without C++ dependencies.\n\n**Quick-Start Functions:**\n- Highs_lpCall(): Solve LP in one call with solution returned\n- Highs_mipCall(): Solve MIP in one call\n- Highs_qpCall(): Solve QP in one call\n\n**Full API (via Highs pointer):**\n- Highs_create()/Highs_destroy(): Instance management\n- Highs_passLp()/Highs_passMip(): Pass model data\n- Highs_run(): Solve current model\n- Highs_getSolution()/Highs_getBasis(): Retrieve results\n- Highs_setOption*(): Configure solver options\n- Highs_getInfo*(): Query solver statistics\n\n**Status Constants:**\n- kHighsStatus{Ok,Warning,Error}: Return codes\n- kHighsModelStatus*: Optimal, Infeasible, Unbounded, etc.\n- kHighsBasisStatus*: Lower, Basic, Upper, Zero, Nonbasic\n\n**Type Constants:**\n- kHighsVarType*: Continuous, Integer, SemiContinuous, etc.\n- kHighsOptionType*: Bool, Int, Double, String\n- kHighsMatrixFormat*: Colwise, Rowwise", "see": ["Highs.h for C++ API", "lp_data/HConst.h for C++ enum definitions"], "param": ["num_col   The number of columns.", "num_row   The number of rows.", "num_nz    The number of nonzeros in the constraint matrix.", "a_format  The format of the constraint matrix as a\n                 `kHighsMatrixFormat` constant.", "sense     The optimization sense as a `kHighsObjSense` constant.", "offset    The objective constant.", "col_cost  An array of length [num_col] with the column costs.", "col_lower An array of length [num_col] with the column lower bounds.", "col_upper An array of length [num_col] with the column upper bounds.", "row_lower An array of length [num_row] with the row lower bounds.", "row_upper An array of length [num_row] with the row upper bounds.", "a_start   The constraint matrix is provided to HiGHS in compressed\n                 sparse column form (if `a_format` is\n                 `kHighsMatrixFormatColwise`, otherwise compressed sparse row\n                 form). The sparse matrix consists of three arrays,\n                 `a_start`, `a_index`, and `a_value`. `a_start` is an array\n                 of length [num_col] containing the starting index of each\n                 column in `a_index`. If `a_format` is\n                 `kHighsMatrixFormatRowwise` the array is of length [num_row]\n                 corresponding to each row.", "a_index   An array of length [num_nz] with indices of matrix entries.", "a_value   An array of length [num_nz] with values of matrix entries.", "col_value      An array of length [num_col], to be filled with the\n                      primal column solution.", "col_dual       An array of length [num_col], to be filled with the\n                      dual column solution.", "row_value      An array of length [num_row], to be filled with the\n                      primal row solution.", "row_dual       An array of length [num_row], to be filled with the\n                      dual row solution.", "col_basis_status  An array of length [num_col], to be filled with the\n                         basis status of the columns in the form of a\n                         `kHighsBasisStatus` constant.", "row_basis_status  An array of length [num_row], to be filled with the\n                         basis status of the rows in the form of a\n                         `kHighsBasisStatus` constant.", "model_status      The location in which to place the termination\n                         status of the model after the solve in the form of a\n                         `kHighsModelStatus` constant.", "integrality   An array of length [num_col], containing a\n                     `kHighsVarType` constant for each column.", "q_num_nz  The number of nonzeros in the Hessian matrix.", "q_format  The format of the Hessian matrix in the form of a\n                 `kHighsHessianStatus` constant. If q_num_nz > 0, this must\n                 be `kHighsHessianFormatTriangular`.", "q_start   The Hessian matrix is provided to HiGHS as the lower\n                 triangular component in compressed sparse column form\n                 (or, equivalently, as the upper triangular component\n                 in compressed sparse row form). The sparse matrix consists\n                 of three arrays, `q_start`, `q_index`, and `q_value`.\n                 `q_start` is an array of length [num_col].", "q_index   An array of length [q_num_nz] with indices of matrix\n                 entries.", "q_value   An array of length [q_num_nz] with values of matrix entries.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "filename  The filename to read.", "highs     A pointer to the Highs instance.", "filename  The filename to write.", "highs     A pointer to the Highs instance.", "filename  The filename to write.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs       A pointer to the Highs instance.", "col_value   An array of length [num_col] with the column solution\n                   values.", "col_dual    An array of length [num_col] with the column dual\n                   values, or a null pointer if not known.", "row_dual    An array of length [num_row] with the row dual values,\n                   or a null pointer if not known.", "highs     A pointer to the Highs instance.", "filename  The name of the file to write the results to.", "highs     A pointer to the Highs instance.", "filename  The name of the file to write the results to.", "highs       A pointer to the Highs instance.", "num_col     The number of columns.", "num_row     The number of rows.", "num_nz      The number of elements in the constraint matrix.", "q_num_nz    The number of elements in the Hessian matrix.", "a_format    The format of the constraint matrix to use in the form of\n                   a `kHighsMatrixFormat` constant.", "q_format    The format of the Hessian matrix to use in the form of a\n                   `kHighsHessianFormat` constant.", "sense       The optimization sense in the form of a `kHighsObjSense`\n                   constant.", "offset      The constant term in the objective function.", "col_cost    An array of length [num_col] with the objective\n                   coefficients.", "col_lower   An array of length [num_col] with the lower column bounds.", "col_upper   An array of length [num_col] with the upper column bounds.", "row_lower   An array of length [num_row] with the upper row bounds.", "row_upper   An array of length [num_row] with the upper row bounds.", "a_start     The constraint matrix is provided to HiGHS in compressed\n                   sparse column form (if `a_format` is\n                   `kHighsMatrixFormatColwise`, otherwise compressed sparse\n                   row form). The sparse matrix consists of three arrays,\n                   `a_start`, `a_index`, and `a_value`. `a_start` is an array\n                   of length [num_col] containing the starting index of each\n                   column in `a_index`. If `a_format` is\n                   `kHighsMatrixFormatRowwise` the array is of length\n                   [num_row] corresponding to each row.", "a_index     An array of length [num_nz] with indices of matrix\n                   entries.", "a_value     An array of length [num_nz] with values of matrix entries.", "q_start     The Hessian matrix is provided to HiGHS as the lower\n                   triangular component in compressed sparse column form\n                   (or, equivalently, as the upper triangular component\n                   in compressed sparse row form). The sparse matrix consists\n                   of three arrays, `q_start`, `q_index`, and `q_value`.\n                   `q_start` is an array of length [num_col]. If the model\n                   is linear, pass NULL.", "q_index     An array of length [q_num_nz] with indices of matrix\n                   entries. If the model is linear, pass NULL.", "q_value     An array of length [q_num_nz] with values of matrix\n                    entries. If the model is linear, pass NULL.", "integrality An array of length [num_col] containing a `kHighsVarType`\n                   constant for each column.", "highs     A pointer to the Highs instance.", "dim       The dimension of the Hessian matrix. Should be [num_col].", "num_nz    The number of non-zero elements in the Hessian matrix.", "format    The format of the Hessian matrix as a `kHighsHessianFormat`\n                 constant. This must be `kHighsHessianFormatTriangular`.", "start     The Hessian matrix is provided to HiGHS as the lower\n                 triangular component in compressed sparse column form\n                 (or, equivalently, as the upper triangular component\n                 in compressed sparse row form), using `q_start`, `q_index`,\n                 and `q_value`.The Hessian matrix is provided to HiGHS as the\n                 lower triangular component in compressed sparse column form.\n                 The sparse matrix consists of three arrays, `start`,\n                 `index`, and `value`. `start` is an array of length\n                 [num_col] containing the starting index of each column in\n                 `index`.", "index     An array of length [num_nz] with indices of matrix entries.", "value     An array of length [num_nz] with values of matrix entries.", "highs         A pointer to the Highs instance.", "weight        A pointer to the weights of the linear objective, with\n                     its positive/negative sign determining whether it is\n                     minimized or maximized during lexicographic optimization", "offset        A pointer to the objective offsets", "coefficients  A pointer to the objective coefficients", "abs_tolerance A pointer to the absolute tolerances used when\n                     constructing objective constraints during lexicographic\n                     optimization", "rel_tolerance A pointer to the relative tolerances used when\n                     constructing objective constraints during lexicographic\n                     optimization", "priority      A pointer to the priorities of the objectives during\n                     lexicographic optimization", "highs         A pointer to the Highs instance.", "weight        The weight of the linear objective, with its\n                     positive/negative sign determining whether it is\n                     minimized or maximized during lexicographic\n                     optimization", "offset        The objective offset", "coefficients  A pointer to the objective coefficients", "abs_tolerance The absolute tolerance used when constructing an\n                     objective constraint during lexicographic optimization", "rel_tolerance The relative tolerance used when constructing an\n                     objective constraint during lexicographic optimization", "priority      The priority of this objective during lexicographic\n                     optimization", "highs A pointer to the Highs instance.", "highs A pointer to the Highs instance.", "row   The row for which the name is supplied.", "name  The name of the row.", "highs A pointer to the Highs instance.", "col   The column for which the name is supplied.", "name  The name of the column.", "highs A pointer to the Highs instance.", "name  The name of the model.", "highs     A pointer to the Highs instance.", "filename  The filename from which to read the option values.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The location in which the current value of the option should\n                 be placed.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The location in which the current value of the option should\n                 be placed.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The location in which the current value of the option should\n                 be placed.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     A pointer to allocated memory (of at least\n                 `kMaximumStringLength`) to store the current value of the\n                 option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "type      A HighsInt in which the corresponding `kHighsOptionType`\n                 constant should be placed.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "filename  The filename to write the options to.", "highs     A pointer to the Highs instance.", "filename  The filename to write the options to.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "index     The index of the option.", "name      The name of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "default_value A pointer to the default value of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "min_value     A pointer to the minimum value of the option.", "max_value     A pointer to the maximum value of the option.", "default_value A pointer to the default value of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "min_value     A pointer to the minimum value of the option.", "max_value     A pointer to the maximum value of the option.", "default_value A pointer to the default value of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "default_value A pointer to the default value of the option.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "value     A reference to an integer that the result will be stored in.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "value     A reference to a double that the result will be stored in.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "value     A reference to an int64 that the result will be stored in.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "type      A HighsInt in which the corresponding `kHighsOptionType`\n                 constant is stored.", "highs      A pointer to the Highs instance.", "col_value  An array of length [num_col], to be filled with primal\n                  column values.", "col_dual   An array of length [num_col], to be filled with dual column\n                  values.", "row_value  An array of length [num_row], to be filled with primal row\n                  values.", "row_dual   An array of length [num_row], to be filled with dual row\n                  values.", "highs       A pointer to the Highs instance.", "col_status  An array of length [num_col], to be filled with the column\n                   basis statuses in the form of a `kHighsBasisStatus`\n                   constant.", "row_status  An array of length [num_row], to be filled with the row\n                   basis statuses in the form of a `kHighsBasisStatus`\n                   constant.", "highs     A pointer to the Highs instance.", "highs             A pointer to the Highs instance.", "has_dual_ray      A pointer to a HighsInt to store 1 if a dual ray\n                         currently exists.", "dual_ray_value    An array of length [num_row] filled with the\n                         unbounded ray.", "highs                                   A pointer to the Highs\n                                               instance.", "has_dual_unboundedness_direction        A pointer to a HighsInt to\n                                               store 1 if the dual\n                                               unboundedness direction\n                                               exists.", "dual_unboundedness_direction_value      An array of length [num_col]\n                                               filled with the unboundedness\n                                               direction.", "highs             A pointer to the Highs instance.", "has_primal_ray    A pointer to a HighsInt to store 1 if the primal ray\n                         exists.", "primal_ray_value  An array of length [num_col] filled with the\n                         unbounded ray.", "highs     A pointer to the Highs instance.", "highs             A pointer to the Highs instance.", "basic_variables   An array of size [num_rows], filled with the indices\n                         of the basic variables.", "highs         A pointer to the Highs instance.", "row           The index of the row to compute.", "row_vector    An array of length [num_row] in which to store the\n                     values of the non-zero elements.", "row_num_nz    The number of non-zeros in the row.", "row_index     An array of length [num_row] in which to store the\n                     indices of the non-zero elements.", "highs         A pointer to the Highs instance.", "col           The index of the column to compute.", "col_vector    An array of length [num_row] in which to store the\n                     values of the non-zero elements.", "col_num_nz    The number of non-zeros in the column.", "col_index     An array of length [num_row] in which to store the\n                     indices of the non-zero elements.", "highs             A pointer to the Highs instance.", "rhs               The right-hand side vector ``b``.", "solution_vector   An array of length [num_row] in which to store the\n                         values of the non-zero elements.", "solution_num_nz   The number of non-zeros in the solution.", "solution_index    An array of length [num_row] in which to store the\n                         indices of the non-zero elements.", "highs             A pointer to the Highs instance.", "rhs               The right-hand side vector ``b``", "solution_vector   An array of length [num_row] in which to store the\n                         values of the non-zero elements.", "solution_num_nz   The number of non-zeros in the solution.", "solution_index    An array of length [num_row] in which to store the\n                         indices of the non-zero elements.", "highs         A pointer to the Highs instance.", "row           The index of the row to compute.", "row_vector    An array of length [num_col] in which to store the\n                     values of the non-zero elements.", "row_num_nz    The number of non-zeros in the row.", "row_index     An array of length [num_col] in which to store the\n                     indices of the non-zero elements.", "highs         A pointer to the Highs instance.", "col           The index of the column to compute.", "col_vector    An array of length [num_row] in which to store the\n                      values of the non-zero elements.", "col_num_nz    The number of non-zeros in the column.", "col_index     An array of length [num_row] in which to store the\n                      indices of the non-zero elements.", "highs       A pointer to the Highs instance.", "col_status  an array of length [num_col] with the column basis status\n                   in the form of `kHighsBasisStatus` constants", "row_status  an array of length [num_row] with the row basis status\n                   in the form of `kHighsBasisStatus` constants", "highs     A pointer to the Highs instance.", "highs       A pointer to the Highs instance.", "col_value   An array of length [num_col] with the column solution\n                   values.", "row_value   An array of length [num_row] with the row solution\n                   values.", "col_dual    An array of length [num_col] with the column dual values.", "row_dual    An array of length [num_row] with the row dual values.", "highs       A pointer to the Highs instance.", "num_entries Number of variables in the set", "index       Indices of variables in the set", "value       Values of variables in the set", "highs              A pointer to the Highs instance.", "user_callback      A pointer to the user callback", "user_callback_data A pointer to the user callback data", "highs         A pointer to the Highs instance.", "callback_type The type of callback to be started", "highs         A pointer to the Highs instance.", "callback_type The type of callback to be stopped", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs         A pointer to the Highs instance.", "cost          The objective coefficient of the column.", "lower         The lower bound of the column.", "upper         The upper bound of the column.", "num_new_nz    The number of non-zeros in the column.", "index         An array of size [num_new_nz] with the row indices.", "value         An array of size [num_new_nz] with row values.", "highs         A pointer to the Highs instance.", "num_new_col   The number of new columns to add.", "costs         An array of size [num_new_col] with objective\n                     coefficients.", "lower         An array of size [num_new_col] with lower bounds.", "upper         An array of size [num_new_col] with upper bounds.", "num_new_nz    The number of new nonzeros in the constraint matrix.", "starts        The constraint coefficients are given as a matrix in\n                     compressed sparse column form by the arrays `starts`,\n                     `index`, and `value`. `starts` is an array of size\n                     [num_new_cols] with the start index of each row in\n                     indices and values.", "index         An array of size [num_new_nz] with row indices.", "value         An array of size [num_new_nz] with row values.", "highs         A pointer to the Highs instance.", "lower         The lower bound of the column.", "upper         The upper bound of the column.", "highs         A pointer to the Highs instance.", "num_new_var   The number of new variables to add.", "lower         An array of size [num_new_var] with lower bounds.", "upper         An array of size [num_new_var] with upper bounds.", "highs         A pointer to the Highs instance.", "lower         The lower bound of the row.", "upper         The upper bound of the row.", "num_new_nz    The number of non-zeros in the row", "index         An array of size [num_new_nz] with column indices.", "value         An array of size [num_new_nz] with column values.", "highs         A pointer to the Highs instance.", "num_new_row   The number of new rows to add", "lower         An array of size [num_new_row] with the lower bounds of\n                     the rows.", "upper         An array of size [num_new_row] with the upper bounds of\n                     the rows.", "num_new_nz    The number of non-zeros in the rows.", "starts        The constraint coefficients are given as a matrix in\n                     compressed sparse row form by the arrays `starts`,\n                     `index`, and `value`. `starts` is an array of size\n                     [num_new_rows] with the start index of each row in\n                     indices and values.", "index         An array of size [num_new_nz] with column indices.", "value         An array of size [num_new_nz] with column values.", "highs         A pointer to the Highs instance.", "highs         A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "sense     The new optimization sense in the form of a `kHighsObjSense`\n                 constant.", "highs     A pointer to the Highs instance.", "offset    The new objective offset.", "highs         A pointer to the Highs instance.", "col           The column index to change.", "integrality   The new integrality of the column in the form of a\n                     `kHighsVarType` constant.", "highs         A pointer to the Highs instance.", "from_col      The index of the first column whose integrality changes.", "to_col        The index of the last column whose integrality\n                     changes.", "integrality   An array of length [to_col - from_col + 1] with the new\n                     integralities of the columns in the form of\n                     `kHighsVarType` constants.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to change.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to change.", "integrality       An array of length [num_set_entries] with the new\n                         integralities of the columns in the form of\n                         `kHighsVarType` constants.", "highs         A pointer to the Highs instance.", "mask          An array of length [num_col] with 1 if the column\n                     integrality should be changed and 0 otherwise.", "integrality   An array of length [num_col] with the new\n                     integralities of the columns in the form of\n                     `kHighsVarType` constants.", "highs         A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "col       The index of the column fo change.", "cost      The new objective coefficient.", "highs     A pointer to the Highs instance.", "from_col  The index of the first column whose cost changes.", "to_col    The index of the last column whose cost changes.", "cost      An array of length [to_col - from_col + 1] with the new\n                 objective coefficients.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to change.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to change.", "cost              An array of length [num_set_entries] with the new\n                         costs of the columns.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_col] with 1 if the column\n                 cost should be changed and 0 otherwise.", "cost      An array of length [num_col] with the new costs.", "highs     A pointer to the Highs instance.", "col       The index of the column whose bounds are to change.", "lower     The new lower bound.", "upper     The new upper bound.", "highs     A pointer to the Highs instance.", "from_col  The index of the first column whose bound changes.", "to_col    The index of the last column whose bound changes.", "lower     An array of length [to_col - from_col + 1] with the new\n                 lower bounds.", "upper     An array of length [to_col - from_col + 1] with the new\n                 upper bounds.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to change.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to change.", "lower             An array of length [num_set_entries] with the new\n                         lower bounds.", "upper             An array of length [num_set_entries] with the new\n                         upper bounds.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_col] with 1 if the column\n                 bounds should be changed and 0 otherwise.", "lower     An array of length [num_col] with the new lower bounds.", "upper     An array of length [num_col] with the new upper bounds.", "highs     A pointer to the Highs instance.", "row       The index of the row whose bounds are to change.", "lower     The new lower bound.", "upper     The new upper bound.", "highs     A pointer to the Highs instance.", "from_row  The index of the first row whose bound changes.", "to_row    The index of the last row whose bound changes.", "lower     An array of length [to_row - from_row + 1] with the new\n                 lower bounds.", "upper     An array of length [to_row - from_row + 1] with the new\n                 upper bounds.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of rows to change.", "set               An array of size [num_set_entries] with the indices\n                         of the rows to change.", "lower             An array of length [num_set_entries] with the new\n                         lower bounds.", "upper             An array of length [num_set_entries] with the new\n                         upper bounds.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_row] with 1 if the row\n                 bounds should be changed and 0 otherwise.", "lower     An array of length [num_row] with the new lower bounds.", "upper     An array of length [num_row] with the new upper bounds.", "highs     A pointer to the Highs instance.", "row       The index of the row to change.", "col       The index of the column to change.", "value     The new constraint coefficient.", "highs     A pointer to the Highs instance.", "sense     The location in which the current objective sense should be\n                 placed. The sense is a `kHighsObjSense` constant.", "highs     A pointer to the Highs instance.", "offset    The location in which the current objective offset should be\n                 placed.", "highs         A pointer to the Highs instance.", "from_col      The first column for which to query data for.", "to_col        The last column (inclusive) for which to query data for.", "num_col       A HighsInt populated with the number of columns got from\n                     the model (this should equal `to_col - from_col + 1`).", "costs         An array of size [to_col - from_col + 1] for the column\n                     cost coefficients.", "lower         An array of size [to_col - from_col + 1] for the column\n                     lower bounds.", "upper         An array of size [to_col - from_col + 1] for the column\n                     upper bounds.", "num_nz        A HighsInt to be populated with the number of non-zero\n                     elements in the constraint matrix.", "matrix_start  An array of size [to_col - from_col + 1] with the start\n                     indices of each column in `matrix_index` and\n                     `matrix_value`.", "matrix_index  An array of size [num_nz] with the row indices of each\n                     element in the constraint matrix.", "matrix_value  An array of size [num_nz] with the non-zero elements of\n                     the constraint matrix.", "num_set_indices   The number of indices in `set`.", "set               An array of size [num_set_entries] with the column\n                         indices to get.", "mask  An array of length [num_col] containing a `1` to get the column\n             and `0` otherwise.", "highs         A pointer to the Highs instance.", "from_row      The first row for which to query data for.", "to_row        The last row (inclusive) for which to query data for.", "num_row       A HighsInt to be populated with the number of rows got\n                     from the model.", "lower         An array of size [to_row - from_row + 1] for the row\n                     lower bounds.", "upper         An array of size [to_row - from_row + 1] for the row\n                     upper bounds.", "num_nz        A HighsInt to be populated with the number of non-zero\n                     elements in the constraint matrix.", "matrix_start  An array of size [to_row - from_row + 1] with the start\n                     indices of each row in `matrix_index` and\n                     `matrix_value`.", "matrix_index  An array of size [num_nz] with the column indices of\n                     each element in the constraint matrix.", "matrix_value  An array of size [num_nz] with the non-zero elements of\n                     the constraint matrix.", "num_set_indices   The number of indices in `set`.", "set               An array of size [num_set_entries] containing the\n                         row indices to get.", "mask  An array of length [num_row] containing a `1` to get the row and\n             `0` otherwise.", "row   The index of the row to query.", "name  A pointer in which to store the name of the row. This must have\n             length `kHighsMaximumStringLength`.", "name A pointer of the name of the row to query.", "row  A pointer in which to store the index of the row", "col   The index of the column to query.", "name  A pointer in which to store the name of the column. This must\n             have length `kHighsMaximumStringLength`.", "name A pointer of the name of the column to query.", "col  A pointer in which to store the index of the column", "col          The index of the column to query.", "integrality  A HighsInt in which the integrality of the column should\n                    be placed. The integer is one of the `kHighsVarTypeXXX`\n                    constants.", "highs     A pointer to the Highs instance.", "from_col  The index of the first column to delete.", "to_col    The index of the last column to delete.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to delete.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to delete.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_col] with 1 if the column\n                 should be deleted and 0 otherwise.", "highs     A pointer to the Highs instance.", "from_row  The index of the first row to delete.", "to_row    The index of the last row to delete.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of rows to delete.", "set               An array of size [num_set_entries] with the indices\n                         of the rows to delete.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_row] with `1` if the row should be\n                 deleted and `0` otherwise. The new index of any column not\n                 deleted is stored in place of the value `0`.", "highs     A pointer to the Highs instance.", "col       The index of the column to scale.", "scaleval  The value by which to scale the column. If `scaleval < 0`,\n                 the variable bounds flipped.", "highs     A pointer to the Highs instance.", "row       The index of the row to scale.", "scaleval  The value by which to scale the row. If `scaleval < 0`, the\n                 row bounds are flipped.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs      A pointer to the Highs instance.", "num_col    The number of variables.", "num_row    The number of rows.", "col_value  An array of length [num_col] with optimal primal solution\n                  for each column.", "col_dual   An array of length [num_col] with optimal dual solution for\n                  each column. May be `NULL`, in which case no dual solution\n                  is passed.", "row_dual   An array of length [num_row] with optimal dual solution for\n                  each row. . May be `NULL`, in which case no dual solution\n                  is passed.", "highs                  A pointer to the Highs instance.", "col_cost_up_value      The upper range of the cost value", "col_cost_up_objective  The objective at the upper cost range", "col_cost_up_in_var     The variable entering the basis at the upper\n                              cost range", "col_cost_up_ou_var     The variable leaving the basis at the upper\n                              cost range", "col_cost_dn_value      The lower range of the cost value", "col_cost_dn_objective  The objective at the lower cost range", "col_cost_dn_in_var     The variable entering the basis at the lower\n                              cost range", "col_cost_dn_ou_var     The variable leaving the basis at the lower\n                              cost range", "col_bound_up_value     The upper range of the column bound value", "col_bound_up_objective The objective at the upper column bound range", "col_bound_up_in_var    The variable entering the basis at the upper\n                              column bound range", "col_bound_up_ou_var    The variable leaving the basis at the upper\n                              column bound range", "col_bound_dn_value     The lower range of the column bound value", "col_bound_dn_objective The objective at the lower column bound range", "col_bound_dn_in_var    The variable entering the basis at the lower\n                              column bound range", "col_bound_dn_ou_var    The variable leaving the basis at the lower\n                              column bound range", "row_bound_up_value     The upper range of the row bound value", "row_bound_up_objective The objective at the upper row bound range", "row_bound_up_in_var    The variable entering the basis at the upper\n                              row bound range", "row_bound_up_ou_var    The variable leaving the basis at the upper row\n                              bound range", "row_bound_dn_value     The lower range of the row bound value", "row_bound_dn_objective The objective at the lower row bound range", "row_bound_dn_in_var    The variable entering the basis at the lower\n                              row bound range", "row_bound_dn_ou_var    The variable leaving the basis at the lower row\n                              bound range", "highs                             A pointer to the Highs instance.", "const double global_lower_penalty The penalty for violating lower\nbounds on variables", "const double global_upper_penalty The penalty for violating upper\nbounds on variables", "const double global_rhs_penalty   The penalty for violating constraint\nRHS values", "const double* local_lower_penalty The penalties for violating specific\nlower bounds on variables", "const double* local_upper_penalty The penalties for violating specific\nupper bounds on variables", "const double* local_rhs_penalty   The penalties for violating specific\nconstraint RHS values", "highs                      A pointer to the Highs instance.", "const HighsInt iis_num_col Number of columns in the IIS.", "const HighsInt iis_num_row Number of rows in the IIS.", "const HighsInt* col_index  An array of length [iis_num_col], to be\n                                  filled with the indices of original\n                                  variables in the IIS.", "const HighsInt* row_index  An array of length [iis_num_col], to be\n                                  filled with the indices of original\n                                  constraints in the IIS.", "const HighsInt* col_bound  An array of length [iis_num_col], to be\n                                  filled with the bound status of variables\n                                  in the IIS.", "const HighsInt* row_bound  An array of length [iis_num_col], to be\n                                  filled with the bound status of constraints\n                                  in the IIS.", "const HighsInt* col_status An array of length [num_col], to be\n                                  filled with the IIS status of all original\n                                  variables.", "const HighsInt* row_status n array of length [num_col], to be\n                                  filled with the IIS status of all original\n                                  constraints.", "blocking   If the `blocking` parameter has a nonzero value, then this\n                  function will not return until all memory is freed, which\n                  might be desirable when debugging heap memory, but it\n                  requires the calling thread to wait for all scheduler\n                  threads to wake-up which is usually not necessary.", "data_out      A pointer to the HighsCallbackDataOut instance.", "item_name     The name of the item.", "data_in     A pointer to the callback input data instance.", "num_entries Number of variables in the set", "value       An array of length [num_entries <= num_col] with\n                   column solution values.", "data_in     A pointer to the callback input data instance.", "num_entries Number of variables in the set", "index       Indices of variables in the set", "value       Values of variables in the set"], "has_pass2": false}, "highs/pdlp/CupdlpWrapper.h": {"path": "layer-4/HiGHS/highs/pdlp/CupdlpWrapper.h", "filename": "CupdlpWrapper.h", "file": "pdlp/CupdlpWrapper.h", "brief": "Wrapper for CUPDLP first-order LP solver\n\nIntegrates CUPDLP (CUDA/CPU Primal-Dual LP) solver into HiGHS.\nCUPDLP uses first-order methods (PDHG) for LP solving without pivoting.\n\n**Main Functions:**\n- solveLpCupdlp(): Solve LP using CUPDLP algorithm\n- formulateLP_highs(): Convert HighsLp to CUPDLP format (CSC, rhs, bounds)\n- getCupdlpLogLevel(): Map HiGHS verbosity to CUPDLP log level\n\n**Problem Setup:**\n- problem_create()/problem_alloc(): Allocate CUPDLP problem structure\n- data_alloc(): Allocate matrix data (supports GPU if CUPDLP_GPU defined)\n\n**Memory Macros:**\n- cupdlp_init_*: Allocation helpers for int, double, work, problem, data\n- cupdlp_copy_vec: CPU vector copy (GPU uses CUDA memcpy)\n\n**GPU Support:**\nCompile with CUPDLP_GPU for CUDA acceleration via cuPDLP backend.", "see": ["pdlp/cupdlp/cupdlp.h for CUPDLP algorithm implementation", "ipm/IpxWrapper.h for alternative interior point method"], "has_pass2": false}, "highs/lp_data/HighsLpUtils.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsLpUtils.h", "filename": "HighsLpUtils.h", "file": "lp_data/HighsLpUtils.h", "brief": "Utility functions for LP manipulation, I/O, and validation\n\nCollection of free functions operating on HighsLp and related structures.\n\n**Basis I/O:**\n- writeBasisFile(): Export basis to file\n- readBasisFile()/readBasisStream(): Import basis from file/stream\n\n**Index/Name Lookup:**\n- getIndexFromName(): Resolve variable/constraint name to index\n\n**LP Assessment:**\n- assessLp(): Validate LP data consistency\n- lpDimensionsOk(): Check matrix dimensions\n- assessCosts(): Validate objective coefficients\n- assessBounds(): Validate variable/constraint bounds\n- applyScalingToLp()/applyScalingToLpCol/Row(): Apply scaling factors\n\n**LP Modification:**\n- appendColsToLp(), appendRowsToLp(): Extend LP\n- transformIntoEqualityProblem(): Convert to standard form\n\n**Solution Utilities:**\n- getLpCosts(), analyseObjective(): Cost analysis\n- writeSolution(): Export solution to file", "see": ["lp_data/HighsLp.h for LP data structure", "lp_data/HighsSolution.h for solution structure", "lp_data/HighsModelUtils.h for model-level utilities"], "has_pass2": false}, "highs/lp_data/HighsIis.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsIis.h", "filename": "HighsIis.h", "file": "lp_data/HighsIis.h", "brief": "Irreducible Infeasible Set (IIS) computation for infeasible LPs\n\nIdentifies minimal subset of constraints that cannot be satisfied.\n\n**IIS Definition:**\n- Minimal infeasible subsystem: removing any constraint makes it feasible\n- Helps diagnose why an LP is infeasible\n- Also identifies which variable bounds participate\n\n**IisBoundStatus:**\n- kDropped: Bound removed from IIS\n- kNull: Not yet classified\n- kFree: Variable is free (not in IIS)\n- kLower/kUpper/kBoxed: Which bound(s) are in IIS\n\n**Algorithm:**\n- compute(): Main IIS computation using simplex iterations\n- trivial(): Check for obviously infeasible (single row/col)\n- rowValueBounds(): Check row activity vs bounds\n\n**Strategy:**\n- kIisStrategyMin: Minimize IIS size (more iterations)\n- Iteratively removes constraints until minimal set remains\n\n**Output:**\n- col_index_/row_index_: Variables/constraints in IIS\n- col_bound_/row_bound_: Which bounds contribute\n- info_: Simplex stats per iteration", "see": ["Highs.h for getIis() API"], "has_pass2": false}, "highs/lp_data/HighsAnalysis.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsAnalysis.h", "filename": "HighsAnalysis.h", "file": "lp_data/HighsAnalysis.h", "brief": "Timer clock aggregation for performance analysis\n\nGroups named timer clocks for measuring algorithm phases.\n\n**HighsTimerClock:**\n- timer_pointer_: Reference to parent HighsTimer\n- clock_[]: Vector of clock indices for this group\n\n**Usage:**\n- Simplex phases (pricing, ratio test, update)\n- IPM iterations\n- MIP operations (cuts, branching, heuristics)", "see": ["util/HighsTimer.h for clock management"], "has_pass2": false}, "highs/lp_data/HighsLp.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsLp.h", "filename": "HighsLp.h", "file": "lp_data/HighsLp.h", "brief": "Linear programming model data structure\n\n**HighsLp Class:**\nCore LP representation: min/max c'x s.t. row_lower <= Ax <= row_upper,\ncol_lower <= x <= col_upper\n\n**Data Members:**\n- num_col_, num_row_: Problem dimensions\n- col_cost_: Objective coefficients (c)\n- col_lower_, col_upper_: Variable bounds\n- row_lower_, row_upper_: Constraint bounds\n- a_matrix_: Constraint matrix A (HighsSparseMatrix, CSC or CSR)\n- sense_: Minimize (1) or Maximize (-1)\n- offset_: Constant objective offset\n- integrality_: Variable types (continuous, integer, semi-continuous, etc.)\n\n**Naming:**\n- col_names_, row_names_: Optional variable/constraint names\n- col_hash_, row_hash_: Name lookup hash tables\n\n**Scaling:**\n- scale_: Row/column scaling factors\n- is_scaled_: Whether scaling has been applied", "see": ["model/HighsModel.h for LP + Hessian (QP)", "HStruct.h for supporting types (ObjSense, HighsScale, etc.)", "util/HighsSparseMatrix.h for matrix storage"], "has_pass2": false}, "highs/lp_data/HighsRanging.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsRanging.h", "filename": "HighsRanging.h", "file": "lp_data/HighsRanging.h", "brief": "Sensitivity analysis (ranging) for LP optimal solutions\n\nComputes allowable ranges for objective coefficients and bounds.\n\n**HighsRangingRecord:**\n- value_[]: New coefficient/bound value at range limit\n- objective_[]: Objective value at range limit\n- in_var_[]: Variable entering basis at limit\n- ou_var_[]: Variable leaving basis at limit\n\n**HighsRanging Structure:**\n- col_cost_up/dn: Objective coefficient increase/decrease\n- col_bound_up/dn: Variable bound increase/decrease\n- row_bound_up/dn: Constraint RHS increase/decrease\n\n**Sensitivity Analysis:**\n- For costs: Range where current basis remains optimal\n- For bounds: Range where current basis remains primal feasible\n- Shadow prices from dual variables", "see": ["lp_data/HighsLpSolverObject.h for solver state"], "has_pass2": false}, "highs/lp_data/HighsSolve.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsSolve.h", "filename": "HighsSolve.h", "file": "lp_data/HighsSolve.h", "brief": "Top-level LP solve dispatch and special case handling\n\nEntry points for solving LPs with solver selection and preprocessing.\n\n**Main Solve Functions:**\n- solveLp(): Primary entry point dispatching to simplex/IPM\n- solveUnconstrainedLp(): Handle LPs with no constraints\n\n**Solver Selection:**\n- useIpm(): Check if IPM should be used based on options\n- useHipo(): Check if parallel HIPO IPM is appropriate\n  - Considers problem size, structure, and options\n\n**Preprocessing:**\n- assessExcessiveObjectiveBoundScaling(): Detect numerical issues\n  - Warns if objective/bound ratio is extreme\n  - Populates user_scale_data for remediation\n\n**Dispatch Logic:**\n1. Check for special cases (unconstrained, trivially infeasible)\n2. Select solver (simplex vs IPM vs HIPO)\n3. Call appropriate solver with prepared solver object", "see": ["lp_data/HighsLpSolverObject.h for solver state container", "simplex/HEkk.h for simplex implementation", "ipm/IpxWrapper.h for IPM implementation"], "has_pass2": false}, "highs/lp_data/HighsModelUtils.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsModelUtils.h", "filename": "HighsModelUtils.h", "file": "lp_data/HighsModelUtils.h", "brief": "Utility functions for model analysis and solution output\n\nFree functions for analyzing models and writing solution files.\n\n**Model Analysis:**\n- analyseModelBounds(): Report statistics on variable/constraint bounds\n- hasNamesWithSpaces(): Check for problematic whitespace in names\n\n**Solution Output:**\n- writeModelBoundSolution(): Write variable/constraint solution to file\n- writeModelObjective(): Write objective function evaluation\n- writeLpObjective(): Write LP objective evaluation\n- writeObjectiveValue(): Write scalar objective value\n- writePrimalSolution()/writeDualSolution(): Export primal/dual values\n- writeSolutionFile(): Write complete solution to file\n\n**Model Status:**\n- utilModelStatusToString(): Convert HighsModelStatus to string\n- utilBasisStatusToString(): Convert HighsBasisStatus to string", "see": ["lp_data/HighsLpUtils.h for LP-specific utilities", "model/HighsModel.h for model data structure", "lp_data/HighsSolution.h for solution structure"], "has_pass2": false}, "highs/lp_data/HStruct.h": {"path": "layer-4/HiGHS/highs/lp_data/HStruct.h", "filename": "HStruct.h", "file": "lp_data/HStruct.h", "brief": "Core data structures for HiGHS solver\n\nDefines fundamental structs used throughout HiGHS.\n\n**Solution/Basis Structs:**\n- HighsSolution: Primal values (col_value, row_value) and duals (col_dual, row_dual)\n- HighsBasis: Column/row basis status (Lower, Basic, Upper, Zero, Nonbasic)\n- HighsObjectiveSolution: Objective value with column values (for MIP solutions)\n\n**Scaling Structs:**\n- HighsScale: Row/column scaling factors (strategy, col[], row[], cost)\n- HighsUserScaleData: User-specified objective/bound scaling parameters\n\n**Model Modification Structs:**\n- HighsLpMods: Tracks modifications for semi-variables and infinite costs\n- HighsLinearObjective: Multi-objective support (weight, offset, coefficients, priority)\n\n**Utility Structs:**\n- HighsFiles: File paths for reading/writing solutions, bases, models\n- HighsNameHash: Hash table for name-to-index lookup\n- RefactorInfo/HotStart: Basis refactorization data (deprecated HotStart)\n\n**Logging/Statistics:**\n- HighsPresolveRuleLog/HighsPresolveLog: Presolve rule application counts\n- HighsSimplexStats: Iteration count, invert frequency, density statistics\n- HighsIllConditioning: Records for ill-conditioned basis detection\n- HighsSubSolverCallTime: Sub-solver timing breakdown", "see": ["HighsLp.h for LP model structure", "HConst.h for HighsBasisStatus, HighsVarType enums"], "has_pass2": false}, "highs/lp_data/HConst.h": {"path": "layer-4/HiGHS/highs/lp_data/HConst.h", "filename": "HConst.h", "file": "lp_data/HConst.h", "brief": "Constants, enums, and type definitions for HiGHS\n\nCentral definitions for HiGHS types and enumerations.\n\n**Numeric Constants:**\n- kHighsInf: Infinity value (std::numeric_limits<double>::infinity())\n- kHighsIInf: Integer infinity (max HighsInt)\n- kHighsTiny, kHighsMacheps, kHighsZero: Numerical tolerances\n\n**Core Enums:**\n- HighsModelStatus: Optimization result (kOptimal, kInfeasible, kUnbounded, etc.)\n- HighsVarType: Variable types (kContinuous, kInteger, kSemiContinuous, etc.)\n- HighsBasisStatus: Basis state (kLower, kBasic, kUpper, kZero, kNonbasic)\n- ObjSense: Minimize (+1) or Maximize (-1)\n\n**Format Enums:**\n- MatrixFormat: kColwise (CSC), kRowwise (CSR), kRowwisePartitioned\n- HessianFormat: kTriangular, kSquare\n\n**Status Enums:**\n- HighsPresolveStatus: Presolve outcomes\n- SolutionStatus: kNone, kInfeasible, kFeasible\n- BasisValidity: kInvalid, kValid\n\n**Callback Types:**\n- HighsCallbackType: Logging, interrupt, MIP solution events\n\n**Presolve Rules:**\n- PresolveRuleType: EmptyRow, SingletonRow, FixedCol, ForcingRow, etc.", "see": ["HighsStatus.h for function return codes", "HighsOptions.h for option types"], "has_pass2": false}, "highs/lp_data/HighsDebug.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsDebug.h", "filename": "HighsDebug.h", "file": "lp_data/HighsDebug.h", "has_pass2": false}, "highs/lp_data/HighsSolutionDebug.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsSolutionDebug.h", "filename": "HighsSolutionDebug.h", "file": "lp_data/HighsSolutionDebug.h", "has_pass2": false}, "highs/lp_data/HighsSolution.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsSolution.h", "filename": "HighsSolution.h", "file": "lp_data/HighsSolution.h", "brief": "Solution validation and KKT condition checking utilities\n\nProvides functions to validate solutions and compute optimality metrics.\n\n**KKT Failure Analysis:**\n- getKktFailures(): Compute primal/dual infeasibilities for LP/QP\n- getLpKktFailures(): LP-specific KKT checking\n- getVariableKktFailures(): Per-variable bound/dual violations\n\n**Error Tracking (HighsError, HighsPrimalDualErrors):**\n- Absolute/relative primal infeasibility\n- Absolute/relative dual infeasibility\n- Nonzero basic duals, off-bound nonbasics\n- Glpsol-compatible residual output\n\n**Solution Utilities:**\n- computeObjectiveValue(): Evaluate c'x\n- computeDualObjectiveValue(): Dual objective for LP/QP\n- getComplementarityViolations(): x_i * s_i deviations\n- refineBasis(): Improve basis from solution values\n\n**IPX Integration:**\n- ipxSolutionToHighsSolution(): Convert IPX (interior point) solution\n- ipxBasicSolutionToHighsBasicSolution(): Convert IPX basis + crossover result", "see": ["HStruct.h for HighsSolution, HighsBasis structs", "HighsInfo.h for solver info output"], "has_pass2": false}, "highs/lp_data/HighsInfo.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsInfo.h", "filename": "HighsInfo.h", "file": "lp_data/HighsInfo.h", "brief": "Solver output information and statistics\n\nProvides typed info records for solver statistics accessible via API.\n\n**InfoRecord Hierarchy:**\n- InfoRecord: Base class (type, name, description, advanced)\n- InfoRecordInt64: 64-bit integer info (mip_node_count)\n- InfoRecordInt: Integer info (iteration counts, status codes)\n- InfoRecordDouble: Double info (objective, infeasibilities)\n\n**HighsInfoStruct/HighsInfo:**\n- Iteration counts: simplex, ipm, crossover, pdlp, qp\n- Solution status: primal_solution_status, dual_solution_status\n- Objective: objective_function_value\n- MIP statistics: mip_node_count, mip_dual_bound, mip_gap\n- Infeasibility metrics: num/max/sum primal/dual infeasibilities\n- Residuals: primal/dual residual errors (absolute and relative)\n- Complementarity: violation counts and max violation\n\n**Functions:**\n- getInfoIndex(): Look up info by name\n- getLocalInfoValue(): Retrieve int/int64/double values\n- writeInfoToFile()/reportInfo(): Output info records", "see": ["HConst.h for HighsInfoType enum", "HighsOptions.h for solver configuration"], "has_pass2": false}, "highs/lp_data/HighsCallbackStruct.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsCallbackStruct.h", "filename": "HighsCallbackStruct.h", "file": "lp_data/HighsCallbackStruct.h", "has_pass2": false}, "highs/lp_data/HighsLpSolverObject.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsLpSolverObject.h", "filename": "HighsLpSolverObject.h", "file": "lp_data/HighsLpSolverObject.h", "brief": "Aggregation of all objects needed for LP solving\n\n**HighsLpSolverObject:**\nBundles references to all components needed to solve an LP:\n- lp_: The LP model data (HighsLp)\n- basis_: Current simplex basis (HighsBasis)\n- solution_: Primal/dual solution vectors (HighsSolution)\n- highs_info_: Solver statistics and output (HighsInfo)\n- ekk_instance_: Edinburgh simplex kernel (HEkk)\n- callback_: User callback handler (HighsCallback)\n- options_: Solver options (HighsOptions)\n- timer_: Performance timer (HighsTimer)\n- sub_solver_call_time_: Timing breakdown for sub-solvers\n- model_status_: Current model status (kNotset, kOptimal, etc.)\n\nUsed to pass solver context between functions without long parameter lists.", "see": ["simplex/HEkk.h for simplex implementation", "lp_data/HighsLp.h for LP data structure"], "has_pass2": false}, "highs/lp_data/HighsOptions.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsOptions.h", "filename": "HighsOptions.h", "file": "lp_data/HighsOptions.h", "brief": "Solver configuration options system\n\n**OptionRecord Hierarchy:**\nBase class with derived types for each option type:\n- OptionRecordBool: Boolean options\n- OptionRecordInt: Integer options with bounds\n- OptionRecordDouble: Double options with bounds\n- OptionRecordString: String options\n\n**HighsOptions Class:**\nContainer holding all solver options:\n- records: Vector of OptionRecord pointers\n- Options by category: solving, tolerances, output, limits, etc.\n\n**Key Option Categories:**\n- Solver selection: solver (\"simplex\", \"ipm\", \"choose\"), run_crossover\n- Tolerances: primal_feasibility_tolerance, dual_feasibility_tolerance\n- Limits: time_limit, iteration_limit, solution_limit\n- Output: output_flag, log_to_console, log_file\n- Presolve: presolve (\"on\", \"off\", \"choose\")\n- Simplex: simplex_strategy, simplex_scale_strategy\n- IPM: ipm_iteration_limit\n- MIP: mip_max_nodes, mip_abs_gap, mip_rel_gap", "see": ["Highs.h for setOptionValue/getOptionValue API", "HConst.h for option type enums"], "has_pass2": false}, "highs/lp_data/HighsCallback.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsCallback.h", "filename": "HighsCallback.h", "file": "lp_data/HighsCallback.h", "brief": "Callback mechanism for solver events and user interaction\n\nEnables user code to receive solver events and inject solutions.\n\n**HighsCallbackOutput:**\nData provided to callback during solver execution:\n- running_time, iteration counts (simplex, ipm, pdlp)\n- objective_function_value\n- MIP data: node_count, primal/dual bounds, gap, incumbent solution\n- Cut pool data for callback-based cuts\n\n**HighsCallbackInput:**\nData provided by user callback:\n- user_interrupt: Signal early termination\n- user_solution: Provide heuristic solution to MIP\n- setSolution(): Set solution (dense or sparse)\n- repairSolution(): Fix partial solution to feasibility\n\n**HighsCallback:**\nMain callback container:\n- user_callback: std::function for C++/Python\n- c_callback: C function pointer\n- active[]: Bitmask of enabled callback types\n- callbackAction(): Trigger callback and process response\n\n**Callback Types (from HighsCallbackStruct.h):**\nLogging, SimplexInterrupt, IpmInterrupt, MipSolution, MipLogging, etc.", "see": ["interfaces/highs_c_api.h for C callback constants", "lp_data/HighsCallbackStruct.h for callback type enum"], "has_pass2": false}, "highs/lp_data/HighsStatus.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsStatus.h", "filename": "HighsStatus.h", "file": "lp_data/HighsStatus.h", "brief": "Return status enum for HiGHS API calls\n\n**HighsStatus enum:**\n- kError (-1): Operation failed\n- kOk (0): Operation succeeded\n- kWarning (1): Operation succeeded with warnings\n\n**Functions:**\n- highsStatusToString(): Convert status to string\n- interpretCallStatus(): Combine call and return status with logging\n- worseStatus(): Return max(status0, status1) for status propagation", "see": ["HConst.h for HighsModelStatus (solver termination status)"], "has_pass2": false}, "highs/lp_data/HighsInfoDebug.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsInfoDebug.h", "filename": "HighsInfoDebug.h", "file": "lp_data/HighsInfoDebug.h", "has_pass2": false}, "highs/simplex/HSimplexNla.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplexNla.h", "filename": "HSimplexNla.h", "file": "simplex/HSimplexNla.h", "brief": "Numerical linear algebra interface for simplex\n\nWraps HFactor with scaling, product-form updates, and iterate storage.\n\n**ProductFormUpdate:**\nStores basis updates as explicit η-vectors when HFactor update fails:\n- pivot_index/value[]: Pivot positions and values\n- start/index/value[]: Eta-file sparse storage\n- btran/ftran(): Apply product form to vectors\n- Allows continued solve after HFactor becomes unstable\n\n**SimplexIterate:**\nCheckpoint storage for basis state:\n- basis_: Basis column indices and status\n- invert_: Factorization representation\n- dual_edge_weight_[]: DSE weights for restoration\n\n**Scaling Operations:**\n- btranInScaledSpace/ftranInScaledSpace(): Solve in scaled coordinates\n- applyBasisMatrixColScale/RowScale(): Transform vectors\n- variableScaleFactor(): Individual column/row scales\n\n**Key Methods:**\n- invert(): Refactorize basis via HFactor\n- btran(): B^(-T) * rhs (row pricing)\n- ftran(): B^(-1) * rhs (column evaluation)\n- update(): Basis update with pivot\n- putInvert/getInvert(): Save/restore iterate state\n\n**Debug Support:**\n- debugCheckInvert(): Verify factorization accuracy\n- debugInvertResidualError(): Measure solution error", "see": ["util/HFactor.h for LU factorization", "simplex/HEkk.h for simplex kernel using HSimplexNla"], "has_pass2": false}, "highs/simplex/SimplexConst.h": {"path": "layer-4/HiGHS/highs/simplex/SimplexConst.h", "filename": "SimplexConst.h", "file": "simplex/SimplexConst.h", "brief": "Constants and enums for HiGHS simplex solvers\n\n**SimplexStrategy Enum:**\n- kSimplexStrategyDual: Serial dual simplex\n- kSimplexStrategyDualTasks: SIP parallel dual\n- kSimplexStrategyDualMulti: PAMI parallel dual\n- kSimplexStrategyPrimal: Primal simplex\n\n**SimplexSolvePhase Enum:**\n- kSolvePhase1: Finding feasible basis\n- kSolvePhase2: Optimizing objective\n- kSolvePhaseOptimalCleanup: Removing perturbations\n\n**EdgeWeightMode Enum:**\n- kDantzig: Original pricing\n- kDevex: Approximate steepest edge\n- kSteepestEdge: Exact steepest edge\n\n**RebuildReason Enum:**\nReasons to reinvert basis matrix (INVERT):\n- kRebuildReasonUpdateLimitReached: Too many eta factors\n- kRebuildReasonSyntheticClockSaysInvert: Time-based trigger\n- kRebuildReasonPossiblyOptimal: Check optimality\n\n**Nonbasic Status Constants:**\n- kNonbasicMoveUp/Dn/Ze: Variable movement directions\n- kNonbasicFlagTrue/False: Basic/nonbasic indicator", "see": ["simplex/HEkk.h for simplex implementation", "lp_data/HConst.h for general constants"], "has_pass2": false}, "highs/simplex/HEkk.h": {"path": "layer-4/HiGHS/highs/simplex/HEkk.h", "filename": "HEkk.h", "file": "simplex/HEkk.h", "brief": "Edinburgh simplex kernel - high-performance LP solver core\n\nHEkk (Edinburgh Kernel) is the main simplex implementation in HiGHS,\nsupporting both dual and primal simplex methods.\n\n**HEkk Class:**\nCentral simplex solver managing LP data, basis, and solve state:\n- solve(): Run simplex algorithm (auto-selects dual/primal)\n- setBasis(): Initialize from HighsBasis\n- getSolution(): Extract primal/dual solution\n\n**Key Components:**\n- lp_: The LP being solved (may be scaled/dualized copy)\n- basis_: SimplexBasis with basic variable indices and status\n- simplex_nla_: Numeric linear algebra (factorization)\n- dual_edge_weight_: Steepest edge or Devex weights\n\n**Simplex Operations:**\n- btran/ftran: Backward/forward transformation with basis\n- pivotColumnFtran: Compute pivot column for ratio test\n- unitBtran: Compute row of B^{-1}\n\n**Transformations:**\n- dualize/undualize: Convert LP to/from dual form\n- permute/unpermute: Reorder LP for efficiency\n\n**Parallelism:**\n- chooseSimplexStrategyThreads(): Configure parallel strategy", "algorithm": "Hyper-sparse Computation:\n  Exploits sparsity in FTRAN/BTRAN when < 10% of elements nonzero.\n  Uses specialized scatter/gather for cache efficiency.", "complexity": "Per-iteration: O(nnz(B^{-1}·v)) for FTRAN/BTRAN\n  Hyper-sparse: O(nnz(result)) when exploiting sparsity\n  Parallelization: independent FTRAN/BTRAN across columns", "ref": ["Huangfu, Q. and Hall, J.A.J. (2018). \"Parallelizing the dual revised\n  simplex method\". Math. Prog. Computation 10:119-142.", "Hall, J.A.J. and McKinnon, K.I.M. (2005). \"Hyper-sparsity in the\n  revised simplex method and how to exploit it\". Comp. Opt. Appl. 32:259-283."], "see": ["HEkkDual.h for dual simplex implementation", "HEkkPrimal.h for primal simplex implementation", "HSimplexNla.h for basis factorization", "SimplexStruct.h for SimplexBasis, HighsSimplexStatus"], "has_pass2": true}, "highs/simplex/HSimplexReport.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplexReport.h", "filename": "HSimplexReport.h", "file": "simplex/HSimplexReport.h", "brief": "Simplex iteration progress reporting\n\nFunctions for logging simplex solve progress.\n\n**reportSimplexPhaseIterations():**\nReports iteration counts and phase transitions:\n- iteration_count: Total simplex iterations\n- info: HighsSimplexInfo with infeasibility counts\n- initialise: Reset counters for new phase\n\n**Output Includes:**\n- Phase 1/2 transition points\n- Iteration milestones\n- Infeasibility reduction progress", "see": ["simplex/HEkk.h for simplex solver using this", "simplex/HighsSimplexAnalysis.h for detailed analysis"], "has_pass2": false}, "highs/simplex/HEkkPrimal.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkPrimal.h", "filename": "HEkkPrimal.h", "file": "simplex/HEkkPrimal.h", "brief": "Phase 2 primal simplex solver for HiGHS\n\nImplements primal simplex with column selection (CHUZC) and row\nselection (CHUZR). Primarily used for Phase 1 when dual is infeasible.\n\n**Key Operations:**\n- chuzc()/chooseColumn(): Select entering variable (violating bound)\n- chooseRow(): Ratio test to find leaving variable\n- considerBoundSwap(): Handle bound-to-bound moves\n- updateDual(): Maintain reduced costs after pivot\n\n**Hyper-Sparse CHUZC:**\n- hyper_chuzc_candidate[]: Top candidates for entering\n- hyperChooseColumn*(): Efficient candidate tracking\n\n**Edge Weights:**\n- edge_weight_[]: Pricing weights (Devex or Steepest Edge)\n- devex_index_[]: Reference set for Devex framework\n- updatePrimalSteepestEdgeWeights(): Exact weight updates\n\n**Phase 1:**\n- phase1ChooseRow(): Modified ratio test for feasibility\n- phase1ComputeDual(): Dual values for infeasibility measure", "see": ["simplex/HEkk.h for main simplex class", "simplex/HEkkDual.h for dual simplex"], "has_pass2": false}, "highs/simplex/HEkkDualRHS.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkDualRHS.h", "filename": "HEkkDualRHS.h", "file": "simplex/HEkkDualRHS.h", "brief": "Dual simplex optimality test for HiGHS", "has_pass2": false}, "highs/simplex/HSimplex.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplex.h", "filename": "HSimplex.h", "file": "simplex/HSimplex.h", "brief": "Simplex utility functions\n\nStandalone functions for simplex basis and scaling operations.\n\n**Basis Extension:**\n- appendNonbasicColsToBasis(): Add new columns as nonbasic\n- appendBasicRowsToBasis(): Add new rows as basic (slack variables)\nWorks with both HighsBasis and SimplexBasis.\n\n**Solution Status:**\n- getUnscaledInfeasibilities(): Compute infeasibilities in original space\n- setSolutionStatus(): Set primal/dual solution status in HighsInfo\n\n**Scaling:**\n- scaleSimplexCost(): Scale objective coefficients\n- unscaleSimplexCost(): Reverse objective scaling\n\n**Validation:**\n- isBasisRightSize(): Check SimplexBasis dimensions match LP", "see": ["simplex/HEkk.h for main simplex solver class", "simplex/SimplexStruct.h for SimplexBasis definition"], "has_pass2": false}, "highs/simplex/SimplexTimer.h": {"path": "layer-4/HiGHS/highs/simplex/SimplexTimer.h", "filename": "SimplexTimer.h", "file": "simplex/SimplexTimer.h", "brief": "Timer clock indices for profiling simplex operations\n\nDefines clock identifiers for fine-grained simplex timing.\n\n**iClockSimplex Enum:**\nClock indices for HighsTimer used by simplex:\n\n**Top-Level Clocks:**\n- SimplexTotalClock: Total simplex time\n- SimplexDualPhase1/2Clock: Dual simplex phases\n- SimplexPrimalPhase1/2Clock: Primal simplex phases\n\n**Iteration Clocks:**\n- IterateClock: Per-iteration timing\n- IterateChuzrClock: CHUZR (row selection)\n- IterateChuzcClock: CHUZC (column selection)\n- IterateFtranClock: FTRAN operations\n- IterateDualClock: Dual value updates\n- IteratePrimalClock: Primal value updates\n\n**Infrastructure Clocks:**\n- InvertClock: Basis factorization\n- BasisConditionClock: Condition estimation\n- DseIzClock: DSE weight initialization", "see": ["util/HighsTimer.h for timer implementation", "simplex/HighsSimplexAnalysis.h for timing analysis"], "has_pass2": false}, "highs/simplex/HSimplexDebug.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplexDebug.h", "filename": "HSimplexDebug.h", "file": "simplex/HSimplexDebug.h", "brief": "Debug utilities for simplex algorithm validation\n\nFunctions to detect numerical issues and algorithm failures.\n\n**CHUZC Failure Diagnosis:**\n- debugDualChuzcFailNorms(): Compute norms for failure analysis\n- debugDualChuzcFailQuad0/1(): Check quadratic pricing failures\n- debugDualChuzcFailHeap(): Check heap-based pricing failures\n\n**Basis Validation:**\n- debugNonbasicFlagConsistent(): Verify nonbasicFlag matches basis\n\n**Usage:**\nCalled when simplex encounters unexpected conditions:\n- Ratio test returns no candidate\n- Basis becomes numerically unstable\n- Iteration cycling detected\n\n**Return Type:**\nHighsDebugStatus indicates severity:\n- kOk: No issues detected\n- kWarning: Potential numerical concern\n- kError: Algorithmic failure", "see": ["simplex/HEkkDual.h for dual simplex CHUZC", "simplex/HEkkPrimal.h for primal simplex CHUZR"], "has_pass2": false}, "highs/simplex/HEkkDual.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkDual.h", "filename": "HEkkDual.h", "file": "simplex/HEkkDual.h", "brief": "Dual simplex solver for HiGHS\n\nImplements dual simplex algorithm with CHUZR (row selection), PRICE\n(pivot row computation), CHUZC (column selection), and basis update.\n\n**Parallelization Strategies:**\n- Plain: Serial dual simplex (kSimplexStrategyDualPlain)\n- SIP: Suboptimization with Independent Parallelism (Tasks)\n- PAMI: Parallel Minor Iterations (Multi)\n\n**Key Phases:**\n- Phase 1: Minimize sum of infeasibilities to find feasible basis\n- Phase 2: Optimize objective maintaining dual feasibility\n\n**Edge Weight Modes:**\n- Dantzig: Simple pricing\n- Devex: Approximate steepest edge\n- Steepest Edge: Exact steepest edge with DSE vector updates\n\n**PAMI Data Structures:**\n- MChoice: Multiple row candidates from CHUZR\n- MFinish: Minor iteration data for parallel updates\n- slice_*: Partitioned matrix for parallel PRICE", "algorithm": "PAMI - Parallel Minor Iterations (iterateMulti):\n  Exploit parallelism by batching multiple pivots:\n  1. majorChooseRow: Select k candidate leaving rows in parallel\n  2. majorChooseRowBtran: Parallel BTRAN for all k row_ep vectors\n  3. For each minor iteration i = 1..k:\n     a. minorChooseRow: Pick best from remaining candidates\n     b. chooseColumnSlice: Parallel PRICE across matrix slices\n     c. minorUpdate: Update data structures locally\n  4. majorUpdate: Apply all k updates to LU factors at once", "ref": ["Maros, I. (2003). \"Computational Techniques of the Simplex Method\".\n       Springer, Chapters 9-10.", "Forrest, J.J. and Goldfarb, D. (1992). \"Steepest-edge simplex\n       algorithms for linear programming\". Math. Programming 57:341-374.", "Harris, P.M.J. (1973). \"Pivot selection methods of the Devex\n       LP code\". Math. Programming Study 4:30-57.", "Hall, J.A.J. and McKinnon, K.I.M. (2005). \"Hyper-sparsity in the\n       revised simplex method and how to exploit it\". CMS 2(1):21-40."], "math": "Edge weight γ_i = ||B^{-1} e_i||² = ||τ_i||²\n  After basis change with pivot row r and column s:\n  - Compute τ_s = B^{-1} e_s (FTRAN of unit vector)\n  - Update weights: γ_i' = γ_i - 2(τ_r · a_q)α_i/α_r + γ_r(α_i/α_r)²\n  Cost: one extra FTRAN per iteration, but ~40% fewer iterations.", "complexity": "Serial iteration: O(nnz) for BTRAN/FTRAN, O(m) for ratio test\n  PAMI: O(k × nnz/p + k² × m) for k pivots on p processors\n  Total iterations: typically O(m) to O(2m) for dual simplex", "see": ["simplex/HEkk.h for main simplex class", "simplex/HEkkDualRow.h for pivot row handling", "simplex/HEkkDualRHS.h for RHS management"], "has_pass2": true}, "highs/simplex/HighsSimplexAnalysis.h": {"path": "layer-4/HiGHS/highs/simplex/HighsSimplexAnalysis.h", "filename": "HighsSimplexAnalysis.h", "file": "simplex/HighsSimplexAnalysis.h", "brief": "Analyse simplex iterations, both for run-time control and data\ngathering", "has_pass2": false}, "highs/simplex/SimplexStruct.h": {"path": "layer-4/HiGHS/highs/simplex/SimplexStruct.h", "filename": "SimplexStruct.h", "file": "simplex/SimplexStruct.h", "brief": "Core data structures for HiGHS simplex solvers\n\n**SimplexBasis:**\nLow-level basis representation:\n- basicIndex_[row]: Variable index of basic variable in each row\n- nonbasicFlag_[var]: 0=basic, 1=nonbasic\n- nonbasicMove_[var]: Direction to move (-1=lower, +1=upper, 0=fixed/free)\n- hash: Basis fingerprint for debugging\n\n**HighsSimplexStatus:**\nSimplex solver state flags:\n- has_basis, has_ar_matrix, has_nla, has_invert\n- has_dual_steepest_edge_weights\n- is_dualized, is_permuted (problem transformations)\n\n**HighsSimplexInfo:**\nWorking data for simplex iterations:\n- workCost_/workDual_/workShift_: Objective and dual values\n- workLower_/workUpper_/workRange_/workValue_: Nonbasic variable bounds\n- baseLower_/baseUpper_/baseValue_: Basic variable data\n- Perturbation flags: costs_perturbed, bounds_perturbed\n- Infeasibility counts: num_primal/dual_infeasibilities\n- Backtracking data for singular basis recovery\n- DSE (Dual Steepest Edge) control parameters\n\n**HighsSimplexBadBasisChangeRecord:**\nTracks tabooed pivot operations to avoid cycling.\n\n**HighsRayRecord:**\nStores primal/dual ray for unbounded/infeasible detection.", "see": ["simplex/HEkk.h for simplex solver using these structures", "simplex/SimplexConst.h for simplex constants"], "has_pass2": false}, "highs/simplex/HEkkDualRow.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkDualRow.h", "filename": "HEkkDualRow.h", "file": "simplex/HEkkDualRow.h", "brief": "Dual simplex ratio test for HiGHS", "has_pass2": false}, "highs/parallel/HighsParallel.h": {"path": "layer-4/HiGHS/highs/parallel/HighsParallel.h", "filename": "HighsParallel.h", "file": "parallel/HighsParallel.h", "brief": "High-level parallel execution API with spawn/sync pattern\n\nTask-based parallelism modeled on Intel TBB / Cilk Plus semantics.\n\n**Initialization:**\n- initialize_scheduler(): Set up thread pool\n  - Default: hardware_concurrency/2 threads\n  - HIGHS_NO_DEFAULT_THREADS: Force single-threaded\n\n**Spawn/Sync Pattern:**\n- spawn(f): Push task to local deque, may be stolen\n- sync(): Wait for most recent spawn to complete\n- Child stealing: spawned tasks run depth-first or stolen\n\n**TaskGroup:**\n- RAII wrapper for spawn/sync blocks\n- taskWait(): Wait for all spawned tasks\n- cancel(): Mark pending tasks as cancelled\n- Destructor ensures cleanup\n\n**Parallel Loops:**\n- for_each(start, end, f, grainSize): Recursive binary splitting\n  - Splits until range <= grainSize\n  - Spawns right half, executes left half\n  - Task parallelism with load balancing\n\n**Thread Info:**\n- num_threads(): Total worker count\n- thread_num(): Current worker ID", "see": ["parallel/HighsTaskExecutor.h for executor implementation", "parallel/HighsSplitDeque.h for work-stealing deque"], "has_pass2": false}, "highs/parallel/HighsTask.h": {"path": "layer-4/HiGHS/highs/parallel/HighsTask.h", "filename": "HighsTask.h", "file": "parallel/HighsTask.h", "brief": "Fixed-size task for work-stealing scheduler\n\nInline callable storage with atomic state for task synchronization.\n\n**Memory Layout:**\n- taskData[]: 64-sizeof(Metadata) bytes for callable storage\n- metadata.stealer: Atomic for stealer pointer + status flags\n- Total: kMaxTaskSize = 64 bytes (cache line sized)\n\n**Status Flags (packed in stealer pointer):**\n- kFinishedFlag: Task execution completed\n- kCancelFlag: Task marked for cancellation\n- Pointer bits: Which worker stole this task\n\n**Callable Storage:**\n- Type-erased via virtual CallableBase interface\n- Placement new into taskData buffer\n- Requires trivially destructible callables\n\n**State Transitions:**\n- setTaskData(): Initialize with callable, stealer=0\n- run() by owner: Execute if not cancelled\n- run(stealer): Execute and mark stealer, return owner to notify\n- markAsFinished(): Set finished flag, return waiting owner\n- cancel(): Set cancel flag atomically\n\n**Interrupt Handling:**\n- HighsTask::Interrupt exception for task cancellation\n- Propagates up through task tree on cancel", "see": ["parallel/HighsSplitDeque.h for task queuing", "parallel/HighsTaskExecutor.h for execution"], "has_pass2": false}, "highs/parallel/HighsSplitDeque.h": {"path": "layer-4/HiGHS/highs/parallel/HighsSplitDeque.h", "filename": "HighsSplitDeque.h", "file": "parallel/HighsSplitDeque.h", "brief": "Chase-Lev work-stealing deque with split point optimization\n\nLock-free concurrent deque enabling efficient task-parallel execution.\n\n**Chase-Lev Deque:**\n- Owner pushes/pops from head (LIFO for locality)\n- Thieves steal from tail (FIFO for load balancing)\n- Single-word CAS for conflict resolution\n\n**Split Point Optimization:**\n- tail/split packed in 64-bit atomic (ts)\n- Split divides deque: [tail, split) available for stealing\n- Owner controls split growth via growShared()/shrinkShared()\n- Reduces contention when deque not fully shared\n\n**Memory Layout (Cache-Aligned):**\n- OwnerData (64B): head, split copy, workers, RNG\n- splitRequest (64B): Flag for thieves requesting more work\n- StealerData (64B): semaphore, injectedTask, ts atomic\n- WorkerBunkData (64B): nextSleeper for sleep stack\n- taskArray: 8192 task slots\n\n**WorkerBunk:**\n- Global sleep/wake coordination across workers\n- Lock-free sleeper stack with ABA-safe CAS\n- publishWork(): Wake sleepers when work available\n\n**Pop Status:**\n- kEmpty: No tasks\n- kStolen: Task was stolen, need sync\n- kWork: Task available for execution\n- kOverflown: Queue full, task executed inline", "see": ["parallel/HighsTaskExecutor.h for executor using this deque", "parallel/HighsTask.h for task representation"], "has_pass2": false}, "highs/parallel/HighsCacheAlign.h": {"path": "layer-4/HiGHS/highs/parallel/HighsCacheAlign.h", "filename": "HighsCacheAlign.h", "file": "parallel/HighsCacheAlign.h", "brief": "Cache-line aligned memory allocation for parallel performance\n\nPrevents false sharing by ensuring separate objects reside on different cache lines.\n\n**cache_aligned Struct:**\nStatic utilities for aligned allocation:\n- alignment() → 64 bytes (typical cache line size)\n- alloc(size): Allocate with 64-byte alignment\n- free(ptr): Deallocate aligned memory\n\n**Implementation:**\n- Over-allocates by alignment bytes\n- Stores original pointer before aligned address\n- Retrieves original pointer for deallocation\n\n**Smart Pointer Support:**\n- Deleter<T>: Custom deleter calling destructor + free\n- unique_ptr<T>: Alias with custom deleter\n- make_unique<T>(): Factory function\n- make_unique_array<T>(n): Array allocation\n\n**Usage:**\nWrap thread-local data in cache_aligned::unique_ptr to prevent\nperformance degradation from false sharing in parallel algorithms.", "see": ["parallel/HighsCombinable.h for thread-local storage", "parallel/HighsSplitDeque.h for aligned worker data"], "has_pass2": false}, "highs/parallel/HighsSpinMutex.h": {"path": "layer-4/HiGHS/highs/parallel/HighsSpinMutex.h", "filename": "HighsSpinMutex.h", "file": "parallel/HighsSpinMutex.h", "brief": "Lightweight spin lock for short critical sections\n\nBusy-waits instead of blocking, ideal for short-held locks.\n\n**HighsSpinMutex Class:**\nSimple test-and-set spin lock:\n- flag: Atomic bool (false = unlocked)\n- try_lock(): Non-blocking lock attempt\n- lock(): Spin until acquired\n- unlock(): Release with release semantics\n\n**Spinning Strategy:**\n- yieldProcessor(): CPU hint during spin wait\n  - x86: _mm_pause() (reduces power, prevents pipeline stalls)\n  - Others: std::this_thread::yield()\n\n**TTAS Pattern:**\nTest-and-Test-and-Set for reduced bus traffic:\n1. Exchange to try acquiring\n2. If failed, spin on load (cache-local)\n3. Only retry exchange when flag appears free\n\n**When to Use:**\nPrefer over std::mutex when critical sections are very short\nand contention is low. Avoids syscall overhead.", "see": ["parallel/HighsTaskExecutor.h for task queue locking"], "has_pass2": false}, "highs/parallel/HighsCombinable.h": {"path": "layer-4/HiGHS/highs/parallel/HighsCombinable.h", "filename": "HighsCombinable.h", "file": "parallel/HighsCombinable.h", "brief": "Thread-local storage with reduction for parallel algorithms\n\nPer-thread copies that are lazily initialized and can be combined.\n\n**Design (similar to TBB combinable):**\n- One T instance per worker thread\n- Cache-line aligned to prevent false sharing\n- Lazy initialization on first access\n\n**Construction:**\n- Default: T() for each thread's copy\n- Custom: Callable returning T for each thread\n\n**Access:**\n- local(): Get/create this thread's copy\n- Thread ID from HighsTaskExecutor::getThisWorkerDeque()\n\n**Reduction:**\n- combine_each(f): Apply f to each initialized copy\n- combine(f): Reduce all copies with binary operation f\n  - Returns combined result by move\n\n**Usage Pattern:**\n```cpp\nHighsCombinable<int> sum;\nparallel::for_each([&](int i, int j) {\n  sum.local() += compute(i, j);\n});\nint total = sum.combine(std::plus<int>());\n```", "see": ["parallel/HighsTaskExecutor.h for worker identification", "parallel/HighsParallel.h for parallel algorithms"], "has_pass2": false}, "highs/parallel/HighsTaskExecutor.h": {"path": "layer-4/HiGHS/highs/parallel/HighsTaskExecutor.h", "filename": "HighsTaskExecutor.h", "file": "parallel/HighsTaskExecutor.h", "brief": "Work-stealing thread pool for task parallelism\n\nManages worker threads and coordinates task execution via work stealing.\n\n**Thread Pool:**\n- Main thread (worker 0) + N-1 spawned workers\n- Each worker has own HighsSplitDeque for local tasks\n- Thread-local storage for current worker's deque\n\n**Work Stealing Loop:**\n- random_steal_loop(): Try stealing from random victims\n  - Exponential backoff with microsecond timing\n  - Falls back to global sync after timeout\n- Workers sleep when no work available (WorkerBunk)\n\n**Stolen Task Sync:**\n- sync_stolen_task(): Wait for task stolen by another worker\n  - Leapfrog stealing: steal from the stealer\n  - Spin wait with exponential backoff\n  - Sleep with notification when timeout exceeded\n\n**Lifecycle:**\n- initialize(): Create executor singleton\n- shutdown(): Stop all workers, join or detach threads\n- ExecutorHandle: RAII cleanup on thread exit", "see": ["parallel/HighsSplitDeque.h for deque implementation", "parallel/HighsParallel.h for high-level API"], "has_pass2": false}, "highs/parallel/HighsRaceTimer.h": {"path": "layer-4/HiGHS/highs/parallel/HighsRaceTimer.h", "filename": "HighsRaceTimer.h", "file": "parallel/HighsRaceTimer.h", "brief": "Lock-free timer for parallel algorithm racing\n\nAllows multiple threads to race, with early termination when limit reached.\n\n**HighsRaceTimer<T> Class:**\nAtomic limit value that can only decrease:\n- Constructor: Initialize limit to max value (no limit)\n- decreaseLimit(newLimit): Atomically reduce limit (CAS loop)\n- limitReached(currentTime): Check if time exceeds limit\n\n**Memory Ordering:**\nUses relaxed ordering since:\n- Only one direction (decrease) of updates\n- Eventual consistency sufficient for early termination\n- No happens-before relationships required\n\n**Usage:**\nMultiple solvers race; first to find solution decreases limit.\nSlower solvers check limitReached() and abort early.", "see": ["mip/HighsSearch.h for parallel node processing"], "has_pass2": false}, "highs/model/HighsHessianUtils.h": {"path": "layer-4/HiGHS/highs/model/HighsHessianUtils.h", "filename": "HighsHessianUtils.h", "file": "model/HighsHessianUtils.h", "brief": "Utility functions for Hessian matrix manipulation\n\nFree functions for validating, transforming, and operating on Hessian matrices.\n\n**Validation:**\n- assessHessian(): Comprehensive Hessian validation\n- assessHessianDimensions(): Check dimension consistency\n- okHessianDiagonal(): Verify positive diagonal for convexity\n\n**Transformation:**\n- normaliseHessian(): Standardize Hessian format\n- extractTriangularHessian(): Convert to lower-triangular storage\n- triangularToSquareHessian(): Expand to full symmetric matrix\n- completeHessianDiagonal(): Add missing diagonal entries\n- completeHessian(): Extend to full variable dimension\n\n**Scaling:**\n- userScaleHessian(): Apply/remove user-provided scaling factors\n\n**Reporting:**\n- reportHessian(): Print Hessian structure and values", "see": ["model/HighsHessian.h for Hessian data structure", "model/HighsModel.h for combined LP+QP model"], "has_pass2": false}, "highs/model/HighsModel.h": {"path": "layer-4/HiGHS/highs/model/HighsModel.h", "filename": "HighsModel.h", "file": "model/HighsModel.h", "brief": "Combined LP/QP model container\n\n**HighsModel Class:**\nCombines HighsLp (linear constraints/objective) with HighsHessian (quadratic terms).\n\n**Data Members:**\n- lp_: Linear program data (constraints, bounds, linear objective)\n- hessian_: Quadratic objective term Q for QP: min 0.5*x'Qx + c'x\n\n**Model Type Detection:**\n- isQp(): Returns true if hessian_.dim_ != 0\n- isMip(): Delegates to lp_.isMip() (checks integrality constraints)\n- isEmpty(): True if num_col_ == 0 and num_row_ == 0\n\n**Objective Evaluation:**\n- objectiveValue(): Compute c'x + 0.5*x'Qx\n- objectiveGradient(): Compute c + Qx", "see": ["HighsLp.h for LP data structure", "HighsHessian.h for Hessian matrix"], "has_pass2": false}, "highs/model/HighsHessian.h": {"path": "layer-4/HiGHS/highs/model/HighsHessian.h", "filename": "HighsHessian.h", "file": "model/HighsHessian.h", "brief": "Sparse Hessian matrix for QP objective\n\n**HighsHessian Class:**\nStores quadratic objective term Q for QP: min 0.5*x'Qx + c'x\n\n**Sparse Storage (CSC-like):**\n- dim_: Number of variables (Q is dim_ x dim_)\n- format_: kTriangular (lower triangle) or kSquare (full matrix)\n- start_[]: Column start indices\n- index_[]: Row indices\n- value_[]: Non-zero values\n\n**Operations:**\n- product(): Compute Qx (Hessian-vector product)\n- objectiveValue(): Compute 0.5*x'Qx\n- objectiveCDoubleValue(): High-precision objective using HighsCDouble\n- deleteCols(): Remove columns/rows for presolve", "see": ["HighsModel.h for combined LP+Hessian model", "HConst.h for HessianFormat enum"], "has_pass2": false}, "highs/ipm/IpxWrapper.h": {"path": "layer-4/HiGHS/highs/ipm/IpxWrapper.h", "filename": "IpxWrapper.h", "file": "ipm/IpxWrapper.h", "brief": "Wrapper for IPX interior point solver\n\nProvides interface between HiGHS and the IPX interior point solver.\n\n**Main Functions:**\n- solveLpIpx(): Solve LP using IPX with optional crossover to basis\n- fillInIpxData(): Convert HighsLp to IPX's input format\n- getHighsNonVertexSolution(): Extract non-vertex (interior) solution\n\n**Status Reporting:**\n- reportIpxSolveStatus(): Convert IPX status to HighsStatus\n- reportIpxIpmCrossoverStatus(): Report IPM/crossover termination\n- ipxStatusError(): Handle IPX error conditions\n- reportIpmNoProgress(): Log stalled IPM iterations\n\n**HIPO Support (optional):**\nParallel interior point variant (compile-time flag HIPO):\n- solveLpHipo(): High-performance IPM solver\n- getHipoNonVertexSolution(): Extract HIPO solution", "see": ["ipm/ipx/lp_solver.h for IPX solver class", "ipm/IpxSolution.h for IPX solution struct", "lp_data/HighsSolution.h for solution conversion utilities"], "has_pass2": false}, "highs/ipm/IpxSolution.h": {"path": "layer-4/HiGHS/highs/ipm/IpxSolution.h", "filename": "IpxSolution.h", "file": "ipm/IpxSolution.h", "brief": "IPX interior point solution container\n\nStores primal/dual solution and basis status from IPX solver.\n\n**IpxSolution Struct:**\nSolution data in IPX's native format for transfer to HiGHS:\n- num_col, num_row: Problem dimensions\n- ipx_col_value[], ipx_row_value[]: Primal solution (x, slacks)\n- ipx_col_dual[], ipx_row_dual[]: Dual solution (reduced costs, duals)\n- ipx_col_status[], ipx_row_status[]: Basis status for crossover\n\n**Status Values:**\nIPX uses its own status encoding (see ipx_status.h):\n- Basic, AtLower, AtUpper, Free for variables\n- Converted to HighsBasisStatus in IpxWrapper\n\n**Usage:**\n1. IPX populates after solve\n2. IpxWrapper extracts to HighsSolution/HighsBasis\n3. Crossover uses basis status for simplex warm start", "see": ["ipm/IpxWrapper.h for solution extraction", "lp_data/HighsSolution.h for HiGHS solution format"], "has_pass2": false}, "highs/mip/HighsPrimalHeuristics.h": {"path": "layer-4/HiGHS/highs/mip/HighsPrimalHeuristics.h", "filename": "HighsPrimalHeuristics.h", "file": "mip/HighsPrimalHeuristics.h", "brief": "Primal heuristics for finding MIP feasible solutions\n\nCollection of primal heuristics to discover incumbent solutions.\n\n**Sub-MIP Heuristics:**\n- solveSubMip(): Solve restricted MIP with fixed/bounded variables\n- RENS(): Relaxation Enforced Neighborhood Search (fix non-LP-integer vars)\n- RINS(): Relaxation Induced Neighborhood Search (fix incumbent-matching vars)\n\n**Rounding Heuristics:**\n- feasibilityPump(): Iterate between LP and MIP rounding until feasible\n- centralRounding(): Round from analytic center\n- randomizedRounding(): Probabilistic rounding based on fractionality\n- ziRound(): Berthold's ZI-Round shifting\n- tryRoundedPoint(): Simple rounding with constraint repair\n- linesearchRounding(): Round along line between two points\n\n**Other Methods:**\n- rootReducedCost(): Fix variables using reduced costs at root\n- shifting(): Variable shifting to repair constraint violations\n\n**Adaptive Targeting:**\n- determineTargetFixingRate(): Adjust fixing rate based on success history\n- successObservations/infeasObservations: Track heuristic effectiveness", "see": ["mip/HighsMipSolverData.h for heuristic controller", "mip/HighsLpRelaxation.h for LP solution access"], "has_pass2": false}, "highs/mip/HighsRedcostFixing.h": {"path": "layer-4/HiGHS/highs/mip/HighsRedcostFixing.h", "filename": "HighsRedcostFixing.h", "file": "mip/HighsRedcostFixing.h", "brief": "Reduced cost fixing and lurking bounds for MIP\n\nUses LP reduced costs to fix variable bounds based on cutoff.\n\n**Reduced Cost Fixing:**\nIf fixing variable j to its bound would increase objective beyond cutoff,\nthe opposite bound becomes valid. For minimization:\n- If reduced_cost[j] > 0 and x[j] = lb: fixing to ub proves lb valid\n- If reduced_cost[j] < 0 and x[j] = ub: fixing to lb proves ub valid\n\n**Lurking Bounds:**\nBounds that become valid at specific objective values:\n- lurkingColUpper[col]: (objective_threshold, bound_value) pairs\n- lurkingColLower[col]: Maps threshold to tighter bound\n- getLurkingBounds(): Extract bounds valid at current cutoff\n\n**Propagation Methods:**\n- propagateRootRedcost(): Apply fixings at root using stored costs\n- propagateRedCost(): Apply fixings at any node using LP reduced costs\n- addRootRedcost(): Store root LP reduced costs for later use\n\n**Integration:**\n- Called when incumbent improves (new cutoff enables more fixings)\n- Provides global domain tightening from LP dual information", "see": ["mip/HighsDomain.h for bound propagation", "mip/HighsLpRelaxation.h for LP solution access"], "has_pass2": false}, "highs/mip/HighsDebugSol.h": {"path": "layer-4/HiGHS/highs/mip/HighsDebugSol.h", "filename": "HighsDebugSol.h", "file": "mip/HighsDebugSol.h", "brief": "Debug solution tracking for MIP solver validation\n\nConditionally compiled (HIGHS_DEBUGSOL) facility to verify MIP solver\ncorrectness against a known optimal solution.\n\n**When HIGHS_DEBUGSOL Defined:**\n- debugSolution[]: Known optimal solution values\n- debugSolObjective: Optimal objective value\n- conflictingBounds: Tracks domain changes inconsistent with debug solution\n\n**Validation Methods:**\n- checkCut(): Verify cut doesn't exclude debug solution\n- checkRow(): Verify row bounds contain debug solution\n- checkClique(): Verify clique is satisfied by debug solution\n- checkVub/Vlb(): Verify variable bounds contain debug solution\n- nodePruned(): Alert if node containing debug solution is pruned\n\n**Domain Tracking:**\n- registerDomain(): Track a HighsDomain instance\n- boundChangeAdded/Removed(): Monitor bound changes\n- resetDomain(): Clear domain tracking\n\n**Conflict Analysis:**\n- checkConflictReasonFrontier(): Verify conflict derivation\n- checkConflictReconvergenceFrontier(): Verify reconvergence\n\n**When HIGHS_DEBUGSOL Not Defined:**\nAll methods become empty stubs with zero overhead.", "see": ["mip/HighsDomain.h for domain propagation", "mip/HighsConflictPool.h for conflict constraints"], "has_pass2": false}, "highs/mip/HighsSearch.h": {"path": "layer-4/HiGHS/highs/mip/HighsSearch.h", "filename": "HighsSearch.h", "file": "mip/HighsSearch.h", "brief": "Branch-and-bound tree search for MIP solver\n\nImplements depth-first search with backtracking and node evaluation.\n\n**Node Management:**\n- nodestack[]: Stack of NodeData for current branch\n- NodeData: {lower_bound, estimate, branchingdecision, nodeBasis, etc.}\n- localdom: HighsDomain for current node's variable bounds\n\n**NodeResult Enum:**\n- kBoundExceeding: Node bound exceeds cutoff\n- kDomainInfeasible: Domain propagation found infeasibility\n- kLpInfeasible: LP relaxation infeasible\n- kBranched: Node branched on integer variable\n- kSubOptimal: Node proven suboptimal\n\n**Child Selection Rules:**\n- kUp/kDown: Always branch up/down first\n- kRootSol: Prefer direction toward root solution\n- kBestCost/kWorstCost: Use pseudocost estimates\n- kHybridInferenceCost: Combine inference and cost scores\n\n**Key Operations:**\n- dive(): Descend tree solving LP at each node\n- evaluateNode(): Solve LP relaxation, check bounds\n- branch(): Select variable and create child nodes\n- selectBranchingCandidate(): Strong branching + pseudocosts\n- backtrack(): Return to parent node\n- backtrackPlunge(): Intelligent backtracking with node queue\n\n**Heuristic Support:**\n- setRINSNeighbourhood(): Fix variables from incumbent\n- setRENSNeighbourhood(): Round LP solution", "see": ["mip/HighsNodeQueue.h for best-first node storage", "mip/HighsPseudocost.h for branching scores"], "has_pass2": false}, "highs/mip/HighsTableauSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsTableauSeparator.h", "filename": "HighsTableauSeparator.h", "file": "mip/HighsTableauSeparator.h", "brief": "Gomory mixed-integer cuts from LP tableau\n\nGenerates cuts by applying MIR procedure to simplex tableau rows.\n\n**Gomory Cut Generation:**\nFor each fractional basic integer variable:\n1. Extract tableau row: x_B[i] = f_0 - sum(a_j * x_N[j])\n2. Apply mixed-integer rounding (MIR)\n3. Produce cut: sum(floor(a_j) * x_j) ≤ floor(f_0) (simplified)\n\n**Separation Flow:**\n1. Get fractional integer variables from LP relaxation\n2. For each candidate, extract tableau row via BTRAN\n3. Transform using HighsTransformedLp (bound substitution)\n4. Apply MIR strengthening\n5. Add valid cuts to cut pool\n\n**Implementation:**\n- numTries: Counter for separation attempts (controls effort)\n- Uses kTableauSepaString identifier for statistics\n- Inherits run() timing from HighsSeparator base class", "see": ["mip/HighsSeparator.h for base class interface", "mip/HighsTransformedLp.h for bound substitution", "mip/HighsSeparation.h for separator orchestration"], "has_pass2": false}, "highs/mip/HighsGFkSolve.h": {"path": "layer-4/HiGHS/highs/mip/HighsGFkSolve.h", "filename": "HighsGFkSolve.h", "file": "mip/HighsGFkSolve.h", "brief": "Linear system solver over finite field GF(k)\n\nSolves congruence systems for mod-k cut generation.\n\n**HighsGFk<k> Template:**\nCompile-time multiplicative inverse via Fermat's little theorem:\n- inverse(a) = a^(k-2) mod k (when k prime)\n- powk(): Recursive repeated squaring for a^k\n- Specializations for k=2,3 (trivial inverses)\n\n**HighsGFkSolve Class:**\nSparse LU factorization in GF(k):\n\n**Storage:**\n- Triplet format: Arow[], Acol[], Avalue[] (mod k)\n- Column-wise linked list: colhead, Anext, Aprev\n- Row-wise splay tree: rowroot, ARleft, ARright\n\n**Factorization:**\n- Markowitz-style pivot selection (min row × col size)\n- factorColPerm/factorRowPerm: Pivot order recording\n- colBasisStatus/rowUsed: Track basic columns and used rows\n\n**Solution:**\n- solve<k>(): Enumerate basic solutions\n- SolutionEntry: (index, weight) for solution vector\n- Reports multiple solutions via basis swapping\n\n**Usage:**\n1. fromCSC<k>(): Load constraint matrix modulo k\n2. setRhs<k>(): Set right-hand side (k-1 for mod-k MIR)\n3. solve<k>(): Find row weights yielding maximally violated cuts", "see": ["mip/HighsModkSeparator.h for mod-k cut generation using this solver"], "has_pass2": false}, "highs/mip/HighsCutPool.h": {"path": "layer-4/HiGHS/highs/mip/HighsCutPool.h", "filename": "HighsCutPool.h", "file": "mip/HighsCutPool.h", "brief": "Cutting plane storage and separation for MIP solver\n\n**HighsCutSet:**\nContainer for cuts to add to LP relaxation:\n- cutindices: Indices into cut pool\n- ARstart_/ARindex_/ARvalue_: CSR storage for cut coefficients\n- lower_/upper_: Cut bounds (typically -inf, rhs)\n\n**HighsCutPool:**\nManages all generated cutting planes:\n- matrix_: HighsDynamicRowMatrix storing all cuts\n- rhs_[]: Right-hand sides\n- ages_[]: Cut age (incremented when not binding, reset when used)\n- hashToCutMap: Duplicate detection via row hashing\n\n**Cut Lifecycle:**\n- addCut(): Add new cut with duplicate detection and clique extraction\n- separate(): Select violated cuts for LP (scoring + density limits)\n- performAging(): Increment ages, remove old cuts (age > agelim_)\n- lpCutRemoved(): Called when cut leaves LP\n\n**Cut Propagation:**\n- propagationDomains: Registered domains for cut-based bound tightening\n- propRows: Active propagation rows", "see": ["mip/HighsDynamicRowMatrix.h for cut storage", "mip/HighsDomain.h for domain propagation"], "has_pass2": false}, "highs/mip/HighsMipSolver.h": {"path": "layer-4/HiGHS/highs/mip/HighsMipSolver.h", "filename": "HighsMipSolver.h", "file": "mip/HighsMipSolver.h", "brief": "Branch-and-cut MIP solver\n\n**HighsMipSolver Class:**\nMain MIP solver using branch-and-cut with LP relaxations.\n\n**Key State:**\n- model_/orig_model_: Problem LP with integrality constraints\n- solution_: Best incumbent solution found\n- dual_bound_/primal_bound_: Bounds on optimal value\n- gap_: Optimality gap (primal_bound - dual_bound) / primal_bound\n- node_count_: Number of B&B nodes explored\n\n**Sub-MIP Support:**\n- submip/submip_level: For recursive sub-MIP solves (e.g., in heuristics)\n- rootbasis: Starting basis from parent MIP\n- pscostinit: Warm-start pseudocosts\n- clqtableinit/implicinit: Warm-start conflict graph structures\n\n**HighsTerminator:**\nParallel termination coordination for concurrent MIP instances.\n\n**Main Entry:**\n- run(): Execute MIP solve (presolve, root LP, branch-and-cut)\n- runMipPresolve(): Standalone presolve for warm-start scenarios", "see": ["mip/HighsMipSolverData.h for internal B&B data", "mip/HighsCutPool.h for cutting plane management", "mip/HighsCliqueTable.h for clique detection"], "has_pass2": false}, "highs/mip/HighsDynamicRowMatrix.h": {"path": "layer-4/HiGHS/highs/mip/HighsDynamicRowMatrix.h", "filename": "HighsDynamicRowMatrix.h", "file": "mip/HighsDynamicRowMatrix.h", "brief": "Dynamic row matrix with efficient column-sign iteration\n\nSparse row-wise matrix supporting efficient row add/remove with\nseparate iteration over positive and negative column entries.\n\n**Row Storage:**\n- ARrange_[row]: (start, end) range in ARindex_/ARvalue_\n- ARindex_[]/ARvalue_[]: Column indices and values\n- ARrowindex_[]: Row index for each nonzero position\n\n**Column Iteration by Sign:**\nSeparate doubly-linked lists for positive and negative entries:\n- AheadPos_[col], AnextPos_[], AprevPos_[]: Positive entry list\n- AheadNeg_[col], AnextNeg_[], AprevNeg_[]: Negative entry list\n\n**Template Iterators:**\n- forEachPositiveColumnEntry(): Iterate positive entries in column\n- forEachNegativeColumnEntry(): Iterate negative entries in column\n- Useful for implication detection (sign determines bound type)\n\n**Dynamic Operations:**\n- addRow(): Insert row, reusing deleted space from freespaces_ set\n- removeRow(): Remove row, add index to deletedrows_ for reuse\n- unlinkColumns(): Disconnect row from column lists", "see": ["mip/HighsDomain.h for domain propagation using sign iteration", "mip/HighsCutPool.h for similar dynamic cut storage"], "has_pass2": false}, "highs/mip/HighsTransformedLp.h": {"path": "layer-4/HiGHS/highs/mip/HighsTransformedLp.h", "filename": "HighsTransformedLp.h", "file": "mip/HighsTransformedLp.h", "brief": "LP bound transformations for cutting plane separation\n\nTransforms LP rows into single-row relaxations suitable for cut generation\nby substituting bounds and handling complementation.\n\n**Bound Substitution Types (BoundType enum):**\n- kSimpleLb/kSimpleUb: Direct variable bounds (x ≥ lb, x ≤ ub)\n- kVariableLb/kVariableUb: VLB/VUB from implications (x ≥ a*y + b)\n\n**Bound Selection:**\n- bestVlb/bestVub[]: Tightest variable bound for each column\n- simpleLbDist/simpleUbDist[]: Distance of LP value from simple bounds\n- lbDist/ubDist[]: Distance considering all bound types\n- boundDist[]: Minimum distance (used for coefficient strengthening)\n\n**Transform Operations:**\n- transform(): Convert row to standard form for cut generation\n  - Substitutes bounds to get non-negative variables\n  - Tracks which bound type used per variable\n  - Returns integralPositive flag for MIR applicability\n- untransform(): Reverse transformation to original space\n\n**Cut Generation Flow:**\n1. Aggregate LP rows (via HighsLpAggregator)\n2. Transform to standard form (substitute bounds)\n3. Apply cut procedure (Gomory, MIR, etc.)\n4. Untransform back to original variables", "see": ["mip/HighsLpAggregator.h for row aggregation", "mip/HighsImplications.h for VUB/VLB structures", "mip/HighsSeparation.h for cut orchestration"], "has_pass2": false}, "highs/mip/MipTimer.h": {"path": "layer-4/HiGHS/highs/mip/MipTimer.h", "filename": "MipTimer.h", "file": "mip/MipTimer.h", "brief": "Indices of mip iClocks", "has_pass2": false}, "highs/mip/HighsMipSolverData.h": {"path": "layer-4/HiGHS/highs/mip/HighsMipSolverData.h", "filename": "HighsMipSolverData.h", "file": "mip/HighsMipSolverData.h", "brief": "Central data container for MIP solver state\n\nAggregates all MIP solver components and statistics.\n\n**MipSolutionSource enum:**\nTracks origin of solutions for statistics:\n- B: Branching, C: CentralRounding, F: FeasibilityPump\n- H: Heuristic, J: FeasibilityJump, L: SubMip\n- R: RandomizedRounding, S: SolveLp, X: UserSolution, Z: ZiRound\n\n**HighsPrimaDualIntegral:**\nTracks primal-dual gap integral for solution quality metric.\n\n**HighsMipSolverData Components:**\n\n*Core Structures:*\n- cutpool: Global cut storage\n- conflictPool: Learned conflict constraints\n- domain: Global domain with bounds and propagation\n- lp: LP relaxation solver\n- nodequeue: B&B tree node priority queue\n\n*Inference:*\n- cliquetable: Binary variable cliques\n- implications: Variable implications (VUB/VLB)\n- pseudocost: Branching score history\n- redcostfixing: Reduced cost-based bound fixing\n\n*Heuristics:*\n- heuristics: Primal heuristic controller\n- objectiveFunction: Objective analysis for probing\n\n*Symmetry:*\n- symmetries/globalOrbits: Symmetry detection/handling\n- SymmetryDetectionData: Async symmetry computation\n\n*Statistics:*\n- num_nodes/num_leaves: Tree exploration counts\n- total_lp_iterations: LP iteration counters by type\n- lower_bound/upper_bound: Best bounds\n- incumbent: Best solution vector\n\n**Key Methods:**\n- init/runSetup(): Initialize solver state\n- evaluateRootNode(): Root node processing\n- trySolution/addIncumbent(): Solution handling\n- performRestart(): Restart MIP search", "see": ["mip/HighsMipSolver.h for solver interface", "mip/HighsSearch.h for tree search"], "has_pass2": false}, "highs/mip/HighsObjectiveFunction.h": {"path": "layer-4/HiGHS/highs/mip/HighsObjectiveFunction.h", "filename": "HighsObjectiveFunction.h", "file": "mip/HighsObjectiveFunction.h", "brief": "Objective function analysis for MIP solving\n\nAnalyzes objective structure to enable specialized bound propagation.\n\n**Objective Structure:**\n- objectiveNonzeros[]: Columns with nonzero objective (binaries first)\n- objectiveVals[]: Packed objective coefficients\n- numBinary/numIntegral: Variable counts by type\n\n**Integrality Detection:**\n- objIntScale: Scale factor making all integer coefficients integral\n- isIntegral(): True if objective is integer for integer solutions\n- checkIntegrality(): Verify/update integrality with epsilon\n- Enables objective cutoff rounding\n\n**Clique Partitioning:**\nPartitions binary objective variables into cliques:\n- cliquePartitionStart[]: Start indices per partition\n- colToPartition[]: Maps column to its partition\n- At most one variable per clique can be 1\n- Enables tighter objective bounds from clique constraints\n\n**Usage:**\n- Objective bound computation during propagation\n- Detecting integral objectives for cutoff strengthening\n- Clique-based objective probing", "see": ["mip/HighsCliqueTable.h for clique detection", "mip/HighsDomain.h for bound propagation using objective"], "has_pass2": false}, "highs/mip/HighsModkSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsModkSeparator.h", "filename": "HighsModkSeparator.h", "file": "mip/HighsModkSeparator.h", "brief": "Class for separating maximally violated mod-k MIR cuts.\n\nContrary to mod-k CG cuts as described in the literature, continuous\nvariables are allowed to appear in the rows used for separation. In case an\nLP row is already an integral row it is included into the congruence system\nin the same way as for mod-k CG cuts. Should the LP row contain continuous\nvariables that have a non-zero solution value after bound substitution, then\nit is discarded, as it can not participate in a maximally violated mod-K MIR\ncut.\n\nIf a row contains continuous variables that sit at zero after bound\nsubstitution, then those rows are included in the congruence system, as the\npresence of such variables does not reduce the cuts violation when applying\nthe MIR procedure. In order to handle their presence the row must simply be\nscaled, such that all integer variables that have a non-zero solution value\nafter bound substitution, as well as the right hand side value, attain an\nintegral value. If we succeed in finding such a scale that is not too large,\nthe resulting row might get a non-zero weight in the solution of the\ncongruence system. The aggregated row therefore can contain continuous\nvariables. These variables, however, all sit at zero in the current LP\nsolution. Using the weights from the solution of the congruence system all\ninteger variables with non-zero solution value will attain a coefficient that\nis divisible by k, and the integral right hand side value will have a\nremainder of k - 1 when dividing by k. All other variables do not contribute\nto the activity of the cut in this LP solution, hence applying the MIR\nprocedure will yield a cut that is violated by (k-1)/k. However, we prefer to\ngenerate inequalities with superadditive lifting from the aggregated row\nwhenever all integer variables are bounded.", "has_pass2": false}, "highs/mip/HighsPseudocost.h": {"path": "layer-4/HiGHS/highs/mip/HighsPseudocost.h", "filename": "HighsPseudocost.h", "file": "mip/HighsPseudocost.h", "brief": "Pseudocost branching for MIP variable selection\n\nEstimates objective change from branching based on historical observations.\n\n**Score Components:**\n- pseudocostup/down[col]: Average unit objective change per direction\n- inferencesup/down[col]: Average domain reductions (bound tightenings)\n- ncutoffsup/down[col]: Count of subtree cutoffs per direction\n- conflictscoreup/down[col]: Conflict analysis contribution\n\n**Reliability:**\n- nsamplesup/down[col]: Number of observations per direction\n- minreliable: Threshold for reliable pseudocost estimate\n- isReliable(): True when min(up_samples, down_samples) >= minreliable\n- Unreliable estimates blend with global average (cost_total)\n\n**Scoring (getScore):**\nCombines multiple signals into branching score:\n- costScore: Pseudocost product (up * down) / avg^2\n- inferenceScore: Domain reduction product\n- cutoffScore: Historical cutoff frequency\n- conflictScore: Conflict analysis contribution\n- degeneracyFactor: Adjusts weighting for degenerate LPs\n\n**Key Methods:**\n- addObservation(): Record objective change from branching\n- addInferenceObservation(): Record domain reductions\n- getPseudocostUp/Down(): Get estimated change for given fraction", "see": ["mip/HighsSearch.h for branching variable selection", "mip/HighsConflictPool.h for conflict scoring"], "has_pass2": false}, "highs/mip/HighsMipAnalysis.h": {"path": "layer-4/HiGHS/highs/mip/HighsMipAnalysis.h", "filename": "HighsMipAnalysis.h", "file": "mip/HighsMipAnalysis.h", "brief": "Analyse MIP iterations, both for run-time control and data\ngathering", "has_pass2": false}, "highs/mip/HighsCutGeneration.h": {"path": "layer-4/HiGHS/highs/mip/HighsCutGeneration.h", "filename": "HighsCutGeneration.h", "file": "mip/HighsCutGeneration.h", "brief": "Class that generates cuts from single row relaxations", "has_pass2": false}, "highs/mip/HighsDomain.h": {"path": "layer-4/HiGHS/highs/mip/HighsDomain.h", "filename": "HighsDomain.h", "file": "mip/HighsDomain.h", "brief": "Bound tracking, propagation, and conflict analysis for MIP\n\n**HighsDomain Class:**\nManages variable bounds during MIP branch-and-bound with:\n- col_lower_[], col_upper_[]: Current variable bounds\n- domchgstack_[]: Stack of bound changes (for backtracking)\n- domchgreason_[]: Reason for each bound change (branching, row, clique)\n\n**Bound Propagation:**\n- propagate(): Deduce implied bounds from constraints\n- activitymin_[]/activitymax_[]: Row activity bounds for propagation\n- markPropagate(): Queue row for propagation\n- ObjectivePropagation: Deduce bounds from objective cutoff\n\n**Reason Tracking:**\nEach bound change records its cause:\n- kBranching: B&B branching decision\n- kModelRowUpper/Lower: Implied by constraint\n- kCliqueTable: Clique conflict\n- kObjective: Objective bound\n- cut(pool,index): Cut from cut pool\n\n**Conflict Analysis (ConflictSet):**\nLearns from infeasibility:\n- conflictAnalysis(): Build conflict clause from infeasibility proof\n- reasonSideFrontier: Domain changes causing infeasibility\n- Generates learned conflicts for HighsConflictPool\n\n**Cut Pool Integration (CutpoolPropagation):**\n- Tracks activity of cuts for cut-based propagation\n- Updates when bounds change\n\n**Backtracking:**\n- backtrack(): Undo bound changes\n- backtrackToGlobal(): Reset to root node bounds\n- branchPos_[]: Positions of branching decisions in stack", "algorithm": "Objective Propagation:\n  Given incumbent z* and objective c'x:\n  For each variable, derive bound from c'x < z* constraint.\n  Particularly effective when objective has large coefficients.", "complexity": "propagate(): O(nnz × propagation_rounds) amortized\n  conflictAnalysis(): O(conflict_clause_length × reason_chain_depth)\n  backtrack(): O(number_of_bound_changes_to_undo)", "ref": ["Achterberg, T. (2007). \"Conflict analysis in mixed integer\n  programming\". Discrete Optimization 4:4-20."], "see": ["mip/HighsDomainChange.h for bound change struct", "mip/HighsConflictPool.h for learned conflicts"], "has_pass2": true}, "highs/mip/HighsSeparation.h": {"path": "layer-4/HiGHS/highs/mip/HighsSeparation.h", "filename": "HighsSeparation.h", "file": "mip/HighsSeparation.h", "brief": "Cut generation orchestration for MIP solver\n\nCoordinates multiple separators to generate cutting planes.\n\n**Separation Management:**\n- separators[]: Collection of HighsSeparator instances\n- cutset: HighsCutSet accumulating generated cuts\n- implBoundClock, cliqueClock: Timing for specific separator types\n\n**Key Methods:**\n- separationRound(): Run all separators once\n- separate(): Multiple rounds until no more cuts found\n- setLpRelaxation(): Bind to LP for solution access\n\n**Separator Types (via HighsSeparator subclasses):**\n- Gomory cuts from optimal basis\n- MIR (mixed-integer rounding)\n- Clique cuts from conflict graph\n- Implied bound cuts from VUB/VLB\n- Flow cover cuts", "see": ["mip/HighsSeparator.h for separator base class", "mip/HighsCutPool.h for cut storage", "mip/HighsLpRelaxation.h for LP interface"], "has_pass2": false}, "highs/mip/HighsLpAggregator.h": {"path": "layer-4/HiGHS/highs/mip/HighsLpAggregator.h", "filename": "HighsLpAggregator.h", "file": "mip/HighsLpAggregator.h", "brief": "Row aggregation for cut generation\n\nCombines LP rows with weights to create mixed-integer Gomory source rows.\n\n**Core Operations:**\n- addRow(): Add weighted LP row to current aggregation\n- getCurrentAggregation(): Extract combined row as sparse vector\n- clear(): Reset for next aggregation\n\n**Aggregation Format:**\nResult includes slack variables making it an equation with RHS = 0:\n  sum(a_ij * x_j) + s_i = b_i  →  aggregated as equality\n\n**Implementation:**\n- Uses HighsSparseVectorSum for efficient sparse combination\n- Tracks nonzero indices incrementally\n- negate parameter for sign flip (row sense conversion)\n\n**Cut Generation Context:**\n1. Start from LP tableau row (basis row)\n2. Aggregate additional rows to eliminate non-integer variables\n3. Pass to HighsTransformedLp for bound substitution\n4. Generate Gomory mixed-integer cuts", "see": ["mip/HighsTransformedLp.h for bound transformation", "mip/HighsSeparation.h for cut orchestration", "util/HighsSparseVectorSum.h for sparse vector arithmetic"], "has_pass2": false}, "highs/mip/HighsSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsSeparator.h", "filename": "HighsSeparator.h", "file": "mip/HighsSeparator.h", "brief": "Abstract base class for cut separators\n\nDefines interface for generating cutting planes from LP relaxation.\n\n**Separator Types (string constants):**\n- kImplboundSepaString: Implied bound cuts from VUB/VLB\n- kCliqueSepaString: Clique cuts from conflict graph\n- kTableauSepaString: Tableau-based cuts (Gomory)\n- kPathAggrSepaString: Path aggregation cuts\n- kModKSepaString: Mod-k cuts\n\n**Abstract Interface:**\n- separateLpSolution(): Generate cuts from current LP solution\n  - lpRelaxation: Access to LP solution and basis\n  - lpAggregator: Row aggregation helper\n  - transLp: Transformed LP with bound substitutions\n  - cutpool: Destination for generated cuts\n\n**Statistics:**\n- numCutsFound: Total cuts generated\n- numCalls: Times separator was invoked\n- clockIndex: Timer index for profiling\n\n**Usage:**\n- run(): Wrapper that times and counts separateLpSolution()\n- Subclasses implement specific cut types", "see": ["mip/HighsSeparation.h for separator orchestration", "mip/HighsCutPool.h for cut storage"], "has_pass2": false}, "highs/mip/HighsPathSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsPathSeparator.h", "filename": "HighsPathSeparator.h", "file": "mip/HighsPathSeparator.h", "brief": "Path aggregation cuts from network structure\n\nHeuristically identifies network paths and generates flow-based cuts.\n\n**Network Path Detection:**\n1. Find rows with network structure (±1 coefficients)\n2. Aggregate rows to form source-sink paths\n3. Resulting aggregation gives flow conservation cuts\n\n**Separation Heuristic:**\n- Start from fractional integer variables\n- Follow flow through network-like constraints\n- Randomized selection (HighsRandom) for diverse cuts\n\n**Cut Generation:**\n- Aggregated row passed to MIR procedure\n- Path structure often yields strong cuts\n- Effective on transportation/assignment problems", "see": ["mip/HighsSeparator.h for base class interface", "mip/HighsModkSeparator.h for related mod-k aggregation", "util/HighsRandom.h for randomization"], "has_pass2": false}, "highs/mip/HighsCliqueTable.h": {"path": "layer-4/HiGHS/highs/mip/HighsCliqueTable.h", "filename": "HighsCliqueTable.h", "file": "mip/HighsCliqueTable.h", "brief": "Clique detection and storage for MIP solver\n\nMaintains cliques (sets of binary variables where at most one can be 1).\n\n**CliqueVar:**\nRepresents a literal (variable=0 or variable=1):\n- col: Variable index\n- val: 0 or 1 (whether literal is x or (1-x))\n- weight(sol): Contribution to clique violation (val ? sol : 1-sol)\n\n**Clique:**\nA set of CliqueVars where sum <= 1 (or ==1 for equality cliques).\n- start/end: Range in cliqueentries[] array\n- equality: True for \"exactly one\" (SOS1) constraints\n- origin: Row index if derived from constraint\n\n**Key Operations:**\n- addClique(): Store clique with subsumption checking\n- haveCommonClique(): Test if two literals conflict (fast conflict detection)\n- separateCliques(): Find violated clique inequalities (Bron-Kerbosch)\n- extractCliques(): Derive cliques from constraints\n- runCliqueMerging(): Extend cliques using transitivity\n\n**Substitution Support:**\nWhen x1 + x2 = 1 is detected, substitute x2 = 1 - x1:\n- colsubstituted[], substitutions[]: Track substitutions\n- resolveSubstitution(): Apply substitutions to variables\n\n**Conflict Graph:**\n- invertedHashList[]: Maps literal→cliques containing it\n- sizeTwoCliques: Fast lookup for pairwise conflicts\n- addImplications(): Add implied fixings when literal is set", "see": ["mip/HighsCutPool.h for cut management", "mip/HighsDomain.h for domain propagation"], "has_pass2": false}, "highs/mip/HighsConflictPool.h": {"path": "layer-4/HiGHS/highs/mip/HighsConflictPool.h", "filename": "HighsConflictPool.h", "file": "mip/HighsConflictPool.h", "brief": "Storage for learned conflict constraints\n\nManages nogood constraints derived from infeasible subproblems.\n\n**Conflict Storage:**\n- conflictEntries_[]: Domain changes forming each conflict\n- conflictRanges_[]: {start, end} pairs indexing entries\n- Each conflict: set of HighsDomainChange that together are infeasible\n\n**Aging System:**\n- ages_[]: Tracks how long since conflict was active\n- ageDistribution_[]: Count of conflicts at each age\n- performAging(): Increment ages, remove old conflicts\n- resetAge(): Reset age when conflict becomes active\n- agelim_: Maximum age before deletion\n\n**Memory Management:**\n- freeSpaces_: Ordered set of reusable entry ranges\n- deletedConflicts_: Reusable conflict indices\n- softlimit_: Target maximum conflicts\n\n**Key Methods:**\n- addConflictCut(): Store conflict from infeasibility analysis\n- addReconvergenceCut(): Store conflict with reconvergence point\n- removeConflict(): Delete conflict and free space\n\n**Propagation Integration:**\n- propagationDomains[]: Domains using conflicts for propagation\n- modification_[]: Version counter for conflict updates", "see": ["mip/HighsDomain.h for conflict derivation", "mip/HighsSearch.h for conflict generation during search"], "has_pass2": false}, "highs/mip/HighsImplications.h": {"path": "layer-4/HiGHS/highs/mip/HighsImplications.h", "filename": "HighsImplications.h", "file": "mip/HighsImplications.h", "brief": "Implication graphs and variable bound relationships for MIP\n\n**HighsImplications Class:**\nTracks logical implications and variable bounds derived from binary variables.\n\n**Implications Storage:**\n- implications[2*col + val]: Bound changes implied when col=val\n- computeImplications(): Derives implications via probing\n- getImplications(): Returns cached or computes implications\n\n**Variable Upper/Lower Bounds (VUB/VLB):**\nLinear bounds of form: x_j <= coef * y + constant (VUB)\n- vubs[col], vlbs[col]: Hash trees of variable bounds\n- VarBound: {coef, constant} with minValue()/maxValue()\n- getBestVub/Vlb(): Find tightest VUB/VLB given LP solution\n\n**Key Operations:**\n- runProbing(): Test fixing binary variable and propagate\n- separateImpliedBounds(): Generate cuts from VUB/VLB\n- addVUB/addVLB(): Store new variable bounds\n- columnTransformed(): Update VBDs after variable scaling\n\n**Substitutions:**\n- substitutions[], colsubstituted[]: Track variable substitutions\n- storeLiftingOpportunity: Callback for cut strengthening", "see": ["mip/HighsDomain.h for bound propagation", "mip/HighsCliqueTable.h for clique-based implications"], "has_pass2": false}, "highs/mip/HighsLpRelaxation.h": {"path": "layer-4/HiGHS/highs/mip/HighsLpRelaxation.h", "filename": "HighsLpRelaxation.h", "file": "mip/HighsLpRelaxation.h", "brief": "LP relaxation management for MIP branch-and-bound\n\nManages the LP relaxation at each B&B node with cut handling.\n\n**Status enum:**\n- kOptimal: LP solved to optimality\n- kInfeasible: LP is infeasible (node can be pruned)\n- kUnscaledDualFeasible: Dual feasible (valid bound)\n- kUnscaledPrimalFeasible: Primal feasible (valid solution)\n- kUnbounded/kError: Problem states\n\n**LpRow Tracking:**\n- origin: kModel (original constraint) or kCutPool (added cut)\n- index: Row index in model or cut pool\n- age: Iterations since cut was binding (for aging/removal)\n\n**Key Operations:**\n- run(): Solve LP relaxation\n- addCuts(): Add cuts from HighsCutSet\n- flushDomain(): Apply bound changes from HighsDomain\n- performAging(): Age cuts, remove old non-binding cuts\n- removeObsoleteRows(): Delete aged-out cuts\n\n**Solution Access:**\n- getObjective(): LP objective value\n- getSolution(): Primal/dual values\n- getFractionalIntegers(): Integer vars with fractional values\n- integerFeasible(): True if LP solution is MIP-feasible\n\n**Dual Proofs:**\n- computeDualProof(): Derive valid inequality from LP dual\n- storeDualInfProof(): Store proof when LP is infeasible\n- getDualProof(): Retrieve stored proof for conflict analysis\n\n**Playground (nested class):**\nRAII wrapper for temporary LP modifications (e.g., strong branching).", "see": ["mip/HighsMipSolver.h for MIP solver using this", "mip/HighsCutPool.h for cut management"], "has_pass2": false}, "highs/mip/HighsNodeQueue.h": {"path": "layer-4/HiGHS/highs/mip/HighsNodeQueue.h", "filename": "HighsNodeQueue.h", "file": "mip/HighsNodeQueue.h", "brief": "Priority queue for branch-and-bound nodes\n\nManages open nodes for best-first search with efficient memory allocation.\n\n**OpenNode Structure:**\n- domchgstack: Vector of bound changes from root to node\n- branchings: Indices of branching decisions\n- lower_bound, estimate: Node bounds and estimates\n- depth: Node depth in tree\n- lowerLinks, hybridEstimLinks: Red-black tree links for ordering\n\n**Custom Allocator:**\n- AllocatorState: Chunk-based allocation with freelist\n- NodesetAllocator: STL-compatible allocator for NodeSet\n- Reduces allocation overhead for many small nodes\n\n**Node Indexing:**\n- colLowerNodes/colUpperNodes: Nodes indexed by bound changes\n- Enables efficient global bound propagation\n- numNodesUp/Down(col, val): Count nodes with given bound\n\n**Selection Strategies:**\n- popBestBoundNode(): Best-first search by lower_bound\n- popBestNode(): Hybrid using estimate\n- performBounding(): Prune nodes exceeding upper limit\n\n**Key Operations:**\n- emplaceNode(): Add new node with bound changes\n- pruneInfeasibleNodes(): Remove nodes with tightened global bounds\n- checkGlobalBounds(): Update tree weight from bound changes", "see": ["mip/HighsSearch.h for depth-first search", "mip/HighsDomainChange.h for bound change structure"], "has_pass2": false}, "highs/util/stringutil.h": {"path": "layer-4/HiGHS/highs/util/stringutil.h", "filename": "stringutil.h", "file": "util/stringutil.h", "brief": "String manipulation utilities for file parsing\n\nHelper functions for trimming, case conversion, and tokenization.\n\n**Case Conversion:**\n- tolower(): Convert string to lowercase in-place\n- toupper(): Convert string to uppercase in-place\n\n**Trimming:**\n- ltrim(): Remove leading whitespace\n- rtrim(): Remove trailing whitespace\n- trim(): Remove leading and trailing whitespace\n- default_non_chars: Standard whitespace characters\n\n**Predicates:**\n- is_empty(): Check if string contains only whitespace\n- is_end(): Check if position is at end of content\n\n**Tokenization:**\n- first_word(): Extract first word from position\n- first_word_end(): Find end position of first word\n\n**Usage:**\nUsed by MPS/LP parsers for section and field extraction.", "see": ["io/HMpsFF.h for MPS parsing", "io/FilereaderLp.h for LP parsing"], "has_pass2": false}, "highs/util/HighsUtils.h": {"path": "layer-4/HiGHS/highs/util/HighsUtils.h", "filename": "HighsUtils.h", "file": "util/HighsUtils.h", "brief": "Index collections, sparse transpose, and statistical utilities\n\nMiscellaneous utility types and functions used across HiGHS.\n\n**HighsIndexCollection:**\nFlexible index specification for API operations:\n- interval: Range [from_, to_]\n- set: Explicit index array\n- mask: Boolean array\n\n**Sparse Matrix Operations:**\n- highsSparseTranspose(): CSC to CSR conversion\n\n**Value Distribution:**\n- HighsValueDistribution: Histogram for analyzing value ranges\n- HighsScatterData: Scatter plot data for analysis\n\n**Index Collection Status:**\n- kIndexCollectionCreateOk: Valid collection\n- kIndexCollectionCreateIllegalInterval/Set/Mask: Error codes", "see": ["lp_data/HighsLp.h for LP index operations", "model/HighsModel.h for model modifications using indices"], "has_pass2": false}, "highs/util/HSet.h": {"path": "layer-4/HiGHS/highs/util/HSet.h", "filename": "HSet.h", "file": "util/HSet.h", "brief": "O(1) integer set with fast membership test and removal\n\nUnordered set of distinct non-negative integers with O(1) operations.\n\n**Data Structure:**\n- entry_[]: Dense array of current entries\n- pointer_[]: Sparse array mapping entry → position in entry_[]\n- count_: Number of elements\n\n**Operations (all O(1)):**\n- add(entry): Append to entry_[], record position in pointer_[]\n- remove(entry): Swap with last, update swapped entry's pointer\n- in(entry): Check pointer_[] for valid position\n\n**Design:**\n- Similar to \"swap and pop\" deletion pattern\n- pointer_[i] = no_pointer (-1) means i not in set\n- Trades O(max_entry) space for O(1) operations\n\n**Usage:**\n- Track active row/column indices in simplex\n- Maintain sets of candidates during pricing", "see": ["simplex/HEkkDual.h for dual pricing candidate sets"], "has_pass2": false}, "highs/util/HighsInt.h": {"path": "layer-4/HiGHS/highs/util/HighsInt.h", "filename": "HighsInt.h", "file": "HighsInt.h", "brief": "The definition for the integer type to use", "has_pass2": false}, "highs/util/HighsHashTree.h": {"path": "layer-4/HiGHS/highs/util/HighsHashTree.h", "filename": "HighsHashTree.h", "file": "util/HighsHashTree.h", "brief": "Adaptive hash-array mapped trie (HAMT) container\n\nCache-efficient hash map using trie structure with occupation bitmaps.\n\n**Node Types:**\n- InnerLeaf<SizeClass>: Leaf with 6-54 entries (4 size classes)\n- BranchNode: Internal node with 64-way branching via hash chunks\n- ListLeaf: Collision list at maximum depth (rare)\n\n**Hash Structure:**\n- 6 bits per level → 64-way branching (kBranchFactor)\n- Up to 9 depth levels (54 bits of 64-bit hash)\n- Occupation bitmap tracks which children exist\n\n**InnerLeaf Features:**\n- Linear probing within leaf for hash collisions\n- 16-bit hash prefix for fast filtering\n- Size class promotion/demotion on insert/erase\n\n**Burst Behavior:**\n- Leaf bursts to branch when exceeding kLeafBurstThreshold (54)\n- Branch merges back to leaf when children fit in threshold\n\n**Key Methods:**\n- insert/insert_or_get(): Add entry, return value pointer\n- find(): Lookup by key\n- erase(): Remove entry\n- find_common(): Find intersection with another tree\n- for_each(): Iterate all entries\n\n**Usage:**\nUsed for lifting opportunities in presolve (HPresolve.liftingOpportunities)", "see": ["util/HighsHash.h for hash functions", "presolve/HPresolve.h for usage in presolve"], "has_pass2": false}, "highs/util/HighsSparseMatrix.h": {"path": "layer-4/HiGHS/highs/util/HighsSparseMatrix.h", "filename": "HighsSparseMatrix.h", "file": "util/HighsSparseMatrix.h", "brief": "Sparse matrix in CSC or CSR format\n\n**HighsSparseMatrix Class:**\nStores constraint matrix A for LP/QP in compressed format.\n\n**Storage Format (MatrixFormat):**\n- kColwise (CSC): start_[col], index_[]/value_[] are row indices and values\n- kRowwise (CSR): start_[row], index_[]/value_[] are column indices and values\n- p_end_[]: Optional end pointers for partitioned/incremental updates\n\n**Format Conversion:**\n- ensureColwise()/ensureRowwise(): Convert to desired format\n- createColwise()/createRowwise(): Create from existing matrix\n- setFormat(): Set target format\n\n**Operations:**\n- product()/productTranspose(): Ax, A'x multiplication\n- productQuad(): Quad-precision multiplication\n- alphaProductPlusY(): y += alpha*Ax\n\n**Simplex PRICE Operations:**\n- priceByColumn()/priceByRow(): Compute pi'A\n- priceByRowWithSwitch(): Switch strategy based on density\n- createRowwisePartitioned(): Partition for hyper-sparse PRICE\n\n**Modification:**\n- addCols()/addRows(): Extend matrix\n- deleteCols()/deleteRows(): Remove columns/rows\n- scaleCol()/scaleRow(): Apply scaling", "see": ["lp_data/HighsLp.h for HighsLp.a_matrix_ usage", "HConst.h for MatrixFormat enum"], "has_pass2": false}, "highs/util/HighsSparseVectorSum.h": {"path": "layer-4/HiGHS/highs/util/HighsSparseVectorSum.h", "filename": "HighsSparseVectorSum.h", "file": "util/HighsSparseVectorSum.h", "brief": "Accumulator for sparse vector linear combinations\n\nEfficient accumulation of multiple sparse vectors with quad precision.\n\n**Hybrid Storage:**\n- values[]: Full-length HighsCDouble array for accumulation\n- nonzeroinds[]: Packed indices of nonzero entries\n- O(nnz) iteration with O(1) random access\n\n**Key Operations:**\n- add(): Accumulate value at index, track new nonzeros\n  - Uses DBL_MIN sentinel to distinguish zero from never-set\n- getNonzeros(): Return accumulated nonzero indices\n- getValue(): Extract accumulated value at index\n- clear(): Reset (O(nnz) or O(n) based on density)\n\n**Template Methods:**\n- partition(): Partition nonzeros by predicate\n- cleanup(): Remove entries satisfying isZero predicate\n\n**Usage:**\nRow aggregation for cut generation (HighsLpAggregator uses this)", "see": ["mip/HighsLpAggregator.h for row aggregation", "util/HighsCDouble.h for quad precision arithmetic"], "has_pass2": false}, "highs/util/FactorTimer.h": {"path": "layer-4/HiGHS/highs/util/FactorTimer.h", "filename": "FactorTimer.h", "file": "util/FactorTimer.h", "brief": "Indices of factor iClocks", "has_pass2": false}, "highs/util/HighsRbTree.h": {"path": "layer-4/HiGHS/highs/util/HighsRbTree.h", "filename": "HighsRbTree.h", "file": "util/HighsRbTree.h", "brief": "Array-based Red-Black tree with CRTP interface\n\nSelf-balancing BST using indices instead of pointers for cache efficiency.\n\n**CRTP Pattern:**\n- RbTreeTraits<Impl>: Define KeyType and LinkType\n- Impl::getRbTreeLinks(): Return reference to RbTreeLinks\n- Impl::getKey(): Return comparable key value\n\n**RbTreeLinks Structure:**\n- child[2]: Left/right child indices\n- parentAndColor: Parent packed with color bit (saves storage)\n- Works with both integer and pointer LinkTypes\n\n**Tree Operations:**\n- link(): Insert node, rebalance via red-black rotations\n- unlink(): Remove node with transplant and fixup\n- find(): Search returning (node, exact_match) pair\n- first()/last(): Minimum/maximum in subtree\n- successor()/predecessor(): In-order traversal\n\n**CacheMinRbTree Variant:**\n- Maintains cached pointer to minimum element\n- O(1) access to first() for priority queue usage\n\n**Usage in HiGHS:**\n- HighsNodeQueue: Priority queue for B&B nodes by bound\n- Efficient ordered containers with O(log n) operations", "see": ["mip/HighsNodeQueue.h for node selection", "util/HighsSplay.h for alternative tree structure"], "has_pass2": false}, "highs/util/HighsMatrixPic.h": {"path": "layer-4/HiGHS/highs/util/HighsMatrixPic.h", "filename": "HighsMatrixPic.h", "file": "util/HighsMatrixPic.h", "brief": "Class-independent utilities for HiGHS", "has_pass2": false}, "highs/util/HighsLinearSumBounds.h": {"path": "layer-4/HiGHS/highs/util/HighsLinearSumBounds.h", "filename": "HighsLinearSumBounds.h", "file": "util/HighsLinearSumBounds.h", "brief": "Row activity bounds for bound tightening (FBBT)\n\nIncrementally maintains min/max activity bounds for linear constraints.\n\n**Activity Computation:**\nFor row sum = Σ(a_i * x_i):\n- sumLower: Minimum activity using a_i>0 with lb_i, a_i<0 with ub_i\n- sumUpper: Maximum activity using a_i>0 with ub_i, a_i<0 with lb_i\n- numInfSumLower/Upper: Count of infinite contributions\n\n**Original vs Implied Bounds:**\n- sumLowerOrig/sumUpperOrig: Using original variable bounds only\n- sumLower/sumUpper: Using tightest of original and implied bounds\n- implVarLower/implVarUpper: Implied bounds from other constraints\n- Source tracking: implVarLowerSource[] for circular dependency detection\n\n**Incremental Updates:**\n- add(): Add variable contribution to sum\n- remove(): Remove variable contribution from sum\n- updatedVarLower/Upper(): Update after bound tightening\n- updatedImplVarLower/Upper(): Update after implied bound discovery\n\n**Residual Computation:**\n- getResidualSumLower/Upper(): Activity excluding specific variable\n  - Used for FBBT: derive x_i bounds from row ≤ rhs - residual\n  - Optional boundVar parameter for conditional bounds\n\n**Quad Precision:**\nUses HighsCDouble for activity sums to avoid numerical issues.", "see": ["presolve/HPresolve.h for presolve bound tightening", "mip/HighsDomain.h for MIP propagation", "util/HighsCDouble.h for compensated double arithmetic"], "has_pass2": false}, "highs/util/HVectorBase.h": {"path": "layer-4/HiGHS/highs/util/HVectorBase.h", "filename": "HVectorBase.h", "file": "util/HVectorBase.h", "brief": "Template sparse vector for simplex operations\n\nHybrid-storage vector supporting both sparse and dense access patterns.\n\n**Storage Model:**\n- array[]: Full-length value storage (dense access)\n- index[]: Packed nonzero indices (sparse iteration)\n- count: Number of nonzeros\n- Enables O(nnz) operations while allowing O(1) element access\n\n**Core Operations:**\n- setup(): Initialize to dimension\n- clear(): Reset all values and count\n- saxpy(): y += alpha * x (sparse BLAS-1)\n- pack(): Compress to packIndex/packValue arrays\n- reIndex(): Rebuild index[] from array[] (after cancellation)\n- norm2(): Squared 2-norm computation\n\n**Packing System:**\n- packFlag: Enable/disable packing\n- packIndex/packValue: Compressed representation\n- tight(): Zero small values exceeding kHighsTiny\n\n**PAMI Support:**\n- next: Link pointer for parallel minor iteration chains\n- cwork/iwork: Scratch buffers for UPDATE operations\n- synthetic_tick: Profiling counter for operation costs", "see": ["simplex/HEkkDual.h for PAMI parallelization", "util/HFactor.h for BTRAN/FTRAN using HVector"], "has_pass2": false}, "highs/util/HighsTimer.h": {"path": "layer-4/HiGHS/highs/util/HighsTimer.h", "filename": "HighsTimer.h", "file": "util/HighsTimer.h", "brief": "Profiling facility for HiGHS computational components\n\n**HighsTimer Class:**\nWall-clock timing with named clocks for performance profiling.\n\n**Clock Management:**\n- clock_def(): Define a named clock, returns clock index\n- start()/stop(): Start/stop a clock (asserts proper pairing)\n- read(): Get elapsed time (handles running clocks)\n- running(): Check if clock is running\n- numCall(): Get number of times clock was stopped\n\n**Built-in Clocks:**\n- Clock 0: \"Run HiGHS\" (total time)\n- presolve_clock, solve_clock, postsolve_clock: Phase timings\n\n**Reporting:**\n- report(): Print timing breakdown for clock list (grep-friendly format)\n- reportOnTolerance(): Only report clocks above percentage threshold\n- writeAllClocks(): Dump all clock times\n\n**Implementation:**\nUses std::chrono::high_resolution_clock. Clock start stores negative\nwall time; positive indicates stopped. This enables running/stopped detection.", "see": ["simplex/SimplexTimer.h for simplex-specific clocks", "mip/HighsMipAnalysis.h for MIP timing"], "has_pass2": false}, "highs/util/HighsRandom.h": {"path": "layer-4/HiGHS/highs/util/HighsRandom.h", "filename": "HighsRandom.h", "file": "util/HighsRandom.h", "brief": "Deterministic random number generator for HiGHS\n\nXorshift-based PRNG with strongly universal hash output functions.\n\n**State Management:**\n- 64-bit state with xorshift advance\n- initialise(): Seed with mixing to avoid weak states\n- Deterministic for reproducible optimization\n\n**Output Methods:**\n- integer(): Random in [0, 2^31-1] or [0, sup) or [min, sup)\n- fraction(): Open interval (0, 1)\n- closedFraction(): Closed interval [0, 1]\n- real(a, b): Uniform in [a, b]\n- bit(): Random boolean\n- shuffle(): Fisher-Yates shuffle\n\n**Uniform Range Algorithm:**\nUses rejection sampling with hash function outputs:\n- Multiple hash outputs per state (up to 32)\n- Minimizes state advances for bounded integers\n\n**Hash-Based Mixing:**\nApplies HighsHashHelpers::pair_hash to state for high-quality output\nin both high and low bits (unlike raw xorshift).", "see": ["util/HighsHash.h for hash functions", "mip/HighsPathSeparator.h for randomized cut selection"], "has_pass2": false}, "highs/util/HighsHash.h": {"path": "layer-4/HiGHS/highs/util/HighsHash.h", "filename": "HighsHash.h", "file": "util/HighsHash.h", "brief": "Hash functions and hash table for HiGHS\n\nProvides high-quality hashing with Mersenne prime arithmetic.\n\n**HighsHashHelpers:**\n- hash<T>(): Fast hash for trivially copyable types (up to 64 bytes)\n- vector_hash(): Hash for vectors using pair_hash with random constants\n- sparse_combine(): Order-independent hash for sparse vectors\n- multiply_modM61(): Multiply mod Mersenne prime 2^61-1\n- double_hash_code(): Hash for doubles handling epsilon equality\n\n**Sparse Hashing (for symmetry detection):**\nEvaluates polynomial over finite field GF(2^61-1):\n- sparse_combine(hash, index, value): Add term to polynomial\n- sparse_inverse_combine(): Remove term (for partition refinement)\n- Enables order-independent hashing of sparse vectors\n\n**HighsHashTable<K,V>:**\nRobin Hood hashing with linear probing:\n- 7-bit hash stored in metadata for fast comparison\n- Automatic grow/shrink at 7/8 and 1/4 load factors\n- Steals positions from entries closer to ideal slot\n\n**HighsHashTableEntry<K,V>:**\n- Specialized for void value (set behavior)\n- key(), value() accessors\n- forward() for functional-style iteration", "see": ["util/HighsHashTree.h for ordered hash structures", "mip/HighsCliqueTable.h for hash-based clique storage"], "has_pass2": false}, "highs/util/HFactorDebug.h": {"path": "layer-4/HiGHS/highs/util/HFactorDebug.h", "filename": "HFactorDebug.h", "file": "util/HFactorDebug.h", "has_pass2": false}, "highs/util/HighsSplay.h": {"path": "layer-4/HiGHS/highs/util/HighsSplay.h", "filename": "HighsSplay.h", "file": "util/HighsSplay.h", "brief": "Splay tree operations for array-based binary search trees\n\nTop-down splay tree using indices instead of pointers for cache efficiency.\n\n**Splay Operation:**\n- highs_splay(): Move accessed node to root via rotations\n- Amortized O(log n) operations through self-adjusting\n- Zig-zig and zig-zag rotations for balanced restructuring\n\n**Tree Operations:**\n- highs_splay_link(): Insert new node, splay to root\n- highs_splay_unlink(): Remove node, restructure tree\n\n**Lambda Interface:**\nTemplate parameters for flexible storage:\n- GetLeft: lambda(index) → reference to left child index\n- GetRight: lambda(index) → reference to right child index\n- GetKey: lambda(index) → comparable key value\n\n**Usage in HiGHS:**\n- HPresolve: Row-wise nonzero storage (ARleft, ARright, rowroot)\n- HighsGFkSolve: Sparse storage for GF(k) systems\n- Efficient O(1) access to recently accessed elements", "see": ["presolve/HPresolve.h for row iteration via splay tree", "mip/HighsGFkSolve.h for GF(k) sparse storage"], "has_pass2": false}, "highs/util/HighsCDouble.h": {"path": "layer-4/HiGHS/highs/util/HighsCDouble.h", "filename": "HighsCDouble.h", "file": "util/HighsCDouble.h", "brief": "Compensated double for quad precision arithmetic\n\nImplements quad precision using error-free transformations (Knuth/Rump).\n\n**Representation:**\n- hi: Primary double value\n- lo: Compensation term (error from previous operations)\n- Value = hi + lo with |lo| << |hi|\n\n**Error-Free Transformations:**\n- two_sum(x,y,a,b): Exact a+b=x+y with x=double(a+b), 6 FLOPs\n- two_product(x,y,a,b): Exact a*b=x+y with x=double(a*b), 17 FLOPs\n- split(x,y,a): Split 53-bit mantissa into two 26-bit parts\n\n**Supported Operations:**\n- +, -, *, / with double and HighsCDouble operands\n- Comparison operators (using double(hi+lo))\n- abs(), sqrt(), floor(), ceil(), round(), ldexp()\n- renormalize(): Restore |lo| << |hi| invariant\n\n**Usage:**\nCritical for accurate summation in LP (objective, constraint activity)\nand avoiding catastrophic cancellation in bound computations.", "see": ["util/HVector.h for HVectorQuad using HighsCDouble", "mip/HighsNodeQueue.h for tree weight tracking"], "has_pass2": false}, "highs/util/HighsDataStack.h": {"path": "layer-4/HiGHS/highs/util/HighsDataStack.h", "filename": "HighsDataStack.h", "file": "util/HighsDataStack.h", "brief": "Type-erased byte stack for presolve reduction storage\n\nLIFO storage for heterogeneous data with compile-time type safety.\n\n**Design:**\n- Stores arbitrary trivially-copyable types as raw bytes\n- push<T>(): Append value to stack\n- pop<T>(): Remove and return value from top\n- Vectors stored with trailing size for self-describing pops\n\n**Presolve Integration:**\n- HighsPostsolveStack uses this for reduction storage\n- Each presolve reduction pushes undo data\n- Postsolve pops in reverse order to restore solution\n- Compact binary format (no per-element overhead)\n\n**Memory Layout:**\n- Scalar: |--value bytes--|\n- Vector: |--element bytes--|--size_t count--|\n- Position tracks pop location; data.size() for push\n\n**Type Safety:**\n- IS_TRIVIALLY_COPYABLE enforces memcpy-safe types\n- Compatible with GCC < 5 via __has_trivial_copy", "see": ["presolve/HighsPostsolveStack.h for reduction undo storage", "presolve/HPresolve.h for presolve reductions"], "has_pass2": false}, "highs/util/HighsSort.h": {"path": "layer-4/HiGHS/highs/util/HighsSort.h", "filename": "HighsSort.h", "file": "util/HighsSort.h", "brief": "Heap-based sorting routines for HiGHS\n\nIn-place heapsort implementations for indices and value-index pairs.\n\n**Algorithm:**\n- Build max-heap in O(n) via buildMaxheap()\n- Extract elements in O(n log n) via maxHeapsort()\n- Result: increasing order (max-heap yields ascending sort)\n\n**Functions:**\n- maxheapsort(): Combined build + sort\n- buildMaxheap() + maxHeapsort(): Separate phases\n- maxHeapify(): Sift-down operation\n\n**Overloads:**\n- (HighsInt*): Sort indices only\n- (double*, HighsInt*): Sort values with corresponding indices\n- (HighsInt*, HighsInt*): Sort HighsInt values with indices\n\n**Decreasing Heap Utilities:**\n- addToDecreasingHeap(): Insert into bounded min-heap\n- sortDecreasingHeap(): Extract in decreasing order\n\n**Validation:**\n- increasingSetOk(): Check sorted order and bounds\n- sortSetData(): Sort indices and reorder associated data arrays\n\n@note Arrays use 1-based indexing: heap[1..n]", "see": ["simplex/HEkkDual.h for CHUZC ratio test using heaps"], "has_pass2": false}, "highs/util/HighsMatrixUtils.h": {"path": "layer-4/HiGHS/highs/util/HighsMatrixUtils.h", "filename": "HighsMatrixUtils.h", "file": "util/HighsMatrixUtils.h", "brief": "Class-independent utilities for HiGHS", "has_pass2": false}, "highs/util/HighsDisjointSets.h": {"path": "layer-4/HiGHS/highs/util/HighsDisjointSets.h", "filename": "HighsDisjointSets.h", "file": "util/HighsDisjointSets.h", "brief": "Union-Find data structure for disjoint sets\n\nEfficient partition management with path compression and union-by-size.\n\n**Template Parameter:**\n- kMinimalRepresentative: If true, smaller index becomes representative\n  (useful for symmetry detection where order matters)\n\n**Data Structure:**\n- sets[]: Parent pointers (self-loop = representative)\n- sizes[]: Set sizes for union-by-size\n- linkCompressionStack: Workspace for path compression\n\n**Operations:**\n- getSet(): Find representative with path compression\n  - Amortized O(α(n)) via iterative compression\n- merge(): Union two sets\n  - Union-by-size keeps tree balanced\n  - Or union-by-min-index if kMinimalRepresentative\n- getSetSize(): Size of set containing item\n\n**Usage in HiGHS:**\n- Symmetry detection: orbit computation (HighsSymmetry)\n- Clique merging (HighsCliqueTable)\n- Connected component identification", "see": ["presolve/HighsSymmetry.h for orbit computation", "mip/HighsCliqueTable.h for clique management"], "has_pass2": false}, "highs/util/HVector.h": {"path": "layer-4/HiGHS/highs/util/HVector.h", "filename": "HVector.h", "file": "util/HVector.h", "brief": "Sparse vector types for HiGHS simplex operations\n\nType aliases for HVectorBase with different precision.\n\n**Types:**\n- HVector: HVectorBase<double> - Standard precision sparse vector\n- HVectorQuad: HVectorBase<HighsCDouble> - Quad precision for accuracy\n- HVector_ptr, HVectorQuad_ptr: Pointer types for array storage\n\n**HVectorBase Features (from HVectorBase.h):**\n- Sparse representation: index[] + array[] with count\n- Dense operations: clear(), setup(), collectDense()\n- SAXPY: Add scalar multiple of another vector\n- Packed format for BLAS-style operations", "see": ["util/HVectorBase.h for implementation details", "util/HighsCDouble.h for quad precision type"], "has_pass2": false}, "highs/util/HighsComponent.h": {"path": "layer-4/HiGHS/highs/util/HighsComponent.h", "filename": "HighsComponent.h", "file": "HighsComponent.h", "brief": "The HiGHS class", "has_pass2": false}, "highs/util/HFactorConst.h": {"path": "layer-4/HiGHS/highs/util/HFactorConst.h", "filename": "HFactorConst.h", "file": "util/HFactorConst.h", "brief": "Constants for basis matrix factorization, update and solves for HiGHS", "has_pass2": false}, "highs/util/HighsMatrixSlice.h": {"path": "layer-4/HiGHS/highs/util/HighsMatrixSlice.h", "filename": "HighsMatrixSlice.h", "file": "util/HighsMatrixSlice.h", "brief": "Polymorphic iterators for sparse matrix storage formats\n\nSTL-compatible iterators over matrix rows/columns regardless of storage.\n\n**Storage Format Specializations:**\n- HighsEmptySlice: Zero nonzeros (null iterators)\n- HighsCompressedSlice: CSC/CSR format (index[], value[], len)\n- HighsIndexedSlice: Sparse index + dense values (hybrid storage)\n- HighsTripletListSlice: Linked list via next[] array\n- HighsTripletTreeSliceInOrder: BST traversal (sorted iteration)\n- HighsTripletTreeSlicePreOrder: BST pre-order (fast iteration)\n- HighsTripletPositionSlice: Position array indirection\n\n**Iterator Interface:**\n- HighsSliceNonzero: (index, value) pair via index()/value()\n- Forward iterator with begin()/end()\n- Compatible with range-based for loops\n\n**Usage Pattern:**\n```cpp\nfor (auto nz : HighsCompressedSlice(idx, val, len)) {\n  process(nz.index(), nz.value());\n}\n```\n\n**Usage in HiGHS:**\n- HPresolve: Unified iteration over row/column storage\n- Generic algorithms independent of underlying format\n- Splay tree and linked list iteration in presolve", "see": ["presolve/HPresolve.h for row-wise splay tree iteration", "util/HighsSparseMatrix.h for CSC matrix storage"], "has_pass2": false}, "highs/util/HFactor.h": {"path": "layer-4/HiGHS/highs/util/HFactor.h", "filename": "HFactor.h", "file": "util/HFactor.h", "brief": "Basis matrix factorization, update and solves for HiGHS", "return": "0 if successful, otherwise rank_deficiency>0", "has_pass2": false}, "highs/util/HighsIntegers.h": {"path": "layer-4/HiGHS/highs/util/HighsIntegers.h", "filename": "HighsIntegers.h", "file": "util/HighsIntegers.h", "brief": "Integer arithmetic utilities for cut generation\n\nNumber-theoretic functions critical for MIP cut generation.\n\n**Basic Operations:**\n- mod(): Proper modulo (always non-negative result)\n- gcd(): Euclidean algorithm for GCD\n- nearestInteger(): Round to nearest int64\n- isIntegral(): Test if double is within eps of integer\n\n**Modular Arithmetic:**\n- modularInverse(): Extended Euclidean algorithm for a^{-1} mod m\n  - Used in mod-k cut generation (GF(k) arithmetic)\n\n**Rational Approximation:**\n- denominator(): Continued fraction algorithm to find\n  smallest denominator d such that |x - p/d| < eps\n  - Used for detecting integer structure in coefficients\n\n**Integral Scaling:**\n- integralScale(): Find multiplier to make all values integral\n  - Combines continued fractions with GCD reduction\n  - Used for strengthening MIR/GMI cuts\n  - Returns 0 if no small denominator exists\n\n**Usage in HiGHS:**\n- HighsGFkSolve: Modular inverse for GF(k) systems\n- HighsTableauSeparator: Finding integral structure in cuts\n- Cut coefficient strengthening", "see": ["mip/HighsGFkSolve.h for GF(k) cut generation", "mip/HighsTableauSeparator.h for Gomory cuts"], "has_pass2": false}, "highs/util/HighsMemoryAllocation.h": {"path": "layer-4/HiGHS/highs/util/HighsMemoryAllocation.h", "filename": "HighsMemoryAllocation.h", "file": "HighsMemoryAllocation.h", "brief": "Utilities for memory allocation that return true if successful", "has_pass2": false}, "highs/io/HMpsFF.h": {"path": "layer-4/HiGHS/highs/io/HMpsFF.h", "filename": "HMpsFF.h", "file": "io/HMpsFF.h", "brief": "Free-format MPS file parser with full section support\n\nComprehensive MPS reader supporting LP, QP, MIP, and extensions.\n\n**MPS Sections Parsed:**\n- NAME, OBJSENSE: Problem metadata and optimization direction\n- ROWS: Constraint types (N=free, L/G/E=bounds)\n- COLUMNS: Variable coefficients, integrality markers\n- RHS: Right-hand sides for constraints\n- RANGES: Two-sided constraint bounds\n- BOUNDS: Variable bounds (LO/UP/FX/FR/MI/PL/BV/LI/UI)\n\n**Quadratic Extensions:**\n- QSECTION, QMATRIX, QUADOBJ: Objective Hessian\n- QCMATRIX: Quadratic constraints (per-row)\n\n**Additional Sections:**\n- SOS: Special ordered sets (Type 1, Type 2)\n- CSECTION: Conic constraints (MOSEK format)\n- INDICATORS: Indicator constraints (ignored)\n\n**Parsing Features:**\n- Triplet matrix accumulation, then CSC conversion\n- Duplicate row/column name detection\n- Timeout support for large files\n- Fixed-format detection and fallback\n\n**Data Flow:**\n- loadProblem() → parse() → fillMatrix()/fillHessian()\n- Populates HighsModel with LP/QP/MIP data", "see": ["io/FilereaderMps.h for Filereader wrapper", "io/HMPSIO.h for fixed-format legacy parser"], "has_pass2": false}, "highs/io/Filereader.h": {"path": "layer-4/HiGHS/highs/io/Filereader.h", "filename": "Filereader.h", "file": "io/Filereader.h", "brief": "Abstract file reader/writer interface for optimization models\n\nPolymorphic I/O with format detection from file extension.\n\n**Supported Formats:**\n- MPS: Mathematical Programming System (fixed/free format)\n- LP: CPLEX LP format (human-readable)\n- EMS: HiGHS extended model format\n\n**Factory Pattern:**\n- getFilereader(): Returns appropriate reader for extension\n  - \".mps\", \".mps.gz\" → FilereaderMps\n  - \".lp\" → FilereaderLp\n  - \".ems\" → FilereaderEms\n\n**Return Codes:**\n- kOk: Success\n- kFileNotFound: File doesn't exist\n- kParserError: Malformed input\n- kNotImplemented: Format not supported for operation\n- kTimeout: Read exceeded time limit", "see": ["io/FilereaderMps.h for MPS format", "io/FilereaderLp.h for LP format", "io/HMpsFF.h for free-format MPS parser"], "has_pass2": false}, "highs/io/FilereaderMps.h": {"path": "layer-4/HiGHS/highs/io/FilereaderMps.h", "filename": "FilereaderMps.h", "file": "io/FilereaderMps.h", "brief": "MPS format file reader/writer wrapper\n\nImplements Filereader interface for MPS format (industry standard).\n\n**FilereaderMps Class:**\nThin wrapper delegating to HMpsFF (free-format) or HMPSIO (fixed-format):\n- readModelFromFile(): Detect format, parse MPS into HighsModel\n- writeModelToFile(): Export HighsModel to MPS format\n\n**MPS Format Support:**\n- Standard sections: ROWS, COLUMNS, RHS, BOUNDS, RANGES\n- Extensions: OBJSENSE, QSECTION, SOS, integer markers\n- Both free-format and fixed-format variants\n\n**Read Flow:**\n1. Attempts free-format parse (HMpsFF)\n2. Falls back to fixed-format (HMPSIO) if needed\n3. Handles QP and MIP extensions", "see": ["io/HMpsFF.h for free-format parser implementation", "io/HMPSIO.h for fixed-format parser implementation", "io/Filereader.h for base class interface"], "has_pass2": false}, "highs/io/HMPSIO.h": {"path": "layer-4/HiGHS/highs/io/HMPSIO.h", "filename": "HMPSIO.h", "file": "io/HMPSIO.h", "brief": "Fixed-format MPS file parser (legacy format)\n\nParses classic fixed-column MPS format from IBM's MP/360.\n\n**Fixed-Format Layout:**\nColumns are at fixed character positions:\n- field_1_start (1): Indicator field (row type, bound type)\n- field_2_start (4): Name field (8 chars)\n- field_3_start (14): Name field 2 (8 chars)\n- field_4_start (24): Value field (12 chars)\n- field_5_start (39): Name field 3 (8 chars)\n- field_6_start (49): Value field 2 (12 chars)\n\n**Row Type Constants:**\n- MPS_ROW_TY_N: Free row (objective)\n- MPS_ROW_TY_E: Equality constraint\n- MPS_ROW_TY_L: Less-than-or-equal\n- MPS_ROW_TY_G: Greater-than-or-equal\n\n**readMps():**\nMain parsing function extracting:\n- Constraint matrix (Astart, Aindex, Avalue)\n- Bounds and costs\n- Integrality constraints\n- Quadratic terms (Qstart, Qindex, Qvalue)", "see": ["io/HMpsFF.h for free-format parser (preferred)", "io/FilereaderMps.h for unified MPS interface"], "has_pass2": false}, "highs/io/LoadOptions.h": {"path": "layer-4/HiGHS/highs/io/LoadOptions.h", "filename": "LoadOptions.h", "file": "io/LoadOptions.h", "brief": "Load solver options from file\n\nParses options files into HighsOptions structure.\n\n**HighsLoadOptionsStatus:**\n- kError: Parse failure or invalid option\n- kOk: Options loaded successfully\n- kEmpty: File exists but contains no options\n\n**loadOptionsFromFile():**\nReads option file with key=value pairs:\n- Recognizes all HighsOptions fields\n- Validates option names and value types\n- Logs warnings for unknown options\n\n**File Format:**\nSimple text format with one option per line:\n```\npresolve = on\ntime_limit = 3600.0\nmip_rel_gap = 0.01\n```", "see": ["lp_data/HighsOptions.h for option definitions", "Highs::readOptions() for user-facing API"], "has_pass2": false}, "highs/io/FilereaderEms.h": {"path": "layer-4/HiGHS/highs/io/FilereaderEms.h", "filename": "FilereaderEms.h", "file": "io/FilereaderEms.h", "brief": "EMS format file reader/writer (internal HiGHS format)\n\nImplements Filereader interface for EMS format.\n\n**EMS Format:**\nHiGHS internal text format for model serialization:\n- Compact representation of LP/MIP data\n- Preserves all model metadata\n- Used for debugging and testing\n\n**FilereaderEms Class:**\n- readModelFromFile(): Parse EMS file into HighsModel\n- writeModelToFile(): Export HighsModel to EMS format\n\n**Compared to MPS/LP:**\n- Less standardized but more compact\n- Primarily for internal use\n- Full round-trip fidelity with HighsModel", "see": ["io/Filereader.h for base class interface", "io/FilereaderMps.h for standard MPS format"], "has_pass2": false}, "highs/io/FilereaderLp.h": {"path": "layer-4/HiGHS/highs/io/FilereaderLp.h", "filename": "FilereaderLp.h", "file": "io/FilereaderLp.h", "brief": "CPLEX LP format file reader/writer\n\nImplements Filereader interface for LP format (human-readable).\n\n**LP Format:**\nCPLEX-style format with sections:\n- Minimize/Maximize: Objective function\n- Subject To: Linear constraints\n- Bounds: Variable bounds\n- General/Binary: Integer/binary declarations\n- End: File terminator\n\n**FilereaderLp Class:**\n- readModelFromFile(): Parse LP file into HighsModel\n- writeModelToFile(): Export HighsModel to LP format\n\n**Writing Utilities:**\n- writeToFile(): Buffered output with line-length management\n- writeToFileValue(): Format numeric coefficients\n- writeToFileVar(): Output variable names (quotes if needed)\n- writeToFileMatrixRow(): Output constraint row\n\n**Constants:**\n- LP_MAX_LINE_LENGTH (560): Maximum line length\n- LP_MAX_NAME_LENGTH (255): Maximum identifier length", "see": ["io/Filereader.h for base class interface", "io/HMpsFF.h for MPS format alternative"], "has_pass2": false}, "highs/io/HighsIO.h": {"path": "layer-4/HiGHS/highs/io/HighsIO.h", "filename": "HighsIO.h", "file": "io/HighsIO.h", "brief": "Logging and output utilities for HiGHS\n\nProvides structured logging with log levels and callbacks.\n\n**Log Levels (LogDevLevel):**\n- kHighsLogDevLevelNone: No output\n- kHighsLogDevLevelInfo: Standard progress info\n- kHighsLogDevLevelDetailed: More detailed info\n- kHighsLogDevLevelVerbose: Maximum detail\n\n**Log Types (HighsLogType):**\nTagged output: WARNING, ERROR, etc. (see HighsLogTypeTag[])\n\n**HighsLogOptions:**\nConfiguration for logging destination and callbacks:\n- log_stream: FILE* for output (nullptr = stdout)\n- output_flag/log_to_console: Enable/disable flags\n- user_log_callback: Custom logging callback\n- user_callback: Full callback with HighsCallbackOutput\n\n**Logging Functions:**\n- highsLogUser(): Single-line user messages with type tag\n- highsLogDev(): Development logging (respects log_dev_level)\n- highsLogHeader(): Print version and copyright\n- highsReportDevInfo(): Development info (accepts null log_options)\n\n**File Types (HighsFileType):**\nkMinimal, kFull, kMps, kLp, kMd for output format selection.", "see": ["lp_data/HighsCallback.h for callback structures"], "has_pass2": false}, "highs/presolve/ICrashUtil.h": {"path": "layer-4/HiGHS/highs/presolve/ICrashUtil.h", "filename": "ICrashUtil.h", "file": "presolve/ICrashUtil.h", "brief": "Low-level utilities for ICrash algorithms", "author": "Julian Hall, Ivet Galabova, Qi Huangfu and Michael Feldmeier\n\nHelper functions for iterative crash procedures.\n\n**Problem Transformation:**\n- convertToMinimization(): Negate objective for maximization\n- isEqualityProblem(): Check if all constraints are equations\n\n**Linear Algebra:**\n- vectorProduct(): Compute dot product of two vectors\n- muptiplyByTranspose(): Compute A^T * v\n\n**Initialization:**\n- initialize(): Set up initial solution and dual multipliers\n\n**Component-Wise Minimization:**\n- minimizeComponentQP(): Solve 1D quadratic subproblem\n- minimizeComponentIca(): Solve ICA subproblem for single variable\n\n**Diagnostics:**\n- printMinorIterationDetails(): Log iteration progress", "see": ["presolve/ICrash.h for main crash interface", "presolve/ICrashX.h for crossover support"], "has_pass2": false}, "highs/presolve/HPresolve.h": {"path": "layer-4/HiGHS/highs/presolve/HPresolve.h", "filename": "HPresolve.h", "file": "presolve/HPresolve.h", "brief": "LP/MIP presolve engine\n\n**HPresolve Class:**\nReduces problem size and tightens bounds before solving.\n\n**Matrix Storage:**\nTriplet format with linked list (column) and splay tree (row) for fast access:\n- Avalue[], Arow[], Acol[]: Non-zero storage\n- colhead[], Anext[], Aprev[]: Column-wise linked list\n- rowroot[], ARleft[], ARright[]: Row-wise splay tree\n- rowsize[], colsize[]: Current row/column lengths\n\n**Bound Tracking:**\n- implColLower[]/implColUpper[]: Implied variable bounds\n- rowDualLower[]/rowDualUpper[]: Dual bounds\n- impliedRowBounds, impliedDualRowBounds: Row activity bounds\n\n**Presolve Techniques (Result enum):**\n- singletonRow()/singletonCol(): Remove singleton rows/columns\n- emptyCol(): Remove columns with no constraints\n- doubletonEq(): Eliminate doubleton equalities\n- dominatedColumns(): Remove dominated variables\n- aggregator(): Aggregate rows/columns\n- runProbing(): Probing for integer variables\n- sparsify(): Reduce matrix density\n- detectParallelRowsAndCols(): Remove parallel constraints/variables", "algorithm": "Sparsification (sparsify):\n  Combine rows to create zeros in dense columns:\n  For equation row i with coefficient a_ij in column j:\n  - For each row k with coefficient a_kj:\n    - Add scaled row i to row k: row_k += (−a_kj/a_ij) * row_i\n    - If fill-in acceptable, apply permanently", "math": "Compute implied lower bound on x_k:\n    x_k >= (L - sum_{j≠k}(a_j * u_j if a_j>0 else a_j * l_j)) / a_k\n  If tighter than current l_k, update and propagate.\n  Dual analog: implied dual bounds from reduced cost constraints.", "ref": ["Savelsbergh, M.W.P. (1994). \"Preprocessing and probing techniques\n       for mixed integer programming problems\". ORSA J. Computing 6(4).", "Achterberg, T. et al. (2020). \"Presolve reductions in mixed\n       integer programming\". INFORMS J. Computing 32(2)."], "complexity": "Matrix access: O(1) amortized via splay trees (row) and linked lists (col)\n  Singleton elimination: O(nnz) per pass\n  Probing: O(#binaries * propagation_depth)\n  Full presolve: typically O(nnz * #passes), bounded by reductionLimit", "see": ["presolve/HighsPostsolveStack.h for solution recovery", "presolve/HPresolveAnalysis.h for presolve statistics"], "has_pass2": true}, "highs/presolve/ICrash.h": {"path": "layer-4/HiGHS/highs/presolve/ICrash.h", "filename": "ICrash.h", "file": "presolve/ICrash.h", "brief": "Iterative crash procedure for LP starting points", "author": "Julian Hall, Ivet Galabova, Qi Huangfu and Michael Feldmeier\n\nFinds feasible starting points by solving penalized/regularized subproblems.\n\n**ICrashStrategy Enum:**\n- kPenalty: Penalty method for constraint violations\n- kAdmm: Alternating Direction Method of Multipliers\n- kICA: Iterative Constraint Activation\n- kUpdatePenalty/kUpdateAdmm: Variants with parameter updates\n\n**ICrashIterationDetails:**\nPer-iteration statistics for analysis:\n- num, weight, lambda_norm_2: Iteration metadata\n- lp_objective, quadratic_objective: Objective values\n- residual_norm_2: Constraint violation\n\n**ICrashInfo:**\nOverall crash procedure results:\n- num_iterations, total_time: Execution stats\n- final_lp_objective, final_quadratic_objective: Terminal objectives\n- x_values[]: Final solution vector\n\n**ICrashOptions:**\nConfiguration for crash procedure:\n- dualize: Work with dual LP\n- Strategy selection and parameters", "see": ["presolve/ICrashUtil.h for helper functions", "presolve/ICrashX.h for extended crash variants"], "has_pass2": false}, "highs/presolve/HPresolveAnalysis.h": {"path": "layer-4/HiGHS/highs/presolve/HPresolveAnalysis.h", "filename": "HPresolveAnalysis.h", "file": "presolve/HPresolveAnalysis.h", "brief": "Presolve statistics and rule control\n\nTracks presolve reductions and controls which rules are enabled.\n\n**HPresolveAnalysis Class:**\n- model, options: References to problem being presolved\n- allow_rule_[]: Per-rule enable flags (from options->presolve_rule_off)\n- numDeletedRows, numDeletedCols: Reduction counts\n- presolve_log_: Detailed reduction history\n\n**Rule Control:**\n- setup(): Initialize rule flags from options\n- allow_rule_[]: Boolean array indexed by PresolveRuleType\n- Disabled rules are skipped during presolve passes\n\n**Logging:**\n- allow_logging_/logging_on_: Control verbosity\n- startPresolveRuleLog()/stopPresolveRuleLog(): Bracket rule execution\n- analysePresolveRuleLog(): Summarize effectiveness of rules\n- presolveReductionTypeToString(): Convert rule type to name\n\n**Statistics:**\n- original_num_col_/row_: Problem size before presolve\n- num_deleted_rows0_/cols0_: Running deletion counts", "see": ["presolve/HPresolve.h for presolve engine using this", "lp_data/HConst.h for PresolveRuleType enum"], "has_pass2": false}, "highs/presolve/HighsPostsolveStack.h": {"path": "layer-4/HiGHS/highs/presolve/HighsPostsolveStack.h", "filename": "HighsPostsolveStack.h", "file": "presolve/HighsPostsolveStack.h", "brief": "Postsolve stack to recover original-space solutions\n\nRecords all presolve transformations to enable solution recovery.\n\n**HighsPostsolveStack Class:**\nStack of reduction operations that can be undone in reverse order.\n\n**Reduction Types (ReductionType enum):**\nEach presolve operation has a corresponding undo:\n- kLinearTransform: x = scale * x' + constant\n- kFreeColSubstitution: Free variable substituted via equation\n- kDoubletonEquation: Two-variable equation eliminated\n- kSingletonRow/kFixedCol: Single-entry row or fixed column\n- kRedundantRow/kForcingRow: Redundant or forcing constraints\n- kDuplicateRow/kDuplicateColumn: Parallel rows/columns merged\n\n**Index Mapping:**\n- origColIndex[], origRowIndex[]: Map reduced indices to original\n- compressIndexMaps(): Update after columns/rows removed\n\n**Postsolve Operations:**\n- undo(): Reverse all reductions (primal, dual, basis)\n- undoPrimal(): Reverse only primal solution\n- getReducedPrimalSolution(): Map original solution to reduced space\n\n**Data Storage:**\n- reductionValues: HighsDataStack for reduction parameters\n- reductions[]: Vector of (type, stack_position) pairs\n- rowValues[], colValues[]: Temporary storage for sparse vectors", "see": ["presolve/HPresolve.h for presolve engine", "util/HighsDataStack.h for stack implementation"], "has_pass2": false}, "highs/presolve/ICrashX.h": {"path": "layer-4/HiGHS/highs/presolve/ICrashX.h", "filename": "ICrashX.h", "file": "presolve/ICrashX.h", "brief": "Crossover from interior point to basic solution\n\nProvides simplex crossover after IPM solve to obtain vertex solution.\n\n**callCrossover():**\nConverts interior point solution to basic feasible solution:\n- Input: IPM solution (may be interior to polytope)\n- Output: Basis + vertex solution for post-processing\n\n**Why Crossover:**\n- IPM returns interior solutions, not vertices\n- Basic solutions needed for warm-starting, sensitivity analysis\n- Crossover pushes solution to vertex via simplex pivots\n\n**Process:**\n1. Identify near-bound variables\n2. Construct initial basis from binding constraints\n3. Run simplex cleanup to reach vertex", "see": ["ipm/IpxWrapper.h for IPM solver calling crossover", "simplex/HEkk.h for simplex cleanup phase"], "has_pass2": false}, "highs/presolve/PresolveComponent.h": {"path": "layer-4/HiGHS/highs/presolve/PresolveComponent.h", "filename": "PresolveComponent.h", "file": "presolve/PresolveComponent.h", "brief": "Presolve component for HiGHS pipeline integration\n\nEncapsulates presolve as a modular component with data and statistics.\n\n**PresolveComponentData:**\nState managed by presolve component:\n- reduced_lp_: Presolved LP (smaller problem)\n- postSolveStack: Operations to undo presolve\n- recovered_solution_/recovered_basis_: Original-space solution\n- presolve_log_: Reduction history\n\n**PresolveComponentInfo:**\nStatistics exposed for reporting:\n- n_rows_removed, n_cols_removed, n_nnz_removed: Reduction counts\n- Timing information from HighsComponentInfo base\n\n**PresolveComponentOptions:**\nComponent-specific settings (extends HighsComponentOptions):\n- iteration_limit: Maximum presolve passes\n- Timeout and other controls\n\n**PresolveComponent Class:**\nMain component implementing HighsComponent interface:\n- init(): Setup with options\n- run(): Execute presolve\n- has_run_: Track execution state", "see": ["presolve/HPresolve.h for presolve engine", "util/HighsComponent.h for component base class", "presolve/HighsPostsolveStack.h for solution recovery"], "has_pass2": false}, "highs/presolve/HighsSymmetry.h": {"path": "layer-4/HiGHS/highs/presolve/HighsSymmetry.h", "filename": "HighsSymmetry.h", "file": "presolve/HighsSymmetry.h", "brief": "Symmetry detection and orbital fixing for MIP", "author": "Leona Gottwald\n\nDetects variable permutation symmetries and uses them to reduce search.\n\n**HighsMatrixColoring:**\nAssigns distinct colors to distinct coefficient values for graph coloring.\nUsed to build colored graph where symmetries preserve colors.\n\n**HighsSymmetries:**\nStores detected symmetry group:\n- permutationColumns[], permutations[]: Generator permutations\n- orbitPartition[], orbitSize[]: Variable orbits under symmetry\n- orbitopes[]: Detected orbitope structures for faster propagation\n- numPerms, numGenerators: Symmetry group statistics\n\n**StabilizerOrbits:**\nOrbits under stabilizer of fixed variables:\n- orbitCols[], orbitStarts[]: Orbit structure\n- stabilizedCols[]: Already-fixed variables\n- orbitalFixing(): Apply orbital fixing to domain\n\n**HighsOrbitopeMatrix:**\nSpecial symmetry structure (matrix of symmetric binaries):\n- Type::kFull or Type::kPacking (set-packing rows)\n- orbitalFixing(): Specialized propagation for orbitopes\n- getBranchingColumn(): Symmetry-aware branching\n\n**HighsSymmetryDetection:**\nGraph-based symmetry detection algorithm:\n- loadModelAsGraph(): Build vertex-colored graph from LP\n- initializeDetection(): Set up partition refinement\n- run(): Find automorphism generators via search tree\n- Uses nauty-style partition refinement with certificate comparison", "see": ["mip/HighsDomain.h for orbital fixing application", "mip/HighsMipSolverData.h for symmetry integration"], "has_pass2": false}, "highs/ipm/ipx/basis.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/basis.h", "filename": "basis.h", "file": "ipm/ipx/basis.h", "brief": "Basis management for IPX interior point solver\n\nManages simplex-style basis for IPM preconditioning and crossover.\n\n**BasicStatus Enum:**\n- NONBASIC_FIXED: Fixed at bound, never enters basis\n- NONBASIC: At bound, may enter basis\n- BASIC: In basis, may leave\n- BASIC_FREE: In basis, never leaves\n\n**Basis Storage:**\n- basis_[p]: Column index at position p (0 <= p < m)\n- map2basis_[j]: Position of variable j, or -1/-2 if nonbasic\n- StatusOf(), PositionOf(), IsBasic(): Query methods\n\n**Linear Algebra Operations:**\n- Factorize(): LU factorization with stability check\n- SolveDense(): FTRAN/BTRAN for dense RHS\n- SolveForUpdate(): Prepare for basis exchange\n- TableauRow(): Compute simplex tableau row\n- ExchangeIfStable(): Pivot with stability check\n\n**Basis Construction:**\n- SetToSlackBasis(): Initialize to identity\n- Load(): Load user-provided basis\n- ConstructBasisFromWeights(): Crash procedure\n- CrashBasis(): Fast weighted crash\n- Repair(): Fix singularities with slack columns\n\n**Statistics:**\n- factorizations(), updates_total(): Counts\n- frac_ftran_sparse(), frac_btran_sparse(): Sparsity metrics\n- mean_fill(), max_fill(): LU fill factors", "see": ["ipm/ipx/lu_update.h for LU factorization", "ipm/ipx/lp_solver.h for usage context"], "has_pass2": false}, "highs/ipm/ipx/lp_solver.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/lp_solver.h", "filename": "lp_solver.h", "file": "ipm/ipx/lp_solver.h", "brief": "IPX interior point LP solver main class\n\nImplements primal-dual interior point method with crossover to basic solution.\n\n**Model Loading:**\n- LoadModel(): LP in standard form (c, lb, ub, A, rhs, constr_type)\n- LoadIPMStartingPoint(): User-provided initial point (x, xl, xu, y, zl, zu)\n\n**Solution Methods:**\n- Solve(): Run IPM + optional crossover to basis\n- CrossoverFromStartingPoint(): Convert complementary point to basis\n\n**Solution Access:**\n- GetInfo(): Solver status and statistics\n- GetInteriorSolution(): IPM iterate (primal, slack, dual)\n- GetBasicSolution(): Basic solution with basis status (vbasis, cbasis)\n\n**IPM Pipeline (InteriorPointSolve):**\n1. RunInitialIPM(): Initial centering iterations\n2. BuildStartingBasis(): Construct basis from weighted columns\n3. RunMainIPM(): Main IPM iterations with basis preconditioning\n4. RunCrossover(): Push to basis via dual/primal pushes\n\n**Key Components:**\n- control_: Parameters and logging\n- model_: Preprocessed LP data\n- iterate_: Current IPM point (x, y, z, slacks)\n- basis_: Basis for preconditioning and crossover", "see": ["ipm/ipx/basis.h for basis management", "ipm/ipx/ipm.h for IPM algorithm details", "ipm/IpxWrapper.h for HiGHS integration"], "has_pass2": false}}}, "OS": {"name": "OS", "file_count": 58, "pass2_count": 0, "files": {"OS/src/OSModelInterfaces/OSgams2osil.hpp": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSgams2osil.hpp", "filename": "OSgams2osil.hpp", "return": "whether the instance is created successfully.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSosrl2gams.hpp": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSosrl2gams.hpp", "filename": "OSosrl2gams.hpp", "param": ["gmo_ GMO handler.", "osresult Optimization result as object.", "osrl Optimization result as string."], "has_pass2": false}, "OS/applications/columnGen/code/OSDecompSolverFactory.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompSolverFactory.h", "filename": "OSDecompSolverFactory.h", "file": "OSDecompSolverFactory.h\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSBearcatSolverXij.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSBearcatSolverXij.h", "filename": "OSBearcatSolverXij.h", "file": "OSBearcatSolverXij.h\n\n\\remarks\nCopyright (C) 2005-2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSNode.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSNode.h", "filename": "OSNode.h", "file": "OSNode.h\n\n\\remarks\nCopyright (C) 2005-2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSColGenApp.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSColGenApp.h", "filename": "OSColGenApp.h", "file": "OSColGenApp.h\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSDecompSolver.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompSolver.h", "filename": "OSDecompSolver.h", "file": "OSDecompSolver.h\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["int numNewRows -- number of new rows generated", "int* numNonz -- number of nonzeros in each row", "int** colIdx -- vectors column indexes of new rows", "double** values -- vectors of matrix coefficient values of new rows", "double* rowLB -- vector of row lower bounds", "double* rowUB -- vector of row upper bounds\n\nINPUT:", "double* thetaVar -- the vector of primal master values", "int numThetaVar -- size of master primal vector", "iny numNewColumns -- number of new columns generated", "int* numNonz -- number of nonzeros in each column", "double* cost -- the objective function coefficient on each new column", "double** rowIdx -- vectors row indexes of new columns", "double** values -- vectors of matrix coefficient values of new columns", "double* yA -- the vector of dual values on the coupling constraints", "int numARows -- size of the yA dual vector", "int numBRows -- size of the yA dual vector", "varIdx -- the variable number x_{ij} for branching", "numNonz -- number of theta indexes in the cut", "indexes -- the indexes of the theta variables", "values -- the number of times the theta indexed\nin indexes appears in the cut\nnote -- set numNonz to zero if the generated cut\nvariable already appears in varConMap\n\nINPUT:\ndouble* thetaVar -- the vector of primal master values\nint numThetaVar -- size of master primal vector\nvarConMap -- the map of variables in x_{ij} space to\na consraint number", "varIdx -- the variable number x_{ij} for branching", "indexes -- the indexes of the theta variables", "values -- the number of times the theta indexed\nin indexes appears in the cut\nnote -- set numNonz to zero if the generated cut\nvariable already appears in varConMap\n\nINPUT:", "double* theta -- the vector of primal master values", "int numThetaVar -- size of master primal vector", "dstd::map<int, int> &inVars -- the mapping of variables, the first\nindex is the variable number before resetting, the second index is the variable\nnumber after the reset", "OsiSolverInterface *si -- the solver interface that corresponds to the master\nthis is what gets rebuilt"], "has_pass2": false}, "OS/applications/columnGen/code/OSDecompParam.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompParam.h", "filename": "OSDecompParam.h", "file": "OSColGenApp.cpp\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSBearcatSolverXkij.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSBearcatSolverXkij.h", "filename": "OSBearcatSolverXkij.h", "file": "OSBearcatSolverXkij.h\n\n\\remarks\nCopyright (C) 2005-2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSDecompFactoryInitializer.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompFactoryInitializer.h", "filename": "OSDecompFactoryInitializer.h", "file": "OSDecompFactoryInitializer.cpp\n\n\\remarks\nCopyright (C) 2005-2008, Gus Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSAgent/OSWSUtil.h": {"path": "layer-4/OS/OS/src/OSAgent/OSWSUtil.h", "filename": "OSWSUtil.h", "file": "OSWSUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["solverURI is the location of remote solver or scheduler", "theSOAP is a string that SOAP message sent to the Web service", "servIP is a string with IP address or domain name of the server", "solverPortNumber is a string with the port number of Web server (assume 80 by default)", "theXmlString is the string to modify to out in the SOAP envelop", "useCDATA is true if just encase the XML in a CDATA statement", "theXmlString is the string from the SOAP envelop to modify", "useCDATA is true if just encasing the XML in a CDATA statement", "numInputs is the number of OSxL protocols (e.g. osil, osol) in the SOAP message", "solverAddress is the address of the scheduler or solver used", "postURI is the path to the solver that follows the first\n/ in the solverAddress", "smethod is the method invoked, e.g. solve, kill, send, etc.", "msInputs is string pointer to an array of strings are the OSxL protocols\nprotocols that go into the message, e.g. osil, osol", "msInputNames is a string pointer to an array of string names of the OSxL protocols", "sSoapAction is the name of the solver service plus the method, e.g. OSSolverService#solve", "numInputs is the number of OSxL protocols (e.g. osil, osol) in the SOAP message", "solverAddress is the address of the scheduler or solver used", "postURI is the path to the solver that follows the first\n/ in the solverAddress", "smethod is the method invoked, e.g. solve, kill, send, etc.", "msInputs is string pointer to an array of strings are the OSxL protocols\nprotocols that go into the message, e.g. osil, osol", "msInputNames is string pointer to an array of string names of the OSxL protocols", "sSoapAction is the name of the solver service plus the method, e.g. OSSolverService#solve", "soapstring the soap envelop returned from the Web service", "serviceMethod -- extract the string between the <serviceMethodReturn> and </serviceMethodReturn> tags."], "return": "the reply from the Web service in a SOAP message.\n</p>", "has_pass2": false}, "OS/src/OSAgent/OSSolverAgent.h": {"path": "layer-4/OS/OS/src/OSAgent/OSSolverAgent.h", "filename": "OSSolverAgent.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["solverURI is the location of remote solver or scheduler", "osil a string that holds the problem instance", "osol is a string of options for the solver", "osol is the string with the options in OSoL format", "osil is the string with the instance in OSiL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "ospl is the string with the process information in OSpL format", "osol is the string with the options in OSoL format", "osilFileName is the name of the file\nwith the OSiL instance to be written on the server", "osil is a string with the OSiL problem instance\n</p>"], "return": "osrl which is a string with the result.\n</p>", "has_pass2": false}, "OS/src/OSAgent/OShL.h": {"path": "layer-4/OS/OS/src/OSAgent/OShL.h", "filename": "OShL.h", "file": "OShL.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osil is the string with the instance in OSiL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "osil is the string with the instance in OSiL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "ospl is the string with the process information in OSpL format", "osol is the string with the options in OSoL format"], "return": "a string which is the result in OSrL format.\n</p>", "has_pass2": false}, "OS/src/OSUtils/OSBase64.h": {"path": "layer-4/OS/OS/src/OSUtils/OSBase64.h", "filename": "OSBase64.h", "file": "Base64.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["bytes is the input to be encoded.", "size is the size of the pointer in bytes", "b64bytes is the input to be decoded"], "return": "a string in base 64 format.", "has_pass2": false}, "OS/src/OSUtils/OSFileUtil.h": {"path": "layer-4/OS/OS/src/OSUtils/OSFileUtil.h", "filename": "OSFileUtil.h", "file": "FileUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["fname holds the name of the file.", "fname holds the name of the file.", "fname holds the name of the file to be written.", "thestring holds the string to be written to the file.", "fname holds the name of the file to be written.", "thestring holds the string to be written to the file.", "fname holds the name of the file to be written.", "ch holds a pointer to a char array to be written to the file."], "return": "the file contents as a string.", "has_pass2": false}, "OS/src/OSUtils/OSErrorClass.h": {"path": "layer-4/OS/OS/src/OSUtils/OSErrorClass.h", "filename": "OSErrorClass.h", "file": "ErrorClass.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["errormsg_ holds the error message as a string."], "has_pass2": false}, "OS/src/OSUtils/OSMathUtil.h": {"path": "layer-4/OS/OS/src/OSUtils/OSMathUtil.h", "filename": "OSMathUtil.h", "file": "MathUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\n<p>The <code>MathUtil</code> class contains methods for performing\nmathematics related operations used by many classes in the\nOptimization Services (OS) framework. </p>", "param": ["X holds the number to be rounded.", "precision holds the number of digit after (or before if negative) the decimal point.", "x holds the number before the mod operator.", "x holds the number after the mod operator.", "isColumnMajor holds whether the coefMatrix (AMatrix) holding linear program\ndata is stored by column. If false, the matrix is stored by row.", "startSize holds the size of the start array", "valueSize holds the size of the index and value arrays", "start holds an integer array of start elements in coefMatrix (AMatrix),\nwhich points to the start of a column (row) of nonzero elements in coefMatrix (AMatrix).", "index holds an integer array of rowIdx (or colIdx) elements in coefMatrix (AMatrix).\nIf the matrix is stored by column (row), rowIdx (colIdx) is the array of row (column) indices.", "value holds a double array of value elements in coefMatrix (AMatrix),\nwhich contains nonzero elements.", "dimension holds the column count if the input matrix is row major (row count = start.length-1)\nor the row number if the input matrix is column major (column count = start.length -1)", "x is the double that gets converted into a string\nthis takes the David Gay dtoa and converts to a formatted string", "str is the char* string that gets converted to double\nthis method actually wraps around os_strtod (which is really the\nDavid Gay version of strtod) and will throw an exception\nif the str contains text or is in anyway not a valid number\nstr should be null terminated", "str is the char* string that gets converted to double", "strEnd should point to the end of str\nthis method actually wraps around os_strtod (which is really the\nDavid Gay version of strtod) and will throw an exception\nif the str contains text or is in anyway not a valid number", "i holds a pointer to the array to be processed.", "mult holds the length of the run. This parameter is passed by reference", "incr holds the increment. This parameter is also passed by reference", "size holds the number of elements in the array. This parameter is passed by value", "defaultIncr holds the default value for incr from the schema file. Using just <el mult=\"...\"\nsaves space whenever a run of two or more elements has been encountered, whereas <el mult=\"...\" incr=\"...\"\nsaves space only for runs of three or more elements. Thus the defaultIncr must be treated specially\n(and it might change from one schema element to the next).", "i holds a pointer to the array to be processed.", "mult holds the length of the run. This parameter is passed by reference", "incr holds the increment. This parameter is also passed by reference", "size holds the number of elements in the array. This parameter is passed by value", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array.", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array.", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array.", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array."], "return": "the rounded number.", "has_pass2": false}, "OS/src/OSUtils/OSDataStructures.h": {"path": "layer-4/OS/OS/src/OSUtils/OSDataStructures.h", "filename": "OSDataStructures.h", "file": "OShL.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSUtils/OSParameters.h": {"path": "layer-4/OS/OS/src/OSUtils/OSParameters.h", "filename": "OSParameters.h", "file": "OSParameters.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSUtils/OSOutput.h": {"path": "layer-4/OS/OS/src/OSUtils/OSOutput.h", "filename": "OSOutput.h", "file": "OSOutput.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2012-2013, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["name holds the name of the file or device\n that applies to this output device in all code areas", "area holds the area of the code to which this option is to be applied", "level holds a valid print level", "level holds a valid print level", "level holds an array of valid print levels", "dim holds the number of entries in the array level", "area:  the area in which the output string originated", "level: the print level associated with the string", "str:   the string that is to be printed", "level:  the print level associated with the string", "area:   the area of the code in which the output was generated", "outStr: the string to be output", "name: The name of the channel (\"stdout\" and \"stderr\" are reserved names)", "level: The array of print levels used for the output to this channel", "dim: The number of entries in this array", "name: The name of the channel (\"stdout\" and \"stderr\" are reserved names)", "level: The print level used for the output to this channel\n     if < ENUM_OUTPUT_LEVEL_NUMBER_OF_LEVELS, set the (same) print level in all areas\n     otherwise set the print level only in one particular area", "name: The name of the channel (\"stdout\" and \"stderr\" are reserved names)", "name: The name of the channel", "name: The name of the channel"], "return": "whether the set() was successful", "has_pass2": false}, "OS/src/OSUtils/OSdtoa.h": {"path": "layer-4/OS/OS/src/OSUtils/OSdtoa.h", "filename": "OSdtoa.h", "file": "MathUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\n<p>The <code>MathUtil</code> class contains methods for performing\nmathematics related operations used by many classes in the\nOptimization Services (OS) framework. </p>", "has_pass2": false}, "OS/src/OSUtils/OSStringUtil.h": {"path": "layer-4/OS/OS/src/OSUtils/OSStringUtil.h", "filename": "OSStringUtil.h", "file": "OSStringUtil.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["str holds the string to be output.\nIf the string does not contain double quotes, it is output surrounded by double quotes,\nif the string contains double quotes, it is output surrounded by single quotes,"], "return": "the prepared string, ready to be printed", "has_pass2": false}, "OS/src/OSModelInterfaces/OSmps2OS.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSmps2OS.h", "filename": "OSmps2OS.h", "file": "OSmps2OS.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2013, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "whether the objects are created successfully.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSmps2osil.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSmps2osil.h", "filename": "OSmps2osil.h", "file": "OSmps2osil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "whether the instance is created successfully.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSnl2OS.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSnl2OS.h", "filename": "OSnl2OS.h", "file": "OSnl2osol.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2011, Horand Gassmann, Jun Ma, Kipp Martin,\nand the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["name carries the name of the ASL object\n(there are three of them: asl, rw, cw)", "stub is the (relevant part of the) file name", "asl carries a pointer to the object named \"asl\"", "rw  carries a pointer to the object named \"rw\"", "cw  carries a pointer to the object named \"cw\"\n(asl should point to the same location as either rw or cw)", "osinstance: a pointer to the OSInstance object", "lower: index of the first variable to be set in this call", "upper: set all variables from lower...upper-1", "vartype: the type of the variable (in AMPL this is 'C', 'B' or 'I')", "osinstance: a pointer to the OSInstance object", "lower: index of the first variable to be set in this call", "upper: set all variables from lower...upper-1"], "return": "the pointer to the object named", "has_pass2": false}, "OS/src/OSModelInterfaces/OSosrl2ampl.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSosrl2ampl.h", "filename": "OSosrl2ampl.h", "file": "OSosrl2ampl.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2012, Horand Gassmann, Jun Ma, Kipp Martin,\nand the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osrl is a string containing the result information", "asl is a pointer to an ASL data structure", "filename is the name of the output file\n(e.g., as returned from the solver)."], "return": "whether the .sol file was created successfully.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSMatlabSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSMatlabSolver.h", "filename": "OSMatlabSolver.h", "file": "OSMatlab.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "a string with the solution in OSrL format", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSDefaultSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSDefaultSolver.h", "filename": "OSDefaultSolver.h", "file": "DefaultSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSLindoSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSLindoSolver.h", "filename": "OSLindoSolver.h", "file": "LindoSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "true if an exception is not thrown.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSKnitroSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSKnitroSolver.h", "filename": "OSKnitroSolver.h", "file": "KnitroSolver.h", "author": "Robert Fourer,  Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSCsdpSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSCsdpSolver.h", "filename": "OSCsdpSolver.h", "file": "OSCsdpSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSBonminSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSBonminSolver.h", "filename": "OSBonminSolver.h", "file": "OSBonmnSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSIpoptSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSIpoptSolver.h", "filename": "OSIpoptSolver.h", "file": "IpoptSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2014, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSCouenneSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSCouenneSolver.h", "filename": "OSCouenneSolver.h", "file": "OSCouenneSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSRunSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSRunSolver.h", "filename": "OSRunSolver.h", "file": "OSRunSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2013, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osol: A string containing the user options in osol format", "osinstance: A pointer to an OSInstance object containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osoption: A pointer to an OSOption object containing the options to be passed to the solver", "osil: A string containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osol: A string containing the user options in osol format", "osil: A string containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osoption: A pointer to an OSOption object containing the options to be passed to the solver", "osinstance: A pointer to an OSInstance object containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected based on the characteristics of the problem", "osinstance: A pointer to an OSInstance object containing the instance to be optimized"], "return": "the solution (or error message) in OSrL format", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSCoinSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSCoinSolver.h", "filename": "OSCoinSolver.h", "file": "CoinSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSnLNode.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSnLNode.h", "filename": "OSnLNode.h", "file": "OSnLNode.h\n\\brief This file defines the OSnLNode class along with its derived classes.", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\nIn this file we define classes for a subset of the nodes defined in the OSnL schema\nThese nodes fall into three broad classes:\nThose that evaluate to real values (which inherit from OSnLNode),\nthose that evaluate to matrices (and inherit from OSnLMNode), and\nthose that evaluate to complex values (and inherit from OSnLCNode).\nReal and complex-valued nodes derive from a common parent, ScalarNode,\nwhich is derived from ExprNode.\nOSnLNodes can have OSnLMNode children (e.g., matrixDeterminant)\nand vice versa (e.g., matrixScalarTimes); similarly for OSnLCNodes.", "return": "the value of the operator name", "param": ["recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "a pointer prefixVector to a vector of pointers of ExprNodes", "a pointer postfixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to ExprNodes in prefix format", "nlNodeVec holds a vector of pointers to ExprNodes in postfix format", "x holds the values of the variables in a double array.", "x holds the values of the variables in a double array.", "x holds the values of the variables in a double array.", "x holds the values of the variables in a double array.", "a pointer to a map of the variables in the OSnLNode and its children", "x holds the values of the variables in a double array.", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin prefix format", "a pointer prefixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to OSnLNodes\nin postfix format", "a pointer postfixVector to a vector of pointers of ExprNodes", "recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin prefix format", "a pointer prefixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin postfix format", "a pointer postfixVector to a vector of pointers of ExprNodes", "x holds the values of the variables in a double array.", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "x holds the values of the variables in a double array.", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin prefix format", "a pointer prefixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to OSnLNodes\nin postfix format", "a pointer postfixVector to a vector of pointers of ExprNodes"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSCommandLine.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSCommandLine.h", "filename": "OSCommandLine.h", "file": "OSCommandLine.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2011-2012, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSrLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSrLWriter.h", "filename": "OSrLWriter.h", "file": "OSrLWriter.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nDalhousie University, Northwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["theosresult is a pointer to an OSResult object", "theosresult is a pointer to an OSResult object"], "return": "a string with the OSResult data that validates against the OSrL schema.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSrLReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSrLReader.h", "filename": "OSrLReader.h", "file": "OSrLReader.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nDalhousie University, Northwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osrl an OSrL string."], "return": "the OSResult object corresponding to the OSrL string.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSMatrix.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSMatrix.h", "filename": "OSMatrix.h", "file": "OSMatrix.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2010-2015, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "the value of nType", "param": ["a pointer postfixVector to a vector of pointers of MatrixNodes", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "isRowMajor_ holds whether the matrix is stored by column.\n       If true, the matrix is stored by row.", "numberOfRows holds the number of rows", "numberOfColumns holds the number of columns", "startSize_ holds the size of the start array.", "valueSize_ holds the size of the value and index arrays.", "type_ describes the type of values held in the matrix (see OSParameters.h).", "nValues gives the size of the value->el array", "sourceElements is the source array from which to copy", "sourceType gives the type of elements in the source array", "sourceIndex gives the location within the source's value array to be copied from", "targetIndex gives the location within the target's value array to be copied to", "scalarMult if present, gives a real scalar by which to multiply the value to be copied", "scalarImag if present, gives the imaginary part of the scalar multiplier", "sourceElements is the source array from which to copy", "sourceType gives the type of elements in the source array", "sourceIndex gives the location within the source's value array to be copied from", "targetIndex gives the location within the target's value array to be copied to", "scalarMult if present, gives a real scalar by which to multiply the value to be copied", "scalarImag if present, gives the imaginary part of the scalar multiplier", "convertTo_ gives the type of elements to be stored into the target matrix", "transpose_ specifies whether the matrix is to be transposed in the process\n        (i.e., the identity of the index array is to be adjusted)", "convertTo_ gives the type of elements to be stored into the target matrix", "scalarMult if present, gives a real scalar by which to multiply the values", "scalarImag if present, gives the imaginary part of the scalar multiplier", "symmetry_ gives the type of symmetry to be used in the target matrix", "copyValues_ determines whether the arrays need to be duplicated", "symmetry_ gives the type of symmetry to be used in the target matrix", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the blocks are stored in row major form or not.", "valueType_ indicates in which form to store the disassembled matrix\n        The default for this optional parameter is ENUM_MATRIX_TYPE_unknown", "symmetry_ determines what kind of symmetry to use in representing the blocks.\n        If this parameter is missing, the default value is NO symmetry", "rowIdx is the row index of the block to be retrieved", "colIdx is the column index of the block to be retrieved", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "symmetry_ contains the symmetry type (see OSParameters.h)", "mtxArray provides pointers to all defined matrices in case expansion is necessary.", "mtxArray   provides pointers to all defined matrices for use within transformations.", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "transpose_ controls whether the expansion is of the matrix or its transpose\n                   The default is to expand the matrix in its natural form", "idx indicates which entry in the vector of expansions is to be printed", "idx indicates which entry in the vector of expansions is to be printed", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor can be used to store the objects in row major form.", "isColumnMajor holds whether the matrix is currently stored by column.\n       If true, the matrix is converted to row major form.\n       If false, the matrix is stored by row and is converted to column major.", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the baseMatrix should be stored in row major (if true)\n        or column major.", "convertTo_ indicates the form of the value array in the expanded matrix\n        The default is \"unknown\", which infers the value type from the content of the matrix", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "currentBlocks is a pointer to the collection of blocks that is to be expanded", "mtxArray  provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the expanded matrix should be stored in row major (if true)\n        or column major.", "convertTo_ indicates the form of the value array in the expanded matrix\n        The default is \"unknown\", which infers the value type from the content of the matrix", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "nConst indicates the position of the constructor in the array m_mChildren, the child\n        elements of the current matrix (i.e., the list of constructors)", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the expanded matrix should be stored in row major (if true)\n        or column major.", "convertTo_ indicates the form of the value array in the expanded matrix\n        The default is \"unknown\", which infers the value type from the content of the matrix", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "nConst gives the number of the constructor within the constructor list\n        of the parent matrix or block", "rowMajor indicates whether the baseMatrix should be stored in row major (if true)\n        or column major.", "convertTo_ can be used to force conversion of matrix elements to a specific form.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "nConst     indicates the position of the constructor in the array m_mChildren, the child\n                   elements of the current matrix (i.e., the list of constructors)", "mtxArray   provides pointers to all defined matrices for use within transformations.", "rowMajor_  indicates whether the baseMatrix should be stored in row major (if true)\n                   or column major.", "convertTo_ can be used to force conversion of matrix elements to a specific form.\n-    *", "symmetry_  can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "mtxArray   provides pointers to all defined matrices for use within transformations.", "rowMajor_  indicates whether the blocks should be stored in row major (if true)\n                   or column major.", "convertTo_ is an optional parameter that can be used to covert the elements\n                   of all blocks to a different type", "symmetry_  can be used to store only the upper or lower triangle, depending\n                   on the parameter value --- see OSParameters.h for definitions.\n                   The default is \"none\".", "mtxArray  provides pointers to all defined matrices for use within transformations.", "rowOffset defines a partition of the matrix rows into the blocks", "rowOffsetSize gives the number of elements in the rowOffset array", "colOffset defines a partition of the matrix columns into the blocks", "colOffsetSize gives the number of elements in the colOffset array", "rowMajor_ controls whether the blocks are stored by row or by column", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "firstrow gives the first row of the block", "firstcol gives the first column of the block", "lastrow gives the last row of the block", "lastcol gives the last column of the block", "rowMajor_ can be used to store the objects in row major form.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "appendToBlockArray determines whether the blocks should be created if not found.", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor indicates whether the blocks are stored in row major form or not.", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "appendToBlockArray determines whether the blocks should be created if not found.", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor indicates whether the blocks are stored in row major form or not.", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "i is the number of the <blocks> constructor in the array of matrix constructors.", "appendToBlockArray determines whether the blocks should be created if not found.", "rowMajor indicates whether the blocks are stored in row major form or not.", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the blocks are stored in row major form or not.", "valueType_ indicates in which form to store the disassembled matrix\n        The default for this optional parameter is ENUM_MATRIX_TYPE_unknown", "symmetry_ determines what kind of symmetry to use in representing the blocks.\n        If this parameter is missing, the default value is NO symmetry", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n                   (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)\n                   if false, then side constraints are not enforced,\n                   which can be useful for parser stress and compliance tests", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "mtxConstructorVec holds a vector of pointers to matrix constructors,\nmtxConstructorVec and blocks in prefix format", "rowOffset defines a partition of the matrix rows into the blocks", "colOffset defines a partition of the matrix columns into the blocks", "rowMajor controls whether the blocks are stored by row or by column", "symmetry can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor can be used to store the objects in row major form.", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "name holds the matrix name; use null or empty std::string (\"\") if no matrix name.", "numberOfRows holds the number of rows. It is required. Use 1 for column vectors.", "numberOfColumns holds the number of columns. It is required. Use 1 for row vectors.", "symmetry holds the type of symmetry used in the definition of the matrix.\n       For more information  see the enumeration ENUM_MATRIX_SYMMETRY in OSGeneral.h.\n       If no symmetry, use ENUM_MATRIX_SYMMETRY_unknown.", "declaredMatrixType tracks the type of elements contained in this matrix.\n       For more information  see the enumeration ENUM_MATRIX_TYPE in OSGeneral.h.\n       If unsure, use ENUM_MATRIX_TYPE_unknown.", "inumberOfChildren is the number of MatrixNode child elements,\n       i.e., the number of matrix constructors in the m_mChildren array.", "m_mChildren is the array of matrix constructors used in the definition of this matrix.", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that an objective reference in this matrix can take", "iMax: greatest index value (inclusive) that an objective reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that an objective reference in this matrix can take", "iMax: greatest index value (inclusive) that an objective reference can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a constraint reference in this matrix can take", "iMax: greatest index value (inclusive) that a constraint reference can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor can be used to store the objects in row major form.", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSoLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSoLWriter.h", "filename": "OSoLWriter.h", "file": "OSoLWriter.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["theosoption is a pointer to an OSOption object"], "return": "a string with the OSOption data that validates against the OSoL schema.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSoLReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSoLReader.h", "filename": "OSoLReader.h", "file": "OSoLReader.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osol is a string that holds the solver options."], "return": "the instance as an OSOption object.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSExpressionTree.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSExpressionTree.h", "filename": "OSExpressionTree.h", "file": "OSExpressionTree.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2014, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "the expression tree as a vector of ExprNodes in prefix.", "param": ["x holds the values of the variables in a double array.", "new_x is false if any evaluation method was previously called for the current x", "x holds the values of the variables in a double array.", "new_x is false if any evaluation method was previously called for the current x", "x holds the values of the variables in a double array.", "new_x is false if any evaluation method was previously called for the current x"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSResult.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSResult.h", "filename": "OSResult.h", "file": "OSResult.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nDalhousie University, Northwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "name: the name of this file or instance", "source: the source (e.g., in BiBTeX format)", "fileCreator: the creator of this file", "description: further description about this file and/or its contents", "licence: licence information if applicable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "i the number of the substatus (must be between 0 and numberOfSubstatuses)", "i the number of the substatus (must be between 0 and numberOfSubstatuses)", "i holds the number of the result whose name is saught.", "solIdx holds the solution index to get the solution status.", "solIdx holds the solution index to get the solution status type.", "solIdx holds the solution index to get the solution status description.", "solIdx holds the solution index to get the solution status description.", "solIdx holds the solution index to get the solution message.", "solIdx holds the solution index the optimal solution corresponds to.", "solIdx holds the solution index for the current solution", "object describes the kind of indices to be retrieved\n(legal values are described in ENUM_BASIS_STATUS --- see OSGeneral.h)", "status gives the basis status type", "solIdx holds the solution index for the current solution", "object describes the kind of indices to be retrieved\n(legal values are described in ENUM_BASIS_STATUS --- see OSGeneral.h)", "status gives the basis status (basic, atLower, atUpper, etc.)", "j is the (zero-based) position of the desired entry within the index array", "solIdx is the solution index", "object describes the kind of indices to be retrieved\n (legal values are described in ENUM_PROBLEM_COMPONENT --- see OSGeneral.h)", "resultArray is the array that returns the basis information", "dim is the dimension of the resultArray", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n @enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "enumIdx is the index of the current enumeration level", "j is the (zero-based) position of the index within the index array", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "resultArray is the array that returns the content of the <var> or <enumeration> array", "dim is the array dimension", "objIdx holds the objective index the optimal value corresponds to.", "solIdx holds the solution index the optimal value corresponds to.", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "j is the (zero-based) position of the index in the array", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "resultArray is the array that returns the content of the <obj> or <enumeration> array", "dim is the array dimension", "solIdx holds the solution index the optimal solution corresponds to.", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "j is the (zeo-based) position of the entry in the array", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "resultArray is the array that returns the content of the <con> or <enumeration> array", "dim is the array dimension", "status holds the general status.", "type holds the general status type", "num holds the number of substatuses (a nonegative integer)", "description holds the general status description.", "name holds the general substatus name", "idx holds the index of the substatus in the array", "description holds the general substatus description.", "idx holds the index of the substatus in the array", "message holds the general message.", "serviceName holds the name of the service.", "serviceURI holds the uri of the service.", "instanceName holds the name of the instance.", "jobID holds the job id.", "solverInvoked holds the solver invoked.", "time holds the time stamp.", "num holds the number of other general results.", "name holds the general otherResult name", "idx holds the index of the otherResult in the array", "value holds the general otherResult value", "idx holds the index of the otherResult in the array", "description holds the general otherResult description", "idx holds the index of the otherResult in the array", "idx holds the index of the otherResult in the array", "name holds the general otherResult description", "value holds the general otherResult value", "description holds the general otherResult description", "systemInformation holds the system information", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte, petabyte)", "description holds further information about available disk space", "value holds the number of disk space units", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte)", "description holds further information about available memory", "value holds the number of memory units", "unit holds unit", "description holds further information about the CPU speed", "value holds the available CPU speed", "description is used to impart further info about the CPUs", "value holds the available number of CPUs", "num holds the number of other system results.", "name holds the system otherResult name", "idx holds the index of the otherResult in the array", "name holds the system otherResult value", "idx holds the index of the otherResult in the array", "name holds the system otherResult description", "idx holds the index of the otherResult in the array", "currentState holds the current state", "jobCount holds the current job count", "number holds the total number of jobs", "startTime holds the starting time", "value holds the service utilitzation", "num holds the number of other service results.", "name holds the service otherResult name", "idx holds the index of the otherResult in the array", "name holds the service otherResult value", "idx holds the index of the otherResult in the array", "name holds the service otherResult description", "idx holds the index of the otherResult in the array", "status holds the job status", "submitTime holds the time when the job was submitted", "scheduledStartTime holds the scheduled start time", "actualStartTime holds the actual start time", "endTime holds the time when the job finished", "time holds the time.", "type holds the timer type (cpuTime/elapsedTime/other).", "category holds the timer category (total/input/preprocessing, etc.)", "unit holds the timer unit (tick/milliscond/second/minute/etc.)", "description holds further information about the timer.", "value holds the time measurement.", "idx holds the index within the time array of the item to be set", "type holds the timer type (cpuTime/elapsedTime/other).", "category holds the timer category (total/input/preprocessing, etc.)", "unit holds the timer unit (tick/milliscond/second/minute/etc.)", "description holds further information about the timer.", "value holds the time measurement.", "numberOfTimes holds the number of measurements", "timeNumber holds the number of measurements", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte, petabyte)", "description holds further information about used disk space", "value holds the number of disk space units", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte)", "description holds further information about used memory", "value holds the number of memory units", "unit holds unit", "description holds further information about the CPU speed", "value holds the used CPU speed", "description is used to impart further info about the CPUs", "value holds the used number of CPUs", "num holds the number of other job results.", "name holds the job otherResult name", "idx holds the index of the otherResult in the array", "name holds the job otherResult value", "idx holds the index of the otherResult in the array", "name holds the job otherResult description", "idx holds the index of the otherResult in the array", "variableNumber holds the number of variables", "objectiveNumber holds the number of objectives", "constraintNumber holds the number of constraints", "number holds the number of solutions to set.", "solIdx holds the solution index to set the solution status.", "status holds the optimization solution status to set.", "solIdx holds the solution index whose status to set.", "type holds the solution status type", "solIdx holds the solution index whose status to set.", "num holds the number of substatuses (a nonegative integer)", "solIdx holds the solution index whose status to set.", "description holds the solution status description.", "solIdx holds the solution index whose status to set.", "substatusIdx holds the index of the substatus in the array", "type holds the general substatus type", "solIdx holds the solution index whose status to set.", "substatusIdx holds the index of the substatus in the array", "description holds the general substatus description.", "solIdx holds the solution index to set the objective index.", "objectiveIdx holds the objective index to set.\nAll the objective indexes are negative starting from -1 downward.", "solIdx holds the solution index to set the objective index.", "objectiveName holds the objective indexname to set.", "solIdx holds the solution index to set the objective index.", "weightedObjectives holds the value \"true\" or \"false\".", "solIdx holds the solution index to set the objective index.", "msg holds the solution message to set.", "solIdx holds the solution index to set the primal variable values.", "n holds the number of elements in the array x", "solIdx holds the solution index to set the primal variable values.", "x holds a vector of type IndexValuePair; the idx component holds the index of the variable;\nthe value component holds its value. The vector could be null if all variables are 0.", "solIdx holds the solution index to set the primal variable values.", "x holds a double dense array of variable values to set; it could be null if all variables are 0.", "solIdx holds the solution index to set the primal variable values.", "numberOfVar holds the number of primal variables that are to be set", "solIdx holds the solution index to set the primal variable values.", "number holds the location within the sparse array var that is to be used", "idx holds the index of the primal variable that is to be set", "name holds the variable name (or an empty string).", "val holds the variable value to set.", "solIdx holds the solution index to set the primal variable values.", "numberOfVar holds the number of primal variables that are to be set", "solIdx holds the solution index to set the primal variable values.", "number holds the location within the sparse array var that is to be used", "idx holds the index of the primal variable that is to be set", "name holds the variable name (or an empty string).", "str holds the variable value to set.", "solIdx holds the index of the solution to which the basis values belong.", "object holds the type of basis object to be used\n    (legal values are taken from the ENUM_PROBLEM_COMPONENT enumeration --- see OSGeneral.h))", "status holds the status which is to be used\n    (legal values are taken from the ENUM_BASIS_STATUS enumeration --- see OSGeneral.h)", "i holds the integer array whose values are to be transferred.\n\n    (NOTE WELL: This method does not handle individual variables --- the entire basis must be processed at once.)", "ni holds the number of elements of i", "solIdx is the solution index", "numberOfOtherVariableResult holds the number of OtherVariableResult objects\nEach other variable result contains the name (required), an optional description (std::string) and an optional\nvalue (std::string). Each other variable result can also optionally contain an array OtherVarResult for each variable.\nThe OtherVarResult contains a variable idx (required) and an optional std::string value.", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "idx holds a pointer to the indexes of the var element", "s holds a pointer to the array of values of the var element", "n holds the number of elements of the array", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "idx holds a pointer to the indexes of the var element", "s holds a pointer to the array of values of the var element", "n holds the number of elements of the array", "type holds the type of the <other> element's value attribute", "varType holds the type of the <other> element's <var> array", "enumType holds the type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "s holds a pointer to the array of values of the var element", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "s holds a pointer to the array of values of the var element", "type holds the type of the <other> element's value attribute", "varType holds the type of the <other> element's <var> array", "enumType holds the type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "numberOfVar holds the number of <var> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "numberOfEnumerations holds the number of <enumeration> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "name holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "type holds the type of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varType holds the data type of the <var> array of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "enumType holds the data type of the <enumeration> array of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "value holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "description holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "solver holds the type of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "category holds the type of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varIdx holds the index of the location to which the information is stored", "idx holds the index of the variable to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varIdx holds the index of the location to which the information is stored", "name holds the name of the variable to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varIdx holds the index of the location to which the information is stored", "value holds the value of the variable to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "object holds the object to which this enumeration pertains\n    (legal values are taken from the ENUM_PROBLEM_COMPONENT enumeration --- see OSGeneral.h))", "enumIdx holds the index of the OtherOptionOrResultEnumeration object", "value holds the value of this result", "description holds a description of this result", "i holds the indices of the variables that take on this value", "ni holds the dimension of the index vector i", "solIdx is the solution index", "numberOfOtherObjectiveResult holds the number of OtherObjectiveResult objects\nEach other objective result contains the name (required), an optional description (std::string) and an optional\nvalue (std::string). Each other objective result can also optionally contain an array OtherObjResult for each objective.\nThe OtherObjResult contains an objective idx (required) and an optional std::string value.", "solIdx holds the solution index to set the objective values.", "numberOfObj holds the number of objectives that are to be set", "solIdx holds the solution index to set the constraint values.", "n holds the number of elements in the array x", "solIdx holds the solution index to set the objective values.", "x holds a vector of type IndexValuePair; the idx component holds the index of the objective;\nthe value component holds its value. The vector could be null if all objectives are 0.\nPossibly only the objective that the solution is based on has the value, and the rest of the objective\nvalues all get a Double.NaN value, meaning that they are not calculated.", "solIdx holds the solution index to set the objective values.", "objectiveValues holds the double sparse array of objective values to set.\nPossibly only the objective that the solution is based on has the value, and the rest of the objective\nvalues all get a Double.NaN value, meaning that they are not calculated.", "solIdx holds the solution index to set the objective values.", "number holds the location within the sparse array obj that is to be used", "idx holds the index of the objective that is to be set", "name holds the objective name (or an empty string).", "val holds the objective value to set.", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "numberOfObj holds the number of <obj> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "numberOfObj holds the number of <obj> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the type of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the data type of the <other> element's <var> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the data type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "value holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "description holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "solver holds the solver of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "category holds the category of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "objIdx holds the index of the location to which the information is stored", "idx holds the index of the objective to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "objIdx holds the index of the location to which the information is stored", "name holds the name of the objective to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "objIdx holds the index of the location to which the information is stored", "value holds the value of the objective to which the information belongs", "solIdx is the solution index", "numberOfOtherConstraintResults holds the number of OtherConstraintResult objects\nEach other objective result contains the name (required), an optional description (std::string) and an optional\nvalue (std::string). Each other constraint result can also optionally contain an array OtherConResult for each constraint.\nThe OtherConResult contains a constraint idx (required) and an optional std::string value.", "solIdx holds the solution index to set the constraint values.", "numberOfCon holds the number of constraint that are to be set", "solIdx holds the solution index to set the dual variable values.", "n holds the number of elements in the array x", "solIdx holds the solution index to set the dual variable values.", "x holds a vector of type IndexValuePair; the idx component holds the index of the constraint;\nthe value component holds its value. The vector could be null if all dual variables are 0.", "solIdx holds the solution index to set the dual variable values.", "y holds a double dense array of variable dual values; it could be NULL if all values are 0.", "solIdx holds the solution index to set the constraint values.", "constraintValues holds the a double dense array of constraint values to set; it could be null if all constraint values are 0.", "solIdx holds the solution index to set the constraint values.", "number holds the location within the sparse array con that is to be used", "idx holds the index of the constraint that is to be set", "name holds the constraint name (or an empty string).", "val holds the constraint value to set.", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "numberOfCon holds the number of <con> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "numberOfCon holds the number of <con> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the type of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the type of the <other> element's <con> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "value holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "description holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "solver holds the solver of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "category holds the category of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "conIdx holds the index of the location to which the information is stored", "idx holds the index of the onstraint to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "conIdx holds the index of the location to which the information is stored", "name holds the name of the constraint to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "conIdx holds the index of the location to which the information is stored", "value holds the value of the constraint to which the information belongs", "solIdx is the solution index", "numberOfMatrixVar_ holds the number of matrixVar elements\n       for which values are to be provided", "numberOfOtherMatrixVariableResults_ holds the number of <other> elements\n       for which values are to be provided", "solIdx is the solution index", "idx holds the index of the matrixVar (in the MatrixVariableValues array)", "matrixVarIdx holds the index of the matrixVar (as defined in the OS instance)", "numberOfRows holds the number of rows in the matrixVar", "numberOfColumns holds the number of columns in the matrixVar", "symmetry (optional) holds the type of symmetry\n       (if not present, the default value is \"none\"", "type (optional) holds the type of values in the nonzeroes of the matrix\n       (if not present, the default value is \"unknown\")", "name (optional) holds the name of this matrixVar (the default is the empty string)\n       for which values are to be provided", "solIdx is the solution index", "idx holds the index of the matrixVar for which values are to be provided\n       (as derived from the <values> element in matrixProgramming", "colOffset is an array of column offsets that define\n       the partition of the columns within the block structure", "colOffsetSize gives the size of the colOffset array", "rowOffset is an array of row offsets that define\n       the partition of the rows within the block structure", "rowOffsetSize gives the size of the rowOffset array", "numberOfBlocks gives the number of blocks", "blocksConstructorIdx gives the index of the MatrixBlocks node\n       in the array of constructors of the matrixVar. The default is 0.", "solIdx is the solution index", "idx holds the index of the matrixVar for which values are to be provided\n       (as derived from the <values> element in matrixProgramming", "blkno is the number of the block for which elements are provided\n       the partition of the columns within the block structure", "blkRowIdx gives the index of the block row in which the block is located", "blkColIdx gives the index of the block column in which the block is located", "nz gives the number of (nonzero) values", "start gives the start elements (column or row starts, depending on\n       whether rowMajor is false or true)", "index gives the array of row or column (depending on rowMajor) indices", "value gives the data structure for (nonzero) values that need to be stored", "valueType gives the type of values (see OSParameters.h)", "symmetry gives the form of symmetry. (The default is NONE.)", "rowMajor indicates whther the elements are stored column by column (if rowMajor is false)\n       or row by row (if rowMajor is true). The default is rowMajor = false.\n\n@remark each block object can handle only one type of elements,\n        although different blocks may contain different types of values", "solIdx is the solution index", "idx holds the index of the other result (in the array of <other> solutions)", "name holds the name of the <other> result", "description can be used to hold a further description of the result", "value holds a scalar value associated with the <other> result", "type describes the type of value represented by the scalar result", "solver gives the solver with which this result is associated", "category can be used to specify a further category within the solver", "numberOfMatrixVar gives the number of matrixVar elements associated with this result.\n       This argument is optional and defaults to 0.", "matrixType can be used to associate a type with the values of the matrixVar elements.\n       This argument is optional and defaults to the empty string (\"\").", "numberOfEnumerations gives the number of levels associated with an enumeration of matrixVar\n       elements pertaining to this result. The argument is optional and defaults to 0.", "enumType can be used to associate a type with the values of the enumeration.\n       This argument is optional and defaults to the empty string (\"\").", "solIdx is the solution index", "otherIdx holds the index of the other matrix variables result", "matrixVarIdx holds the index of the matrixVar (as defined in the OS instance)", "numberOfRows holds the number of rows in the matrixVar", "numberOfColumns holds the number of columns in the matrixVar", "symmetry (optional) holds the type of symmetry\n       (if not present, the default value is \"none\"", "type (optional) holds the type of values in the nonzeroes of the matrix\n       (if not present, the default value is \"unknown\")", "name (optional) holds the name of this matrixVar (the default is the empty string)\n       for which values are to be provided", "solIdx is the solution index", "otherIdx holds the index of the other solution", "matrixVarIdx holds the index of the matrixVar for which values are to be provided", "colOffset is an array of column offsets that define\n       the partition of the columns within the block structure", "colOffsetSize gives the size of the colOffset array", "rowOffset is an array of row offsets that define\n       the partition of the rows within the block structure", "rowOffsetSize gives the size of the rowOffset array", "numberOfBlocks gives the number of blocks", "blocksConstructorIdx gives the index of the MatrixBlocks node\n       in the array of constructors of the matrixVar. The default is 0.", "solIdx is the solution index", "otherIdx holds the index of the other solution", "matrixVarIdx holds the index of the matrixVar for which values are to be provided", "blkno is the number of the block for which elements are provided\n       the partition of the columns within the block structure", "blkRowIdx gives the index of the block row in which the block is located", "blkColIdx gives the index of the block column in which the block is located", "nz gives the number of (nonzero) values", "start gives the start elements (column or row starts, depending on\n       whether rowMajor is false or true)", "index gives the array of row or column (depending on rowMajor) indices", "value gives the data structure for (nonzero) values that need to be stored", "valueType gives the type of values (see OSParameters.h)", "symmetry gives the form of symmetry. (The default is NONE.)", "rowMajor indicates whther the elements are stored column by column (if rowMajor is false)\n       or row by row (if rowMajor is true). The default is rowMajor = false.\n\n@remark each block object can handle only one type of elements,\n        although different blocks may contain different types of values", "solIdx is the solution index", "numberOfOtherSolutionResults holds the number of OtherSolutionResult objects\nEach other objective result contains the name (required), an optional\ndescription (std::string) and an optional category (std::string).\nEach other solution result can also optionally contain an array Item for each result.\nThe Item content is string-valued.", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "name holds the name of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "value holds the value of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "category holds the category of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "category holds the description of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "numberOfItems holds the number of items", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "itemIdx holds the index of the item", "item holds the value of the item", "solIdx holds the solution index i.", "name holds the name of the other solution result", "value holds the value of the other solution result", "category holds the category of the result", "description holds a description of the result", "numberOfItems holds the number of items", "item holds a pointer to the array of items (can be NULL if numberOfItems is 0)", "numberOfOtherSolverOutputs holds the number of SolverOutput objects\nEach solver output can also optionally contain an array Item for each result.\nThe Item content is string-valued.", "otherIdx holds the index of the solverOutput object", "name holds the name of the solver output", "otherIdx holds the index of the solverOutput object", "name holds the category of the solver output", "otherIdx holds the index of the solverOutput object", "name holds the description of the solver output", "otherIdx holds the index of the solverOutput object", "numberOfItems holds the number of items", "otherIdx holds the index of the otherSolutionResult", "itemIdx holds the index of the item", "item holds the value of the item"], "return": "whether the copy was created successfully", "see": ["org.optimizationservices.oscommon.datastructure.osresult.OptimizationSolutionStatus", "#setVariableNumber(int)", "#setObjectiveNumber(int)", "#setConstraintNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OptimizationSolutionStatus", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherSolutionResult", "org.optimizationservices.oscommon.datastructure.osresult.Item", "#setSolutionNumber(int)"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSOption.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSOption.h", "filename": "OSOption.h", "file": "OSOption.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "name: the name of the <other> element to be added (required)", "value: the value of the <other> element to be added (optional)", "description: a description of the <other> element (optional)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfJobIDs: number of <jobID> elements to be set", "jobID: the array of <jobID> elements that are to be set", "jobID: the name of the <jobID> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfPaths: number of <path> elements to be set", "path: the array of <path> elements that are to be set", "path: the path to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfPathPairs: number of <pathPair> elements to be set", "path: the array of <pathPair> elements that are to be set", "from: array containing a list of objects to be moved", "to: array containing a list of destinations", "makeCopy: records whether each object is to be moved or copied", "numberOfPathPairs: number of <pathPair> elements to be set", "fromPath: the path from which to copy or move", "toPath: the path to which to copy or move", "makecopy: tracks whether a copy is to be made", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfProcesses: number of <process> elements to be set", "path: the array of <process> elements that are to be set", "process: the ID of the process to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial string value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements to be that are to be set", "idx: the index of the variable to be given an initial basis status", "value: the initial basis status to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements to be that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given a branching weight", "value: the branching weight to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable to be given a branching weight", "value: the branching weight to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfSOS: number of <sos> elements to be set", "sos: the array of <sos> elements that are to be set", "sosIdx: the index of the SOS that is to be added (refer back to OSiL file)", "nvar: the number of variables in this SOS that are to be given weights", "weight: a selection weight for the entire group of variables", "idx: an array of variable indices", "value: the array of corresponding selection weights", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfObj: number of <obj> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of objective names", "idx: the index of the objective to be given an initial value", "value: the initial value to be added", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfObj: number of <obj> elements to be set", "idx: the array of indices", "lbValue: the array of corresponding lower bounds", "ubValue: the array of corresponding upper bounds", "name: the array of objective names", "idx: the index of the objective to be given initial bounds", "lbValue: the initial lower bound for the objective", "ubValue: the initial upper bound for the objective", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "idx: the index of the objective", "value: the value associated with this objective", "lbValue: a lower bound associated with this objective", "ubValue: an upper bound associated with this objective", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "con: the array of <con> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfCon: number of <con> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of constraint names", "idx: the index of the constraint to be given an initial value", "value: the initial value to be added", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "con: the array of <con> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfCon: number of <con> elements to be set", "idx: the array of indices", "lbValue: the array of dual values for the lower bound", "ubValue: the array of dual values for the upper bound", "name: the array of constraint names", "idx: the index of the constraint to be given initial dual variables", "lbDualValue: an initial value for the dual variable associated with the lower bound", "ubDualValue: an initial value for the dual variable associated with the upper bound", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <matrixVar> elements to be set", "var: the array of <matrixVar> elements that are to be set", "numberOfVar: number of <matrixVar> elements to be set", "var: the array of <matrixVar> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <matrixVar> elements to be set", "matrixVar: the array of <matrixVar> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of solver options to be set", "solverOption: the array of solver options that are to be set", "name: the name of the solver option (required)", "value: a value associated with the option (optional)", "solver: the solver to which the option applies (optional)", "category: the category (and subcategories) of the option (optional)", "type: the type of the option (optional)", "description: a description associated with the option (optional)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "name: the name of this file or instance", "source: the source (e.g., in BiBTeX format)", "fileCreator: the creator of this file", "description: further description about this file and/or its contents", "licence: licence information if applicable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the OSOption object from which information is to be copied", "iSOS the number of the SOS", "numberOfVariables holds the dimension of the vector", "numberOfVariables holds the dimension of the vector", "numberOfVariables is the dimension of the array", "type: the type of variable or problem component\n\t(contained in ENUM_PROBLEM_COMPONENT --- see OSGeneral.h)", "status: the basis status\n\t(contained in ENUM_BASIS_STATUS --- see OSGeneral.h)", "type: the type of variable or problem component\n\t(contained in ENUM_PROBLEM_COMPONENT --- see OSGeneral.h)", "status: the basis status\n\t(contained in ENUM_BASIS_STATUS --- see OSGeneral.h)", "elem: pointer to the memory location where the user wants to store the returned values", "numberOfVariables holds the dimension of the vector", "solver_name is the name of the solver whose options we want", "optionNumber is the index of the option in the array", "numberOfObjectives holds the dimension of the vector", "numberOfObjectives holds the dimension of the vector", "numberOfObjectives holds the dimension of the vector", "numberOfObjectives is the dimension of the array", "solver_name is the name of the solver whose options we want", "optionNumber is the index of the option in the array", "numberOfConstraints holds the dimension of the vector", "numberOfConstraints holds the dimension of the vector", "numberOfConstraints holds the dimension of the vector", "numberOfConstraints is the dimension of the array", "solver_name is the name of the solver whose options we want", "optionNumber is the index of the option in the array", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxArray is a pointer to the <matrices> array, which may be needed\n         for the block expansion", "rowPartition is an array of integer values describing the desired row partition", "rowPartitionSize gives the size of the rowPartition array", "colPartition is an array of integer values describing the desired column partition", "colPartitionSize gives the size of the colPartition array", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxArray is a pointer to the <matrices> array, which may be needed\n         for the block expansion", "rowPartition is an array of integer values describing the desired row partition", "rowPartitionSize gives the size of the rowPartition array", "colPartition is an array of integer values describing the desired column partition", "colPartitionSize gives the size of the colPartition array", "solver_name is the name of the solver whose options we want", "solver_name is the name of the solver whose options we want", "getFreeOptions is a boolean set to true if the free\n       options (not associated with a solver name) should be returned", "numberOfOptions contains the number of other options to be set", "other is a pointer to an array of OtherOption objects", "name - the identifying anme of the option. This string cannot be empty", "value - optional value associated with this option", "description - further information (can be used for documentation)", "unit - select the unit (Kb, Mb, etc.)", "description - further description (can be used for documentation)", "value - number of units of disk space required", "unit - select the unit (Kb, Mb, etc.)", "description - further description (can be used for documentation)", "value - number of units of memory size required", "unit - select the unit (MHz, GHz, TFlops etc.)", "description - further description (can be used for documentation)", "value - number of units of CPU speed required", "number - number of CPU cores required", "description - further description (can be used for documentation)", "object describes the type of pathpairs\n      legal values are\n          ENUM_PATHPAIR_input_dir,\n          ENUM_PATHPAIR_input_file,\n          ENUM_PATHPAIR_output_file,\n          ENUM_PATHPAIR_output_dir", "from is a pointer to an array of strings containing the\n          location of the original object", "to   is a pointer to an array of strings containing the\n          location of the destination object", "makeCopy is a pointer to an array of boolean, describing\n          for each object whether it is to be copied or moved", "numberOfPathPairs is an integer giving the number of PathPairs\n          this must equal the number of entries in the from, to and makeCopy arrays", "type: type of this element (see ENUM_PROBLEM_COMPONENT - OSGeneral.h)", "idx: index of this element (nonnegative for variable or constraint,\n\tnegative for objective)", "status: basis status (see ENUM_BASIS_STATUS - OSGeneral.h)", "iOther: position of this element in the array of <other>", "numberOfVar: number of <var> children contained in this <other> element", "numberOfEnumerations: number of <enumeration> children", "name: name of this <other> element", "value: a value associated with this <other> element", "solver: the solver associated with this <other> element", "category: the category of this <other> element", "type: type of this <other> element", "varType: type of the data in the <var> array", "enumType: type of the data in the <enumeration> array", "description: further description of this <other> element", "object: the object into which the enumeration is to be stored\n\t(legal values see ENUM_PROBLEM_COMPONENT in OSGeneral.h)", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "enumerationNumber: number of the <enumeration> in the list of enumerations (zero-based)", "numberOfEl: number of objects sharing the value of this enumeration", "value: value of the enumeration (as a string)", "description: further information about the enumeration and its value", "idxArray: the array of indices for the objects sharing this enumeration", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "varNumber: number of the <var> in the array (zero-based)", "idx: index of the variable to which this value belongs", "value: value of the option (as a string)", "lbValue: value associated with the lower bound of the variable (as a string)", "ubValue: value associated with the upper bound of the variable (as a string)", "iOther: position of this element in the array of <other>", "numberOfObj: number of <obj> children contained in this <other> element", "numberOfEnumerations: number of <enumeration> children", "name: name of this <other> element", "value: a value associated with this <other> element", "solver: the solver associated with this <other> element", "category: the category of this <other> element", "type: type of this <other> element", "objType: type of the data in the <var> array", "enumType: type of the data in the <enumeration> array", "description: further description of this <other> element", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "objNumber: number of the <obj> in the array (zero-based)", "idx: index of the objective to which this value belongs", "name: name of the objective", "value: value of the option (as a string)", "lbValue: value associated with the lower bound of the objective (as a string)", "ubValue: value associated with the upper bound of the objective (as a string)", "iOther: position of this element in the array of <other>", "numberOfCon: number of <con> children contained in this <other> element", "numberOfEnumerations: number of <enumeration> children", "name: name of this <other> element", "value: a value associated with this <other> element", "solver: the solver associated with this <other> element", "category: the category of this <other> element", "type: type of this <other> element", "conType: type of the data in the <var> array", "enumType: type of the data in the <enumeration> array", "description: further description of this <other> element", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "conNumber: number of the <obj> in the array (zero-based)", "idx: index of the constraint to which this value belongs", "name: name of the constraint", "value: value of the option (as a string)", "lbValue: value associated with the lower bound of the constraint (as a string)", "ubValue: value associated with the upper bound of the constraint (as a string)", "iOption: position of this element in the array of options", "numberOfMatricess: number of <matrix> children contained in this <other> element", "numberOfItems: number of <item> children contained in this <other> element", "name: name of this solver option", "value: a value associated with this option", "solver: the solver to which this option applies", "category: the category of this option (solver specific)", "type: type of this option (e.g., numeric or string)", "description: further description of this option", "matrix: an array of matrices associated with this option (could be NULL)", "itemList: the list of items associated with this option (could be NULL)"], "return": "whether the copy was created successfully", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSGeneral.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSGeneral.h", "filename": "OSGeneral.h", "file": "OSGeneral.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\nThis file contains the following classes that correspond to ComplexTypes in the OSgL schema:\n     GeneralFileHeader\n     SparseVector\n     SparseIntVector\n     SparseMatrix\n     SparseJacobianMatrix\n     SparseHessianMatrix\n     QuadraticTerms\n     IntVector\n     DoubleVector\n     IndexValuePair\n     BasisStatus\n     StorageCapacity\n     CPUSpeed\n     CPUNumber\n     TimeSpan\n     OtherOptionOrResultElementString\n     OtherObjOptionOrResultElementString\n     OtherOptionOrResultEnumeration : public IntVector\n     OtherVariableOptionOrResult\n     OtherObjectiveOptionOrResult\n     OtherConstraintOptionOrResult\n     OtherSOSOptionOrResult\n     OtherSpecialOrderedSetsOptionOrResult\n     OtherMatrixVariableOptionOrResult\n     OtherMatrixObjectiveOptionOrResult\n     OtherMatrixConstraintOptionOrResult\n     OtherOptionOrResult\n     SolverOptionOrResult", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "item: the type of information sought (name, source, description, fileCreator, licence)", "name: the name of this file or instance", "source: the source (e.g., in BiBTeX format)", "description: further description about this file and/or its contents", "fileCreator: the creator of this file", "licence: licence information if applicable", "number holds the size of the vector.", "number holds the size of the vector.", "isColumnMajor holds whether the coefMatrix (AMatrix) holding linear program\ndata is stored by column. If false, the matrix is stored by row.", "startSize holds the size of the start array.", "valueSize holds the size of the value and index arrays.", "startSize holds the size of the start array.", "valueSize holds the size of the value and index arrays.", "startSize holds the size of the arrays.", "valueSize holds the size of the value and index arrays.", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest value (inclusive) that an entry in this vector can take", "iMax: greatest value (inclusive) that an entry in this vector can take", "that: the instance from which information is to be copied", "ni contains the dimension of the IntVector", "i contains the array of values", "i contains the value to be appended", "j is the index of the entry that is to be retrieved", "i is the location where the user wants to store the array", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that an entry in this basis can take", "iMax: greatest index value (inclusive) that an entry in this basis can take", "that: the instance from which information is to be copied", "status is a string representing the allowed statuses\n       (as defined in enumeration ENUM_BASIS_STATUS - see below)", "i contains the array of indices", "ni contains the number of elements in i", "status is a string representing the allowed statuses\n       (as defined in enumeration ENUM_BASIS_STATUS - see below)", "idx contains the value of the index", "status is a string representing the allowed statuses\n (at present \"basic\", \"atLower\", \"atUpper\", \"isFree\", \"superbasic\", \"unknown\")", "status is an integer representing the allowed statuses\n (as governed by enumeration ENUM_BASIS_STATUS --- see below)", "j is the (zero-based) position of the entry within the array", "status is a string representing the allowed statuses\n (as governed by enumeration ENUM_BASIS_STATUS --- see below)", "i is the location where the user wants to store the array", "resultArray is the location where the user wants to store the array", "dim is the size of the resultArray", "flipIdx indicates whether the index values need to be flipped\n  (used for representations of objective rows)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest value (inclusive) that an entry in this vector can take", "iMax: greatest value (inclusive) that an entry in this vector can take", "that: the instance from which information is to be copied", "value represents the value of this enumeration member", "description holds additional information about this value", "i contains the array of indices", "ni contains the number of elements in i", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "idx: the index of the objective", "value: the value associated with this objective", "lbValue: a lower bound associated with this objective", "ubValue: an upper bound associated with this objective", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfSOS: number of <sos> elements to be set", "obj: the array of <sos> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfSOS: number of <sos> elements to be set", "obj: the array of <sos> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <matrixObj> elements to be set", "matrixObj: the array of <matrixObj> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <matrixCon> elements to be set", "matrixCon: the array of <matrixCon> elements that are to be set", "idx: the index of the matrix constraint", "value: the value associated with this matrix constraint", "lbValue: a lower bound associated with this matrix constraint", "ubValue: an upper bound associated with this matrix constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied"], "return": "whether the copy was created successfully", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSInstance.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSInstance.h", "filename": "OSInstance.h", "file": "OSInstance.h\n\\brief This file defines the OSInstance class along with its supporting classes.", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2012, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\n<p> 1. Elements become objects of class type (the ComplexType is the class) </p>\n\n<p> 2. The attributes, children of the element, and text correspond to members of the class.  </p>\n      (Note text does not have a name and becomes .value)\n\n<p> 3. Model groups such as choice and sequence and all correspond to arrays </p>\n\n<p><b>Exceptions:</b> </p>\n<ol>\n<li> anything specific to XML such as base64, multi, incr does not go into classes </li>\n<li> The root OSnLNode of each <nl> element is called ExpressionTree </li>\n<li> Root is not called osil; it is called osinstance </li>\n</ol>", "return": "the type of cone as a string", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "rowIdx is the index of the row we want to express in infix.", "n is the index number associated with the matrix.\n\n@remark only the most general element type is returned.\n(e.g., if matrix contains both constants and general expressions,\nmatrix type is ENUM_MATRIX_TYPE_general\n@remark for possible types see OSParameters.h", "n is the index number associated with the matrix.\n\n@remark for possible symmetry types see OSParameters.h", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "rowMajor determines whether the elements should be\n       in row major form or colum major form. The default is column major form.", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                  The default value does not change the symmetry", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "columnIdx is the column index of the block's location", "rowIdx is the row index of the block's location", "rowIdx is the index of the row we want to express in infix.", "name holds the instance name.", "source holds the instance source.", "description holds the instance description.", "fileCreator holds the instance creator.", "licence holds the instance licence.", "number holds the number of variables.", "index holds the variable index. It is required.", "name holds the variable name; use null or empty std::string (\"\") if no variable name.", "lowerBound holds the variable lower bound; use -OSDBL_MAX if no lower bound.", "upperBound holds the variable upper bound; use  OSDBL_MAX if no upper bound.", "type holds the variable type character: C for Continuous, B for Binary, I for Integer,\n       S for String, D for semi-continuous, J for semi-integer (i.e., either 0 or integer >=n).", "number holds the number of variables. It is required.", "names holds a std::string array of variable names; use null if no variable names.", "lowerBounds holds a double array of variable lower bounds; use null if all lower bounds are 0;\n use -OSDBL_MAX if no lower bound for a specific variable in the array.", "upperBounds holds a double array of variable upper bounds; use null if no upper bounds;\n use OSDBL_MAX if no upper bound for a specific variable in the array.", "types holds a char array of variable types; use null if all variables are continuous;\n       for a specfic variable in the array use C for Continuous, B for Binary, I for Integer,\n       S for String, D for semi-continuous, J for semi-integer (i.e., either 0 or integer >=n).", "inits holds a double array of variable initial values; use null if no initial values. -- deprecated", "initsString holds a std::string array of varible initial values; use null\n       if no initial std::string values.  -- deprecated", "number holds the number of objectives.", "index holds the objective index. Remember the first objective index is -1, second -2, ...", "name holds the objective name; use null or empty std::string (\"\") if no objective name.", "maxOrMin holds the objective sense or direction; it can only take two values: \"max\" or \"min\".", "constant holds the objective constant; use 0.0 if no objective constant.", "weight holds the objective weight; use 1.0 if no objective weight.", "objectiveCoefficients holds the objective coefficients (null if no objective coefficients)\n       in a sparse representation that holds two arrays: index array and a value array.", "number holds the number of objectives. It is required.", "names holds a std::string array of objective names; use null if no objective names.", "maxOrMins holds a std::string array of objective objective senses or directions: \"max\" or \"min\"; use null if all objectives are \"min\".", "constants holds a double array of objective constants; use null if all objective constants are 0.0.", "weights holds a double array of objective weights; use null if all objective weights are 1.0.", "objectiveCoefficients holds an array of objective coefficients, (null if no objective has any coefficients)\nFor each objective, the coefficients are stored in a sparse representation that holds two arrays: index array and a value array.\nIf for a specific objective, there are no objective coefficients, use null for the corresponding array member.", "number holds the number of constraints.", "index holds the constraint index. It is required.", "name holds the constraint name; use null or empty std::string (\"\") if no constraint name.", "lowerBound holds the constraint lower bound; use -OSDBL_MAX if no lower bound.", "upperBound holds the constraint upper bound; use  OSDBL_MAX if no upper bound.", "number holds the number of constraints. It is required.", "names holds a std::string array of constraint names; use null if no constraint names.", "lowerBounds holds a double array of constraint lower bounds; use null if no lower bounds;\n       use -OSDBL_MAX if no lower bound for a specific constraint in the array.", "upperBounds holds a double array of constraint upper bounds; use null if no upper bounds;\n       use  OSDBL_MAX if no upper bound for a specific constraint in the array.", "numberOfValues holds the number of specified coefficient values (usually nonzero) in the coefficient matrix.", "isColumnMajor holds whether the coefficient matrix is stored in column major (true) or row major (false).", "values holds a double array coefficient values in the matrix.", "valuesBegin holds the begin index of the values array to copy from (usually 0).", "valuesEnd holds the end index of the values array to copy till (usually values.length - 1).", "indexes holds an integer array column/row indexes for each value in the values array.", "indexesBegin holds the begin index of the indexes array to copy from (usually 0).", "indexesEnd holds the end index of the indexes array to copy till (usually indexes.length - 1).", "starts holds an integer array start indexes in the matrix; the first value of starts should always be 0.", "startsBegin holds the begin index of the starts array to copy from (usually 0).", "startsEnd holds the end index of the starts array to copy till (usually starts.length - 1).", "numberOfValues holds the number of specified coefficient values (usually nonzero) in the coefficient matrix.", "isColumnMajor holds whether the coefficient matrix is stored in column major (true) or row major (false).", "values holds a double array coefficient values in the matrix.", "valuesBegin holds the begin index of the values array to copy from (usually 0).", "valuesEnd holds the end index of the values array to copy till (usually values.length - 1).", "indexes holds an integer array column/row indexes for each value in the values array.", "indexesBegin holds the begin index of the indexes array to copy from (usually 0).", "indexesEnd holds the end index of the indexes array to copy till (usually indexes.length - 1).", "starts holds an integer array start indexes in the matrix; the first value of starts should always be 0.", "startsBegin holds the begin index of the starts array to copy from (usually 0).", "startsEnd holds the end index of the starts array to copy till (usually starts.length - 1).", "nq holds the number of quadratic terms.", "number holds the number of quadratic terms.", "rowIndexes holds an integer array of row indexes of all the quadratic terms.\n       A negative integer corresponds to an objective row, e.g. -1 for 1st objective and -2 for 2nd.", "varOneIndexes holds an integer array of the first  variable indexes of all the quadratic terms.", "varTwoIndexes holds an integer array of the second variable indexes of all the quadratic terms.", "coefficients holds an array of double containing all the quadratic term coefficients.", "begin holds the begin index of all the arrays to copy from (usually = 0).", "end holds the end index of all the arrays to copy till (usually = array length -1).", "number holds the number of quadratic terms.", "rowIndexes holds an integer array of row indexes of all the quadratic terms.\n       A negative integer corresponds to an objective row, e.g. -1 for 1st objective and -2 for 2nd.", "varOneIndexes holds an integer array of the first  variable indexes of all the quadratic terms.", "varTwoIndexes holds an integer array of the second variable indexes of all the quadratic terms.", "coefficients holds a double array all the quadratic term coefficients.", "nexpr holds the number of nonlinear expressions.", "root holds a pointer array to the root nodes of all the nonlinear expressions.", "number holds the number of matrices", "index holds the matrix index. It is required.", "name holds the matrix name; use null or empty std::string (\"\") if no matrix name.", "numberOfRows holds the number of rows. It is required. Use 1 for column vectors.", "numberOfColumns holds the number of columns. It is required. Use 1 for row vectors.", "symmetry holds the type of symmetry used in the definition of the matrix.\n       For more information  see the enumeration ENUM_MATRIX_SYMMETRY in OSGeneral.h.\n       If no symmetry, use ENUM_MATRIX_SYMMETRY_none.", "matrixType tracks the type of elements contained in this matrix.\n       For more information  see the enumeration ENUM_MATRIX_TYPE in OSGeneral.h.\n       If unsure, use ENUM_MATRIX_TYPE_unknown.", "inumberOfChildren is the number of MatrixNode child elements,\n       i.e., the number of matrix constructors in the m_mChildren array.", "m_mChildren is the array of matrix constructors used in the definition of this matrix.", "number holds the number of cones", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be one of\n           ENUM_CONE_TYPE_nonnegative,\n           ENUM_CONE_TYPE_nonpositive,\n           ENUM_CONE_TYPE_copositiveMatrices,\n           ENUM_CONE_TYPE_completelyPositiveMatrices.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be one of\n           ENUM_CONE_TYPE_product,\n           ENUM_CONE_TYPE_intersection.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "numberOfComponents holds the number of components of this cone.", "components holds the indexes of the components of this cone.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be one of\n           ENUM_CONE_TYPE_dual,\n           ENUM_CONE_TYPE_polar,\n           ENUM_CONE_TYPE_polyhedral.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "referenceIdx holds the index of a cone or matrix used in the definition of this cone.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_semidefinite.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "semidefiniteness distinguishes positive and negative semidefinite cones.\n       It must be either \"positive\" or \"negative\".", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_quadratic.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "distortionMatrixIdx holds the index of a distortion matrix. Use -1 if there is none.", "normFactor holds a scale factor for the norm. Use 1 if there is none.", "axisDirection holds the index of the axis direction. The most usual value is 0.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_rotatedQuadratic.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "distortionMatrixIdx holds the index of a distortion matrix. Use -1 if there is none.", "normFactor holds a scale factor for the norm. Use 1 if there is none.", "firstAxisDirection holds the index of the  first axis direction. The most usual value is 0.", "secondAxisDirection holds the index of the second axis direction. The most usual value is 1.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_normed.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "distortionMatrixIdx holds the index of a distortion matrix. Use -1 if there is none.", "normFactor holds a scale factor for the norm. Use 1 if there is none.", "pNorm holds the norm descriptor. It must be greater than or equal to 1.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_nonnegativePolynomials.\n           ENUM_CONE_TYPE_sumOfSquaresPolynomials.\n           ENUM_CONE_TYPE_moment.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "maxDegree holds the maximum degree of the polynomials. Use 1, 2, 3, ..., INF.", "numberOfUB holds the number of (box-type) upper bound constraints. Use 0 if there are none.", "ub holds the upper bound values. Use null if there are no upper bounds.", "numberOfLB holds the number of (box-type) lower bound constraints. Use 0 if there are none.", "lb holds the lower bound values. Use null if there are no lower bounds.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "rowIdx is the index of the row we want to express in infix.", "idx is the index on the constraint (0, 1, 2, 3, ...) or objective function (-1, -2, -3, ...).", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called for the current x\nhas been evaluated for the current iterate x\nuse a value of false if not sure", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called for the current x\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called\nfor the current iterate", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called\nfor the current iterate", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins\n@parma idx is the index of the constraint function gradient", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values\n@parma idx is the index of the constraint function gradient", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins\n@parma objIdx is the index of the objective function being optimized", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objIdx is the index of the objective function being optimized", "new_x is false if any evaluation method was previously called\nfor the current iterate", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called\nfor the current iterate\n@parma idx is the index of the either a constraint or objective\nfunction Hessian", "vdX is a vector of doubles holding the current primal variable values\nthe size of x should equal instanceData->variables->numberOfVariables", "p is the highest order Taylor coefficient", "vdX is a vector of doubles of the current primal variable values\nthe size of vdX m_iNumberOfNonlinearVariables", "p is the order of the sweep", "vdlambda is a vector of doubles of the current dual (lagrange) variable values\nthe size of lambda should equal number of objective functions plus number of constraints", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints", "new_x is false if any evaluation method was previously called", "highestOrder is the highest order derivative to be calculated", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints"], "see": ["org.optimizationservices.oscommon.datastructure.SparseVector", "org.optimizationservices.oscommon.datastructure.SparseMatrix", "org.optimizationservices.oscommon.datastructure.SparseMatrix", "org.optimizationservices.oscommon.datastructure.QuadraticTerms"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSgLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSgLWriter.h", "filename": "OSgLWriter.h", "file": "OSgLWriter.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n@version 1.0, 22/Oct/2010\n@since   OS2.2\n\n\\remarks\nCopyright (C) 2005-2010, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSiLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSiLWriter.h", "filename": "OSiLWriter.h", "file": "OSiLWriter.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["theosinstance is a pointer to an OSInstance object"], "return": "a string with the OSInstance data that validates against the OSiL schema.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSiLReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSiLReader.h", "filename": "OSiLReader.h", "file": "OSiLReader.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osil a string that holds the problem instance."], "return": "the instance as an OSInstance object.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSCommandLineReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSCommandLineReader.h", "filename": "OSCommandLineReader.h", "file": "OSCommandLineReader.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2011-2013, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osss is the command line to be parsed", "osss a command line string.", "osss a command line string."], "return": "the OSCommandLine object corresponding to the command line string.\n@remark Calls method parseString once and if a configFile item is found\n        calls method parseString two more times (with the config file contents\n        and again with the original command line)", "has_pass2": false}, "OS/src/OSParsers/OSrLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSrLParserData.h", "filename": "OSrLParserData.h", "file": "OSrLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSiLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSiLParserData.h", "filename": "OSiLParserData.h", "file": "OSiLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSoLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSoLParserData.h", "filename": "OSoLParserData.h", "file": "OSoLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSgLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSgLParserData.h", "filename": "OSgLParserData.h", "file": "OSgLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSnLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSnLParserData.h", "filename": "OSnLParserData.h", "file": "OSnLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSOptionsStruc.h": {"path": "layer-4/OS/OS/src/OSParsers/OSOptionsStruc.h", "filename": "OSOptionsStruc.h", "file": "OSOptionsStruc.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2012, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}}}, "SHOT": {"name": "SHOT", "file_count": 108, "pass2_count": 15, "files": {"ThirdParty/mc++/include/tmodel.hpp": {"path": "layer-4/SHOT/ThirdParty/mc++/include/tmodel.hpp", "filename": "tmodel.hpp", "file": "tmodel.hpp", "brief": "Taylor Model Arithmetic for Rigorous Bound Propagation", "algorithm": "Convergence Properties:", "math": "|R| = O(diam(D)^{q+1}) as domain shrinks\n  Higher order q → faster convergence but more coefficients\n  Used in global optimization branch-and-bound for bound tightening.", "complexity": "Bernstein conversion: O(n^q * 2^n) but provides hull property", "ref": ["Makino & Berz (2003) - Taylor models and validated numerics", "Neumaier (2002) - Taylor forms for global optimization", "Lin & Rokne (1995) - Bernstein form for polynomial bounds", "Sahlodin & Chachuat (2011) - McCormick-Taylor models", "Bompadre et al. (2012) - Convergence of Taylor models"], "has_pass2": true}, "ThirdParty/boost/boost/core/ref.hpp": {"path": "layer-4/SHOT/ThirdParty/boost/boost/core/ref.hpp", "filename": "ref.hpp", "brief": "Contains a reference to an object of type `T`.\n\n `reference_wrapper` is primarily used to \"feed\" references to\n function templates (algorithms) that take their parameter by\n value. It provides an implicit conversion to `T&`, which\n usually allows the function templates to work on references\n unmodified.", "return": "The stored reference.\n     @remark Does not throw.", "has_pass2": false}, "src/EventHandler.h": {"path": "layer-4/SHOT/src/EventHandler.h", "filename": "EventHandler.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "EventHandler.h", "brief": "Observer pattern for algorithm events and callbacks\n\nAllows external code to respond to solver events.\n\n**Event Types (E_EventType):**\n- NewPrimalSolution: New feasible solution found\n- UserTerminationCheck: Allow user to request termination\n\n**Callback Registration:**\n- registerCallback(event, callback): Register handler\n- Callback signature: void()\n- Multiple callbacks per event supported\n\n**Event Notification:**\n- notify(event): Invoke all registered callbacks\n- Called by solver at appropriate points\n\n**Usage Example:**\n```cpp\neventHandler->registerCallback(E_EventType::NewPrimalSolution,\n    [&]() { std::cout << \"New solution found!\" << std::endl; });\n```", "see": ["Solver.h for event handler access", "Enums.h for E_EventType definition"], "has_pass2": false}, "src/Results.h": {"path": "layer-4/SHOT/src/Results.h", "filename": "Results.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Results.h", "brief": "Solution results storage and optimality gap tracking\n\nCentral repository for primal/dual solutions and algorithm progress.\n\n**Primal Solutions:**\n- primalSolutions: All found feasible points\n- addPrimalSolution(): Submit new incumbent\n- getPrimalBound(): Best objective value\n- primalSolutionSourceStatistics: Track solution origins\n\n**Dual Solutions:**\n- dualSolutions: Lower/upper bounds from MIP\n- getCurrentDualBound(), getGlobalDualBound()\n- setDualBound(): Update relaxation bound\n\n**Optimality Gap:**\n- getAbsoluteGlobalObjectiveGap(): |primal - dual|\n- getRelativeGlobalObjectiveGap(): |gap| / |primal|\n- isRelativeObjectiveGapToleranceMet(): Termination check\n\n**Iteration Tracking:**\n- iterations: Per-iteration state snapshots\n- createIteration(), getCurrentIteration()\n\n**Output Formats:**\n- getResultsOSrL(): XML Optimization Services format\n- getResultsTrace(): GAMS trace format\n- getResultsSol(): AMPL .sol format", "see": ["Solver.h for getPrimalSolution(), getResultsOSrL()"], "has_pass2": false}, "src/Settings.h": {"path": "layer-4/SHOT/src/Settings.h", "filename": "Settings.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Settings.h", "brief": "Solver configuration and option management\n\nHierarchical settings system for SHOT algorithm parameters.\n\n**Settings Categories:**\n- Termination: TimeLimit, IterationLimit, ObjectiveGap\n- Subsolver: MIP backend, NLP backend\n- Algorithm: Cut generation, reformulation options\n- Output: Verbosity, logging levels\n\n**Exception Classes:**\n- SettingKeyNotFoundException: Unknown setting key\n- SettingSetWrongTypeException: Type mismatch on set\n- SettingGetWrongTypeException: Type mismatch on get\n\n**Settings Types:**\n- E_SettingType: Integer, Double, String, Boolean, Enum\n\n**File Formats:**\n- OSoL XML format (Options Service Language)\n- Simple key=value format", "see": ["Solver.h for setOptionsFromFile/String methods"], "has_pass2": false}, "src/Environment.h": {"path": "layer-4/SHOT/src/Environment.h", "filename": "Environment.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Environment.h", "brief": "Shared state container for SHOT solver components\n\nCentral hub connecting all solver subsystems.\n\n**Environment Class Members:**\n- problem: Original problem formulation\n- reformulatedProblem: Convexified/linearized version\n- modelingSystem: GAMS/AMPL/OSiL interface\n\n**Solver Components:**\n- dualSolver: MIP solver for dual problem (CPLEX/Gurobi/CBC)\n- primalSolver: NLP solver for primal bounds (Ipopt)\n- rootsearchMethod: Line search for hyperplanes\n\n**Infrastructure:**\n- settings: Solver configuration parameters\n- results: Solution and statistics\n- output: Logging via spdlog\n- timing: Performance profiling\n- events: Callback event handler\n- tasks: Asynchronous task manager", "see": ["Solver.h for the main solver interface"], "has_pass2": false}, "src/Structs.h": {"path": "layer-4/SHOT/src/Structs.h", "filename": "Structs.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Structs.h", "brief": "Core data structures and type definitions for SHOT\n\nFundamental types, forward declarations, and helper structures.\n\n**Constants:**\n- SHOT_DBL_MIN/MAX/INF/EPS: Double precision limits\n- SHOT_INT_MAX, SHOT_LONG_MAX: Integer limits\n\n**Smart Pointer Typedefs:**\n- ProblemPtr, SettingsPtr, ResultsPtr, etc.\n- Shared ownership via std::shared_ptr\n\n**Solution Structures:**\n- PrimalSolution: Variable values + objective + source\n- DualSolution: Dual bound + iteration + source\n- SolutionPoint: Point with constraint deviation\n\n**Hyperplane Structures:**\n- Hyperplane: Supporting hyperplane cut\n- GeneratedHyperplane: Hyperplane with metadata\n- IntegerCut: No-good cut for integer variables\n\n**Utility Types:**\n- VectorDouble, VectorInteger, VectorString\n- PairIndexValue: (index, value) pair", "see": ["Environment.h for component composition"], "has_pass2": false}, "src/Solver.h": {"path": "layer-4/SHOT/src/Solver.h", "filename": "Solver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Solver.h", "brief": "Main solver interface for convex MINLP problems\n\nPrimary entry point for the SHOT optimizer.\n\n**Solver Class:**\n- setProblem(): Load GAMS/AMPL/OSiL problem file\n- solveProblem(): Execute the selected solution strategy\n- getPrimalSolution(): Retrieve best incumbent solution\n\n**Solution Workflow:**\n1. Initialize settings (setOptionsFromFile/String)\n2. Load problem (setProblem)\n3. Solve (solveProblem)\n4. Retrieve results (getResultsOSrL, getPrimalSolution)\n\n**Callbacks:**\n- registerCallback(): Monitor NewPrimalSolution, UserTerminationCheck\n\n**Results Access:**\n- getCurrentDualBound(): Best lower bound\n- getPrimalBound(): Best upper bound (incumbent)\n- getAbsoluteObjectiveGap(): |primal - dual|\n- getRelativeObjectiveGap(): |primal - dual| / |primal|", "see": ["Environment.h for shared state container", "SolutionStrategy/ISolutionStrategy.h for algorithm implementations"], "has_pass2": false}, "src/Timer.h": {"path": "layer-4/SHOT/src/Timer.h", "filename": "Timer.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Timer.h", "brief": "High-resolution stopwatch for performance measurement\n\nLightweight timer using std::chrono::high_resolution_clock.\n\n**Timer Class:**\n- start()/stop(): Pause/resume timing\n- restart(): Reset to zero and start\n- elapsed(): Get cumulative time in seconds\n\n**Properties:**\n- name: Timer identifier (for Timing registry)\n- description: Human-readable label\n\n**Usage:**\nTimer t(\"SolveMIP\", \"MIP solver time\");\nt.start();\n// ... computation ...\nt.stop();\ndouble seconds = t.elapsed();", "see": ["Timing.h for timer registry"], "has_pass2": false}, "src/PrimalSolver.h": {"path": "layer-4/SHOT/src/PrimalSolver.h", "filename": "PrimalSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "PrimalSolver.h", "brief": "NLP-based primal bound computation and solution repair\n\nFinds feasible solutions and improves the primal bound.\n\n**PrimalSolver Class:**\n- primalSolutionCandidates: Points to check for feasibility\n- fixedPrimalNLPCandidates: Integer-fixed NLP subproblems\n\n**Primal Solution Sources:**\n- MIP solution pool points\n- NLP local search from MIP solutions\n- Rounding heuristics\n\n**Solution Validation:**\n- addPrimalSolutionCandidate(): Submit candidate point\n- checkPrimalSolutionCandidates(): Verify feasibility\n- checkPrimalSolutionPoint(): Full constraint check\n\n**Fixed-Integer NLP:**\n- addFixedNLPCandidate(): Queue NLP subproblem\n- hasFixedNLPCandidateBeenTested(): Avoid re-solving", "see": ["DualSolver.h for dual bound computation", "NLPSolver/ for backend NLP solvers (Ipopt)"], "has_pass2": false}, "src/Timing.h": {"path": "layer-4/SHOT/src/Timing.h", "filename": "Timing.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Timing.h", "brief": "Named timer registry for profiling solver phases\n\nCollection of named timers for performance analysis.\n\n**Timing Class:**\n- createTimer(): Register new timer with name/description\n- startTimer()/stopTimer(): Control by name\n- getElapsedTime(): Query accumulated time\n\n**Typical Timers:**\n- \"Total\": Overall solve time\n- \"DualMIP\": MIP solver calls\n- \"PrimalNLP\": NLP subproblem solves\n- \"Reformulation\": Problem transformation\n\n**Thread Safety:**\n- Timers are not thread-safe\n- Use in single-threaded solver context", "see": ["Timer.h for individual timer class", "Environment.h for accessing via env->timing"], "has_pass2": false}, "src/TaskHandler.h": {"path": "layer-4/SHOT/src/TaskHandler.h", "filename": "TaskHandler.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "TaskHandler.h", "brief": "Task scheduling and execution control\n\nOrchestrates the modular task-based algorithm workflow.\n\n**Task Management:**\n- addTask(): Register task with string identifier\n- getTask(): Retrieve task by ID\n- clearTasks(): Reset for new solve\n\n**Execution Control:**\n- getNextTask(): Dequeue next task to run\n- setNextTask(): Jump to specific task (for goto/branching)\n- terminate(): Signal algorithm termination\n- isTerminated(): Check termination flag\n\n**Workflow Pattern:**\n1. SolutionStrategy adds tasks in order\n2. Solver calls getNextTask() in loop\n3. Tasks execute and may setNextTask() for control flow\n4. Termination tasks set terminate()\n\n**Task ID Naming:**\n- \"InitializeIteration\", \"SolveIteration\"\n- \"CheckTimeLimit\", \"CheckAbsoluteGap\"\n- \"SelectHyperplanes\", \"AddHyperplanes\"", "see": ["TaskBase.h for task interface", "SolutionStrategy/ for task configuration"], "has_pass2": false}, "src/Report.h": {"path": "layer-4/SHOT/src/Report.h", "filename": "Report.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Report.h", "brief": "Formatted console output for solver progress\n\nGenerates iteration tables and summary reports.\n\n**Report Class:**\n- outputSolverHeader(): Version and license info\n- outputOptionsReport(): Active settings summary\n- outputProblemInstanceReport(): Problem statistics\n\n**Iteration Reporting:**\n- outputIterationDetailHeader(): Column headers\n- outputIterationDetail(): Per-iteration progress line\n- Shows: iteration, time, cuts, bounds, gap\n\n**Solution Reporting:**\n- outputSolutionReport(): Final summary\n- outputPrimalSolutionDetailedReport(): Full solution\n\n**Output Control:**\n- Tracks last values to avoid redundant output\n- Respects ES_IterationOutputDetail setting", "see": ["Output.h for underlying logging"], "has_pass2": false}, "src/Utilities.h": {"path": "layer-4/SHOT/src/Utilities.h", "filename": "Utilities.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Utilities.h", "brief": "Helper functions for vectors, hashing, and I/O\n\nStandalone utility functions in SHOT::Utilities namespace.\n\n**Vector Operations:**\n- L2Norm(): Euclidean distance between points\n- calculateCenterPoint(): Centroid of point set\n- displayVector(): Debug output for vectors\n\n**Hashing:**\n- calculateHash(): Hash function for duplicate detection\n- Used by hyperplane/integer cut deduplication\n\n**File I/O:**\n- writeStringToFile(), getFileAsString()\n- saveVariablePointVectorToFile(): Solution output\n\n**Numeric:**\n- isAlmostEqual(), isAlmostZero(): Tolerance comparisons\n- isInteger(): Check for integer values\n\n**Sparse Containers:**\n- SparseVariableVector: Variable→coefficient map\n- SparseVariableMatrix: (Variable,Variable)→coefficient\n- combineSparseVariable*(): Merge operations", "see": ["Structs.h for type definitions"], "has_pass2": false}, "src/DualSolver.h": {"path": "layer-4/SHOT/src/DualSolver.h", "filename": "DualSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "DualSolver.h", "brief": "MIP-based dual bound computation via supporting hyperplanes\n\nManages the linearization-based dual problem.\n\n**DualSolver Class:**\n- MIPSolver: Backend MIP solver (CPLEX/Gurobi/CBC/HiGHS)\n- generatedHyperplanes: All cutting planes added\n- hyperplaneWaitingList: Pending cuts to add\n\n**Hyperplane Management:**\n- addHyperplane(): Add supporting hyperplane cut\n- addGeneratedHyperplane(): Record a hyperplane\n- hasHyperplaneBeenAdded(): Avoid duplicate cuts\n\n**Integer Cuts:**\n- addIntegerCut(): Add no-good cuts for MINLP\n- generatedIntegerCuts: All integer cuts\n\n**Interior Points:**\n- interiorPointCandidates: Candidate interior points\n- interiorPts: Verified interior points for ESH", "algorithm": "Extended Supporting Hyperplane (ESH) method adds\n           linearizations at boundary and interior points", "see": ["PrimalSolver.h for primal bound computation", "MIPSolver/ for backend implementations"], "has_pass2": true}, "src/Output.h": {"path": "layer-4/SHOT/src/Output.h", "filename": "Output.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Output.h", "brief": "Logging infrastructure using spdlog\n\nCentralized logging with console and file sinks.\n\n**Output Class:**\n- outputCritical/Error/Warning/Info/Debug/Trace(): Log methods\n- setLogLevels(): Configure console and file verbosity\n- setConsoleSink(): Custom console output\n- setFileSink(): Enable file logging\n\n**E_LogLevel (Enums.h):**\n- Off, Critical, Error, Warning, Info, Debug, Trace\n\n**OutputStream Class:**\n- std::ostream wrapper for solver output redirection\n- Routes external solver output through SHOT logging\n- Used by CPLEX, Gurobi, Ipopt adapters\n\n**Implementation:**\n- Uses spdlog library for fast, thread-safe logging\n- consoleSink: stdout sink\n- fileSink: basic_file_sink for persistence", "see": ["Environment.h for Output access"], "has_pass2": false}, "src/Iteration.h": {"path": "layer-4/SHOT/src/Iteration.h", "filename": "Iteration.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Iteration.h", "brief": "Per-iteration state snapshot for algorithm progress tracking\n\nCaptures the state of the solver at each outer approximation iteration.\n\n**Problem State:**\n- dualProblemClass: LP, QP, MIP, MIQP, etc.\n- isDualProblemDiscrete: Whether MIP or LP relaxation\n- solutionStatus: Optimal, Feasible, Infeasible, etc.\n\n**Solution Data:**\n- solutionPoints: All solutions from MIP solution pool\n- objectiveValue: Best objective value\n- currentObjectiveBounds: (dual, primal) bounds\n\n**Constraint Violation:**\n- constraintDeviations: Per-constraint violations\n- maxDeviation, maxDeviationConstraint: Worst violation\n- usedConstraintTolerance: Tolerance for this iteration\n\n**Hyperplane Statistics:**\n- numHyperplanesAdded: Cuts added this iteration\n- totNumHyperplanes: Cumulative cuts\n- hyperplanePoints: Generation points\n\n**Node Statistics:**\n- numberOfExploredNodes, numberOfOpenNodes: B&B tree\n\n**Helper Methods:**\n- getSolutionPointWithSmallestDeviation()\n- isMIP(): Check if discrete problem", "see": ["Results.h for iterations collection"], "has_pass2": false}, "src/Enums.h": {"path": "layer-4/SHOT/src/Enums.h", "filename": "Enums.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Enums.h", "brief": "Enumeration types for SHOT solver states and options\n\nComprehensive enums for algorithm configuration and status tracking.\n\n**Problem Classification:**\n- E_Convexity: Linear, Convex, Concave, Nonconvex, Unknown\n- E_DualProblemClass: LP, QP, QCQP, MIP, MIQP, MIQCQP\n\n**Auxiliary Variable Types:**\n- E_AuxiliaryVariableType: NonlinearObjectiveFunction (epigraph),\n  BilinearTerms, MonomialPartitioning, etc.\n\n**Hyperplane Sources (E_HyperplaneSource):**\n- MIPOptimalRootsearch, MIPSolutionPoolRootsearch\n- LPRelaxedRootsearch, InteriorPointSearch\n- PrimalSolutionSearch, ObjectiveCuttingPlane\n\n**Event Types (E_EventType):**\n- NewPrimalSolution: New incumbent found\n- UserTerminationCheck: Allow user termination\n\n**Termination Reasons (E_TerminationReason):**\n- Optimal, ObjectiveGapTolerance, TimeLimit\n- IterationLimit, InfeasibleProblem, NoDualCutsAdded", "see": ["Solver.h for using these enums"], "has_pass2": false}, "src/ModelingSystem/ModelingSystemOSiL.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemOSiL.h", "filename": "ModelingSystemOSiL.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemOSiL.h", "brief": "OSiL XML format reader for optimization problems\n\nReads Optimization Services instance Language (OSiL) files.\n\n**ModelingSystemOSiL Class:**\n- createProblem(): Parse OSiL XML into Problem\n- augmentSettings(): Add OSiL-specific options\n- finalizeSolution(): Write OSrL result format\n\n**OSiL Format:**\n- XML-based optimization instance representation\n- <instanceData> contains variables, objectives, constraints\n- <nonlinearExpressions> for NLP terms (OSnL format)\n\n**XML Parsing:**\n- Uses tinyxml2 for XML processing\n- convertNonlinearNode(): OSnL to SHOT expression conversion\n\n@note OSiL is part of COIN-OR Optimization Services", "see": ["ModelingSystemOS.h for OS library integration"], "has_pass2": false}, "src/ModelingSystem/ModelingSystemGAMS.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemGAMS.h", "filename": "ModelingSystemGAMS.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemGAMS.h", "brief": "GAMS interface for optimization problem input\n\nReads GAMS models via GMO/GEV API.\n\n**ModelingSystemGAMS Class:**\n- setModelingObject(): Accept gmoHandle_t from GAMS\n- createProblem(): Convert GMO model to SHOT Problem\n- finalizeSolution(): Return solution to GAMS\n\n**GAMS Integration:**\n- gmoHandle_t: GAMS Model Object (GMO)\n- gevHandle_t: GAMS Environment (GEV)\n- palHandle_t: GAMS Audit Licensing (PAL)\n\n**Model Conversion:**\n- copyVariables/Constraints/ObjectiveFunction()\n- parseGamsInstructions(): Bytecode to expression conversion\n\n**GamsOutputSink:**\n- spdlog sink routing to GAMS log\n- gevLogPChar() for normal messages\n- gevLogStatPChar() for warnings/errors", "see": ["GMO API documentation (gams.com)"], "has_pass2": false}, "src/ModelingSystem/IModelingSystem.h": {"path": "layer-4/SHOT/src/ModelingSystem/IModelingSystem.h", "filename": "IModelingSystem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/IModelingSystem.h", "brief": "Abstract interface for problem input formats\n\nBase interface for parsing optimization problem files.\n\n**E_ProblemCreationStatus Enum:**\n- NormalCompletion: Problem loaded successfully\n- FileDoesNotExist, ErrorInFile: I/O errors\n- ErrorInVariables/Constraints/Objective: Parse errors\n- CapabilityProblem: Unsupported problem features\n\n**IModelingSystem Interface:**\n- augmentSettings(): Add format-specific settings\n- updateSettings(): Extract settings from problem file\n- finalizeSolution(): Write solution back to format\n\n**Implementations:**\n- ModelingSystemOSiL: XML OSiL format (Optimization Services)\n- ModelingSystemAMPL: .nl format from AMPL\n- ModelingSystemGAMS: GAMS interface\n- ModelingSystemOS: Optimization Services framework", "see": ["Solver.h for setProblem() to load problems", "Results.h for solution output"], "has_pass2": false}, "src/ModelingSystem/ModelingSystemOS.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemOS.h", "filename": "ModelingSystemOS.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemOS.h", "brief": "Optimization Services library integration\n\nFull OS library integration for OSiL and AMPL files.\n\n**ModelingSystemOS Class:**\n- createProblem(filename, format): Read OSiL or nl files\n- createProblem(OSInstance): Use existing OS instance\n- finalizeSolution(): Generate OSrL output\n\n**File Formats:**\n- E_OSInputFileFormat::OSiL: XML format\n- E_OSInputFileFormat::Ampl: nl binary via OSnl2OS\n\n**OS Library Classes:**\n- OSiLReader: Parse OSiL XML\n- OSInstance: In-memory problem representation\n- OSnl2OS: Convert nl to OSInstance\n- OSnLNode: Nonlinear expression tree nodes\n\n**Expression Conversion:**\n- convertOSNonlinearNode(): OSnLNode to SHOT expression\n- Preserves convexity annotations if available\n\n@note Uses COIN-OR OS library (github.com/coin-or/OS)", "has_pass2": false}, "src/ModelingSystem/ModelingSystemAMPL.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemAMPL.h", "filename": "ModelingSystemAMPL.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemAMPL.h", "brief": "AMPL nl-file format reader for optimization problems\n\nReads AMPL .nl (nonlinear) binary problem files.\n\n**ModelingSystemAMPL Class:**\n- createProblem(): Parse .nl file into Problem\n- augmentSettings(): Add AMPL-specific options\n- finalizeSolution(): Write .sol solution file\n\n**AMPL nl Format:**\n- Binary format for optimization instances\n- Contains variables, bounds, constraints, expressions\n- Used by AMPL-compatible solvers\n\n**Usage:**\n- Standalone: SHOT problem.nl\n- Via AMPL: option solver shot; solve;", "see": ["ASL (AMPL Solver Library) for nl format details"], "has_pass2": false}, "src/NLPSolver/NLPSolverIpoptRelaxed.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverIpoptRelaxed.h", "filename": "NLPSolverIpoptRelaxed.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverIpoptRelaxed.h", "brief": "Ipopt solver for continuous relaxation problems\n\nSolves NLP with integer variables relaxed to continuous.\n\n**NLPSolverIpoptRelaxed Class:**\n- Multiple inheritance: NLPSolverBase + NLPSolverIpoptBase\n- setSolverSpecificInitialSettings(): Configure for relaxed solve\n- getSolution(): Return continuous solution values\n\n**Use Case:**\n- Find interior points for ESH algorithm\n- Check feasibility of relaxed problem\n- Generate starting points for primal heuristics", "see": ["NLPSolverIpoptBase.h for fixed-integer variant"], "has_pass2": false}, "src/NLPSolver/NLPSolverCuttingPlaneMinimax.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverCuttingPlaneMinimax.h", "filename": "NLPSolverCuttingPlaneMinimax.h", "author": "Andreas Lundell, Åbo Akademi University\n\n    @section LICENSE\n    This software is licensed under the Eclipse Public License 2.0.\n    Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverCuttingPlaneMinimax.h", "brief": "Cutting-plane solver for minimax LP problems\n\nBuilt-in LP-based solver for simple minimax problems.\n\n**NLPSolverCuttingPlaneMinimax Class:**\n- Uses MIP solver (CPLEX/Gurobi/Cbc) as LP engine\n- Iteratively adds cutting planes\n- No external NLP solver dependency\n\n**Minimax Problem Form:**\n- min t\n- s.t. f_i(x) <= t for all i\n\n**Use Case:**\n- Finding interior points when Ipopt unavailable\n- Solving auxiliary minimax subproblems", "algorithm": "Iterative cutting plane for convex minimax", "has_pass2": true}, "src/NLPSolver/INLPSolver.h": {"path": "layer-4/SHOT/src/NLPSolver/INLPSolver.h", "filename": "INLPSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/INLPSolver.h", "brief": "Abstract interface for NLP solver backends\n\nPure virtual interface for primal bound NLP solvers.\n\n**Starting Point:**\n- setStartingPoint(): Initialize solver from MIP solution\n- clearStartingPoint(): Reset to default\n\n**Variable Fixing:**\n- fixVariables(): Fix integer variables for NLP subproblem\n- unfixVariables(): Restore full problem\n\n**Solution Methods:**\n- solveProblem(): Execute NLP solver\n- getSolution(): Retrieve primal point\n- getObjectiveValue(): Primal objective value\n\n**Bound Management:**\n- getVariableLowerBounds(), getVariableUpperBounds()\n- updateVariableLowerBound(), updateVariableUpperBound()\n\n**Implementations:**\n- NLPSolverIpoptBase: Ipopt interior point solver\n- NLPSolverGAMS: GAMS NLP solvers\n- NLPSolverSHOT: Recursive SHOT for NLP", "see": ["PrimalSolver.h for fixed-integer NLP subproblems", "NLPSolverBase.h for implementation utilities"], "has_pass2": false}, "src/NLPSolver/NLPSolverBase.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverBase.h", "filename": "NLPSolverBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverBase.h", "brief": "Base implementation for NLP solver interface\n\nProvides shared solve logic for all NLP solvers.\n\n**NLPSolverBase Class:**\n- solveProblem(): Template method calling solveProblemInstance()\n- Derived classes implement solveProblemInstance() hook\n\n**Inheritance:**\n- Virtually inherits from INLPSolver\n- Used with multiple inheritance (NLPSolverIpoptRelaxed)", "see": ["INLPSolver.h for interface contract", "NLPSolverIpoptBase.h for Ipopt implementation"], "has_pass2": false}, "src/NLPSolver/NLPSolverIpoptBase.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverIpoptBase.h", "filename": "NLPSolverIpoptBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverIpoptBase.h", "brief": "Ipopt implementation of INLPSolver interface\n\nProvides NLP solving for SHOT's primal problem using Ipopt interior point.\n\n**IpoptProblem Class (Ipopt::TNLP):**\n- Implements Ipopt's TNLP interface\n- Provides callbacks for function/gradient/Hessian evaluation\n- Sparse Jacobian/Hessian via index placement maps\n\n**TNLP Callbacks:**\n- get_nlp_info(): Problem dimensions and sparsity\n- get_bounds_info(): Variable and constraint bounds\n- eval_f(): Objective function value\n- eval_grad_f(): Objective gradient\n- eval_g(): Constraint function values\n- eval_jac_g(): Constraint Jacobian (sparse)\n- eval_h(): Lagrangian Hessian (sparse)\n- finalize_solution(): Store optimal point\n\n**NLPSolverIpoptBase Class:**\n- Wraps IpoptApplication for solve control\n- Variable fixing for integer-fixed NLP subproblems\n- Starting point management\n\n**IpoptJournal:**\n- Routes Ipopt output through SHOT logging system\n\n@note Used for fixed-integer NLP subproblems in primal bound computation", "see": ["PrimalSolver.h for NLP subproblem dispatch"], "algorithm": "Warm Starting (setStartingPoint):\n  Initialize from previous NLP solution or MIP point:\n  - Improves convergence for similar subproblems\n  - Critical for efficiency in repeated NLP solves", "math": "l_i = u_i = x̄_i for integer variables i ∈ I\n  Ipopt solves reduced-space problem over continuous variables.\n  Original bounds restored by unfixVariables().", "ref": ["Bonami et al. (2008) - BONMIN algorithm description", "Wächter & Biegler (2006) - Ipopt implementation", "Nocedal & Wright (2006) - Warm starting in IPM"], "complexity": "O(nnz) vs O(n²) for dense evaluation", "has_pass2": true}, "src/NLPSolver/NLPSolverSHOT.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverSHOT.h", "filename": "NLPSolverSHOT.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverSHOT.h", "brief": "SHOT as NLP solver for convex subproblems\n\nRecursive use of SHOT to solve fixed-integer NLP.\n\n**NLPSolverSHOT Class:**\n- Creates nested SHOT solver instance\n- Solves convex NLP after fixing integer variables\n- Useful when Ipopt is unavailable\n\n**Fixed-Integer NLP:**\n- fixVariables(): Fix discrete variables to integer values\n- solveProblemInstance(): Solve continuous subproblem\n- getSolution(): Return primal solution\n\n**Nested Architecture:**\n- Outer SHOT: Full MINLP (uses this as primal solver)\n- Inner SHOT: Convex NLP (fixed integers)\n\n@note Self-referential design for solver-agnostic NLP solving", "has_pass2": false}, "src/NLPSolver/NLPSolverGAMS.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverGAMS.h", "filename": "NLPSolverGAMS.h", "author": "Stefan Vigerske, GAMS Development Corp.\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverGAMS.h", "brief": "GAMS-based NLP solver for fixed-integer subproblems\n\nUses GAMS modeling system to call NLP solvers.\n\n**NLPSolverGAMS Class:**\n- Uses gmoHandle_t from GAMS modeling system\n- Calls user-selected NLP solver (CONOPT, IPOPT, etc.)\n- Manages variable bounds and starting points\n\n**GAMS Handles:**\n- modelingObject: GMO for problem data\n- modelingEnvironment: GEV for GAMS environment\n\n**Solver Configuration:**\n- nlpsolver: Solver name (e.g., \"CONOPT\", \"IPOPT\")\n- timelimit, iterlimit: Resource limits\n- solvelink: Execution mode\n\n@note Requires GAMS license for commercial NLP solvers", "see": ["ModelingSystemGAMS.h for GAMS input processing"], "has_pass2": false}, "src/Tasks/TaskSelectHyperplanePointsObjectiveFunction.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectHyperplanePointsObjectiveFunction.h", "filename": "TaskSelectHyperplanePointsObjectiveFunction.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectHyperplanePointsObjectiveFunction.h", "brief": "Hyperplane selection for nonlinear objective\n\nGenerates cuts for epigraph of nonlinear objective.\n\n**TaskSelectHyperplanePointsObjectiveFunction Class:**\n- run(): Process objective at current solutions\n- run(solPoints): Process specific solution points\n\n**Epigraph Linearization:**\n- Nonlinear objective reformulated as: min t, f(x) <= t\n- Generates cuts to approximate f(x) <= t constraint", "see": ["AuxiliaryVariables.h for epigraph variable", "TaskSelectHyperplanePointsESH.h for constraint cuts"], "has_pass2": false}, "src/Tasks/TaskRepairableBase.h": {"path": "layer-4/SHOT/src/Tasks/TaskRepairableBase.h", "filename": "TaskRepairableBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskRepairableBase.h", "brief": "Base class for tasks that can repair failures\n\nExtension point for failure recovery logic.\n\n**TaskRepairableBase Class:**\n- repair(): Attempt to fix failed task\n\n**Usage:**\n- Subclassed by tasks that can recover from errors\n- Called when initial run() fails", "see": ["TaskRepairInfeasibleDualProblem.h for example"], "has_pass2": false}, "src/Tasks/TaskPerformBoundTightening.h": {"path": "layer-4/SHOT/src/Tasks/TaskPerformBoundTightening.h", "filename": "TaskPerformBoundTightening.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskPerformBoundTightening.h", "brief": "Tighten variable bounds via optimization\n\nUses optimization-based bound tightening (OBBT).\n\n**TaskPerformBoundTightening Class:**\n- POASolver: Polyhedral outer approximation solver\n- createPOA(): Build relaxed problem for bound tightening\n\n**OBBT Algorithm:**\n- For each variable: min/max subject to relaxation\n- Tightens bounds beyond constraint propagation\n- Improves relaxation quality", "algorithm": "Optimization-Based Bound Tightening", "see": ["NLPSolverSHOT.h for POA solver", "Problem.h for bound storage"], "has_pass2": true}, "src/Tasks/TaskPrintIterationReport.h": {"path": "layer-4/SHOT/src/Tasks/TaskPrintIterationReport.h", "filename": "TaskPrintIterationReport.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskPrintIterationReport.h", "brief": "Output iteration progress to console\n\nFormatted progress line for each iteration.\n\n**TaskPrintIterationReport Class:**\n- lastNumHyperplane: Track hyperplanes added\n- run(): Format and print iteration status\n\n**Report Contents:**\n- Iteration number, elapsed time\n- Primal bound, dual bound, gap\n- Number of hyperplanes added", "see": ["Report.h for output formatting", "Results.h for bound information"], "has_pass2": false}, "src/Tasks/TaskSimple.h": {"path": "layer-4/SHOT/src/Tasks/TaskSimple.h", "filename": "TaskSimple.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSimple.h", "brief": "Lightweight task wrapper for lambda functions\n\nAdapts arbitrary callable to task interface.\n\n**TaskSimple Class:**\n- setFunction(fn): Set callable to execute\n- run(): Invoke the stored function\n\n**Usage:**\n- Quick inline tasks without subclassing\n- Adapts existing functions to task framework", "see": ["TaskBase.h for task interface"], "has_pass2": false}, "src/Tasks/TaskTerminate.h": {"path": "layer-4/SHOT/src/Tasks/TaskTerminate.h", "filename": "TaskTerminate.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskTerminate.h", "brief": "Signals end of optimization algorithm\n\nFinal task that stops the main loop.\n\n**TaskTerminate Class:**\n- run(): Set termination flag in TaskHandler\n\n**Called When:**\n- Optimal solution found (gap closed)\n- Resource limit exceeded (time, iteration)\n- Infeasibility proven", "see": ["TaskGoto.h for jump to termination", "TaskHandler.h for loop control"], "has_pass2": false}, "src/Tasks/TaskInitializeIteration.h": {"path": "layer-4/SHOT/src/Tasks/TaskInitializeIteration.h", "filename": "TaskInitializeIteration.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskInitializeIteration.h", "brief": "Per-iteration setup and housekeeping\n\nPrepares state for new iteration cycle.\n\n**TaskInitializeIteration Class:**\n- run(): Reset iteration-local state\n\n**Initialization Steps:**\n- Increment iteration counter\n- Clear iteration-specific storage\n- Update timing information", "see": ["TaskPrintIterationReport.h for iteration summary", "Timing.h for time tracking"], "has_pass2": false}, "src/Tasks/TaskCheckIterationError.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckIterationError.h", "filename": "TaskCheckIterationError.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckIterationError.h", "brief": "Detect errors during iteration\n\nHandles solver failures and numerical issues.\n\n**TaskCheckIterationError Class:**\n- taskIDIfTrue: Jump target when error detected\n- run(): Check for MIP solver errors, infeasibility, etc.\n\n**Error Types:**\n- MIP solver failure or timeout\n- Numerical issues in cut generation\n- Unexpected infeasibility", "see": ["TaskTerminate.h for error termination", "DualSolver.h for MIP solve status"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalCandidatesFromSolutionPool.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalCandidatesFromSolutionPool.h", "filename": "TaskSelectPrimalCandidatesFromSolutionPool.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalCandidatesFromSolutionPool.h", "brief": "Check MIP solution pool for feasible MINLP solutions\n\nExtracts primal candidates from MIP solver's solution pool.\n\n**TaskSelectPrimalCandidatesFromSolutionPool Class:**\n- run(): Iterate through MIP solution pool\n- Check each solution for original constraint feasibility\n\n**Primal Candidate Processing:**\n- MIP solution may violate nonlinear constraints\n- Feasible solutions update primal bound\n- Infeasible solutions generate hyperplanes", "see": ["DualSolver.h for solution pool access", "PrimalSolver.h for feasibility checking"], "has_pass2": false}, "src/Tasks/TaskCreateDualProblem.h": {"path": "layer-4/SHOT/src/Tasks/TaskCreateDualProblem.h", "filename": "TaskCreateDualProblem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCreateDualProblem.h", "brief": "Build MIP relaxation from reformulated problem\n\nTranslates SHOT's problem representation to MIP solver format.\n\n**TaskCreateDualProblem Class:**\n- run(): Create initial MIP in DualSolver\n- createProblem(): Transfer variables, constraints, objective\n\n**MIP Construction:**\n- Linear constraints: Added directly\n- Quadratic constraints: Added if solver supports MIQCQP\n- Nonlinear constraints: Omitted (handled via hyperplanes)\n- Integer variables: Marked as such in MIP", "see": ["DualSolver.h for MIP solver interface", "Problem.h for source representation"], "has_pass2": false}, "src/Tasks/TaskCheckAbsoluteGap.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckAbsoluteGap.h", "filename": "TaskCheckAbsoluteGap.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckAbsoluteGap.h", "brief": "Termination check for absolute optimality gap\n\nTerminates when |primal - dual| < tolerance.\n\n**TaskCheckAbsoluteGap Class:**\n- taskIDIfTrue: Jump target when gap closed\n- run(): Check |bestPrimal - bestDual| < AbsoluteGapTol\n\n**Optimality Gap:**\n- Primal bound: Best feasible solution value\n- Dual bound: Relaxation optimal value\n- Gap closed = provably optimal", "see": ["TaskCheckRelativeGap.h for relative gap check", "Results.h for bound storage"], "has_pass2": false}, "src/Tasks/TaskPresolve.h": {"path": "layer-4/SHOT/src/Tasks/TaskPresolve.h", "filename": "TaskPresolve.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskPresolve.h", "brief": "Initial problem simplification\n\nApplies presolve techniques before main algorithm.\n\n**TaskPresolve Class:**\n- isPresolved: Track if presolve completed\n- run(): Apply presolve routines\n\n**Presolve Techniques:**\n- Bound propagation\n- Fixed variable removal\n- Redundant constraint detection\n- Expression simplification", "see": ["TaskReformulateProblem.h for reformulation", "Simplifications.h for expression simplification"], "has_pass2": false}, "src/Tasks/TaskCheckIterationLimit.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckIterationLimit.h", "filename": "TaskCheckIterationLimit.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckIterationLimit.h", "brief": "Termination check for iteration count limit\n\nPart of termination check sequence.\n\n**TaskCheckIterationLimit Class:**\n- taskIDIfTrue: Jump target when limit reached\n- run(): Compare iteration count vs Settings.IterationLimit\n\n**Behavior:**\n- If exceeded: Jump to termination task\n- If not: Continue to next task", "see": ["TaskCheckTimeLimit.h for time-based termination", "TaskTerminate.h for termination handling"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalCandidatesFromNLP.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalCandidatesFromNLP.h", "filename": "TaskSelectPrimalCandidatesFromNLP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalCandidatesFromNLP.h", "brief": "Solve fixed-integer NLP for primal candidates\n\nUses NLP solver (Ipopt) with fixed integer variables.\n\n**TaskSelectPrimalCandidatesFromNLP Class:**\n- solveFixedNLP(): Fix integers, solve continuous NLP\n- createInfeasibilityCut(): Handle infeasible NLP\n- createIntegerCut(): Exclude tested integer assignment\n\n**Fixed-Integer NLP:**\n- Take MIP solution's integer values\n- Solve continuous relaxation with fixed integers\n- Provides locally optimal primal candidates\n\n**Usage:**\n- Triggered by primal stagnation\n- May improve primal bound significantly", "see": ["NLPSolver/ for Ipopt interface", "TaskCheckPrimalStagnation.h for trigger"], "has_pass2": false}, "src/Tasks/TaskClearFixedPrimalCandidates.h": {"path": "layer-4/SHOT/src/Tasks/TaskClearFixedPrimalCandidates.h", "filename": "TaskClearFixedPrimalCandidates.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskClearFixedPrimalCandidates.h", "brief": "Clear pending fixed-integer NLP candidates\n\nResets candidate queue between iterations.\n\n**TaskClearFixedPrimalCandidates Class:**\n- run(): Empty the fixed-NLP candidate list\n\n**Usage:**\n- Called at start of each iteration\n- Ensures fresh candidate selection each cycle", "see": ["TaskSelectPrimalFixedNLPPointsFromSolutionPool.h for selection", "TaskSelectPrimalCandidatesFromNLP.h for NLP solving"], "has_pass2": false}, "src/Tasks/TaskAddPrimalReductionCut.h": {"path": "layer-4/SHOT/src/Tasks/TaskAddPrimalReductionCut.h", "filename": "TaskAddPrimalReductionCut.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskAddPrimalReductionCut.h", "brief": "Add objective cutoff based on incumbent\n\nUses primal bound to tighten MIP relaxation.\n\n**TaskAddPrimalReductionCut Class:**\n- currentLowerBoundForReductionCut: Current cutoff value\n- run(): Update objective bound constraint\n\n**Cutoff Strategy:**\n- Adds constraint: objective <= bestPrimal - epsilon\n- Forces MIP to find improving solutions only\n- Updated when primal bound improves", "see": ["Results.h for primal bound tracking", "TaskCheckMaxNumberOfPrimalReductionCuts.h for limit"], "has_pass2": false}, "src/Tasks/TaskCheckDualStagnation.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckDualStagnation.h", "filename": "TaskCheckDualStagnation.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckDualStagnation.h", "brief": "Detect stalled dual bound improvement\n\nTriggers countermeasures when dual bound stops improving.\n\n**TaskCheckDualStagnation Class:**\n- taskIDIfTrue: Jump target when stagnation detected\n- run(): Check if dual bound unchanged for N iterations\n\n**Stagnation Recovery:**\n- May trigger stronger cuts or reformulation\n- Indicates outer approximation isn't tightening", "see": ["TaskCheckPrimalStagnation.h for primal stagnation", "DualSolver.h for dual bound computation"], "has_pass2": false}, "src/Tasks/TaskCheckConstraintTolerance.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckConstraintTolerance.h", "filename": "TaskCheckConstraintTolerance.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckConstraintTolerance.h", "brief": "Termination check for constraint feasibility\n\nVerifies primal solution satisfies all constraints.\n\n**TaskCheckConstraintTolerance Class:**\n- taskIDIfTrue: Jump target when feasible\n- run(): Check max constraint violation < ConstraintTolerance\n\n**Usage:**\n- Validates feasibility of best primal solution\n- Required for declaring optimality", "see": ["Results.h for constraint violation tracking", "TaskCheckAbsoluteGap.h for gap check"], "has_pass2": false}, "src/Tasks/TaskInitializeRootsearch.h": {"path": "layer-4/SHOT/src/Tasks/TaskInitializeRootsearch.h", "filename": "TaskInitializeRootsearch.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskInitializeRootsearch.h", "brief": "Setup rootsearch method for ESH\n\nOne-time initialization of boundary finding algorithm.\n\n**TaskInitializeRootsearch Class:**\n- run(): Create and configure rootsearch method\n\n**Rootsearch Setup:**\n- Select algorithm (TOMS 748, bisection)\n- Configure tolerances and iteration limits\n- Required for ESH hyperplane generation", "see": ["RootsearchMethod/ for algorithm implementations", "TaskSelectHyperplanePointsESH.h for usage"], "has_pass2": false}, "src/Tasks/TaskSequential.h": {"path": "layer-4/SHOT/src/Tasks/TaskSequential.h", "filename": "TaskSequential.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSequential.h", "brief": "Container task that runs subtasks in sequence\n\nComposite pattern for building task workflows.\n\n**TaskSequential Class:**\n- addTask()/addTasks(): Build task list\n- run(): Execute all subtasks in order\n\n**Usage in SHOT:**\n- Main iteration loop as sequential task chain\n- Groups related operations (all termination checks)", "see": ["TaskBase.h for task interface", "TaskConditional.h for branching logic"], "has_pass2": false}, "src/Tasks/TaskInitializeDualSolver.h": {"path": "layer-4/SHOT/src/Tasks/TaskInitializeDualSolver.h", "filename": "TaskInitializeDualSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskInitializeDualSolver.h", "brief": "Configure and create MIP solver instance\n\nOne-time setup for MIP solver before main loop.\n\n**TaskInitializeDualSolver Class:**\n- useLazyStrategy: Single-tree (true) or multi-tree (false)\n- run(): Create appropriate MIP solver type\n\n**Solver Selection:**\n- CPLEX: MIPSolverCplex or MIPSolverCplexSingleTree\n- Gurobi: MIPSolverGurobi or MIPSolverGurobiSingleTree\n- CBC: MIPSolverCbc (multi-tree only)", "see": ["DualSolver.h for solver management", "MIPSolver/ for solver implementations"], "has_pass2": false}, "src/Tasks/TaskExecuteRelaxationStrategy.h": {"path": "layer-4/SHOT/src/Tasks/TaskExecuteRelaxationStrategy.h", "filename": "TaskExecuteRelaxationStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskExecuteRelaxationStrategy.h", "brief": "Execute LP/MIP relaxation strategy step\n\nControls transition from LP to MIP solving.\n\n**TaskExecuteRelaxationStrategy Class:**\n- run(): Invoke current relaxation strategy\n\n**Strategy Control:**\n- Calls IRelaxationStrategy to decide LP vs MIP\n- May enable/disable integer constraints\n- Affects DualSolver solve behavior", "see": ["IRelaxationStrategy.h for strategy interface", "RelaxationStrategyStandard.h for default strategy"], "has_pass2": false}, "src/Tasks/TaskConditional.h": {"path": "layer-4/SHOT/src/Tasks/TaskConditional.h", "filename": "TaskConditional.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskConditional.h", "brief": "Branching task based on runtime condition\n\nIf-then-else logic for task workflows.\n\n**TaskConditional Class:**\n- setCondition(fn): Boolean predicate to evaluate\n- setTaskIfTrue/False(): Branch targets\n- run(): Evaluate condition and execute appropriate branch\n\n**Usage in SHOT:**\n- Branch on problem type (MINLP vs NLP)\n- Skip tasks based on solver state", "see": ["TaskBase.h for task interface", "TaskSequential.h for sequential execution"], "has_pass2": false}, "src/Tasks/TaskCalculateSolutionChangeNorm.h": {"path": "layer-4/SHOT/src/Tasks/TaskCalculateSolutionChangeNorm.h", "filename": "TaskCalculateSolutionChangeNorm.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCalculateSolutionChangeNorm.h", "brief": "Compute solution movement between iterations\n\nTracks convergence via solution change magnitude.\n\n**TaskCalculateSolutionChangeNorm Class:**\n- run(): Compute ||x_new - x_old|| norm\n\n**Usage:**\n- Detect solution stagnation\n- Used in convergence criteria\n- May trigger algorithm strategy changes", "see": ["Utilities.h for L2Norm computation", "TaskCheckDualStagnation.h for stagnation handling"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalCandidatesFromRootsearch.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalCandidatesFromRootsearch.h", "filename": "TaskSelectPrimalCandidatesFromRootsearch.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalCandidatesFromRootsearch.h", "brief": "Extract primal candidates from ESH rootsearch\n\nByproduct of boundary finding: feasible points near boundary.\n\n**TaskSelectPrimalCandidatesFromRootsearch Class:**\n- run(): Check rootsearch intermediate points\n- run(solPoints): Process specific solution points\n\n**Rootsearch Byproduct:**\n- ESH rootsearch traverses from infeasible to interior\n- Intermediate points may be feasible primal candidates\n- Cheap way to discover feasible solutions", "see": ["RootsearchMethod/ for boundary finding", "TaskSelectHyperplanePointsESH.h for ESH main task"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalFixedNLPPointsFromSolutionPool.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalFixedNLPPointsFromSolutionPool.h", "filename": "TaskSelectPrimalFixedNLPPointsFromSolutionPool.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalFixedNLPPointsFromSolutionPool.h", "brief": "Select MIP solutions for fixed-integer NLP solving\n\nChooses promising candidates for NLP refinement.\n\n**TaskSelectPrimalFixedNLPPointsFromSolutionPool Class:**\n- run(): Select best candidates from solution pool\n\n**Selection Criteria:**\n- Prioritize solutions close to feasibility\n- Avoid already-tested integer assignments\n- Respect NLP solve budget", "see": ["TaskSelectPrimalCandidatesFromNLP.h for NLP solving", "DualSolver.h for solution pool access"], "has_pass2": false}, "src/Tasks/TaskAddHyperplanes.h": {"path": "layer-4/SHOT/src/Tasks/TaskAddHyperplanes.h", "filename": "TaskAddHyperplanes.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskAddHyperplanes.h", "brief": "Add generated hyperplanes to MIP relaxation\n\nTransfers cuts from hyperplane pool to MIP solver.\n\n**TaskAddHyperplanes Class:**\n- run(): Add pending hyperplanes to dual problem\n- itersWithoutAddedHPs: Track stagnation\n\n**Cut Management:**\n- Filters duplicate/dominated cuts\n- Respects cut limit per iteration\n- May trigger lazy vs pool constraint handling", "see": ["DualSolver.h for MIP constraint addition", "TaskSelectHyperplanePointsESH.h for cut generation"], "has_pass2": false}, "src/Tasks/TaskUpdateInteriorPoint.h": {"path": "layer-4/SHOT/src/Tasks/TaskUpdateInteriorPoint.h", "filename": "TaskUpdateInteriorPoint.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskUpdateInteriorPoint.h", "brief": "Update interior point during optimization\n\nImproves ESH quality by updating interior point.\n\n**TaskUpdateInteriorPoint Class:**\n- run(): Recompute or update interior point\n\n**Update Strategy:**\n- Use new primal solutions as interior candidates\n- Average with existing interior point\n- Improves cut quality as algorithm progresses", "see": ["TaskFindInteriorPoint.h for initial computation", "TaskSelectHyperplanePointsESH.h for usage"], "has_pass2": false}, "src/Tasks/TaskGoto.h": {"path": "layer-4/SHOT/src/Tasks/TaskGoto.h", "filename": "TaskGoto.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskGoto.h", "brief": "Jump to labeled task in workflow\n\nEnables non-linear control flow in task sequences.\n\n**TaskGoto Class:**\n- gotoTaskID: Target task identifier\n- run(): Signal TaskHandler to jump to target\n\n**Usage in SHOT:**\n- Loop back to iteration start\n- Jump to termination on error", "see": ["TaskHandler.h for jump resolution", "TaskTerminate.h for exit handling"], "has_pass2": false}, "src/Tasks/TaskCheckTimeLimit.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckTimeLimit.h", "filename": "TaskCheckTimeLimit.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckTimeLimit.h", "brief": "Termination check for wall-clock time limit\n\nPart of termination check sequence.\n\n**TaskCheckTimeLimit Class:**\n- taskIDIfTrue: Jump target when limit exceeded\n- run(): Compare elapsed vs Settings.TimeLimit\n\n**Behavior:**\n- If exceeded: Jump to termination task\n- If not: Continue to next task", "see": ["Timer.h for elapsed time tracking", "TaskTerminate.h for termination handling"], "has_pass2": false}, "src/Tasks/TaskException.h": {"path": "layer-4/SHOT/src/Tasks/TaskException.h", "filename": "TaskException.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskException.h", "brief": "Exception types for task framework errors\n\nSpecialized exceptions for task-related failures.\n\n**Exception Classes:**\n- TaskException: General task error\n- TaskExceptionFunctionNotDefined: Missing run() implementation\n- TaskExceptionNotFound: Invalid task ID in goto/jump\n\n**Usage:**\n- Thrown by TaskHandler and individual tasks\n- Caught and handled in main solve loop", "see": ["TaskHandler.h for exception handling", "TaskGoto.h for task lookups"], "has_pass2": false}, "src/Tasks/TaskCheckRelativeGap.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckRelativeGap.h", "filename": "TaskCheckRelativeGap.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckRelativeGap.h", "brief": "Termination check for relative optimality gap\n\nTerminates when |primal - dual| / |primal| < tolerance.\n\n**TaskCheckRelativeGap Class:**\n- taskIDIfTrue: Jump target when gap closed\n- run(): Check (|bestPrimal - bestDual| / max(1, |bestPrimal|)) < RelativeGapTol\n\n**Usage:**\n- More common than absolute gap for practical problems\n- Scales with solution magnitude", "see": ["TaskCheckAbsoluteGap.h for absolute gap check", "Results.h for bound storage"], "has_pass2": false}, "src/Tasks/TaskAddIntegerCuts.h": {"path": "layer-4/SHOT/src/Tasks/TaskAddIntegerCuts.h", "filename": "TaskAddIntegerCuts.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskAddIntegerCuts.h", "brief": "Add integer cuts to exclude visited solutions\n\nPrevents revisiting same integer assignment.\n\n**TaskAddIntegerCuts Class:**\n- run(): Generate cut excluding current integer solution\n\n**Integer Cut Types:**\n- No-good cuts: Exclude exact binary assignment\n- Local branching: Limit Hamming distance\n\n**Usage:**\n- Multi-tree strategy uses these extensively\n- Prevents cycling through same MIP solutions", "see": ["DualSolver.h for cut addition", "SolutionStrategy/ for multi-tree workflow"], "has_pass2": false}, "src/Tasks/TaskCheckUserTermination.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckUserTermination.h", "filename": "TaskCheckUserTermination.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckUserTermination.h", "brief": "Check for external termination request\n\nAllows user to interrupt solver gracefully.\n\n**TaskCheckUserTermination Class:**\n- taskIDIfTrue: Jump target when user requests stop\n- run(): Check for CTRL+C, callback termination, etc.\n\n**Usage:**\n- Called every iteration to check for interrupts\n- Returns best solution found so far", "see": ["EventHandler.h for termination callbacks", "TaskTerminate.h for graceful shutdown"], "has_pass2": false}, "src/Tasks/TaskSelectHyperplanePointsESH.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectHyperplanePointsESH.h", "filename": "TaskSelectHyperplanePointsESH.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectHyperplanePointsESH.h", "brief": "Extended Supporting Hyperplane point selection\n\nCore ESH algorithm: finds boundary points for hyperplane generation.\n\n**TaskSelectHyperplanePointsESH Class:**\n- run(): Process current MIP solutions\n- run(solPoints): Process specific solution points\n\n**ESH Algorithm:**\n1. Take infeasible MIP solution point\n2. Use rootsearch to find boundary with feasible region\n3. Generate supporting hyperplane at boundary\n\n**Extends ECP:**\n- ESH uses interior point + rootsearch\n- ECP generates cuts at infeasible point directly", "algorithm": "Extended Supporting Hyperplane (Kronqvist 2016)", "see": ["RootsearchMethod/ for boundary finding", "TaskAddHyperplanes.h for cut addition"], "has_pass2": true}, "src/Tasks/TaskCheckPrimalStagnation.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckPrimalStagnation.h", "filename": "TaskCheckPrimalStagnation.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckPrimalStagnation.h", "brief": "Detect stalled primal bound improvement\n\nTriggers NLP solves when primal bound stops improving.\n\n**TaskCheckPrimalStagnation Class:**\n- taskIDIfTrue/False: Branch targets for stagnation state\n- run(): Check if primal bound unchanged for N iterations\n\n**Stagnation Recovery:**\n- May trigger fixed-integer NLP to find better solutions\n- Indicates MIP solutions aren't improving primal", "see": ["TaskCheckDualStagnation.h for dual stagnation", "PrimalSolver.h for primal bound computation"], "has_pass2": false}, "src/Tasks/TaskBase.h": {"path": "layer-4/SHOT/src/Tasks/TaskBase.h", "filename": "TaskBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskBase.h", "brief": "Base class for modular algorithm tasks\n\nFoundation for task-based algorithm composition.\n\n**TaskBase Class:**\n- isActive(), activate(), deactivate(): Enable/disable tasks\n- initialize(): Setup before first run\n- run(): Execute the task logic\n- getType(): Task identifier for debugging\n\n**Task Categories:**\n- Termination: TimeLimit, IterationLimit, Gap checks\n- Hyperplane: SelectHyperplanePointsESH/ECP, AddHyperplanes\n- Primal: SelectPrimalCandidates, NLP solving\n- Dual: SolveIteration, CreateDualProblem\n- Control: Sequential, Conditional, Goto\n\n**Task Composition:**\n- TaskSequential: Run tasks in order\n- TaskConditional: Branch on conditions\n- TaskGoto: Jump to labeled task", "see": ["TaskHandler.h for task scheduling", "SolutionStrategy/ for task workflow definition"], "has_pass2": false}, "src/Tasks/TaskExecuteSolutionLimitStrategy.h": {"path": "layer-4/SHOT/src/Tasks/TaskExecuteSolutionLimitStrategy.h", "filename": "TaskExecuteSolutionLimitStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskExecuteSolutionLimitStrategy.h", "brief": "Control MIP solution pool size dynamically\n\nAdjusts how many MIP solutions are collected per iteration.\n\n**TaskExecuteSolutionLimitStrategy Class:**\n- solutionLimitStrategy: Strategy implementation\n- previousSolLimit: Track limit changes\n- run(): Apply current solution limit\n\n**Strategy Control:**\n- Start with small limit for fast iterations\n- Increase when MIP optimal to explore pool\n- Balance speed vs solution diversity", "see": ["IMIPSolutionLimitStrategy.h for strategy interface", "MIPSolutionLimitStrategyIncrease.h for default strategy"], "has_pass2": false}, "src/Tasks/TaskSolveIteration.h": {"path": "layer-4/SHOT/src/Tasks/TaskSolveIteration.h", "filename": "TaskSolveIteration.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSolveIteration.h", "brief": "Solve MIP relaxation for current iteration\n\nCore MIP solve step in outer approximation loop.\n\n**TaskSolveIteration Class:**\n- run(): Call DualSolver to solve MIP with current cuts\n\n**Iteration Flow:**\n1. MIP has linear approximation of nonlinear constraints\n2. Solve gives candidate solution + dual bound\n3. Candidate checked for feasibility\n4. New cuts generated at infeasible points", "see": ["DualSolver.h for MIP solve call", "TaskSelectHyperplanePointsESH.h for cut generation"], "has_pass2": false}, "src/Tasks/TaskSelectHyperplanePointsECP.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectHyperplanePointsECP.h", "filename": "TaskSelectHyperplanePointsECP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectHyperplanePointsECP.h", "brief": "Extended Cutting Plane point selection\n\nClassic outer approximation: linearize at infeasible points.\n\n**TaskSelectHyperplanePointsECP Class:**\n- run(): Process current MIP solutions\n- run(solPoints): Process specific solution points\n\n**ECP Algorithm:**\n- Generate gradient cut at infeasible solution\n- Simpler than ESH but may converge slower\n- Used as fallback when ESH rootsearch fails", "algorithm": "Extended Cutting Plane (Westerlund-Pettersson)", "see": ["TaskSelectHyperplanePointsESH.h for ESH variant", "TaskAddHyperplanes.h for cut addition"], "has_pass2": true}, "src/Tasks/TaskFindInteriorPoint.h": {"path": "layer-4/SHOT/src/Tasks/TaskFindInteriorPoint.h", "filename": "TaskFindInteriorPoint.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskFindInteriorPoint.h", "brief": "Find strictly feasible interior point for ESH\n\nCritical for ESH: rootsearch needs interior point endpoint.\n\n**TaskFindInteriorPoint Class:**\n- NLPSolvers: Solvers for finding interior\n- run(): Attempt to find strictly feasible point\n\n**Interior Point Methods:**\n- Solve feasibility problem with slack maximization\n- Use NLP solver (Ipopt) with modified objective\n- Required for ESH rootsearch to work", "see": ["TaskUpdateInteriorPoint.h for runtime updates", "RootsearchMethod/ for interior point usage"], "has_pass2": false}, "src/Tasks/TaskReformulateProblem.h": {"path": "layer-4/SHOT/src/Tasks/TaskReformulateProblem.h", "filename": "TaskReformulateProblem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskReformulateProblem.h", "brief": "Transform problem for efficient outer approximation\n\nKey preprocessing: converts problem to reformulated form.\n\n**TaskReformulateProblem Class:**\n- reformulateObjectiveFunction(): Handle nonlinear objective\n- reformulateConstraint(): Process each constraint type\n- createEpigraphConstraint(): Epigraph reformulation\n\n**Reformulation Techniques:**\n- Bilinear terms: McCormick envelopes (addBilinearMcCormickEnvelope)\n- Integer products: Binary expansion\n- Eigenvalue decomposition: Convexify quadratics\n- Term extraction: Separate linear/quadratic/monomial/signomial\n\n**Auxiliary Variables:**\n- Created for bilinear products, squares, absolute values\n- Tracked in squareAuxVariables, bilinearAuxVariables maps", "see": ["AuxiliaryVariables.h for auxiliary variable types", "Problem.h for reformulated problem storage"], "has_pass2": false}, "src/Tasks/TaskCheckMaxNumberOfPrimalReductionCuts.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckMaxNumberOfPrimalReductionCuts.h", "filename": "TaskCheckMaxNumberOfPrimalReductionCuts.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckMaxNumberOfPrimalReductionCuts.h", "brief": "Limit number of objective cutoff updates\n\nPrevents excessive cutoff constraint modifications.\n\n**TaskCheckMaxNumberOfPrimalReductionCuts Class:**\n- taskIDIfTrue: Jump target when limit reached\n- run(): Check cutoff update count\n\n**Usage:**\n- Limit objective bound updates per iteration\n- Avoid MIP solver overhead from repeated bound changes", "see": ["TaskAddPrimalReductionCut.h for cutoff addition", "Settings.h for limit configuration"], "has_pass2": false}, "src/Tasks/TaskRepairInfeasibleDualProblem.h": {"path": "layer-4/SHOT/src/Tasks/TaskRepairInfeasibleDualProblem.h", "filename": "TaskRepairInfeasibleDualProblem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskRepairInfeasibleDualProblem.h", "brief": "Recover from infeasible MIP relaxation\n\nAttempts to restore feasibility when cuts cause infeasibility.\n\n**TaskRepairInfeasibleDualProblem Class:**\n- mainRepairTries, totRepairTries: Repair attempt counters\n- run(): Attempt repair strategies\n\n**Repair Strategies:**\n- Remove recently added cuts\n- Relax cut coefficients\n- Remove dominated cuts\n\n**When Called:**\n- MIP solver returns infeasible\n- May indicate numerical issues with cuts", "see": ["DualSolver.h for MIP status", "TaskAddHyperplanes.h for cut management"], "has_pass2": false}, "src/Model/NonlinearExpressions.h": {"path": "layer-4/SHOT/src/Model/NonlinearExpressions.h", "filename": "NonlinearExpressions.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @note The monotonicity and convexity identification is strongly influenced by that of Suspect\n   (https://github.com/cog-imperial/suspect) by Francesco Ceccon, Imperial College London\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/NonlinearExpressions.h", "brief": "Expression tree nodes for nonlinear functions\n\nDAG-based representation of nonlinear expressions with convexity analysis.\n\n**E_NonlinearExpressionTypes:**\n- Constant, Variable: Leaf nodes\n- Unary: Negate, Invert, SquareRoot, Log, Exp, Square, Sin, Cos, etc.\n- Binary: Divide, Power\n- N-ary: Sum, Product\n\n**NonlinearExpression Base Class:**\n- calculate(point): Evaluate at numeric point\n- calculate(intervals): Interval arithmetic evaluation\n- getFactorableFunction(): CppAD automatic differentiation\n- getConvexity(): Analyze convexity via composition rules\n- getMonotonicity(): Analyze monotonicity\n- tightenBounds(): FBBT bound propagation\n\n**Expression Subclasses:**\n- ExpressionConstant, ExpressionVariable\n- ExpressionNegate, ExpressionSquare, ExpressionSquareRoot\n- ExpressionLog, ExpressionExp\n- ExpressionSin, ExpressionCos\n- ExpressionSum, ExpressionProduct\n- ExpressionDivide, ExpressionPower\n\n**Convexity Rules:**\n- Based on composition theorems (e.g., convex ∘ nondecreasing convex = convex)\n- Interval bounds inform sign-dependent rules\n\n@note Convexity analysis influenced by SUSPECT library", "see": ["Constraints.h for expressions in constraints"], "has_pass2": false}, "src/Model/AuxiliaryVariables.h": {"path": "layer-4/SHOT/src/Model/AuxiliaryVariables.h", "filename": "AuxiliaryVariables.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/AuxiliaryVariables.h", "brief": "Variables introduced during problem reformulation\n\nVariables created to linearize or convexify expressions.\n\n**AuxiliaryVariable Class:**\n- Extends Variable with expression components\n- calculate(): Evaluate auxiliary from original variables\n- Properties: constant, linear/quadratic/monomial/signomial terms\n\n**Auxiliary Variable Types (E_AuxiliaryVariableType):**\n- NonlinearObjectiveFunction: Epigraph reformulation\n- BilinearTerms: McCormick relaxation variables\n- MonomialPartitioning: Product term linearization\n- EigenvalueDecomposition: Convexification variables\n\n**AuxiliaryVariables Collection:**\n- Vector of AuxiliaryVariablePtr with Problem ownership\n- sortByIndex(): Order by variable index", "see": ["Variables.h for base Variable class", "Problem.h for reformulated problem usage"], "has_pass2": false}, "src/Model/Constraints.h": {"path": "layer-4/SHOT/src/Model/Constraints.h", "filename": "Constraints.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Constraints.h", "brief": "Constraint hierarchy: linear, quadratic, and nonlinear\n\nConstraint classes forming an inheritance hierarchy.\n\n**Constraint Properties:**\n- classification: Linear, Quadratic, Signomial, Nonlinear\n- convexity: Linear, Convex, Concave, Nonconvex, Unknown\n- type: Equality, LessThan, GreaterThan\n\n**NumericConstraintValue Struct:**\n- functionValue: f(x) at evaluation point\n- normalizedLHSValue: L - f(x), normalizedRHSValue: f(x) - U\n- error: max(0, max(L - f(x), f(x) - U))\n\n**Constraint Class Hierarchy:**\n- Constraint (abstract): Base with index, name, properties\n- NumericConstraint: LHS/RHS bounds, gradient/Hessian computation\n- LinearConstraint: linearTerms, sparse gradient\n- QuadraticConstraint: + quadraticTerms, Hessian\n- NonlinearConstraint: + monomials, signomials, nonlinearExpression\n\n**Derivative Computation:**\n- calculateGradient(): Sparse first derivatives\n- calculateHessian(): Upper triangular second derivatives\n- Sparsity patterns for efficient NLP solver interfaces", "see": ["Terms.h for LinearTerm, QuadraticTerm definitions", "NonlinearExpressions.h for expression tree evaluation"], "has_pass2": false}, "src/Model/Problem.h": {"path": "layer-4/SHOT/src/Model/Problem.h", "filename": "Problem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Problem.h", "brief": "Core problem representation with variables, constraints, and objective\n\nCentral data structure holding the optimization problem definition.\n\n**ProblemProperties Struct:**\n- Convexity classification (Convex, Nonconvex, NotSet)\n- Problem type flags (MINLP, MIQP, MILP, NLP, etc.)\n- Variable counts by type (real, binary, integer, auxiliary)\n- Constraint counts by type (linear, quadratic, nonlinear)\n\n**SpecialOrderedSet Struct:**\n- SOS1 (at most one variable nonzero) or SOS2 (contiguous nonzeros)\n- Variables and optional weights\n\n**Problem Class:**\n- allVariables, realVariables, binaryVariables, etc.\n- linearConstraints, quadraticConstraints, nonlinearConstraints\n- objectiveFunction (linear, quadratic, or nonlinear)\n- Sparsity patterns for Jacobian and Hessian\n- Feasibility bound propagation (FBBT) for tightening bounds\n\n**Key Methods:**\n- add(): Add variables, constraints, objective\n- finalize(): Compute properties and sparsity patterns\n- getMostDeviatingNumericConstraint(): Find worst violation\n- createCopy(): Clone for reformulation", "see": ["Solver.h for problem loading via setProblem()", "ReformulatedProblem for auxiliary variable introduction"], "algorithm": "Sparsity Pattern Extraction (getConstraintsJacobianSparsityPattern):\n  Identifies nonzero structure for efficient derivative computation:", "math": "Jacobian ∂g_i/∂x_j: sparse pattern for AD evaluation\n  Used by NLP solvers (Ipopt) and for hyperplane construction.", "complexity": "O(nnz) where nnz = number of variable appearances in constraints", "ref": ["Belotti et al. (2009) - Branching and bounds tightening techniques", "ESH (Extended Supporting Hyperplane) uses this for cut generation", "McCormick (1976) - Convex relaxations via factorable functions"], "has_pass2": true}, "src/Model/Variables.h": {"path": "layer-4/SHOT/src/Model/Variables.h", "filename": "Variables.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Variables.h", "brief": "Decision variable representation with bounds and types\n\nVariable classes for MINLP problem modeling.\n\n**VariableProperties Struct:**\n- type: Real, Binary, Integer, Semicontinuous, Semiinteger\n- auxiliaryType: Tracks origin of reformulation variables\n- Context flags: inObjectiveFunction, inLinearConstraints, etc.\n- Bound tightening status\n\n**Variable Class:**\n- index, name: Problem identifier\n- lowerBound, upperBound: Variable domain\n- semiBound: For semicontinuous/semiinteger types\n- calculate(): Evaluate at a point\n- getBound(): Interval arithmetic bounds\n- tightenBounds(): FBBT bound update\n\n**Variables Collection:**\n- Vector wrapper with ownership tracking\n- sortByIndex(): Reorder for solver compatibility\n\n**Interval Arithmetic (CppAD):**\n- Uses mc::Interval for bound propagation\n- FactorableFunction for automatic differentiation", "see": ["Constraints.h for variable usage in constraints", "AuxiliaryVariables.h for reformulation-generated variables"], "has_pass2": false}, "src/Model/ObjectiveFunction.h": {"path": "layer-4/SHOT/src/Model/ObjectiveFunction.h", "filename": "ObjectiveFunction.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/ObjectiveFunction.h", "brief": "Objective function hierarchy with derivative computation\n\nObjective function classes mirroring the constraint hierarchy.\n\n**ObjectiveFunctionProperties:**\n- direction: Minimize or Maximize\n- classification: Linear, Quadratic, Signomial, Nonlinear\n- convexity: Convex, Concave, Linear, Nonconvex\n- Term presence flags for structure detection\n\n**ObjectiveFunction Base:**\n- calculateValue(): Evaluate at point or interval\n- calculateGradient(), calculateHessian(): Derivatives\n- getGradientSparsityPattern(): Nonzero structure\n\n**LinearObjectiveFunction:**\n- linearTerms + constant\n- isDualUnbounded(): Unboundedness detection\n\n**QuadraticObjectiveFunction:**\n- + quadraticTerms\n- Inherits from LinearObjectiveFunction\n\n**NonlinearObjectiveFunction:**\n- + monomialTerms, signomialTerms, nonlinearExpression\n- factorableFunction: CppAD automatic differentiation\n- Inherits from QuadraticObjectiveFunction\n\n@note For minimization with nonlinear objective, SHOT uses\n      epigraph reformulation: min t s.t. f(x) <= t", "see": ["DualSolver.h for objective cut generation"], "has_pass2": false}, "src/Model/Simplifications.h": {"path": "layer-4/SHOT/src/Model/Simplifications.h", "filename": "Simplifications.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Simplifications.h", "brief": "Expression tree simplification and term extraction\n\nAlgebraic simplification and structured term extraction.\n\n**Simplification Functions:**\n- simplify(): Recursive expression simplification\n- simplifyExpression(ExprType): Type-specific handlers\n- Constant folding, negation cancellation, identity removal\n\n**Term Extraction:**\n- extractTermsAndConstant(): Decompose expression into:\n  LinearTerms, QuadraticTerms, MonomialTerms, SignomialTerms,\n  NonlinearExpression remainder, and constant\n\n**Conversion Functions:**\n- convertProductToLinearTerm/QuadraticTerm/MonomialTerm()\n- convertPowerToLinearTerm/QuadraticTerm()\n- convertToSignomialTerm(): Extract signomial structure\n\n**Use Case:**\n- Problem reformulation for better convexity analysis\n- Extract quadratics for QCQP solvers\n- Identify linear/quadratic substructure in NLP", "see": ["NonlinearExpressions.h for expression tree nodes"], "has_pass2": false}, "src/Model/Terms.h": {"path": "layer-4/SHOT/src/Model/Terms.h", "filename": "Terms.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Terms.h", "brief": "Term hierarchy for linear, quadratic, monomial, and signomial expressions\n\nBuilding blocks for constraint and objective function representation.\n\n**Term Base Class:**\n- coefficient: Scaling factor\n- calculate(): Evaluate at point or interval\n- getConvexity(): Linear, Convex, Concave, Nonconvex\n- getMonotonicity(): Nondecreasing, Nonincreasing, Constant\n\n**LinearTerm:** coeff * x\n- Always linear convexity\n- Monotonicity depends on coefficient sign\n\n**QuadraticTerm:** coeff * x1 * x2\n- isSquare: x1 == x2 (convex if coeff > 0)\n- isBilinear: x1 != x2 (nonconvex)\n- isBinary, isInteger: Type detection for reformulation\n\n**QuadraticTerms Collection:**\n- Eigenvalue analysis via Eigen for convexity\n- allSquares, allPositive flags for structure detection\n\n**MonomialTerm:** coeff * x1 * x2 * ... * xn\n- isBinary: All variables binary (linearizable)\n- Generally nonconvex\n\n**SignomialTerm:** coeff * x1^p1 * x2^p2 * ... * xn^pn\n- SignomialElement: (variable, power) pair\n- Convexity rules based on power signs and sums\n\n**Gradient/Hessian Methods:**\n- calculateGradient(): Sparse first derivatives\n- calculateHessian(): Upper triangular second derivatives", "see": ["Constraints.h, ObjectiveFunction.h for term usage"], "has_pass2": false}, "src/SolutionStrategy/ISolutionStrategy.h": {"path": "layer-4/SHOT/src/SolutionStrategy/ISolutionStrategy.h", "filename": "ISolutionStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/ISolutionStrategy.h", "brief": "Abstract interface for solution strategies\n\nBase interface for algorithm selection.\n\n**Strategy Interface:**\n- initializeStrategy(): Configure tasks and solvers\n- solveProblem(): Execute the solution algorithm\n\n**Strategy Implementations:**\n- SolutionStrategyMultiTree: Iterative outer approximation\n  - Solve MIP, add cuts, repeat\n- SolutionStrategySingleTree: Lazy constraint callback\n  - Single MIP with callback cut injection\n- SolutionStrategyNLP: Pure NLP (no integer variables)\n- SolutionStrategyMIQP/MIQCQP: Direct quadratic solving\n\n**Selection Logic:**\n- Problem classification determines best strategy\n- Convex MINLP → ESH (multi-tree or single-tree)\n- QP/QCQP → Direct solver if supported\n- NLP → Interior point only", "see": ["TaskHandler.h for task execution flow", "Solver.h for strategy selection"], "has_pass2": false}, "src/SolutionStrategy/SolutionStrategyMIQCQP.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategyMIQCQP.h", "filename": "SolutionStrategyMIQCQP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategyMIQCQP.h", "brief": "Direct MIQCQP solver for convex quadratic problems\n\nBypasses ESH for problems solvable by CPLEX/Gurobi MIQCQP.\n\n**SolutionStrategyMIQCQP Class:**\n- initializeStrategy(): Configure for direct MIQCQP solve\n- solveProblem(): Single solver call, no outer approximation\n\n**Use Case:**\n- Convex MIQCQP (quadratic constraints, convex)\n- CPLEX and Gurobi support convex QCQP natively\n- Faster than iterative linearization for small problems\n\n**Problem Classification:**\n- All constraints must be convex quadratic\n- Solver must support QCQP (supportsQuadraticConstraints)", "algorithm": "Direct MIQCQP branch-and-bound", "see": ["SolutionStrategyMultiTree.h for general MINLP"], "has_pass2": true}, "src/SolutionStrategy/SolutionStrategySingleTree.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategySingleTree.h", "filename": "SolutionStrategySingleTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategySingleTree.h", "brief": "Lazy constraint callback strategy (single-tree)\n\nCallback-based outer approximation within single B&B tree.\n\n**Algorithm Pattern:**\n1. Solve MIP with lazy constraint callback\n2. Callback checks integer solutions for feasibility\n3. Generate hyperplanes for violated constraints\n4. Add as lazy constraints, solver continues\n\n**Task Flow (initializeStrategy):**\n- CreateDualProblem → ConfigureCallback → SolveProblem\n- (cuts added via callback during solve)\n\n**Advantages:**\n- Single B&B tree exploration\n- Potentially fewer node evaluations\n- Better integration with MIP solver\n\n**Disadvantages:**\n- Requires solver callback support\n- Limited to CPLEX/Gurobi\n- More complex debugging", "algorithm": "Branch-and-cut with lazy ESH constraints", "see": ["MIPSolverCplexSingleTree, MIPSolverGurobiSingleTree"], "has_pass2": true}, "src/SolutionStrategy/SolutionStrategyMultiTree.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategyMultiTree.h", "filename": "SolutionStrategyMultiTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategyMultiTree.h", "brief": "Iterative outer approximation strategy (multi-tree)\n\nClassic outer approximation loop for convex MINLP.\n\n**Algorithm Pattern:**\n1. Solve MIP relaxation to get candidate point\n2. Generate supporting hyperplanes at violated points\n3. Add cuts to MIP and resolve\n4. Repeat until convergence or termination\n\n**Task Flow (initializeStrategy):**\n- CreateDualProblem → SolveIteration → SelectHyperplanes\n- AddHyperplanes → CheckTermination → loop\n\n**Advantages:**\n- No callback complexity\n- Can use any MIP solver\n- Easier debugging/logging\n\n**Disadvantages:**\n- Multiple MIP solves\n- May regenerate same B&B tree work", "algorithm": "Standard ESH outer approximation", "see": ["SolutionStrategySingleTree for callback-based variant"], "has_pass2": true}, "src/SolutionStrategy/SolutionStrategyNLP.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategyNLP.h", "filename": "SolutionStrategyNLP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategyNLP.h", "brief": "Direct NLP solver for continuous problems\n\nBypasses ESH for purely continuous NLP problems.\n\n**SolutionStrategyNLP Class:**\n- initializeStrategy(): Configure for direct NLP solve\n- solveProblem(): Single NLP solver call (Ipopt)\n\n**Use Case:**\n- Problems with no integer variables\n- Convex NLP where outer approximation is unnecessary\n- Falls back to standard NLP solvers (Ipopt)\n\n**When Selected:**\n- Problem type is NLP (no discrete variables)\n- Simpler than ESH for continuous problems", "algorithm": "Direct interior point NLP solve", "see": ["NLPSolver/NLPSolverIpoptBase.h for Ipopt wrapper"], "has_pass2": true}, "src/RootsearchMethod/IRootsearchMethod.h": {"path": "layer-4/SHOT/src/RootsearchMethod/IRootsearchMethod.h", "filename": "IRootsearchMethod.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "RootsearchMethod/IRootsearchMethod.h", "brief": "Interface for constraint boundary rootsearch algorithms\n\nFinds points on constraint boundaries for ESH hyperplane generation.\n\n**Purpose:**\nGiven interior point A and exterior point B, find point on boundary\nwhere constraint function equals zero (feasibility boundary).\n\n**Constraint Rootsearch:**\n- findZero(ptA, ptB, ...): Search along line segment A→B\n- Returns (boundary point, exterior point) pair\n- lambdaTol: Tolerance on line parameter\n- constrTol: Tolerance on constraint violation\n\n**Objective Rootsearch:**\n- findZero(pt, objLB, objUB, ...): Find objective level set\n- Used for objective function cuts\n\n**Algorithm:**\n- Uses bisection or hybrid methods (TOMS 748)\n- Requires f(A) < 0 (feasible) and f(B) > 0 (infeasible)\n\n**Implementations:**\n- RootsearchMethodBoost: Boost.Math TOMS 748 or bisection", "algorithm": "Essential for Extended Supporting Hyperplane (ESH) method", "see": ["DualSolver.h for hyperplane generation workflow"], "has_pass2": true}, "src/RootsearchMethod/RootsearchMethodBoost.h": {"path": "layer-4/SHOT/src/RootsearchMethod/RootsearchMethodBoost.h", "filename": "RootsearchMethodBoost.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "RootsearchMethod/RootsearchMethodBoost.h", "brief": "Boost.Math rootfinding for constraint boundaries\n\nFinds constraint boundary points using TOMS 748 or bisection.\n\n**RootsearchMethodBoost Class:**\n- findZero(ptA, ptB, constraints): Find boundary between points\n- findZero(pt, objLB, objUB, objective): Objective rootsearch\n\n**Helper Classes:**\n- Test: Functor for constraint max-violation evaluation\n- TestObjective: Functor for objective function evaluation\n- TerminationCondition: Convergence criterion\n\n**Algorithm:**\n- TOMS 748: Optimal bracketing algorithm (4th order)\n- Bisection: Fallback for difficult cases\n- Returns boundary point for hyperplane generation", "see": ["IRootsearchMethod.h for interface contract", "Boost.Math TOMS 748 documentation"], "has_pass2": false}, "src/MIPSolver/MIPSolverCplexSingleTreeLegacy.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCplexSingleTreeLegacy.h", "filename": "MIPSolverCplexSingleTreeLegacy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCplexSingleTreeLegacy.h", "brief": "Legacy CPLEX callback API for single-tree ESH\n\nUses older callback API (pre-CPLEX 12.10 generic callbacks).\n\n**Legacy Callback Classes:**\n- HCallbackI: Heuristic callback for primal solutions\n- InfoCallbackI: Progress information callback\n- CtCallbackI: Lazy constraint callback for hyperplanes\n\n**MIPSolverCplexSingleTreeLegacy Class:**\n- Inherits MIPSolverCplex for base functionality\n- Uses IloCplex::LazyConstraintCallbackI/HeuristicCallbackI\n\n@deprecated Prefer MIPSolverCplexSingleTree with generic callbacks", "see": ["MIPSolverCplexSingleTree.h for modern callback API"], "has_pass2": false}, "src/MIPSolver/IRelaxationStrategy.h": {"path": "layer-4/SHOT/src/MIPSolver/IRelaxationStrategy.h", "filename": "IRelaxationStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/IRelaxationStrategy.h", "brief": "Interface for LP relaxation control strategies\n\nManages when to solve LP relaxation vs full MIP.\n\n**IRelaxationStrategy Interface:**\n- executeStrategy(): Apply relaxation decision\n- setActive()/setInactive(): Enable/disable strategy\n- setInitial(): Reset to initial state\n\n**Implementations:**\n- RelaxationStrategyStandard: Periodic LP solves\n- RelaxationStrategyNone: Always solve full MIP\n\n**Use Case:**\n- LP relaxations generate more hyperplanes quickly\n- MIP solves provide better candidate points\n- Strategy balances cut generation vs exploration", "see": ["RelaxationStrategyBase.h for base implementation"], "has_pass2": false}, "src/MIPSolver/MIPSolverGurobi.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverGurobi.h", "filename": "MIPSolverGurobi.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverGurobi.h", "brief": "Gurobi implementation of IMIPSolver interface\n\nProvides MIP solving for SHOT's dual problem using Gurobi C++ API.\n\n**MIPSolverGurobi Class:**\n- Implements full IMIPSolver interface\n- Uses GRBModel for problem representation\n- Supports LP, MIP, QP, MIQP, QCQP problem types\n\n**Key Data Structures:**\n- gurobiModel: Shared GRBModel pointer\n- objectiveLinearExpression, objectiveQuadraticExpression\n- constraintLinearExpression, constraintQuadraticExpression\n\n**GurobiCallbackMultiTree:**\n- GRBCallback for multi-tree algorithm\n- Handles termination checks during solve\n\n**Quadratic Support:**\n- supportsQuadraticObjective(): Yes\n- supportsQuadraticConstraints(): Yes (QCQP)\n\n@note Requires Gurobi C++ headers", "see": ["MIPSolverGurobSingleTree for lazy constraint callback"], "has_pass2": false}, "src/MIPSolver/MIPSolverCallbackBase.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCallbackBase.h", "filename": "MIPSolverCallbackBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCallbackBase.h", "brief": "Shared logic for single-tree callback handlers\n\nBase class for CPLEX and Gurobi callback implementations.\n\n**MIPSolverCallbackBase Class:**\n- addLazyConstraint(): Generate and add ESH cuts\n- checkFixedNLPStrategy(): Decide if NLP should be called\n- checkIterationLimit()/checkUserTermination(): Termination checks\n- printIterationReport(): Console output in callback\n\n**Task Objects:**\n- taskSelectPrimNLPOriginal/Reformulated: Fixed-integer NLP\n- taskSelectHPPts: ESH/ECP hyperplane selection\n- taskSelectPrimalSolutionFromRootsearch: Rootsearch primal\n- tUpdateInteriorPoint: Interior point maintenance", "see": ["MIPSolverCplexSingleTree.h, MIPSolverGurobiSingleTree.h"], "has_pass2": false}, "src/MIPSolver/IMIPSolutionLimitStrategy.h": {"path": "layer-4/SHOT/src/MIPSolver/IMIPSolutionLimitStrategy.h", "filename": "IMIPSolutionLimitStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/IMIPSolutionLimitStrategy.h", "brief": "Interface for MIP solution pool limit strategies\n\nControls how many solutions MIP solver collects per iteration.\n\n**IMIPSolutionLimitStrategy Interface:**\n- updateLimit(): Adjust limit based on progress\n- getNewLimit(): Current solution limit value\n- getInitialLimit(): Starting limit\n\n**Implementations:**\n- MIPSolutionLimitStrategyIncrease: Grow limit over time\n- MIPSolutionLimitStrategyUnlimited: No limit\n- MIPSolutionLimitStrategyAdaptive: Adjust based on gap\n\n**Purpose:**\n- Balance cut generation vs solver time per iteration\n- More solutions = more hyperplane generation points\n- Fewer solutions = faster MIP solves", "see": ["TaskExecuteSolutionLimitStrategy for strategy execution"], "has_pass2": false}, "src/MIPSolver/MIPSolverGurobiSingleTree.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverGurobiSingleTree.h", "filename": "MIPSolverGurobiSingleTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverGurobiSingleTree.h", "brief": "Gurobi with lazy constraint callback for single-tree ESH\n\nExtends MIPSolverGurobi with callback-based cut generation.\n\n**GurobiCallbackSingleTree Class:**\n- Inherits GRBCallback + MIPSolverCallbackBase\n- callback(): Called at candidate solution points\n- createHyperplane()/createIntegerCut(): Add lazy constraints\n- addLazyConstraint(): Generate ESH cuts from solution\n\n**MIPSolverGurobiSingleTree Class:**\n- solveProblem(): Single MIP solve with callbacks enabled\n- isCallbackInitialized: Track callback registration\n\n**Node Information:**\n- lastExploredNodes/lastOpenNodes: B&B tree progress\n- Used for iteration reporting within callback", "see": ["SolutionStrategySingleTree.h for algorithm context"], "has_pass2": false}, "src/MIPSolver/RelaxationStrategyStandard.h": {"path": "layer-4/SHOT/src/MIPSolver/RelaxationStrategyStandard.h", "filename": "RelaxationStrategyStandard.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/RelaxationStrategyStandard.h", "brief": "Standard LP-to-MIP relaxation strategy\n\nImplements the default relaxation phase behavior.\n\n**RelaxationStrategyStandard Class:**\n- executeStrategy(): Decide LP vs MIP solve\n- setActive()/setInactive(): Control strategy state\n- setInitial(): Reset for new problem\n\n**LP Phase Termination:**\n- isIterationLimitReached(): Max LP iterations\n- isTimeLimitReached(): LP time budget\n- isLPStepFinished(): Convergence detection\n- isObjectiveStagnant(): No bound improvement\n\n**Algorithm:**\n- Initial iterations solve LP for fast hyperplanes\n- Switch to MIP after LP phase completion\n- LPFinished flag tracks phase transition", "see": ["IRelaxationStrategy.h for interface"], "has_pass2": false}, "src/MIPSolver/MIPSolverCplexSingleTree.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCplexSingleTree.h", "filename": "MIPSolverCplexSingleTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCplexSingleTree.h", "brief": "CPLEX with lazy constraint callback for single-tree ESH\n\nExtends MIPSolverCplex with callback-based cut generation.\n\n**MIPSolverCplexSingleTree Class:**\n- solveProblem(): Single MIP solve with callbacks enabled\n- Uses generic callback API (Context-based)\n\n**CplexCallback Class:**\n- Inherits IloCplex::Callback::Function + MIPSolverCallbackBase\n- invoke(): Called at candidate solution points\n- createHyperplane()/createIntegerCut(): Add lazy constraints\n- addLazyConstraint(): Generate ESH cuts from solution\n\n**Threading:**\n- callbackMutex: Protect shared state in multi-threaded solve\n- CPLEX invokes callbacks from multiple threads", "see": ["SolutionStrategySingleTree.h for algorithm context"], "has_pass2": false}, "src/MIPSolver/MIPSolutionLimitStrategyIncrease.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolutionLimitStrategyIncrease.h", "filename": "MIPSolutionLimitStrategyIncrease.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolutionLimitStrategyIncrease.h", "brief": "Solution limit strategy with gradual increase\n\nIncreases MIP solution pool limit over iterations.\n\n**MIPSolutionLimitStrategyIncrease Class:**\n- updateLimit(): Increase limit when MIP finds optimal\n- getNewLimit()/getInitialLimit(): Current and start values\n\n**State Tracking:**\n- lastIterSolLimIncreased: When limit was last raised\n- numSolLimIncremented: Total increases performed\n- lastIterOptimal: Last iteration with optimal MIP\n\n**Algorithm:**\n- Start with low limit for fast initial iterations\n- Increase when solver proves optimality\n- More solutions = more hyperplane generation points", "see": ["IMIPSolutionLimitStrategy.h for interface"], "has_pass2": false}, "src/MIPSolver/MIPSolverCplex.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCplex.h", "filename": "MIPSolverCplex.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCplex.h", "brief": "IBM CPLEX implementation of IMIPSolver interface\n\nProvides MIP solving for SHOT's dual problem using CPLEX Concert API.\n\n**MIPSolverCplex Class:**\n- Implements full IMIPSolver interface\n- Uses IloModel, IloCplex for problem representation\n- Supports LP, MIP, QP, MIQP, QCQP problem types\n\n**Key Data Structures:**\n- cplexModel: IloModel for optimization model\n- cplexInstance: IloCplex solver instance\n- cplexVars: Variable array\n- cplexConstrs: Constraint array\n\n**Quadratic Support:**\n- supportsQuadraticObjective(): Yes\n- supportsQuadraticConstraints(): Yes (QCQP)\n\n**UserTerminationCallbackI:**\n- MIP info callback for user termination\n- Allows early termination on events\n\n@note Requires CPLEX Concert Technology headers", "see": ["MIPSolverCplexSingleTree for lazy constraint callback"], "has_pass2": false}, "src/MIPSolver/MIPSolutionLimitStrategyUnlimited.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolutionLimitStrategyUnlimited.h", "filename": "MIPSolutionLimitStrategyUnlimited.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolutionLimitStrategyUnlimited.h", "brief": "No limit on MIP solution pool size\n\nAllows solver to collect all feasible solutions.\n\n**MIPSolutionLimitStrategyUnlimited Class:**\n- getInitialLimit(): Returns maximum integer\n- updateLimit(): No-op (always unlimited)\n\n**Use Case:**\n- Thorough exploration of solution space\n- May slow down iterations", "see": ["IMIPSolutionLimitStrategy.h for interface", "MIPSolutionLimitStrategyIncrease.h for gradual increase"], "has_pass2": false}, "src/MIPSolver/MIPSolutionLimitStrategyAdaptive.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolutionLimitStrategyAdaptive.h", "filename": "MIPSolutionLimitStrategyAdaptive.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolutionLimitStrategyAdaptive.h", "brief": "Adaptive solution limit based on solver progress\n\nAdjusts limit based on optimization state.\n\n**MIPSolutionLimitStrategyAdaptive Class:**\n- lastIterSolLimIncreased: Track when limit changed\n- numSolLimIncremented: Count of increases\n\n**Adaptive Logic:**\n- Increase when finding good solutions\n- Decrease when solver is struggling\n- Balances exploration vs speed", "see": ["IMIPSolutionLimitStrategy.h for interface", "MIPSolutionLimitStrategyIncrease.h for simpler strategy"], "has_pass2": false}, "src/MIPSolver/RelaxationStrategyNone.h": {"path": "layer-4/SHOT/src/MIPSolver/RelaxationStrategyNone.h", "filename": "RelaxationStrategyNone.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/RelaxationStrategyNone.h", "brief": "No-op relaxation strategy (always MIP)\n\nDisables LP relaxation phase, always solves full MIP.\n\n**RelaxationStrategyNone Class:**\n- executeStrategy(): No-op (MIP always active)\n- setActive()/setInactive()/setInitial(): State management\n\n**Use Case:**\n- Problems where LP phase doesn't help\n- When integer solutions are needed immediately\n- Benchmarking MIP-only performance\n\n**Behavior:**\n- Every iteration solves full MIP problem\n- No LP warm-start phase for hyperplane generation", "see": ["RelaxationStrategyStandard.h for LP-then-MIP"], "has_pass2": false}, "src/MIPSolver/MIPSolverCbc.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCbc.h", "filename": "MIPSolverCbc.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCbc.h", "brief": "COIN-OR Cbc implementation of IMIPSolver interface\n\nProvides open-source MIP solving using Cbc branch-and-cut solver.\n\n**MIPSolverCbc Class:**\n- Implements IMIPSolver interface\n- Uses OsiClpSolverInterface for LP subproblems\n- CbcModel for branch-and-cut\n- CoinModel for problem construction\n\n**Key Data Structures:**\n- osiInterface: OSI LP solver (Clp)\n- cbcModel: MIP solver model\n- coinModel: Problem builder\n- objectiveLinearExpression: CoinPackedVector\n\n**CbcMessageHandler:**\n- Custom message handler for SHOT logging\n- Routes Cbc output through SHOT's Output system\n\n**Limitations:**\n- supportsQuadraticObjective(): No\n- supportsQuadraticConstraints(): No\n- LP/MIP only (no MIQP/QCQP)\n\n@note Default open-source MIP solver, no license required", "see": ["Cbc branch-and-cut library"], "algorithm": "MIP Solution Pool (getAllVariableSolutions):\n  CBC can return multiple feasible integer solutions.\n  SHOT uses these as candidate points for NLP evaluation:\n  - More solutions = more chances to find primal feasible point\n  - Controlled by setSolutionLimit()", "math": "For binary x with solution x̂ ∈ {0,1}ⁿ:\n        ∑(x_j : x̂_j=1) - ∑(x_j : x̂_j=0) <= |{j: x̂_j=1}| - 1\n  Prevents cycling when NLP subproblem fails or solution is infeasible.", "complexity": "Each MIP solve is NP-hard; CBC uses B&C with CGL cuts", "ref": ["Duran & Grossmann (1986) - Outer Approximation for MINLP", "Kronqvist et al. (2019) - SHOT solver description", "Kelley's cutting-plane method (1960)", "Solution pool heuristics in MIP solvers"], "has_pass2": true}, "src/MIPSolver/IMIPSolver.h": {"path": "layer-4/SHOT/src/MIPSolver/IMIPSolver.h", "filename": "IMIPSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/IMIPSolver.h", "brief": "Abstract interface for MIP solver backends\n\nPure virtual interface for dual problem MIP solvers.\n\n**Problem Construction:**\n- initializeProblem(), finalizeProblem(): Setup/teardown\n- addVariable(): With type, bounds, and semi-bounds\n- addLinearTermToObjective/Constraint(): Build incrementally\n- addQuadraticTermToObjective/Constraint(): For MIQP/MIQCQP\n\n**Solution Methods:**\n- solveProblem(): Execute MIP solver\n- repairInfeasibility(): Attempt feasibility repair\n- getObjectiveValue(), getDualObjectiveValue(): Bounds\n- getVariableSolution(), getAllVariableSolutions(): Points\n\n**Hyperplane/Cut Management:**\n- createHyperplane(): Add supporting hyperplane linearization\n- createInteriorHyperplane(): Interior point cuts\n- createIntegerCut(): No-good cuts for integer variables\n\n**Bound Management:**\n- setCutOff(): Objective cutoff for pruning\n- fixVariable(), unfixVariables(): For integer fixing\n- presolveAndUpdateBounds(): Bound tightening\n\n**Implementations:**\n- MIPSolverCplex, MIPSolverGurobi, MIPSolverCbc\n- SingleTree variants for callback-based cut addition", "see": ["DualSolver.h for MIP solver orchestration", "IRelaxationStrategy.h for LP relaxation handling"], "algorithm": "MIP Start (Warm Starting) (addMIPStart):\n  Provide known feasible solution to MIP solver:\n  - From NLP subproblem solutions in outer approximation\n  - Improves primal bound immediately\n  - Guides B&B exploration toward good solutions", "math": "Prune node n if: LP(n) >= z* (current best solution)\n  Can be set as hard cutoff or as objective constraint.", "ref": ["Kronqvist et al. (2016) - Extended Supporting Hyperplane Algorithm", "Kelley (1960) - Cutting-plane method for convex programming", "Achterberg et al. (2007) - Conflict analysis in MIP solving", "Achterberg (2007) - SCIP: Solving Constraint Integer Programs"], "has_pass2": true}, "src/MIPSolver/MIPSolverBase.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverBase.h", "filename": "MIPSolverBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverBase.h", "brief": "Common base class for MIP solver implementations\n\nShared functionality for CPLEX, Gurobi, and Cbc backends.\n\n**Problem State:**\n- numberOfVariables, numberOfConstraints: Problem size\n- variableTypes, variableLowerBounds, variableUpperBounds\n- isMinimizationProblem, isProblemDiscrete\n\n**Hyperplane Creation:**\n- createHyperplane(): Add supporting hyperplane cut\n- createInteriorHyperplane(): Interior point cut\n- createHyperplaneTerms(): Compute cut coefficients\n- getConstraintIdentifier(): Named constraint IDs\n\n**Variable Fixing:**\n- fixVariable(), fixVariables(): Fix for NLP subproblem\n- unfixVariables(): Restore original bounds\n- updateVariableBound(): Dynamic bound update\n\n**Relaxation Strategy:**\n- relaxationStrategy: LP relaxation handling\n- executeRelaxationStrategy(): Apply relaxation\n\n**Dual Auxiliary Variable:**\n- Tracks epigraph variable for nonlinear objective\n- hasDualAuxiliaryObjectiveVariable(), getDualAuxiliaryObjectiveVariableIndex()", "see": ["MIPSolverCplex, MIPSolverGurobi, MIPSolverCbc"], "has_pass2": false}, "src/MIPSolver/RelaxationStrategyBase.h": {"path": "layer-4/SHOT/src/MIPSolver/RelaxationStrategyBase.h", "filename": "RelaxationStrategyBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/RelaxationStrategyBase.h", "brief": "Shared helper methods for relaxation strategies\n\nCommon termination checks for LP/MIP relaxation control.\n\n**RelaxationStrategyBase Class:**\n- isRelaxedSolutionInterior(): Check if LP solution is interior\n- isConstraintToleranceReached(): Feasibility achieved\n- isGapReached(): Optimality gap tolerance met\n\n**Use Case:**\n- Determines when to stop LP phase and switch to MIP\n- Checks convergence criteria during relaxation", "see": ["RelaxationStrategyStandard.h for concrete implementation"], "has_pass2": false}, "src/GAMS/GamsNLinstr.h": {"path": "layer-4/SHOT/src/GAMS/GamsNLinstr.h", "filename": "GamsNLinstr.h", "file": "GAMS/GamsNLinstr.h", "brief": "GAMS nonlinear instruction opcodes and function codes\n\nDefines GAMS bytecode for nonlinear expressions.\n\n**GamsOpCode Enum:**\n- Stack operations: nlPushV, nlPushI, nlStore\n- Arithmetic: nlAdd, nlSub, nlMul, nlDiv, nlUMin\n- Function calls: nlCallArg1, nlCallArg2, nlCallArgN\n\n**GamsFuncCode Enum:**\n- Math functions: fnexp, fnlog, fnsqrt, fnabs, fnsin, fncos\n- Comparison: fnmin, fnmax, fnifthen\n- Special: fnpower, fnsignpower, fnentropy\n\n@warning Bytecode format may change without notice", "see": ["ModelingSystemGAMS.h for bytecode interpretation"], "has_pass2": false}, "ThirdParty/eigen/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h": {"path": "layer-4/SHOT/ThirdParty/eigen/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h", "filename": "SyclMemoryModel.h", "brief": "Returns an accessor to the buffer of the given virtual pointer", "param": ["accessMode", "accessTarget", "ptr The virtual pointer", "accessMode", "accessTarget", "ptr The virtual pointer", "cgh Reference to the command group scope", "node A reference to the free node to be fused", "node A reference to the free node to be fused"], "has_pass2": false}}}, "Sonnet": {"name": "Sonnet", "file_count": 12, "pass2_count": 0, "files": {"src/SonnetWrapper/CoinMessageHandler.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CoinMessageHandler.h", "filename": "CoinMessageHandler.h", "file": "SonnetWrapper/CoinMessageHandler.h", "brief": ".NET wrapper for CoinMessageHandler (logging)\n\nC++/CLI wrapper for COIN-OR logging infrastructure.\nCoinMessageHandlerProxy bridges .NET virtual methods to native callbacks.\nCoinOneMessage and CoinMessages for custom message definitions.", "see": ["CoinMpsIO.h for file I/O using message handler"], "has_pass2": false}, "src/SonnetWrapper/CbcEventHandler.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CbcEventHandler.h", "filename": "CbcEventHandler.h", "file": "SonnetWrapper/CbcEventHandler.h", "brief": ".NET wrapper for CBC event handling\n\nCbcEvent enum (node, solution, treeStatus, endSearch, etc.) and\nCbcAction enum (noAction, stop, restart, killSolution).\nCbcDelegateEventHandlerProxy bridges .NET delegates to native callbacks.", "see": ["CbcModel.h for model wrapper", "CbcSolver.h for solver entry points"], "has_pass2": false}, "src/SonnetWrapper/ClpModel.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/ClpModel.h", "filename": "ClpModel.h", "file": "SonnetWrapper/ClpModel.h", "brief": ".NET wrapper for ClpModel (COIN-OR LP solver)\n\nC++/CLI wrapper exposing CLP simplex solver to .NET languages.\nSupports quadratic objectives via loadQuadraticObjective().", "see": ["ClpObjective.h, OsiClpSolverInterface.h"], "has_pass2": false}, "src/SonnetWrapper/CbcModel.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CbcModel.h", "filename": "CbcModel.h", "file": "SonnetWrapper/CbcModel.h", "brief": ".NET wrapper for CbcModel (COIN-OR Branch and Cut)\n\nC++/CLI wrapper exposing CBC MIP solver to .NET languages.\nWraps branch-and-bound with cut generation capabilities.", "see": ["CbcEventHandler.h, CbcCutGenerator.h, CbcStrategy.h"], "has_pass2": false}, "src/SonnetWrapper/CbcSolver.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CbcSolver.h", "filename": "CbcSolver.h", "file": "SonnetWrapper/CbcSolver.h", "brief": ".NET wrapper for CBC solver entry points (CbcMain0/CbcMain1)\n\nC++/CLI wrapper for CBC solver with callback support.\nCbcSolverCallBack delegate for monitoring solve phases.", "see": ["CbcModel.h for model wrapper", "CbcEventHandler.h for event handling"], "has_pass2": false}, "src/SonnetWrapper/CglCutGenerator.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CglCutGenerator.h", "filename": "CglCutGenerator.h", "file": "SonnetWrapper/CglCutGenerator.h", "brief": ".NET wrapper for CglCutGenerator (cut generator base class)\n\nC++/CLI wrapper exposing CGL cut generators to .NET languages.\nGeneric template CglCutGeneratorGeneric<T> for derived generators.", "see": ["CglProbing.h for probing cut generator", "CbcCutGenerator.h for CBC integration"], "has_pass2": false}, "src/SonnetWrapper/OsiSolverInterface.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/OsiSolverInterface.h", "filename": "OsiSolverInterface.h", "file": "SonnetWrapper/OsiSolverInterface.h", "brief": ".NET wrapper for OsiSolverInterface (Open Solver Interface)\n\nC++/CLI wrapper exposing OsiSolverInterface to .NET languages.", "see": ["ClpModel.h, CbcModel.h for solver wrappers"], "has_pass2": false}, "src/SonnetWrapper/OsiClpSolverInterface.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/OsiClpSolverInterface.h", "filename": "OsiClpSolverInterface.h", "file": "SonnetWrapper/OsiClpSolverInterface.h", "brief": ".NET wrapper for OsiClpSolverInterface (CLP via OSI)\n\nC++/CLI wrapper exposing CLP solver through OSI interface to .NET.\nIncludes matrix access methods and optional CPLEX/Gurobi support.", "see": ["ClpModel.h for direct CLP wrapper", "OsiSolverInterface.h for base OSI wrapper"], "has_pass2": false}, "src/SonnetWrapper/CoinPackedMatrix.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CoinPackedMatrix.h", "filename": "CoinPackedMatrix.h", "file": "SonnetWrapper/CoinPackedMatrix.h", "brief": ".NET wrapper for CoinPackedMatrix (sparse matrix)\n\nC++/CLI wrapper exposing CoinUtils sparse matrix to .NET languages.", "see": ["CoinShallowPackedVector.h for vector wrapper"], "has_pass2": false}, "src/SonnetWrapper/Helpers.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/Helpers.h", "filename": "Helpers.h", "file": "SonnetWrapper/Helpers.h", "brief": "Core wrapper infrastructure for Sonnet .NET bindings\n\nTemplate base classes for C++/CLI wrappers around native COIN-OR objects.\nWrapperAbstractBase<T> and WrapperBase<T> handle ownership and lifetime.", "see": ["OsiSolverInterface.h, ClpModel.h, CbcModel.h for concrete wrappers"], "has_pass2": false}, "src/SonnetWrapper/CoinMpsIO.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CoinMpsIO.h", "filename": "CoinMpsIO.h", "file": "SonnetWrapper/CoinMpsIO.h", "brief": ".NET wrapper for CoinMpsIO (MPS file I/O)\n\nC++/CLI wrapper for reading LP problems from MPS format files.\nExposes row/column bounds, objective coefficients, and matrix data.", "see": ["CoinPackedMatrix.h for sparse matrix wrapper", "CoinMessageHandler.h for custom logging"], "has_pass2": false}, "src/SonnetWrapper/OsiCbcSolverInterface.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/OsiCbcSolverInterface.h", "filename": "OsiCbcSolverInterface.h", "file": "SonnetWrapper/OsiCbcSolverInterface.h", "brief": ".NET wrapper for OsiCbcSolverInterface (CBC via OSI)\n\nC++/CLI wrapper exposing CBC MIP solver through OSI interface.\nProvides access to CbcModel for node count, strategy, and solution.", "see": ["CbcModel.h for CBC model wrapper", "OsiClpSolverInterface.h for CLP solver wrapper"], "has_pass2": false}}}, "cuopt": {"name": "cuopt", "file_count": 38, "pass2_count": 0, "files": {"cpp/src/math_optimization/solution_writer.hpp": {"path": "layer-4/cuopt/cpp/src/math_optimization/solution_writer.hpp", "filename": "solution_writer.hpp", "brief": "Writes a solution to a .sol file", "param": ["sol_file_path Path to the .sol file to write", "status Status of the solution", "objective_value Objective value of the solution", "variable_names Vector of variable names", "variable_values Vector of variable values"], "has_pass2": false}, "cpp/src/math_optimization/solution_reader.hpp": {"path": "layer-4/cuopt/cpp/src/math_optimization/solution_reader.hpp", "filename": "solution_reader.hpp", "brief": "Reads a solution file and returns the values of specified variables", "param": ["sol_file_path Path to the .sol file to read", "variable_names Vector of variable names to extract values for"], "return": "std::vector<double> Vector of values corresponding to the variable names", "has_pass2": false}, "cpp/src/routing/solver.hpp": {"path": "layer-4/cuopt/cpp/src/routing/solver.hpp", "filename": "solver.hpp", "brief": "A composable vehicle routing solver.", "tparam": ["i_t Integer type. int (32bit) is expected at the moment. Please\nopen an issue if other type are needed.", "f_t Floating point type. float (32bit) is expected at the moment.\nPlease open an issue if other type are needed."], "return": "assignment_t owning container for the solver output.", "has_pass2": false}, "cpp/src/routing/structures.hpp": {"path": "layer-4/cuopt/cpp/src/routing/structures.hpp", "filename": "structures.hpp", "brief": "Use encoded 32 bit integer with 2 bits for node type, 15 bits for\nnode and 15 bits for location\n@note If we bump it up to 64 bits, we need to make sure to aligned shared memory access", "tparam": ["i_t"], "has_pass2": false}, "cpp/src/routing/fleet_order_constraints.hpp": {"path": "layer-4/cuopt/cpp/src/routing/fleet_order_constraints.hpp", "filename": "fleet_order_constraints.hpp", "brief": "helper function to convert vehicle order match constraints to appropriate service times", "tparam": ["i_t", "f_t"], "param": ["data_model", "fleet_order_constraints_"], "has_pass2": false}, "cpp/src/routing/order_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/order_info.hpp", "filename": "order_info.hpp", "brief": "Helper function to construct order_info object from data model", "tparam": ["i_t", "f_t"], "param": ["data_model", "order_info"], "has_pass2": false}, "cpp/src/routing/fleet_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/fleet_info.hpp", "filename": "fleet_info.hpp", "brief": "helper function to construct the fleet_info object from data model", "tparam": ["i_t", "f_t", "i_t", "f_t"], "param": ["data_model", "fleet_info", "data_model", "fleet_info"], "has_pass2": false}, "cpp/src/linear_programming/saddle_point.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/saddle_point.hpp", "filename": "saddle_point.hpp", "brief": "Structure to hold the current solution to the saddle point problem", "tparam": ["f_t  Data type of the variables and their weights in the equations", "i_t  Data type of indexes", "i_t", "f_t"], "param": ["handle_ptr Pointer to library handle (RAFT) containing hardware resources\ninformation. A default handle is valid.", "primal_size The size of the primal problem", "dual_size The size of the dual problem\n\n@throws cuopt::logic_error if the problem sizes are not larger than 0.", "other saddle_point_state_t object from which the solution should be copied", "stream cuda stream used for the copying\n\n@pre Primal and dual solutions must be of the same size respectively\n\n@throws cuopt::logic_error if the solutions are not of the same size"], "has_pass2": false}, "cpp/src/utilities/strided_span.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/strided_span.hpp", "filename": "strided_span.hpp", "brief": "strided_span container provides a view that can only be accessed in a strided manner\nFor example, if we want to just access one row of a given matrix that is stored in a column\nmajor format, we would have to always calculate the offset to access each entry. This container\nabstracts out that by storing the stride and allows contiguous easy to use indexing", "has_pass2": false}, "cpp/src/utilities/copy_helpers.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/copy_helpers.hpp", "filename": "copy_helpers.hpp", "brief": "Simple utility function to copy device ptr to host", "tparam": ["T", "T", "T", "T", "T", "T", "T", "T", "T", "T"], "param": ["device_ptr", "size", "stream_view", "device_vec", "stream_view", "device_vec", "device_vec", "stream_view", "device_vec", "stream_view", "device_vec", "stream_view"], "return": "auto", "has_pass2": false}, "cpp/src/routing/generator/generator.hpp": {"path": "layer-4/cuopt/cpp/src/routing/generator/generator.hpp", "filename": "generator.hpp", "brief": "Container for allocated dataset.", "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed.", "f_t Floating point type. Needs to be float (32bit) at the moment."], "return": "dataset_t holding matrices, orders and vehicle info.", "has_pass2": false}, "cpp/src/linear_programming/restart_strategy/localized_duality_gap_container.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/restart_strategy/localized_duality_gap_container.hpp", "filename": "localized_duality_gap_container.hpp", "brief": "Gets the device-side view (with raw pointers), for ease of access\n       inside cuda kernels", "has_pass2": false}, "cpp/src/linear_programming/termination_strategy/infeasibility_information.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/termination_strategy/infeasibility_information.hpp", "filename": "infeasibility_information.hpp", "brief": "Gets the device-side view (with raw pointers), for ease of access\n       inside cuda kernels", "has_pass2": false}, "cpp/src/linear_programming/termination_strategy/convergence_information.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/termination_strategy/convergence_information.hpp", "filename": "convergence_information.hpp", "brief": "Gets the device-side view (with raw pointers), for ease of access\n       inside cuda kernels", "has_pass2": false}, "cpp/src/linear_programming/step_size_strategy/adaptive_step_size_strategy.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/step_size_strategy/adaptive_step_size_strategy.hpp", "filename": "adaptive_step_size_strategy.hpp", "brief": "A device-side view of the `adaptive_step_size_strategy_t` structure with the RAII stuffs\n       stripped out, to make it easy to work inside kernels\n\n@note It is assumed that the pointers are NOT owned by this class, but rather\n      by the encompassing `adaptive_step_size_strategy_t` class via RAII abstractions like\n      `rmm::device_uvector`", "has_pass2": false}, "cpp/include/cuopt/logger.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/logger.hpp", "filename": "logger.hpp", "file": "cuopt/logger.hpp", "brief": "Logging infrastructure for cuOpt based on rapids_logger\n\nProvides configurable logging for cuOpt operations.\n\n**Configuration:**\n- CUOPT_DEBUG_LOG_FILE env var: Log to file instead of stderr\n- Log levels: trace, debug, info, warn, error, critical\n\n**Functions:**\n- default_logger(): Get/create the global cuOpt logger\n- reset_default_logger(): Reset to default configuration\n- default_sink(): Get file or stderr sink\n- default_pattern(): Timestamp format string", "see": ["cuopt/logger_macros.hpp for logging macros"], "return": "sink_ptr The sink to use", "has_pass2": false}, "cpp/include/cuopt/error.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/error.hpp", "filename": "error.hpp", "file": "cuopt/error.hpp", "brief": "Error handling and exception types for cuOpt\n\nDefines error types and macros for cuOpt error handling.\n\n**Error Types (error_type_t):**\n- Success: Operation completed successfully\n- ValidationError: Input validation failed\n- OutOfMemoryError: GPU/CPU memory allocation failed\n- RuntimeError: General runtime error\n\n**Exception Class:**\n- logic_error: Exception with error_type_t classification\n\n**Macros:**\n- EXE_CUOPT_EXPECTS(cond, fmt, ...): Assert with formatted message\n- EXE_CUOPT_FAIL(fmt, ...): Unconditional failure", "see": ["cuopt/linear_programming/constants.h for error code constants"], "return": "Pointer to a null-terminated string with explanatory information.", "param": ["error error_type_t type enum value", "variable set of arguments used for fmt\n@throw cuopt::logic_error if the condition evaluates to false.", "args"], "tparam": ["Args"], "has_pass2": false}, "cpp/include/cuopt/routing/data_model_view.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/data_model_view.hpp", "filename": "data_model_view.hpp", "file": "cuopt/routing/data_model_view.hpp", "brief": "VRP problem definition via non-owning GPU memory views\n\nContainer for Vehicle Routing Problem input data.\n\n**data_model_view_t Class:**\n- Non-owning views to GPU memory for routing problem data\n- Supports heterogeneous fleet with multiple vehicle types\n- Configurable constraints and objectives\n\n**Cost/Time Matrices:**\n- add_cost_matrix(): Travel cost between locations\n- add_transit_time_matrix(): Travel time for constraint checking\n- Multiple matrices for heterogeneous fleet\n\n**Constraints:**\n- add_capacity_dimension(): Vehicle capacity limits (CVRP)\n- set_order_time_windows(): Delivery time windows (VRPTW)\n- set_pickup_delivery_pairs(): PDP pickup-before-delivery\n- add_order_precedence(): Order sequencing constraints\n- set_vehicle_time_windows(): Vehicle availability windows\n- add_vehicle_break(): Required driver breaks\n\n**Fleet Configuration:**\n- set_vehicle_locations(): Start/return depot per vehicle\n- set_vehicle_types(): Vehicle type identifiers\n- set_drop_return_trips(): Open-ended routes\n- add_vehicle_order_match(): Vehicle-order compatibility", "see": ["cuopt/routing/solve.hpp for solving", "cuopt/routing/assignment.hpp for solution output"], "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed.", "f_t Floating point type. Needs to be float (32bit) at the moment.\nPlease open an issue if other type are needed."], "param": ["num_locations number of locations to visit, including the depot.", "fleet_size number of vehicles in the fleet. This is primarily used\nto model vehicle properties in a mixed fleet context. cuOpt solution\ncontains the smallest possible number of vehicles which may be smaller or\nequal to the fleet_size.", "vehicle_id", "break_earliest", "break_latest", "break_duration", "break_locations", "num_break_locations", "validate_input", "vehicle_id  vehicle id that has constraints", "orders      device memory pointer to integer values corresponding to\nlist of orders", "norders     number of customer orders that are served by this\nvehicle", "order_id    order id that has constraints", "vehicles    device memory pointer to integer values corresponding to\nlist of vehicles", "nvehicles   number of vehicles that can serve this order", "vehicle_id  vehicle id that has constraints", "service_times      device memory pointer to integer values\ncorresponding to list of service_times", "order_id Order id that has a precedence constraint.", "preceding_orders The orders that need to be scheduled prior to\nnode_id", "n_preceding_orders Number of prior orders."], "return": "Matrix pointer", "has_pass2": false}, "cpp/include/cuopt/routing/routing_structures.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/routing_structures.hpp", "filename": "routing_structures.hpp", "file": "cuopt/routing/routing_structures.hpp", "brief": "Core data structures for VRP problem specification\n\nEnums and helper classes for VRP constraint modeling.\n\n**Objective Types (objective_t):**\n- COST: Total route cost from cost matrix\n- TRAVEL_TIME: Total driving time (excluding waits)\n- VARIANCE_ROUTE_SIZE: Balance route lengths\n- VARIANCE_ROUTE_SERVICE_TIME: Balance service times\n- PRIZE: Sum of collected order prizes\n- VEHICLE_FIXED_COST: Per-vehicle usage cost\n\n**Node Types (node_type_t):**\n- DEPOT: Start/end location for vehicles\n- PICKUP: Pickup location for PDP\n- DELIVERY: Delivery location for PDP\n- BREAK: Driver break location\n\n**Internal Detail Classes:**\n- break_dimension_t: Uniform break constraints\n- vehicle_break_t: Per-vehicle break constraints\n- capacity_t: Capacity dimension (demand/capacity pair)\n- order_time_window_t: Time window bounds\n- vehicle_time_window_t: Vehicle availability bounds", "see": ["cuopt/routing/data_model_view.hpp for using these structures"], "has_pass2": false}, "cpp/include/cuopt/routing/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "cuopt/routing/solver_settings.hpp", "brief": "Configuration options for VRP solver behavior\n\nControls solver runtime behavior and output options.\n\n**solver_settings_t Class:**\n- set_time_limit(): Maximum solving time in seconds\n- set_verbose_mode(): Enable internal progress output\n- set_error_logging_mode(): Log constraint violations\n- dump_best_results(): Write progress to CSV file\n\n**Time Limit Guidelines:**\n- Small problems (<100 locations): ~1 second\n- Large problems: num_locations/5 seconds default\n- More time → better solution quality", "see": ["cuopt/routing/solve.hpp for solver entry point"], "return": "Solving time set in seconds", "has_pass2": false}, "cpp/include/cuopt/routing/solve.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/solve.hpp", "filename": "solve.hpp", "file": "cuopt/routing/solve.hpp", "brief": "GPU-accelerated Vehicle Routing Problem (VRP) solver entry point\n\nMain solve function for vehicle routing optimization on GPU.\n\n**solve() Function:**\n- Takes data_model_view_t (problem) and solver_settings_t (config)\n- Returns assignment_t with optimized routes\n\n**Supported VRP Variants:**\n- CVRP: Capacitated VRP with vehicle capacity constraints\n- VRPTW: VRP with time windows at customer locations\n- VRPPD: VRP with pickup and delivery pairs\n- Heterogeneous fleet: Different vehicle types/costs\n- Multi-depot: Multiple start/return locations\n\n**Algorithm:**\n- Genetic algorithm with GPU-parallel population\n- EAX/SREX crossover operators\n- Local search refinement", "see": ["cuopt/routing/data_model_view.hpp for problem input", "cuopt/routing/solver_settings.hpp for configuration", "cuopt/routing/assignment.hpp for solution output"], "tparam": ["i_t", "f_t"], "return": "assignment_t<i_t> owning container for the solver output", "has_pass2": false}, "cpp/include/cuopt/routing/assignment.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/assignment.hpp", "filename": "assignment.hpp", "file": "cuopt/routing/assignment.hpp", "brief": "VRP solution output container with routes and timing\n\nContainer for Vehicle Routing Problem solver output.\n\n**assignment_t Class:**\n- Optimized routes stored in GPU device memory\n- Arrival times, truck assignments, node types\n- Objective values and solution status\n\n**Solution Data:**\n- get_route(): Ordered sequence of node IDs\n- get_truck_id(): Vehicle assignment per stop\n- get_arrival_stamp(): Arrival time at each stop\n- get_unserviced_nodes(): Orders not serviced (if any)\n\n**Solution Status (solution_status_t):**\n- SUCCESS: Feasible solution found\n- INFEASIBLE: No feasible solution exists\n- TIMEOUT: Time limit reached\n- EMPTY: Solver did not run\n- ERROR: Runtime error occurred\n\n**host_assignment_t:**\n- Host-side copy for CPU access\n- std::vector storage for routes and stamps", "see": ["cuopt/routing/solve.hpp for generating solutions", "cuopt/routing/data_model_view.hpp for problem input"], "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed."], "param": ["status Solution status.", "stream_view Non-owning stream_view object.", "error_status Error status.", "stream_view Non-owning stream_view object.", "vehicle_count Number of vehicles in the solution.", "total_objective_value Total objective value of the solution.", "objective_values Objective value of each objective", "route Device vector containing the ordered node ids.", "arrival_stamp Device vector containing arrival time of each node.", "truck_id Device vector containing truck id of each node.", "route_locations Device vector containing the location of orders", "node_type Device vector containing the type of the order", "unserviced_nodes Device vector containing unserviced orders", "accepted Device vector containing accepted solutions", "status Solution status.", "solution_string Solution string explaining the status.", "filename Name of the output file", "stream_view Non-owning stream view object", "vehicle_count Vehicle count", "status Solution status", "os Output stream to print"], "return": "Best objective value", "has_pass2": false}, "cpp/include/cuopt/linear_programming/optimization_problem.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/optimization_problem.hpp", "filename": "optimization_problem.hpp", "file": "cuopt/linear_programming/optimization_problem.hpp", "brief": "GPU-resident linear/integer programming problem representation\n\nContainer for LP/MIP optimization problems stored in GPU memory.\n\n**optimization_problem_t Class:**\n- Represents: min/max c'x s.t. Ax ≤/=/≥ b, l ≤ x ≤ u\n- Constraint matrix A in CSR (Compressed Sparse Row) format\n- GPU device memory via rmm::device_uvector\n\n**Problem Components:**\n- set_csr_constraint_matrix(): Sparse constraint matrix\n- set_constraint_bounds(): RHS values (b)\n- set_objective_coefficients(): Cost vector (c)\n- set_variable_lower/upper_bounds(): Variable bounds\n- set_variable_types(): CONTINUOUS or INTEGER\n\n**Variable Types:**\n- var_t::CONTINUOUS: Real-valued variable\n- var_t::INTEGER: Integer-valued variable\n\n**Problem Categories:**\n- LP: All continuous variables\n- MIP: Mixed integer and continuous\n- IP: All integer variables", "see": ["cuopt/linear_programming/solve.hpp for solve functions"], "tparam": ["f_t  Data type of the variables and their weights in the equations\n\nThis structure stores all the information necessary to represent the\nfollowing LP:\n\n<pre>\nMinimize:\n  dot(c, x)\nSubject to:\n  matmul(A, x) (= or >= or)<= b\nWhere:\n  x = n-dim vector\n  A = mxn-dim sparse matrix\n  n = number of variables\n  m = number of constraints\n\n</pre>\n\n@note: By default this assumes objective minimization.\n\nObjective value can be scaled and offset accordingly:\nobjective_scaling_factor * (dot(c, x) + objective_offset)\nplease refer to the `set_objective_scaling_factor()` and\n`set_objective_offset()` methods."], "param": ["size_values Size of the A_values array.", "size_indices Size of the A_indices array.", "size_offsets Size of the A_offsets array.", "size Size of the b array.", "size Size of the c array.", "objective_scaling_factor Objective scaling factor value.", "objective_offset Objective offset value.", "size Size of the variable_lower_bounds array", "size Size of the variable_upper_bounds array.", "size Size of the variable_types array.", "size Size of the constraint_lower_bounds array", "size Size of the constraint_upper_bounds array", "size Size of the row_types array"], "has_pass2": false}, "cpp/include/cuopt/linear_programming/solve.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/solve.hpp", "filename": "solve.hpp", "file": "cuopt/linear_programming/solve.hpp", "brief": "GPU-accelerated LP and MIP solving functions\n\nMain entry points for linear programming optimization on GPU.\n\n**LP Solving (PDLP):**\n- solve_lp(): GPU-accelerated first-order LP solver\n- Uses Primal-Dual Hybrid Gradient (PDHG) algorithm\n- Supports warm starting from previous solutions\n\n**MIP Solving:**\n- solve_mip(): Mixed-integer programming with GPU B&B\n- Branch-and-bound with LP relaxations\n\n**Input Formats:**\n- optimization_problem_t: Native cuOpt problem representation\n- mps_data_model_t: MPS file format parser output", "see": ["cuopt/linear_programming/optimization_problem.hpp for problem definition", "cuopt/linear_programming/pdlp/solver_settings.hpp for LP settings", "cuopt/linear_programming/mip/solver_settings.hpp for MIP settings"], "tparam": ["i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations", "i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations", "i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations", "i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations"], "return": "optimization_problem_solution_t<i_t, f_t> owning container for the solver solution", "has_pass2": false}, "cpp/include/cuopt/utilities/timestamp_utils.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/utilities/timestamp_utils.hpp", "filename": "timestamp_utils.hpp", "brief": "Check if extra timestamps should be printed based on environment variable\n\nChecks the CUOPT_EXTRA_TIMESTAMPS environment variable once and caches the result.\nReturns true if the environment variable is set to \"True\", \"true\", or \"1\".", "return": "true if extra timestamps are enabled, false otherwise", "param": ["label The label to print with the timestamp"], "has_pass2": false}, "cpp/include/cuopt/routing/distance_engine/waypoint_matrix.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/distance_engine/waypoint_matrix.hpp", "filename": "waypoint_matrix.hpp", "brief": "A waypoint matrix.\n\nThis class has two purposes:\n\nWhen the cost matrix is not acquirable due to an\nincomplete graph, the latter can be passed to waypoint matrix.\nThe waypoint matrix can then return a cost matrix that can be used by the solver.\n\nThe waypoint matrix can also generate the full path between target locations.\nThe full path represents the set of all vertices (with respect to the graph)\nto go from one target location to another.", "tparam": ["i_t Integer type. int (32bit) is expected at the moment. Please\nopen an issue if other type are needed.", "f_t Floating point type. float (32bit) is expected at the moment.\nPlease open an issue if other type are needed."], "param": ["n_vertices Number of vertices", "n_target_locations Number of target locations.", "n_target_locations Number of target locations", "n_locations Number of locations", "n_target_locations Number of target locations."], "return": "std::pair<rmm::device_uvector<int>, rmm::device_uvector<int>> First is a device buffer\nof size L (L : n_locations) containing an array of offsets. Second is a device buffer\ncontaining the full path for all the route. For each element in the route, the corresponding\nfull route can accessed in the full path through the offsets array.", "has_pass2": false}, "cpp/include/cuopt/routing/cython/generator.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/cython/generator.hpp", "filename": "generator.hpp", "brief": "Container for dataset parameters.\n@note Current generator provides a number of orders equal to\nthe number of locations so only n_locations is specified.", "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed.", "f_t Floating point type. Needs to be float (32bit) at the moment."], "has_pass2": false}, "cpp/include/cuopt/linear_programming/pdlp/solver_solution.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/pdlp/solver_solution.hpp", "filename": "solver_solution.hpp", "brief": "A container of PDLP solver output", "tparam": ["i_t Integer type. Currently only int is supported.", "f_t Floating point type. Currently only float (32bit) and double (64bit) are supported."], "param": ["ms Time in ms", "termination_status termination reason", "filename Name of the output file", "stream_view Non-owning stream view object", "filename Name of the output file", "stream_view Non-owning stream view object", "handle_ptr The handle pointer", "other The other solution object"], "return": "Time in seconds", "has_pass2": false}, "cpp/include/cuopt/linear_programming/pdlp/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/pdlp/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "cuopt/linear_programming/pdlp/solver_settings.hpp", "brief": "PDLP first-order LP solver configuration\n\nSettings for GPU-accelerated Primal-Dual Hybrid Gradient LP solver.\n\n**pdlp_solver_mode_t Modes:**\n- Stable3: Best overall balance (default)\n- Stable2: Legacy stable mode\n- Methodical1: Slower steps but fewer iterations, 1.3-1.7x memory\n- Fast1: Highest speed, lower convergence success\n\n**pdlp_solver_settings_t Class:**\n- set_solver_mode(): Select algorithm mode\n- set_eps_optimal(): Primal/dual convergence tolerance\n- set_time_limit(): Maximum solve time\n- set_warm_start(): Initialize from previous solution\n\n**Algorithm (PDHG):**\n- First-order method, highly parallelizable on GPU\n- O(1/k) convergence for LP\n- Efficient for large sparse problems", "see": ["cuopt/linear_programming/solve.hpp for solve_lp()"], "param": ["eps_optimal Tolerance to optimality", "size Size of the initial_primal_solution array.", "size Size of the initial_dual_solution array.", "pdlp_warm_start_data_view Pdlp warm start data from your solution\nobject to warm start from", "var_mapping Variables indices to scatter to in case the new problem\nhas less variables", "constraint_mapping Constraints indices to scatter to in case the new\nproblem has less constraints"], "return": "pdlp warm start data", "has_pass2": false}, "cpp/include/cuopt/linear_programming/mip/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/mip/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "cuopt/linear_programming/mip/solver_settings.hpp", "brief": "Mixed-integer programming solver configuration\n\nSettings for GPU-accelerated MIP solver.\n\n**mip_solver_settings_t Class:**\n- set_mip_callback(): User callback for incumbent solutions\n- set_time_limit(): Maximum solve time\n- set_mip_gap(): Optimality gap tolerance\n- set_verbosity(): Output level control\n\n**MIP Algorithm:**\n- Branch-and-bound with GPU LP relaxations\n- Population-based heuristics\n- Recombination operators for primal improvement", "see": ["cuopt/linear_programming/solve.hpp for solve_mip()", "cuopt/linear_programming/mip/solver_solution.hpp for output"], "param": ["size Size of the initial_solution array."], "return": "callback pointer", "has_pass2": false}, "cpp/libmps_parser/src/mps_parser.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/src/mps_parser.hpp", "filename": "mps_parser.hpp", "brief": "Different possible types of 'ROWS'", "tparam": ["f_t  data type of the weights and variables\n\n@note this parser assumes that the sections occur in the following order:\n      `NAME -> ROWS -> COLUMNS -> RHS`"], "has_pass2": false}, "cpp/libmps_parser/src/utilities/error.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/src/utilities/error.hpp", "filename": "error.hpp", "brief": "Indicates different type of exceptions which mps parser might throw", "param": ["error error_type_t type enum value", "variable set of arguments used for fmt\n@throw std::logic_error if the condition evaluates to false.", "variable set of arguments used for fmt\n@throw std::logic_error if the condition evaluates to false."], "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/parser.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/parser.hpp", "filename": "parser.hpp", "brief": "Reads the equation from an MPS or QPS file.\n\nThe input file can be a plain text file in MPS-/QPS-format or a compressed MPS/QPS\nfile (.mps.gz or .mps.bz2).\n\nRead this link http://lpsolve.sourceforge.net/5.5/mps-format.htm for more\ndetails on both free and fixed MPS format.\nThis function supports both standard MPS files (for linear programming) and\nQPS files (for quadratic programming). QPS files are MPS files with additional\nsections:\n- QUADOBJ: Defines quadratic terms in the objective function\n\nNote: Compressed MPS files .mps.gz, .mps.bz2 can only be read if the compression\nlibraries zlib or libbzip2 are installed, respectively.", "return": "mps_data_model_t A fully formed LP/QP problem which represents the given file", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/data_model_view.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/data_model_view.hpp", "filename": "data_model_view.hpp", "brief": "A representation of a linear programming (LP) optimization problem", "tparam": ["f_t  Data type of the variables and their weights in the equations\n\nA linear programming optimization problem is defined as follows:\n<pre>\nMinimize:\n  dot(c, x)\nSubject to:\n  matmul(A, x) (= or >= or)<= b\nWhere:\n  x = n-dim vector\n  A = mxn-dim sparse matrix\n  n = number of variables\n  m = number of constraints\n\n</pre>\n\n@note: By default this assumes objective minimization.\n\nObjective value can be scaled and offset accordingly:\nobjective_scaling_factor * (dot(c, x) + objective_offset)\nplease refeto to the `set_objective_scaling_factor()` and `set_objective_offset()` method."], "param": ["size_values Size of the A_values array.", "size_indices Size of the A_indices array.", "size_offsets Size of the A_offsets array.", "size Size of the b array.", "size Size of the c array.", "objective_scaling_factor Objective scaling factor value.", "objective_offset Objective offset value.", "size Size of the variable_lower_bounds array", "size Size of the variable_upper_bounds array.", "size Size of the variable_types array.", "size Size of the row_types array", "size Size of the row_types array", "size Size of the row_types array", "size Size of the initial_primal_solution array.", "size Size of the initial_dual_solution array.", "size_values Size of the Q_values array", "size_indices Size of the Q_indices array", "size_offsets Size of the Q_offsets array"], "return": "Sense value", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/writer.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/writer.hpp", "filename": "writer.hpp", "brief": "Writes the problem to an MPS formatted file\n\nRead this link http://lpsolve.sourceforge.net/5.5/mps-format.htm for more\ndetails on both free and fixed MPS format.", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/mps_writer.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/mps_writer.hpp", "filename": "mps_writer.hpp", "brief": "Main writer class for MPS files", "tparam": ["f_t  data type of the weights and variables", "i_t  data type of the indices"], "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/mps_data_model.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/mps_data_model.hpp", "filename": "mps_data_model.hpp", "brief": "A representation of a linear programming (LP) optimization problem", "tparam": ["f_t  Data type of the variables and their weights in the equations\n\nA linear programming optimization problem is defined as follows:\n<pre>\nMinimize:\n  dot(c, x)\nSubject to:\n  matmul(A, x) (= or >= or)<= b\nWhere:\n  x = n-dim vector\n  A = mxn-dim sparse matrix\n  n = number of variables\n  m = number of constraints\n\n</pre>\n\n@note: By default this assumes objective minimization.\n\nObjective value can be scaled and offset accordingly:\nobjective_scaling_factor * (dot(c, x) + objective_offset)\nplease refer to the `set_objective_scaling_factor()` and `set_objective_offset()` methods."], "param": ["size_values Size of the A_values array.", "size_indices Size of the A_indices array.", "size_offsets Size of the A_offsets array.", "size Size of the b array.", "size Size of the c array.", "objective_scaling_factor Objective scaling factor value.", "objective_offset Objective offset value.", "size Size of the variable_lower_bounds array", "size Size of the variable_upper_bounds array.", "size Size of the constraint_lower_bounds array", "size Size of the constraint_upper_bounds array", "size Size of the row_types array", "size Size of the initial_primal_solution array.", "size Size of the initial_dual_solution array.", "size_values Size of the Q_values array", "size_indices Size of the Q_indices array", "size_offsets Size of the Q_offsets array"], "has_pass2": false}, "cpp/include/cuopt/linear_programming/cuopt_c.h": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/cuopt_c.h", "filename": "cuopt_c.h", "brief": "A ``cuOptOptimizationProblem`` object contains a representation of\nan LP or MIP. It is created by ``cuOptCreateProblem`` or ``cuOptCreateRangedProblem``.\nIt is passed to ``cuOptSolve``. It should be destroyed using ``cuOptDestroyProblem``.", "return": "The size in bytes of the float type.", "has_pass2": false}}}}}}, "stats": {"total_layers": 5, "total_libraries": 28, "total_files": 1197, "total_pass2_files": 89}}