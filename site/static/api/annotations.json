{"version": "1.0", "generated": "2025-12-05T02:06:21.692817", "project": "coin-or-knowledge-base", "description": "Annotated optimization solver library documentation", "layers": {"layer-0": {"name": "layer-0", "library_count": 2, "libraries": {"CoinUtils": {"name": "CoinUtils", "file_count": 68, "pass2_count": 46, "files": {"src/CoinPresolveZeros.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveZeros.hpp", "filename": "CoinPresolveZeros.hpp", "file": "CoinPresolveZeros.hpp", "brief": "Drop and reintroduce explicit zero coefficients", "algorithm": "Explicit Zero Removal:\n  Cleans matrix by removing zero coefficients:\n  1. Scan specified columns for zero coefficients (|a_ij| < tolerance)\n  2. Record (row, col) positions of zeros for postsolve\n  3. Remove zeros from sparse storage\n  4. Update row/column counts and pointers\n  5. Postsolve: Reinsert explicit zeros at recorded positions", "math": "Why explicit zeros exist:\n  From numerical cancellation during presolve transformations\n  Or from user input with sparse format containing zeros\n  Removing improves efficiency of subsequent operations", "complexity": "O(nnz in checked columns) to scan and remove\n  Memory: O(nzeros) to store positions for postsolve\n  Called after transformations that may introduce zeros", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinBronKerbosch.hpp": {"path": "layer-0/CoinUtils/src/CoinBronKerbosch.hpp", "filename": "CoinBronKerbosch.hpp", "file": "CoinBronKerbosch.hpp", "brief": "Bron-Kerbosch Algorithm for maximal clique enumeration", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Bron-Kerbosch with pivoting for weighted cliques:\n  BK(R, P, X):  R = clique being built, P = candidates, X = excluded\n  1. If P ∪ X = ∅: report R as maximal clique\n  2. Choose pivot u ∈ P ∪ X maximizing |N(u) ∩ P|\n  3. For each v ∈ P \\ N(u):\n     - BK(R ∪ {v}, P ∩ N(v), X ∩ N(v))\n     - P := P \\ {v}; X := X ∪ {v}\n  Pivoting prunes branches that cannot produce new maximal cliques.", "math": "Clique: complete subgraph where all vertices are pairwise adjacent.\n  Weight threshold: only enumerate cliques with Σw(v) > minWeight.\n  Pivoting strategies: degree, weight, or modified degree×weight.", "complexity": "Time: O(3^(n/3)) worst case for all maximal cliques\n  With pivoting: much faster in practice, especially for sparse graphs.\n  Space: O(n) for recursion stack", "ref": ["Bron, Kerbosch (1973). \"Algorithm 457: Finding all cliques of an\n     undirected graph\". CACM 16(9):575-577.", "Tomita, Tanaka, Takahashi (2006). \"The worst-case time complexity\n     for generating all maximal cliques\". Theoretical Computer Science.\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["cgraph conflict graph", "weights array containing the weights for each vertex", "pivotingStrategy pivoting strategy used in BK algorithm.", "array containing the weights for each vertex"], "has_pass2": true}, "src/CoinCliqueList.hpp": {"path": "layer-0/CoinUtils/src/CoinCliqueList.hpp", "filename": "CoinCliqueList.hpp", "file": "CoinCliqueList.hpp", "brief": "Sequential storage for cliques found in conflict graphs", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Clique Storage:\n  Stores cliques as variable-length arrays with shared element pool\n  Operations:\n  - addClique(size, elements[]): append new clique\n  - cliqueElements(idx): retrieve clique by index\n  - computeNodeOccurrences: build inverse index (node → cliques)\n\n  Inverse index enables efficient lookup of which cliques contain a node", "math": "Clique C ⊆ V is complete subgraph: ∀u,v ∈ C, (u,v) ∈ E\n  Used in conflict graphs where edges represent incompatibility\n  Clique inequality: Σ_{i∈C} x_i ≤ 1 (at most one var in clique can be 1)", "complexity": "addClique: O(clique_size) for copying elements\n  cliqueElements/cliqueSize: O(1) random access\n  computeNodeOccurrences: O(total_elements) one-time setup\n  nNodeOccurrences: O(1) after setup", "see": ["CoinBronKerbosch for clique enumeration algorithm", "CoinConflictGraph for the underlying conflict structure\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["_iniClqCap initial capacity to store cliques", "_iniClqElCap initial capacity of the elements\nof the cliques", "idxClq index of a clique.", "idxClq index of a clique.", "idxNode index of the node", "idxNode index of the node", "cgraph conflict graph", "idxs indexes of the clique", "size size of the clique"], "has_pass2": true}, "src/CoinPresolveSubst.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveSubst.hpp", "filename": "CoinPresolveSubst.hpp", "file": "CoinPresolveSubst.hpp", "brief": "Variable substitution from equality constraints", "algorithm": "Variable Substitution (Non-Singleton):\n  For implied free variable x_j in equality constraint i:\n  1. Solve constraint i for x_j: x_j = (b_i - Σ_{k≠j} a_ik x_k) / a_ij\n  2. Substitute x_j expression into all other rows containing x_j\n  3. After substitution, x_j becomes column singleton\n  4. Apply implied_free_action to remove x_j and row i\n\n  Fill-in control: only substitute if total nnz doesn't increase too much", "math": "Substitution creates fill-in: originally empty positions become nonzero\n  Fill estimate: (nnz(col_j) - 1) × (nnz(row_i) - 1) new elements\n  Controlled by fill_level parameter to prevent explosion", "complexity": "Time: O(nnz(col_j) × nnz(row_i)) per substitution\n  Can significantly reduce problem when fill-in is controlled\n  Postsolve must undo all substitutions in reverse order", "see": ["CoinPresolveImpliedFree for singleton case", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinDynamicConflictGraph.hpp": {"path": "layer-0/CoinUtils/src/CoinDynamicConflictGraph.hpp", "filename": "CoinDynamicConflictGraph.hpp", "algorithm": "Dynamic Conflict Graph for MIP\n\nCoinConflictGraph implementation which supports modifications.\nFor a static conflict graph implementation with faster queries\ncheck CoinStaticConflictGraph.\n\n**Conflict Detection:**\nTwo binary variables x_i and x_j conflict if they cannot both be 1.\nDetected from constraints like: a_i x_i + a_j x_j ≤ b where\na_i + a_j > b (both coefficients exceed RHS).\n\n**Clique Detection:**\nGroups of mutually conflicting variables form cliques. Large cliques\nstored explicitly to save memory; small cliques expanded to edges.\n\n**Bound Tightening:**\nDuring construction, may discover tighter variable bounds from\nconstraint analysis. Access via updatedBounds().", "math": "Conflict: x_i + x_j ≤ 1 implies x_i and x_j cannot both be 1", "complexity": "Query O(degree) for adjacency lists", "ref": ["Atamtürk, Nemhauser & Savelsbergh, \"Conflict graphs in solving\n     integer programming problems\", EJOR 121(1), 2000"], "file": "CoinDynamicConflictGraph.hpp", "brief": "CoinConflictGraph implementation which supports modifications.", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["_size number of vertices of the\nconflict graph", "numCols number of variables", "colType column types", "colLB column lower bounds", "colUB column upper bounds", "matrixByRow row-wise constraint matrix", "sense row sense", "rowRHS row right hand side", "rowRange row ranges", "node index", "nodeConflicts conflicts to be added", "nConflicts number of conflicts to be added", "idxNode index of the node", "deg degree of the node", "idxNode index of the node", "deg degree of the node"], "return": "a vector of updated bounds with the format (idx, (lb, ub))", "has_pass2": true}, "src/CoinDistance.hpp": {"path": "layer-0/CoinUtils/src/CoinDistance.hpp", "filename": "CoinDistance.hpp", "file": "CoinDistance.hpp", "brief": "Platform-independent iterator distance utilities\n\nProvides wrapper functions around std::distance to ensure consistent\nbehavior across different compilers and platforms. These utilities are\nused throughout COIN-OR for iterator-based calculations.", "tparam": ["ForwardIterator Iterator type (must satisfy ForwardIterator requirements)", "Distance Integral type to store the result", "ForwardIterator Iterator type (must satisfy ForwardIterator requirements)"], "param": ["first Iterator to the beginning of the range", "last Iterator to the end of the range", "first Iterator to the beginning of the range", "last Iterator to the end of the range"], "see": ["coinDistance(ForwardIterator, ForwardIterator)", "coinDistance(ForwardIterator, ForwardIterator, Distance&)"], "return": "Number of elements between first and last", "has_pass2": false}, "src/CoinUtility.hpp": {"path": "layer-0/CoinUtils/src/CoinUtility.hpp", "filename": "CoinUtility.hpp", "file": "CoinUtility.hpp", "brief": "Factory functions for CoinPair and CoinTriple\n\nProvides convenience functions for creating CoinPair and CoinTriple objects\nwith automatic template argument deduction, similar to std::make_pair.", "see": ["CoinSort.hpp for CoinPair and CoinTriple definitions", "CoinPair, CoinMakeTriple()", "CoinTriple, CoinMakePair()"], "tparam": ["S Type of the first element", "T Type of the second element", "S Type of the first element", "T Type of the second element", "U Type of the third element"], "param": ["s First element value", "t Second element value", "s First element value", "t Second element value", "u Third element value"], "return": "A CoinPair containing copies of s and t", "has_pass2": false}, "src/CoinCliqueSet.hpp": {"path": "layer-0/CoinUtils/src/CoinCliqueSet.hpp", "filename": "CoinCliqueSet.hpp", "algorithm": "Hash-Based Clique Set with Deduplication\n\nA class to store a set of cliques. It is an\nextension of class CoinCliqueList.\n\n**Deduplication:**\nUses polynomial rolling hash to detect duplicate cliques.\nCliques are sorted before hashing to ensure canonical form.\nHash collisions resolved via bucket chains with exact comparison.\n\n**Use case:**\nDuring clique enumeration and separation, same clique may be\ngenerated multiple times. This class efficiently filters duplicates.", "math": "Hash: h(C) = Σᵢ (C[i] × sequence_[i mod n]) mod nBuckets_", "complexity": "O(1) amortized with good hash distribution", "file": "CoinCliqueSet.hpp", "brief": "Set of cliques", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["_iniClqCap initial capacity to store cliques", "_iniClqElCap initial capacity of the elements\nof the cliques", "size size of the clique to be added", "els indexes of the clique to be added", "size size of the clique", "els indexes of the clique", "size size of the clique", "els indexes of the clique", "hashCode hash value of\nthe clique (call method vectorHashCode)"], "has_pass2": true}, "src/CoinLpIO.hpp": {"path": "layer-0/CoinUtils/src/CoinLpIO.hpp", "filename": "CoinLpIO.hpp", "file": "CoinLpIO.hpp", "brief": "LP file format reader/writer for linear programming problems\n\nReads/writes CPLEX LP format with Min/Max objective, constraints,\nbounds, and integer/binary variable declarations.", "algorithm": "LP File Parsing:\n  Sections: Minimize/Maximize, Subject To, Bounds, Integers/Generals/Binaries, End\n  Algebraic format: constraint_name: coeff1*var1 + coeff2*var2 <= rhs\n\n  Constraint types: <= (L), >= (G), = (E)\n  Bound keywords: Free, Inf, -Inf\n  Variable types: Integers, Generals (same), Binaries, Semi-Continuous", "math": "Reads: min/max c'x s.t. Ax ≤ b, Ax ≥ b, Ax = b\n  Bounds: lb ≤ x ≤ ub or x free\n  Integrality: x ∈ Z or x ∈ {0,1}", "complexity": "Parsing: O(file_size), token-based lexer\n  More human-readable than MPS but less standardized", "see": ["CoinMpsIO for MPS file format support", "CoinFileIO for underlying file I/O abstraction"], "has_pass2": true}, "src/CoinPragma.hpp": {"path": "layer-0/CoinUtils/src/CoinPragma.hpp", "filename": "CoinPragma.hpp", "file": "CoinPragma.hpp", "brief": "Compiler-specific pragma settings (mainly MSVC warnings)", "has_pass2": false}, "src/CoinNodeHeap.hpp": {"path": "layer-0/CoinUtils/src/CoinNodeHeap.hpp", "filename": "CoinNodeHeap.hpp", "file": "CoinNodeHeap.hpp", "brief": "Monotone min-heap for Dijkstra's algorithm", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Monotone Min-Heap:\n  Binary heap with decreasing-key only (no arbitrary updates).\n  Heap property: parent.cost ≤ children.cost\n  Operations:\n  - update(node, newCost): decrease key, bubble up (newCost < oldCost)\n  - removeFirst(): extract minimum, bubble down replacement\n  - isEmpty(): check if all nodes at infinity", "math": "Used in Dijkstra where distances only decrease.\n  Monotonicity allows simpler implementation than general heap.\n  Position array enables O(1) node lookup for decrease-key.", "complexity": "update: O(log n) for bubble-up\n  removeFirst: O(log n) for bubble-down\n  isEmpty: O(1)\n  Space: O(n) for heap + position array", "see": ["CoinShortestPath for Dijkstra implementation using this heap\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["node used to store the element that was removed"], "has_pass2": true}, "src/CoinPresolveFixed.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveFixed.hpp", "filename": "CoinPresolveFixed.hpp", "file": "CoinPresolveFixed.hpp", "brief": "Remove fixed variables and make variables fixed", "algorithm": "Fixed Variable Removal:\n  For variable x_j with l_j = u_j = v (fixed):\n  1. Substitute x_j = v into all constraints: b_i := b_i - a_ij·v\n  2. Remove column j from constraint matrix\n  3. Adjust objective constant: z += c_j·v\n  4. Store column for postsolve restoration\n\n  make_fixed_action: Force bounds equal to fix variable\n  remove_fixed_action: Remove already-fixed variable from matrix", "math": "Substitution: Ax = b with x_j = v becomes A'x' = b - a_j·v\n  where A' is A without column j, x' is x without x_j\n  Postsolve: restore x_j = v in solution", "complexity": "Time: O(nnz(column_j)) per fixed variable\n  Reduces problem dimension, may create new singletons or empty rows\n  Very common in practical models (constants, derived bounds)", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinMessageHandler.hpp": {"path": "layer-0/CoinUtils/src/CoinMessageHandler.hpp", "filename": "CoinMessageHandler.hpp", "file": "CoinMessageHandler.hpp", "brief": "Flexible message handling with severity levels and i18n support\n\nProvides CoinMessageHandler for formatted output with:\n- Severity-based filtering (warnings, errors, info)\n- Detail/logging levels\n- Multi-language support (messages default to US English)\n- Output to stdout or FILE pointer", "algorithm": "Message Handling System:\n  Message construction: printf-style formatting with type safety\n  Severity levels: 0=nothing, 1=fatal, 2=error, 3=warning, 4+=info\n  Detail filtering: only print if message detail ≤ handler logLevel\n\n  Output chain: CoinMessageHandler << value1 << value2 << CoinMessageEol\n  Supports: int, double, char, string, and custom precision", "complexity": "Message lookup: O(1) by message number\n  Formatting: O(message_length)\n  Filtering: O(1) severity/detail check before formatting", "see": ["CoinMessage for standard COIN-OR message definitions", "CoinOneMessage for individual message container"], "has_pass2": true}, "src/CoinAlloc.hpp": {"path": "layer-0/CoinUtils/src/CoinAlloc.hpp", "filename": "CoinAlloc.hpp", "file": "CoinAlloc.hpp", "brief": "Memory pool allocator for small fixed-size blocks\n\nOptional memory pool that reduces malloc overhead for small allocations.\nConfigure with COINUTILS_MEMPOOL_MAXPOOLED and COINUTILS_MEMPOOL_ALIGNMENT.", "has_pass2": false}, "src/CoinWarmStart.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStart.hpp", "filename": "CoinWarmStart.hpp", "file": "CoinWarmStart.hpp", "brief": "Abstract interfaces for warm start information in optimization solvers", "algorithm": "Warm Start with Delta-Encoded Diffs for B&B\nAbstract base for solver state that enables fast re-optimization.\nThe diff mechanism (generateDiff/applyDiff) compresses incremental changes\nbetween B&B nodes - critical for memory-efficient branch-and-bound.\n\nDerived classes implement solver-specific warm start data (e.g., basis\nstatus for simplex, barrier iterates for interior point methods).", "see": ["CoinWarmStartBasis for a common implementation (simplex basis)", "CoinWarmStartDiff, CoinWarmStartBasis", "applyDiff()", "generateDiff()", "CoinWarmStart::generateDiff(), CoinWarmStart::applyDiff()"], "return": "Pointer to newly allocated copy (caller owns memory)", "param": ["oldWarmStart The baseline warm start to compare against", "diff The diff to apply"], "has_pass2": true}, "src/CoinTime.hpp": {"path": "layer-0/CoinUtils/src/CoinTime.hpp", "filename": "CoinTime.hpp", "file": "CoinTime.hpp", "brief": "Cross-platform timing utilities (CPU and wall clock)\n\nProvides CoinCpuTime() and CoinWallclockTime() that work on\nWindows, Mac, and Unix/Linux platforms.", "has_pass2": false}, "src/CoinSimpFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinSimpFactorization.hpp", "filename": "CoinSimpFactorization.hpp", "file": "CoinSimpFactorization.hpp", "brief": "Simple LU factorization for LP basis matrices\n\nStraightforward LU factorization implementation. Less optimized than\nCoinFactorization but simpler and useful as reference implementation.", "algorithm": "Sparse LU with Markowitz Pivoting:\n  Computes PA = LU minimizing fill-in during elimination.\n  1. Find pivot (r,s) minimizing Markowitz count: (nnz_row - 1)(nnz_col - 1)\n  2. Swap row r to current position, column s to current position\n  3. Perform Gaussian elimination on remaining submatrix\n  4. Store L factors by row and column, U factors by row and column\n  Updates via eta vectors (product form of inverse):\n  B_{k+1}^{-1} = E_k * B_k^{-1} where E_k is eta matrix", "math": "LU factorization: PA = LU, L unit lower triangular, U upper triangular\n  FTRAN: solve Bx = b via Ly = Pb, Ux = y\n  BTRAN: solve B'x = b via U'y = b, L'z = y, x = P'z\n  Update: B_new = B_old + (a_q - B*e_p)*e_p' handled by eta vectors", "complexity": "Factorization: O(n³) worst, typically O(n·nnz) with good pivoting\n  FTRAN/BTRAN: O(nnz(L) + nnz(U)) per solve\n  Update: O(n) per eta vector, refactorize after ~100 pivots", "see": ["CoinOtherFactorization for the base class", "CoinFactorization for the optimized sparse implementation"], "has_pass2": true}, "src/CoinFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinFactorization.hpp", "filename": "CoinFactorization.hpp", "file": "CoinFactorization.hpp", "brief": "LU factorization of sparse basis matrix for simplex\n\nImplements LU factorization with hyper-sparse handling for efficient\nFTRAN/BTRAN operations. Supports rank-one updates during pivoting.", "algorithm": "LU factorization with Markowitz pivot selection", "math": "Decomposes basis matrix B = L·U where:\n      - L is lower triangular (stored by columns, implicit unit diagonal)\n      - U is upper triangular (stored by columns)\n      FTRAN solves Bx = b via: Ly = b, then Ux = y\n      BTRAN solves B'y = c via: U'z = c, then L'y = z", "complexity": "Factorization: O(nnz * fill) where fill depends on pivot order\n            FTRAN/BTRAN: O(nnz(L) + nnz(U)) per solve\n            Rank-one update: O(nnz(column)) via Forrest-Tomlin", "ref": ["Forrest, Tomlin (1972). \"Updated Triangular Factors of the Basis\n     to Maintain Sparsity in the Product Form Simplex Method\".\n     Mathematical Programming 2:263-278.", "Suhl, Suhl (1990). \"Computing Sparse LU Factorizations for\n     Large-Scale Linear Programming Bases\". ORSA J. Computing 2:325-335.", "Forrest, Tomlin (1972). Mathematical Programming 2:263-278."], "author": "John Forrest", "see": ["CoinDenseFactorization for dense matrices", "CoinOslFactorization for OSL-derived implementation", "forrestTomlin() to check which update method is active", "updateColumnTranspose for BTRAN (B'y = c)", "updateColumnFT for FTRAN (Bx = b)"], "param": ["matrix Full constraint matrix A", "rowIsBasic Array marking basic rows (>=0 means basic)", "columnIsBasic Array marking basic columns (>=0 means basic)", "areaFactor Multiplier for memory allocation (0 = auto)", "regionSparse The column entering the basis (after FTRAN)", "pivotRow Row index of the leaving variable", "pivotCheck Expected pivot value for accuracy verification", "checkBeforeModifying If true, validate before modifying factors", "acceptablePivot Minimum acceptable |pivot| for stability", "regionSparse Work vector (must start as zero, returns zero)", "regionSparse2 Right-hand side b on input, solution x on output", "regionSparse Work vector (must start as zero, returns zero)", "regionSparse2 Right-hand side c on input, solution y on output"], "return": "0 = success, -1 = singular, -2 = too many basic, -99 = memory", "has_pass2": true}, "src/CoinPresolveMonitor.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveMonitor.hpp", "filename": "CoinPresolveMonitor.hpp", "file": "CoinPresolveMonitor.hpp", "brief": "Monitor rows/columns for debugging presolve transforms", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinMpsIO.hpp": {"path": "layer-0/CoinUtils/src/CoinMpsIO.hpp", "filename": "CoinMpsIO.hpp", "file": "CoinMpsIO.hpp", "brief": "MPS file format reader/writer for LP and MIP problems\n\nReads/writes standard MPS format including extensions for quadratic,\nconic, and SOS constraints. Supports free format and compression.", "algorithm": "MPS File Parsing:\n  Sections: NAME, ROWS, COLUMNS, RHS, RANGES, BOUNDS, ENDATA\n  Fixed format: columns 1-2 (type), 5-12 (name1), 15-22 (name2), etc.\n  Free format: whitespace-separated fields\n\n  Row types: N (free/objective), E (equality), L (≤), G (≥)\n  Bound types: LO, UP, FX, FR, MI, PL, BV (binary), UI/LI (integer)\n  Extensions: QUADOBJ (Hessian), SOS (special ordered sets)", "math": "Reads: min c'x s.t. l ≤ Ax ≤ u, lb ≤ x ≤ ub\n  RANGES section: row_i has both lower and upper when range specified\n  Integer markers: 'MARKER' 'INTORG'/'INTEND' for integer variables", "complexity": "Parsing: O(file_size), single pass with hash tables for names\n  Supports gzip/bzip2 compression via CoinFileIO abstraction", "see": ["CoinLpIO for LP file format support", "CoinFileIO for underlying file I/O abstraction"], "has_pass2": true}, "src/CoinFinite.hpp": {"path": "layer-0/CoinUtils/src/CoinFinite.hpp", "filename": "CoinFinite.hpp", "file": "CoinFinite.hpp", "brief": "Numeric limit constants and floating-point validation functions\n\nProvides platform-independent definitions for numeric limits (COIN_DBL_MAX,\nCOIN_INT_MAX, etc.) and functions to check for special floating-point values\n(infinity, NaN). These are fundamental utilities used throughout COIN-OR\nfor bounds checking and numerical validation.", "see": ["CoinIsnan(), CoinFinite()", "CoinIsnan()", "CoinFinite()"], "param": ["val The value to test", "val The value to test"], "return": "true if val is a finite number, false if infinity or NaN", "has_pass2": false}, "src/CoinModelUseful.hpp": {"path": "layer-0/CoinUtils/src/CoinModelUseful.hpp", "filename": "CoinModelUseful.hpp", "file": "CoinModelUseful.hpp", "brief": "Helper classes for CoinModel (Link, LinkedList, Hash)", "see": ["CoinModel for the main model building interface"], "has_pass2": false}, "src/CoinBuild.hpp": {"path": "layer-0/CoinUtils/src/CoinBuild.hpp", "filename": "CoinBuild.hpp", "file": "CoinBuild.hpp", "brief": "Efficient row-by-row or column-by-column model construction\n\nCoinBuild accumulates rows or columns for batch addition to a model.\nMore efficient than adding one row/column at a time to CoinPackedMatrix.", "see": ["CoinModel for flexible model building with names and expressions", "CoinPackedMatrix for the final sparse matrix format"], "has_pass2": false}, "src/CoinPackedMatrix.hpp": {"path": "layer-0/CoinUtils/src/CoinPackedMatrix.hpp", "filename": "CoinPackedMatrix.hpp", "file": "CoinPackedMatrix.hpp", "brief": "Sparse matrix stored in compressed row or column format\n\nCoinPackedMatrix represents a sparse matrix using compressed storage.\nCan be stored row-major or column-major. Efficient for major-dimension\noperations (accessing rows in row-major, columns in column-major).", "algorithm": "Compressed Sparse Column/Row (CSC/CSR) format", "math": "Storage arrays for m×n matrix with nnz nonzeros:\n      - elements[nnz]: coefficient values\n      - indices[nnz]: minor indices (row indices for CSC, column for CSR)\n      - starts[major+1]: start position of each major vector\n      Memory: O(nnz + major) vs O(m·n) for dense", "complexity": "Major vector access: O(1) to get start, O(nnz_j) to iterate\n            Minor vector access: O(nnz) full scan required\n            Matrix-vector multiply: O(nnz)\n\n@note Column-major (CSC) is standard for LP constraint matrices since\n      simplex primarily accesses columns (entering variables).", "see": ["CoinPackedVector for sparse vector storage", "CoinShallowPackedVector for row/column views"], "has_pass2": true}, "src/CoinPresolveMatrix.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveMatrix.hpp", "filename": "CoinPresolveMatrix.hpp", "file": "CoinPresolveMatrix.hpp", "brief": "Core presolve data structures and action base class\n\nDefines CoinPrePostsolveMatrix (common base), CoinPresolveMatrix (for\npresolve), CoinPostsolveMatrix (for postsolve), and CoinPresolveAction\n(base class for all presolve transformations).", "algorithm": "LP Presolve - simplifies LP before solving via reversible transforms", "math": "Presolve applies a sequence of transformations T_k to original LP:\n      min c'x s.t. Ax = b, l <= x <= u  -->  min c'x' s.t. A'x' = b', l' <= x' <= u'\n      Each T_k is recorded to enable postsolve recovery of original solution.\n\n@note Presolve typically reduces problem size 30-90%, dramatically speeding solve", "ref": ["Andersen, E., Andersen, K. (1995). \"Presolving in Linear Programming\".\n     Mathematical Programming 71:221-245."], "see": ["CoinPresolveDoubleton, CoinPresolveForcing, etc. for specific transforms"], "has_pass2": true}, "src/CoinWarmStartVector.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartVector.hpp", "filename": "CoinWarmStartVector.hpp", "file": "CoinWarmStartVector.hpp", "brief": "Template warm start storing a single typed vector", "tparam": ["T Element type (typically double)\n\nGeneric warm start class that stores a vector of values.\nUsed as building block for CoinWarmStartDual and CoinWarmStartPrimalDual."], "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartDual which uses this for dual vector storage"], "has_pass2": false}, "src/CoinShallowPackedVector.hpp": {"path": "layer-0/CoinUtils/src/CoinShallowPackedVector.hpp", "filename": "CoinShallowPackedVector.hpp", "file": "CoinShallowPackedVector.hpp", "brief": "Non-owning sparse vector reference (view into external storage)\n\nCoinShallowPackedVector provides read-only access to sparse vector data\nstored elsewhere. It maintains only pointers, not copies. Use when you\nneed a lightweight view into a row/column of a CoinPackedMatrix.", "see": ["CoinPackedVectorBase for the interface", "CoinPackedVector for an owning sparse vector"], "has_pass2": false}, "src/CoinSmartPtr.hpp": {"path": "layer-0/CoinUtils/src/CoinSmartPtr.hpp", "filename": "CoinSmartPtr.hpp", "file": "CoinSmartPtr.hpp", "brief": "Intrusive reference-counted smart pointer", "author": "Carl Laird, Andreas Waechter (IBM)\n\nProvides Coin::SmartPtr<T> for automatic memory management.\nObjects must inherit from Coin::ReferencedObject.", "has_pass2": false}, "src/CoinRational.hpp": {"path": "layer-0/CoinUtils/src/CoinRational.hpp", "filename": "CoinRational.hpp", "file": "CoinRational.hpp", "brief": "Rational number representation with double-to-rational conversion\n\nProvides a simple rational number class that can convert floating-point\nvalues to rational approximations. Useful in optimization for converting\nfloating-point coefficients to exact rational form when needed.", "return": "The denominator value", "param": ["n Numerator", "d Denominator (must not be zero)", "val The floating-point value to approximate", "maxdelta Maximum allowed absolute error", "maxdnom Maximum allowed denominator", "val Value to approximate", "maxdelta Maximum error tolerance", "maxdnom Maximum denominator allowed"], "algorithm": "Stern-Brocot tree / mediant search for best rational approximation", "math": "Given lower bound a/b and upper bound c/d, the mediant (a+c)/(b+d)\n      is the simplest rational between them. Binary search narrows to\n      the best approximation within tolerance.", "complexity": "O(log(maxdnom)) iterations", "ref": ["Stern (1858), Brocot (1861). See also: Graham, Knuth, Patashnik\n     \"Concrete Mathematics\" Ch. 4.5 for Stern-Brocot tree theory."], "has_pass2": true}, "src/CoinSignal.hpp": {"path": "layer-0/CoinUtils/src/CoinSignal.hpp", "filename": "CoinSignal.hpp", "file": "CoinSignal.hpp", "brief": "Cross-platform signal handler typedef (CoinSighandler_t)", "has_pass2": false}, "src/CoinPresolveTripleton.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveTripleton.hpp", "filename": "CoinPresolveTripleton.hpp", "file": "CoinPresolveTripleton.hpp", "brief": "Tripleton row presolve: three-variable equation substitution", "algorithm": "Tripleton Elimination:\n  Given equality constraint ax + by + cz = d with three variables:\n  1. Solve for y: y = (d - ax - cz) / b\n  2. Substitute y into objective and all constraints containing y\n  3. Eliminate the constraint row and variable y\n  Only applied if substitution doesn't increase total nonzeros", "math": "Original: ax + by + cz = d\n  Substitution: y = (d - ax - cz) / b\n  Objective: c_y·y = c_y·d/b - (c_y·a/b)x - (c_y·c/b)z\n  Postsolve recovers y from optimal (x*, z*)", "complexity": "Time: O(nnz(col_y) × avg_row_length) per tripleton\n  Fill-in check prevents growth: only apply if beneficial\n  Less common than doubleton but can still reduce problem size", "see": ["CoinPresolveDoubleton for two-variable case", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinPackedVectorBase.hpp": {"path": "layer-0/CoinUtils/src/CoinPackedVectorBase.hpp", "filename": "CoinPackedVectorBase.hpp", "file": "CoinPackedVectorBase.hpp", "brief": "Abstract base class for read-only sparse vector access\n\nProvides the read-only interface for sparse vectors stored as parallel\nindex/value arrays. This is the base class for CoinPackedVector (owning)\nand CoinShallowPackedVector (non-owning reference).", "see": ["CoinPackedVector for a modifiable sparse vector", "CoinShallowPackedVector for a lightweight view"], "return": "Count of index/value pairs", "param": ["denseSize Length of the dense vector to create", "i Index in the conceptual dense vector", "methodName Name of calling method (for error message)", "className Name of calling class (for error message)", "i Index to search for", "i Index to search for", "dense Pointer to dense vector (must have length >= max index + 1)"], "has_pass2": false}, "src/CoinDenseVector.hpp": {"path": "layer-0/CoinUtils/src/CoinDenseVector.hpp", "filename": "CoinDenseVector.hpp", "file": "CoinDenseVector.hpp", "brief": "Dense vector template with element-wise arithmetic operations\n\nCoinDenseVector<T> stores all elements in a contiguous array.\nUnlike sparse vectors, every position has storage. Supports\narithmetic operators (+, -, *, /) with other dense vectors.", "tparam": ["T Element type (typically double, float, or int)"], "algorithm": "Dense Vector Storage:\n  Contiguous array of size n: elements_[0..n-1]\n  All positions allocated, O(1) random access\n  Element-wise operations: v3 = v1 + v2 applies to all indices\n\n  Supported operations: +, -, *, / (element-wise)\n  Norms: oneNorm (L1), twoNorm (L2/Euclidean), infNorm (L∞/max)", "math": "Dense storage: x[i] stored for all i ∈ {0,...,n-1}\n  oneNorm = Σ|x_i|, twoNorm = √(Σx_i²), infNorm = max|x_i|", "complexity": "Element access: O(1)\n  Vector operations: O(n)\n  Space: O(n) always, regardless of sparsity", "see": ["CoinPackedVector for sparse storage", "CoinIndexedVector for sparse with dense backing"], "has_pass2": true}, "src/CoinPresolveIsolated.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveIsolated.hpp", "filename": "CoinPresolveIsolated.hpp", "file": "CoinPresolveIsolated.hpp", "brief": "Handle isolated constraints (connected only by free variables)", "algorithm": "Isolated Constraint Removal:\n  Removes constraints connected to rest of problem only through free variables:\n  1. Identify constraint where all variables are free (no bounds) or\n     appear only in this constraint\n  2. Such constraints can be satisfied independently\n  3. Store constraint data for postsolve restoration\n  4. Remove row from problem (reduces problem size)\n  5. Postsolve: Compute variable values to satisfy stored constraint", "math": "Isolated constraint feasibility:\n  Constraint: l_i ≤ Σ a_ij·x_j ≤ u_i where all x_j are free\n  Since x_j unbounded, can always find values satisfying constraint\n  Remove row, set x_j values in postsolve to achieve feasibility", "complexity": "O(nnz in row) to detect and remove\n  Uncommon but provides clean removal when found", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinConflictGraph.hpp": {"path": "layer-0/CoinUtils/src/CoinConflictGraph.hpp", "filename": "CoinConflictGraph.hpp", "file": "CoinConflictGraph.hpp", "brief": "Conflict graph for binary variable incompatibilities in MIP", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Conflict Graph Construction:\n  For binary variables x_i ∈ {0,1}, build graph G where:\n  - Nodes: original variables x_i and complements x̄_i = 1 - x_i\n  - Edges: (u,v) if u=1 and v=1 simultaneously infeasible\n  Sources of conflicts:\n  1. Set packing constraints: x_i + x_j ≤ 1 → edge (x_i, x_j)\n  2. Variable bounds: x_i ≤ x_j → edge (x_i, x̄_j)\n  3. Clique constraints: Σx_i ≤ 1 → complete subgraph", "math": "Independent set in conflict graph ↔ feasible partial assignment.\n  Clique C in conflict graph → valid inequality Σ_{i∈C} x_i ≤ 1.\n  Maximum clique gives strongest clique cut.", "complexity": "Space: O(n²) worst case, but typically sparse O(n + m)\n  Conflict check: O(1) with adjacency matrix, O(degree) with lists\n  Clique storage: large cliques stored explicitly to save space", "see": ["CoinBronKerbosch for clique enumeration", "CoinCliqueList for clique storage\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["_cols number of columns in the mixed-integer\nlinear program. The number of elements in the conflict\ngraph will be _cols*2 (it consider complementary variables)", "other conflict graph to be copied", "n1 node index", "n2 node index", "node node index", "temp temporary storage area for storing conflicts,\nshould have space for all elements in the graph (size())", "iv auxiliary incidence array used to eliminate\nduplicates. It should have the size of the graph (size())\nand all elements shoud be initialized as false.", "idxNode index of the node", "deg degree of the node", "idxNode index of the node", "mdegree modified degree of the node", "other conflict graph to be copied"], "return": "true if there is an edge between\nn1 and n2 in the conflict graph, 0 otherwise.", "has_pass2": true}, "src/CoinSnapshot.hpp": {"path": "layer-0/CoinUtils/src/CoinSnapshot.hpp", "filename": "CoinSnapshot.hpp", "file": "CoinSnapshot.hpp", "brief": "Lightweight read-only snapshot of problem state at a B&B node\n\nCoinSnapshot captures problem data for cut generators and branching code.\nMay own or reference external arrays (see owned_ flag). Designed for\npassing problem state without copying full solver structures.", "see": ["CoinPackedMatrix for the constraint matrix format"], "has_pass2": false}, "src/CoinPresolveDoubleton.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveDoubleton.hpp", "filename": "CoinPresolveDoubleton.hpp", "file": "CoinPresolveDoubleton.hpp", "brief": "Doubleton row presolve: substitute y from ax+by=c", "algorithm": "Doubleton Elimination:\n  Given equality constraint ax + by = c with two variables:\n  1. Solve for y: y = (c - ax) / b\n  2. Substitute y in objective: c_x x + c_y y → c_x x + c_y(c-ax)/b\n  3. Substitute y in all constraints containing y\n  4. Transfer bounds: l_y ≤ y ≤ u_y becomes bounds on x\n  5. Remove row and column y from problem", "math": "Original: min c_x·x + c_y·y s.t. ax + by = c, bounds\n  After: min (c_x - c_y·a/b)x + c_y·c/b, modified bounds on x\n  Postsolve recovers y = (c - ax)/b from optimal x*", "complexity": "Time: O(nnz(col_y)) per doubleton - updating all rows with y\n  Typically reduces problem size significantly when equality rows exist\n  Cascading effect: may create new singletons or doubletons", "see": ["CoinPresolveMatrix for the presolve framework", "CoinPresolveTripleton for three-variable equations", "CoinPresolveSingleton for single-variable equations"], "has_pass2": true}, "src/CoinOslFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinOslFactorization.hpp", "filename": "CoinOslFactorization.hpp", "file": "CoinOslFactorization.hpp", "brief": "LU factorization derived from IBM OSL (Optimization Subroutine Library)\n\nPort of OSL's factorization code. Provides alternative to CoinFactorization\nwith different numerical characteristics and update strategies.", "algorithm": "OSL-style Sparse LU Factorization:\n  Implements B = L·U with OSL's eta-file based update scheme:\n  1. Factor sparse basis B using Markowitz pivot selection\n  2. Store L and U in packed sparse format (EKKfactinfo structure)\n  3. After pivot: B_new = B_old·E_k where E_k is elementary matrix\n  4. Accumulate eta vectors in R_etas_* arrays for efficient FTRAN/BTRAN", "math": "Eta vector update representation:\n  After basis change replacing column p with column q:\n  B_new = B_old · E where E = I + (e_p - B_old^{-1}·a_q)·e_p'\n  FTRAN: solve by applying L^{-1}, then U^{-1}, then eta sequence\n  BTRAN: solve by applying eta sequence inverse, then U^{-T}, then L^{-T}", "complexity": "Factorization: O(n·nnz) typical with good pivot ordering\n  FTRAN/BTRAN: O(nnz(L) + nnz(U) + sum(eta_lengths))\n  Eta vectors grow linearly with pivots; refactorize every ~100 iterations", "ref": ["IBM OSL (Optimization Subroutine Library) sparse factorization\n\nKey OSL structures:\n- EKKfactinfo: Contains all factorization state and working arrays\n- R_etas_*: Row-wise storage of accumulated eta vectors\n- kp1adr/kp2adr: Linked lists for sparse row/column management"], "author": "John Forrest (original OSL code from IBM)", "see": ["CoinOtherFactorization for the base class", "CoinFactorization for the primary sparse implementation"], "has_pass2": true}, "src/CoinPresolveDual.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveDual.hpp", "filename": "CoinPresolveDual.hpp", "file": "CoinPresolveDual.hpp", "brief": "Fix variables using dual bounds and reduced cost analysis", "algorithm": "Dual Bound Propagation:\n  Use dual feasibility to fix primal variables:\n  1. For slacks s_i: d_{n+i} = -y_i, bound y_i from slack bounds\n  2. For singletons x_j: d_j = c_j - y_i·a_ij, bound y_i from d_j sign\n  3. Propagate bounds through reduced cost equation d_j = c_j - y'a_j\n  4. If d_j > 0 at optimality: x_j at lower bound (minimization)\n  5. If d_j < 0 at optimality: x_j at upper bound", "math": "Reduced cost: d_j = c_j - Σ_i y_i a_ij\n  Dual feasibility (min): d_j ≥ 0 if x_j at lower, d_j ≤ 0 if at upper\n  Bound propagation: y_i^L ≤ y_i ≤ y_i^U → d_j^L ≤ d_j ≤ d_j^U", "complexity": "Time: O(iterations × nnz) for bound propagation\n  Very powerful for detecting fixed variables without solving LP\n  Can prove unboundedness when required bound is missing", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinIndexedVector.hpp": {"path": "layer-0/CoinUtils/src/CoinIndexedVector.hpp", "filename": "CoinIndexedVector.hpp", "file": "CoinIndexedVector.hpp", "brief": "Sparse vector with dense backing array for O(1) element access\n\nCoinIndexedVector combines sparse index storage with a dense values array,\nenabling O(1) random access while tracking which positions are non-zero.\nDesigned for simplex operations where sparse updates need fast access.\nHas optional \"packed\" mode that behaves more like CoinPackedVector.", "algorithm": "Indexed sparse vector - hybrid sparse/dense storage", "math": "Storage for n-dimensional vector with k nonzeros:\n      - elements[n]: dense array storing values (zeros at unused positions)\n      - indices[k]: list of positions with nonzero values\n      Random access: elements[i] gives value directly\n      Sparse iteration: for j in indices, access elements[indices[j]]", "complexity": "Element access: O(1) via dense array\n            Sparse iteration: O(k) where k = number of nonzeros\n            Insert: O(1) amortized\n            Clear: O(k) - must zero out nonzero positions in dense array\n\n@note Does NOT support negative indices or duplicate checking", "see": ["CoinPackedVector for pure sparse storage", "CoinDenseVector for pure dense storage"], "has_pass2": true}, "src/CoinHelperFunctions.hpp": {"path": "layer-0/CoinUtils/src/CoinHelperFunctions.hpp", "filename": "CoinHelperFunctions.hpp", "file": "CoinHelperFunctions.hpp", "brief": "Low-level utility functions for array manipulation, copying, and comparison\n\nProvides optimized template functions for common array operations used throughout\nCOIN-OR: copying (with Duff's device optimization), filling, zeroing, sorting checks,\niota generation, and element deletion. Also includes random number generation and\nfile I/O utilities.\n\nMany functions offer ~2x speedup over naive loops via loop unrolling and\nDuff's device techniques.", "algorithm": "Duff's device - Tom Duff's loop unrolling technique that uses switch\n           statement fall-through to handle remainder elements, reducing loop\n           overhead from O(n) branches to O(n/8) branches.", "complexity": "Summary of major functions:\n            - CoinCopyN, CoinDisjointCopyN, CoinFillN, CoinZeroN: O(n)\n            - CoinIsSorted: O(n)\n            - CoinIotaN: O(n)\n            - CoinDeleteEntriesFromArray: O(n + k log k) where k = deletions\n            - CoinDrand48: O(1) per call", "ref": ["Duff, T. (1983). \"Duff's Device\" - Usenet comp.lang.c posting.\n     Originally for copying to memory-mapped display registers."], "see": ["CoinSort.hpp for sorting functions", "CoinError.hpp for exception handling", "CoinDisjointCopyN for faster non-overlapping copy", "CoinCopyN for count-based interface", "CoinCopyN for overlap-safe copy", "CoinDrand48 for global (non-thread-safe) random numbers"], "tparam": ["T Element type (must be copy-assignable)", "T Element type (must be copy-assignable)", "T Element type (must be copy-assignable)", "T Element type", "T Element type", "T Type supporting operator<", "T Type supporting operator<", "T Numeric type supporting negation and comparison with 0"], "param": ["from Pointer to source array", "size Number of elements to copy", "to Pointer to destination array", "first Pointer to first source element", "last Pointer past last source element", "to Pointer to destination array\n\n@note Size computed as (last - first), cast to CoinBigIndex", "from Pointer to source array", "size Number of elements to copy", "to Pointer to destination array\n\n@note Arrays must not overlap; behavior undefined if they do\n@note For POD types, CoinMemcpyN may be faster", "array Source array (may be null)", "size Number of elements to copy", "array Source array (may be null)", "size Size of array to allocate", "copySize Number of elements to copy from source (must be <= size)", "name Source string (may be null)", "x1 First value", "x2 Second value", "x1 First value", "x2 Second value", "value Input value"], "return": "Newly allocated copy, or NULL if array was NULL\n\n@note Caller owns returned memory (delete[])", "has_pass2": true}, "src/CoinPresolveSingleton.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveSingleton.hpp", "filename": "CoinPresolveSingleton.hpp", "file": "CoinPresolveSingleton.hpp", "brief": "Singleton row/column presolve transforms", "algorithm": "Singleton Column/Row Elimination:\n  Singleton column (one nonzero): variable x_j appears in one constraint\n  - Compute x_j bounds from constraint bounds and coefficient sign\n  - If bounds tighter than variable bounds, update variable bounds\n  - If bounds prove infeasibility, detect and report\n\n  Slack doubleton: row ax ≤ b with single variable, explicit bound\n  - Convert to column bound: x ≤ b/a (adjust for sign)\n  - Remove the constraint, tighten column bound", "math": "For singleton row ax_j = b: x_j = b/a (fixed value)\n  For singleton column in ax_j ≤ b with a > 0: x_j ≤ b/a\n  For singleton column in ax_j ≥ b with a > 0: x_j ≥ b/a", "complexity": "Time: O(1) per singleton processed\n  Very effective: singletons common in practical LP/MIP models\n  Cascading: removing singletons may create new singletons", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinPresolveUseless.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveUseless.hpp", "filename": "CoinPresolveUseless.hpp", "file": "CoinPresolveUseless.hpp", "brief": "Remove useless (redundant) constraints", "algorithm": "Redundant Constraint Identification:\n  Greedy algorithm identifying constraints implied by variable bounds:\n  1. Initialize column bounds from original variable bounds\n  2. For each constraint, compute implied bounds on variables\n     - If constraint tightens some bound, mark as \"necessary\"\n     - Update working column bounds with tighter values\n  3. After processing all constraints, for non-necessary rows:\n     - Compute row activity bounds: L_i = Σ min{a_ij·l_j, a_ij·u_j}\n     - If l_i ≤ L_i and U_i ≤ u_i, constraint is redundant\n  4. Remove redundant constraints, save for postsolve", "math": "Redundancy condition:\n  Row i redundant if: l_i ≤ min(a_i'x) and max(a_i'x) ≤ u_i\n  where min/max over x ∈ [l, u] (variable bounds)\n  No feasible point can violate such a constraint", "complexity": "O(m · avg_row_length) for full scan\n  Each constraint checked once; bound implications propagated\n  Effective at removing slack constraints from reformulations", "see": ["CoinPresolveForcing for related forcing constraint detection", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinFileIO.hpp": {"path": "layer-0/CoinUtils/src/CoinFileIO.hpp", "filename": "CoinFileIO.hpp", "file": "CoinFileIO.hpp", "brief": "Abstract file I/O with automatic compression detection\n\nProvides CoinFileInput and CoinFileOutput base classes with factory\nmethods that auto-detect gzip/bzip2 compression. Used by CoinMpsIO\nand CoinLpIO for reading/writing problem files.", "see": ["CoinMpsIO for MPS file format", "CoinLpIO for LP file format"], "has_pass2": false}, "src/CoinDenseFactorization.hpp": {"path": "layer-0/CoinUtils/src/CoinDenseFactorization.hpp", "filename": "CoinDenseFactorization.hpp", "file": "CoinDenseFactorization.hpp", "brief": "Dense matrix factorization and CoinOtherFactorization base class\n\nProvides CoinOtherFactorization abstract base class for alternative\nfactorization methods, plus CoinDenseFactorization for small dense\nproblems using LAPACK-style LU.", "algorithm": "Dense LU Factorization (LAPACK-style):\n  Computes PA = LU using partial pivoting (row interchanges).\n  1. For k = 1 to n: find pivot = max|A(i,k)| for i >= k\n  2. Swap rows k and pivot row\n  3. Compute L(i,k) = A(i,k) / A(k,k) for i > k\n  4. Update A(i,j) -= L(i,k) * A(k,j) for i,j > k", "math": "Dense LU: PA = LU where P is permutation, L unit lower triangular,\n  U upper triangular. FTRAN solves Lx = b then Ux = y.\n  BTRAN solves U'y = b then L'x = y.", "complexity": "Time: O(n³/3) for factorization, O(n²) per solve\n  Space: O(n²) for dense storage\n  Use only for small basis matrices (n < 100-200)", "author": "John Forrest", "see": ["CoinFactorization for sparse factorization", "CoinSimpFactorization for simple implementation"], "has_pass2": true}, "src/CoinParam.hpp": {"path": "layer-0/CoinUtils/src/CoinParam.hpp", "filename": "CoinParam.hpp", "file": "CoinParam.hpp", "brief": "Command line parameter parsing with keyword matching\n\nProvides base class for 'keyword value' parameters with support for\ninteger, double, string, and keyword-set values. Includes partial\nkeyword matching, range validation, and help text.", "see": ["CoinParamUtils namespace for parsing utilities"], "has_pass2": false}, "src/CoinPresolveForcing.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveForcing.hpp", "filename": "CoinPresolveForcing.hpp", "file": "CoinPresolveForcing.hpp", "brief": "Forcing and useless constraint detection", "algorithm": "Forcing Constraint Detection:\n  Compute implied bounds on row activity: LB_i ≤ Σ a_ij x_j ≤ UB_i\n  using variable bounds l_j ≤ x_j ≤ u_j\n\n  Useless constraint: LB_i ≥ row_lb and UB_i ≤ row_ub\n  - Constraint can never be violated → remove it\n\n  Forcing constraint: LB_i = row_ub or UB_i = row_lb\n  - All variables forced to bounds that achieve the activity limit\n  - Fix variables, remove constraint", "math": "Activity bounds: LB_i = Σ_{a_ij>0} a_ij l_j + Σ_{a_ij<0} a_ij u_j\n  UB_i = Σ_{a_ij>0} a_ij u_j + Σ_{a_ij<0} a_ij l_j\n  Forcing: if UB_i = row_lb, then each term must achieve its min", "complexity": "Time: O(nnz) to compute activity bounds for all rows\n  Very effective for tightly bounded problems\n  Can fix many variables simultaneously", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinSort.hpp": {"path": "layer-0/CoinUtils/src/CoinSort.hpp", "filename": "CoinSort.hpp", "file": "CoinSort.hpp", "brief": "Sorting utilities for pairs, triples, and parallel arrays\n\nProvides CoinPair, CoinTriple, and sort functions for sorting\nmultiple related arrays together (e.g., indices and values).", "algorithm": "Parallel Array Sorting:\n  Problem: sort array A while permuting arrays B, C, ... identically\n  Approach 1 (standard): Create array of tuples, sort, scatter back\n  Approach 2 (EKK sort): In-place quicksort carrying parallel arrays\n\n  EKK Quicksort variant (COIN_USE_EKK_SORT):\n  1. Check if already sorted (linear scan)\n  2. Median-of-three pivot selection\n  3. Non-recursive stack-based partitioning\n  4. Insertion sort for small subarrays (n ≤ 10)\n  Falls back to std::sort for n > 10000", "complexity": "CoinSort_2/3: O(n log n) average, O(n²) worst\n  CoinShortSort_2: O(n log n) but optimized for small n\n  Space: O(n) for tuple approach, O(log n) stack for EKK", "see": ["CoinPackedVector for typical use case (sorting indices with values)"], "has_pass2": true}, "src/CoinPresolvePsdebug.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolvePsdebug.hpp", "filename": "CoinPresolvePsdebug.hpp", "file": "CoinPresolvePsdebug.hpp", "brief": "Debug and consistency checking for presolve\n\nEnable with PRESOLVE_DEBUG and PRESOLVE_CONSISTENCY defines.", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": false}, "src/CoinPresolveImpliedFree.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveImpliedFree.hpp", "filename": "CoinPresolveImpliedFree.hpp", "file": "CoinPresolveImpliedFree.hpp", "brief": "Detect and process implied free variables", "algorithm": "Implied Free Variable Detection:\n  For singleton variable x_j (one nonzero a_ij in row i):\n  1. Compute implied bounds on x_j from constraint i and other var bounds\n  2. If implied bounds strictly within [l_j, u_j]: x_j is implied free\n  3. Remove constraint i and variable x_j (neither can be binding)\n\n  Non-singleton case: use substitution to reduce to singleton first", "math": "For row: Σ_k a_ik x_k = b_i, with x_j singleton\n  x_j = (b_i - Σ_{k≠j} a_ik x_k) / a_ij\n  Implied bounds from other variables' bounds → [l_j^impl, u_j^impl]\n  If l_j < l_j^impl and u_j > u_j^impl: x_j implied free", "complexity": "Time: O(nnz(row)) per candidate variable\n  Very effective for network and assignment problems\n  Reduces problem by one row and one column per implied free", "see": ["CoinPresolveSubst for non-singleton case", "CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinPresolveTighten.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveTighten.hpp", "filename": "CoinPresolveTighten.hpp", "file": "CoinPresolveTighten.hpp", "brief": "Tighten variable bounds using constraint propagation", "algorithm": "Bound Tightening:\n  For each constraint Σ a_ij x_j ≤ b_i:\n  1. Compute max activity without x_k: M_k = Σ_{j≠k} max(a_ij l_j, a_ij u_j)\n  2. New upper bound: x_k ≤ (b_i - M_k) / a_ik (if a_ik > 0)\n  3. New lower bound: x_k ≥ (b_i - M_k) / a_ik (if a_ik < 0)\n  4. Iterate until no bound changes or fixed point reached\n\n  tighten_zero_cost: fix variables with zero objective at bounds", "math": "Implied bound: given Σa_j x_j ≤ b and x_j ∈ [l_j, u_j]\n  x_k ≤ (b - Σ_{j≠k} a_j l_j) / a_k when a_k > 0\n  Dual bound tightening uses similar logic from reduced costs", "complexity": "Time: O(iterations × nnz) for propagation\n  Very effective for tightly constrained problems\n  May prove infeasibility or fix variables", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinWarmStartPrimalDual.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartPrimalDual.hpp", "filename": "CoinWarmStartPrimalDual.hpp", "file": "CoinWarmStartPrimalDual.hpp", "brief": "Warm start using both primal and dual variable values\n\nStores both primal (column) and dual (row) solution vectors.\nUseful for interior point methods. Includes diff capability.", "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartDual for dual-only warm start"], "has_pass2": false}, "src/CoinFloatEqual.hpp": {"path": "layer-0/CoinUtils/src/CoinFloatEqual.hpp", "filename": "CoinFloatEqual.hpp", "file": "CoinFloatEqual.hpp", "brief": "Function objects for testing equality of real numbers\n\nTwo objects are provided; one tests for equality to an absolute tolerance,\none to a scaled tolerance. The tests will handle IEEE floating point, but\nnote that infinity == infinity. Mathematicians are rolling in their graves,\nbut this matches the behaviour for the common practice of using\n<code>DBL_MAX</code> (<code>numeric_limits<double>::max()</code>, or similar\nlarge finite number) as infinity.", "algorithm": "Floating-Point Comparison:\n  CoinAbsFltEq: |f1 - f2| < ε (absolute tolerance)\n  CoinRelFltEq: |f1 - f2| ≤ ε(1 + max(|f1|, |f2|)) (relative tolerance)\n\n  Special cases:\n  - NaN: never equal to anything (including itself)\n  - Infinity: inf == inf returns true (practical choice)\n  - Exact equality: checked first to handle ±0", "math": "Absolute: suitable when values are O(1) magnitude\n  Relative: suitable when values span orders of magnitude\n  Default ε = 1e-10 (double), 1e-6 (float)", "complexity": "O(1) per comparison\n\n<p>\nExample usage:\n@verbatim\n  double d1 = 3.14159 ;\n  double d2 = d1 ;\n  double d3 = d1+.0001 ;\n\n  CoinAbsFltEq eq1 ;\n  CoinAbsFltEq eq2(.001) ;\n\n  assert(  eq1(d1,d2) ) ;\n  assert( !eq1(d1,d3) ) ;\n  assert(  eq2(d1,d3) ) ;\n@endverbatim\nCoinRelFltEq follows the same pattern.", "see": ["CoinFinite.hpp for infinity/NaN handling utilities"], "has_pass2": true}, "src/CoinModel.hpp": {"path": "layer-0/CoinUtils/src/CoinModel.hpp", "filename": "CoinModel.hpp", "file": "CoinModel.hpp", "brief": "High-level model building with string names and expressions\n\nCoinModel provides a flexible interface for constructing LP/MIP models.\nSupports row/column names, string-based expressions, and incremental\nbuilding. Convert to CoinPackedMatrix for solver use.", "algorithm": "Model Construction Interface:\n  Three building modes:\n  1. Row-by-row: addRow(indices, values, lb, ub)\n  2. Column-by-column: addCol(indices, values, lb, ub, obj)\n  3. Element-by-element: setElement(row, col, value)\n\n  Name handling: hash tables for row/column name lookup\n  Expression parsing: \"2 x1 + 3 x2 <= 5\" string syntax", "complexity": "Insertion: O(1) amortized with dynamic arrays\n  Name lookup: O(1) expected with hash tables\n  Conversion to CoinPackedMatrix: O(nnz)", "see": ["CoinBuild for simpler row-by-row construction", "CoinPackedMatrix for the final sparse matrix format"], "has_pass2": true}, "src/CoinCutPool.hpp": {"path": "layer-0/CoinUtils/src/CoinCutPool.hpp", "filename": "CoinCutPool.hpp", "algorithm": "Cut Pool with Fitness-Based Filtering\n\nClass for storing a pool of cuts, removing\nthe repeated and dominated ones. It also filters the cuts\naccording to their scores.\n\n**Scoring function:**\nGiven LP solution x*, score S(C) = viol(C) / actv(C) where:\n- viol(C) = violation of cut C by x*\n- actv(C) = number of variables in C with x* > 0\n\n**Admission criterion:**\nA cut enters the pool only if it has the best score for at least\none of its variables. This filters out cuts that are unlikely to\nbe useful (every variable already has a better cut).\n\n**Dominance removal:**\nCut A dominates cut B if A is at least as tight as B for all\nfeasible integer points. Dominated cuts are periodically purged.", "math": "Dominance: A dominates B if A ⊆ B and rhs(A) ≤ rhs(B)", "complexity": "O(n × k²) for dominance check where k = cuts per variable", "ref": ["Andreello, Caprara, Fischetti, \"Embedding cuts in a B&C framework\",\n     Annals of Operations Research, 2007"], "file": "CoinCutPool.hpp", "brief": "Class for storing a pool of cuts", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["idxs indexes of variables of the cut", "coefs coefficients of the variables the cut", "nz size of the cut", "rhs right-hand side of the cut", "other cut to be checked.", "x current solution of the LP relaxation of the MILP.", "numCols number of variables of the MILP.", "idxs indexes of the variables of the\ncut to be added.", "coefs coefficients of the variables of\nthe cut to be added.", "nz size of the cut to be added", "rhs right-hand side of the cut to be added"], "has_pass2": true}, "src/CoinSearchTree.hpp": {"path": "layer-0/CoinUtils/src/CoinSearchTree.hpp", "filename": "CoinSearchTree.hpp", "file": "CoinSearchTree.hpp", "brief": "Search tree data structures for branch-and-bound\n\nProvides tree node management with various comparison strategies\n(best-first, depth-first, breadth-first).", "algorithm": "Branch-and-Bound Tree Search:\n  Maintains a priority queue of open nodes, each representing a subproblem.\n  Node selection strategies:\n  - Best-first: select node with best LP bound (minimize gap)\n  - Depth-first: select deepest node (find solutions quickly)\n  - Breadth-first: select shallowest node (explore broadly)\n  - Best-estimate: use estimated integer solution quality", "math": "Node attributes:\n  - depth: distance from root (root = 0)\n  - quality: LP relaxation value (lower bound for minimization)\n  - fractionality: number of integer-infeasible variables\n  - true_lower_bound: valid dual bound (may differ with column generation)", "complexity": "Node operations: O(log n) insertion, O(log n) extraction\n  where n = number of open nodes. Uses heap-based priority queue.\n  Sibling nodes stored together for efficient processing.", "see": ["CoinTreeNode for node data interface", "CoinTreeSiblings for sibling node grouping"], "has_pass2": true}, "src/CoinMessage.hpp": {"path": "layer-0/CoinUtils/src/CoinMessage.hpp", "filename": "CoinMessage.hpp", "file": "CoinMessage.hpp", "brief": "Standard COIN-OR message definitions (enum and preloaded set)\n\nDefines COIN_Message enum for standard messages (MPS errors, presolve\ninfo, etc.) and CoinMessage class that preloads these into a handler.", "see": ["CoinMessageHandler for the message handling facilities", "CoinOneMessage for individual message storage"], "has_pass2": false}, "src/CoinStaticConflictGraph.hpp": {"path": "layer-0/CoinUtils/src/CoinStaticConflictGraph.hpp", "filename": "CoinStaticConflictGraph.hpp", "algorithm": "Static Conflict Graph with Compact Storage\n\nCoinConflictGraph implementation which supports fast queries\nbut doesn't support modifications.\n\n**Compact representation:**\nAfter conflict detection is complete, converts to static structure\nwith contiguous memory layout for cache-efficient queries.\n\n**Clique storage:**\nLarge cliques stored explicitly rather than as O(k²) edges.\nNode-to-clique index allows efficient iteration over conflicts.\n\n**Induced subgraphs:**\nCan create subgraph induced by subset of nodes for focused\nseparation or probing operations.", "complexity": "Memory: O(n + m) where m = edges + clique elements", "ref": ["Atamtürk, Nemhauser & Savelsbergh, \"Conflict graphs in solving\n     integer programming problems\", EJOR 121(1), 2000"], "file": "CoinStaticConflictGraph.hpp", "brief": "static CoinConflictGraph implementation with fast queries", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["numCols number of variables", "colType column types", "colLB column lower bounds", "colUB column upper bounds", "matrixByRow row-wise constraint matrix", "sense row sense", "rowRHS row right hand side", "rowRange row ranges", "cgraph conflict graph", "n number of elements in the induced subgraph", "elements indexes of nodes in the induced subgraph", "idxNode index of the node", "deg degree of the node", "idxNode index of the node", "deg degree of the node"], "return": "a vector of updated bounds with the format (idx, (lb, ub))", "has_pass2": true}, "src/CoinWarmStartBasis.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartBasis.hpp", "filename": "CoinWarmStartBasis.hpp", "file": "CoinWarmStartBasis.hpp", "brief": "Simplex basis warm start with variable status (basic/nonbasic)\n\nStores status of each variable (structural and artificial) using\n2 bits per variable. Includes diff capability for branch-and-bound.", "algorithm": "Simplex Basis Warm Start:\n  For LP with m constraints and n variables, a basis B identifies\n  m basic variables. Restarting from a known basis avoids Phase I.\n  Status encoding (2 bits per variable):\n  - 00 (isFree): nonbasic at zero\n  - 01 (basic): in the basis (value determined by constraints)\n  - 10 (atUpperBound): nonbasic at upper bound\n  - 11 (atLowerBound): nonbasic at lower bound", "math": "Basis matrix B is m×m submatrix of A with basic columns.\n  Basic solution: x_B = B⁻¹b, x_N = 0 (or at bounds)\n  Warm start restores this basis without recomputing from scratch.", "complexity": "Space: O((m+n)/4) bytes using 2-bit packing (4 vars per byte)\n  Diff operations for B&B: O(changed variables) instead of O(m+n)\n  generateDiff/applyDiff enable efficient tree-based warm starting", "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartDual for dual-only warm start"], "has_pass2": true}, "src/CoinCliqueExtender.hpp": {"path": "layer-0/CoinUtils/src/CoinCliqueExtender.hpp", "filename": "CoinCliqueExtender.hpp", "algorithm": "Greedy Clique Extension for Cutting Planes\n\nClass responsible for extending cliques to strengthen cuts.\n\n**Why extend cliques:**\nLarger cliques give tighter cuts. A clique {v₁,...,vₖ} yields\ncut Σxᵢ ≤ 1. Extending to {v₁,...,vₖ,vₖ₊₁} strengthens the cut\nby adding another variable to the LHS.\n\n**Extension methods:**\n- 0: No extension\n- 1: Random selection from candidates\n- 2: Max degree (prefer high-degree nodes)\n- 3: Max modified degree (counts clique containment)\n- 4: Reduced cost (prefer low RC variables)\n- 5: Reduced cost + modified degree hybrid\n\n**Algorithm:**\n1. Build candidate set: nodes adjacent to all clique members\n2. Greedily select from candidates using chosen method\n3. Update candidate set after each addition\n4. Stop when no candidates remain", "complexity": "O(k × d) per extension where k = clique size, d = avg degree", "ref": ["Brito & Santos, \"Conflict-based cut separation for MIP\", 2020"], "file": "CoinCliqueExtender.hpp", "brief": "Clique extender", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["cgraph conflict graph", "extMethod method used to extend cliques (optional).\nValues: 0 = no extension; 1 = random; 2 = max degree;\n3 = max modified degree; 4 = reduced cost (inversely proportional);\n5 = reduced cost (inversely proportional) + modified degree.", "rc reduced cost (optional)", "clqIdxs indexes of the clique to be extended", "clqSize size of the clique to be extended", "clqIdxs indexes of the clique to be extended", "clqSize size of the clique to be extended", "clqIdxs indexes of the clique to be extended", "clqIdxs size of the clique to be extended", "clqIdxs indexes of the clique to be extended", "clqIdxs size of the clique to be extended"], "has_pass2": true}, "src/CoinPackedVector.hpp": {"path": "layer-0/CoinUtils/src/CoinPackedVector.hpp", "filename": "CoinPackedVector.hpp", "file": "CoinPackedVector.hpp", "brief": "Sparse vector that owns its index/value storage\n\nCoinPackedVector stores a sparse vector as parallel arrays of indices\nand values. Unlike CoinShallowPackedVector, this class owns its storage\nand supports modification operations.", "see": ["CoinPackedVectorBase for the read-only interface", "CoinShallowPackedVector for non-owning sparse vector reference", "CoinIndexedVector for sparse vector with dense backing array"], "algorithm": "Merge-style sparse dot product", "math": "Computes x·y = Σ x_i * y_i where x and y share index i\n      Uses sorted merge to find common indices in linear time.", "complexity": "O(k1 + k2) where k1, k2 are the number of nonzeros\n@pre Both vectors must be sorted by increasing index", "has_pass2": true}, "src/CoinPresolveEmpty.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveEmpty.hpp", "filename": "CoinPresolveEmpty.hpp", "file": "CoinPresolveEmpty.hpp", "brief": "Drop and reinsert empty rows/columns\n\nShould be last presolve step. Physical removal of empty entities.", "algorithm": "Empty Row/Column Removal:\n  drop_empty_cols_action:\n  - Compress column arrays, renumber remaining columns\n  - Store original bounds, cost, solution for postsolve\n  - Adjust objective constant for fixed-at-bound variables\n\n  drop_empty_rows_action:\n  - Renumber rows, compress row bounds\n  - Track which row filled each position for postsolve", "math": "Empty column j: a_*j = 0 (all zeros in column)\n  For minimization: set x_j = l_j if c_j > 0, x_j = u_j if c_j < 0\n  Empty row i: a_i* = 0 → constraint always satisfied, remove", "complexity": "Time: O(m + n) for scanning and renumbering\n  Must be done last: other transforms may create empty rows/cols\n  Postsolve reinserts in reverse order, first step after solve", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}, "src/CoinAdjacencyVector.hpp": {"path": "layer-0/CoinUtils/src/CoinAdjacencyVector.hpp", "filename": "CoinAdjacencyVector.hpp", "algorithm": "Dynamic Adjacency List with Deferred Sorting\n\nClass to store a (growable) list of neighbors for each node\nInitially implemented to be used in the Conflict Graph\n\n**Design:**\nEach node has a dynamic vector of neighbors. Supports both\nimmediate sorted insertion and batch insertion with deferred sort.\n\n**Batch mode:**\nDuring graph construction, use addNeighborsBuffer() for bulk additions\nwithout maintaining sorted order. Call flush() once construction is\ncomplete to sort all adjacency lists and remove duplicates.\n\n**Sorted mode:**\nUse addNeighbor() for incremental updates to a constructed graph.\nMaintains sorted order via binary search insertion.", "complexity": "isNeighbor: O(log d) via binary search", "file": "CoinAdjacencyVector.hpp", "brief": "Vector of growable vectors", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["idxRow row index", "idxRow row index", "idxNode graph node", "idxNeigh neighbor that will be searched", "idxNode graph node", "idxNeigh neighbor that will be added to idxNode", "idxNode graph node", "idxNeigh neighbor that will be added to idxNode", "idxNode graph node", "n number of neighbors that will be added to idxNode", "elements neighbors that will be added to idxNode", "el sorted vector", "newEl element to be added to the sorted vector"], "has_pass2": true}, "src/CoinError.hpp": {"path": "layer-0/CoinUtils/src/CoinError.hpp", "filename": "CoinError.hpp", "file": "CoinError.hpp", "brief": "Exception class and assertion macros for COIN-OR error handling\n\nProvides CoinError, a rich exception class that captures context about\nwhere and why an error occurred, plus assertion macros that can optionally\nthrow CoinError instead of calling abort().", "see": ["CoinMessage.hpp for message handling without exceptions", "CoinAssert macros for assertion-based error reporting", "CoinError"], "return": "Description of what went wrong", "param": ["doPrint If false, do nothing (default true)", "message__ Description of the error condition", "methodName__ Name of the method detecting the error", "className__ Name of the class (or hint string for assertions)", "fileName_ Source file name (optional, for assertions)", "line Source line number (optional, -1 if not an assertion)", "source CoinError to copy from", "rhs CoinError to copy from"], "has_pass2": false}, "src/CoinWarmStartDual.hpp": {"path": "layer-0/CoinUtils/src/CoinWarmStartDual.hpp", "filename": "CoinWarmStartDual.hpp", "file": "CoinWarmStartDual.hpp", "brief": "Warm start using dual variable values only\n\nStores dual solution vector for warm starting interior point methods\nor dual simplex. Includes diff capability for branch-and-bound.", "see": ["CoinWarmStart for the abstract base class", "CoinWarmStartPrimalDual for combined primal/dual warm start"], "has_pass2": false}, "src/CoinOddWheelSeparator.hpp": {"path": "layer-0/CoinUtils/src/CoinOddWheelSeparator.hpp", "filename": "CoinOddWheelSeparator.hpp", "algorithm": "Odd-Wheel Separation via Shortest Paths\n\nClass for separating violated odd-cycles. It contains\na lifting module that tries to transform the odd-cycles\ninto odd-wheels.\n\n**Odd-Hole Inequalities:**\nFor an odd cycle C = {v₁,...,v₂ₖ₊₁} in the conflict graph:\nΣᵢ xᵢ ≤ k is valid (at most k of 2k+1 mutually conflicting vars can be 1)\n\n**Odd-Wheel Extension:**\nIf variable w conflicts with all cycle vertices, strengthen to:\nΣᵢ xᵢ + x_w ≤ k (wheel center w tightens the inequality)\n\n**Separation Algorithm:**\nUses shortest path in auxiliary graph where edge weights are\n1 - x_i - x_j. Violated odd holes correspond to negative-cost\nodd-length paths.", "math": "Odd-wheel: Σᵢ∈C xᵢ + |C| × x_w ≤ ⌊|C|/2⌋ with wheel center w", "complexity": "O(n² log n) per separation round using Dijkstra", "ref": ["Grotschel, Lovasz & Schrijver, \"Geometric Algorithms and\n     Combinatorial Optimization\", Springer, 1988", "Brito & Santos, \"Conflict graph based procedures for the\n     weight constrained minimum spanning tree problem\", 2020"], "file": "CoinOddWheelSeparator.hpp", "brief": "Odd-wheel cut separator", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}", "param": ["cgraph conflict graph", "x current solution of the LP relaxation of the MILP", "rc reduced cost of the variables in the LP relaxation\nof the MILP", "extMethod strategy that will be used to lift odd cycles,\ntransforming them into odd wheels: 0 = no lifting, 1 = only one\nvariable as wheel center, 2 = a clique as wheel center."], "has_pass2": true}, "src/CoinStructuredModel.hpp": {"path": "layer-0/CoinUtils/src/CoinStructuredModel.hpp", "filename": "CoinStructuredModel.hpp", "file": "CoinStructuredModel.hpp", "brief": "Block-structured model composed of CoinModel blocks", "algorithm": "Block Decomposition Detection (Dantzig-Wolfe, Benders, Staircase)\nAutomatically detects and decomposes LP/MIP structure for specialized\nalgorithms. decompose() identifies linking constraints and creates block\nstructure suitable for D-W decomposition, Benders cuts, or staircase LP.", "see": ["CoinModel for individual block representation"], "has_pass2": true}, "src/CoinPresolveDupcol.hpp": {"path": "layer-0/CoinUtils/src/CoinPresolveDupcol.hpp", "filename": "CoinPresolveDupcol.hpp", "file": "CoinPresolveDupcol.hpp", "brief": "Detect and remove duplicate columns and rows", "algorithm": "Duplicate Detection via Random Hashing:\n  To find duplicate columns efficiently:\n  1. Assign random weight r_i to each row\n  2. Compute column hash: h_j = Σ_i r_i · a_ij\n  3. Columns with same hash are candidates (verify exactly)\n\n  Duplicate columns (same cost): combine bounds\n  - x_j and x_k identical → replace with x_new, l_new = l_j + l_k\n\n  Duplicate rows: keep tighter constraint, remove redundant\n  - If row_i ⊆ row_k (interval containment), remove row_k", "math": "Column equivalence: a_*j = a_*k (element-wise)\n  Combined variable: x_new = x_j + x_k\n  Postsolve: split x_new* back to feasible (x_j*, x_k*)", "complexity": "Time: O(nnz) for hashing, O(candidates × nnz) for verification\n  Random hashing minimizes false positives\n  Common in models with symmetry or copy-paste construction", "see": ["CoinPresolveMatrix for the presolve framework"], "has_pass2": true}}}, "SuiteSparse": {"name": "SuiteSparse", "file_count": 95, "pass2_count": 49, "files": {"Mongoose/MATLAB/mongoose_mex.hpp": {"path": "layer-0/SuiteSparse/Mongoose/MATLAB/mongoose_mex.hpp", "filename": "mongoose_mex.hpp", "file": "mongoose_mex.hpp", "brief": "MATLAB MEX interface for Mongoose graph partitioning\nCopyright (C) 2017-2018, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nMATLAB interface utilities: conversion between MATLAB sparse matrices\nand Mongoose cs/Graph structures, option struct marshalling, result\nconversion to MATLAB arrays. Enables 'mongoose' MEX function for\ngraph partitioning from MATLAB.", "see": ["Mongoose.hpp for C++ library interface"], "has_pass2": false}, "Mongoose/Include/Mongoose_QPNapUp.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPNapUp.hpp", "filename": "Mongoose_QPNapUp.hpp", "file": "Mongoose_QPNapUp.hpp", "brief": "Upward lambda search in napsack solver\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPNapUp searches for lambda in increasing direction when current\nsolution violates upper balance bound (b > hi). Processes breakpoints\nvia heaps until constraint is satisfied, returning optimal lambda.", "algorithm": "Upward Breakpoint Search:\nIncreases λ to decrease a'x (currently b > hi, need b ≤ hi):\n1. Build heaps of breakpoints: λ values where variables hit bounds\n   - bound_heap: λ where free x_i hits 0 (lower bound)\n   - free_heap: λ where bound x_i = 1 becomes free\n2. Process breakpoints in increasing λ order:\n   - At each breakpoint, variable changes status (free↔bound)\n   - Update b = a'x incrementally\n   - Stop when b ≤ hi\n3. Interpolate final λ if stopping between breakpoints", "math": "Incremental b update as λ increases by δ:\nFor free variable i: Δx_i = -a_i·δ → Δb = -a_i²·δ\nVariables hitting bounds contribute discrete jumps.", "complexity": "O(n log n) for heap operations over n variables.", "see": ["Mongoose_QPNapsack.hpp for main napsack solver", "Mongoose_QPNapDown.hpp for downward search"], "has_pass2": true}, "Mongoose/Include/Mongoose_Refinement.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Refinement.hpp", "filename": "Mongoose_Refinement.hpp", "file": "Mongoose_Refinement.hpp", "brief": "Partition projection during uncoarsening phase\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nRefinement projects partition from coarse to fine graph during\nuncoarsening. Maps coarse partition to fine vertices via inverse\nmatchmap, then applies FM/QP improvement (waterdance) at each level\nfor high-quality final partition.", "algorithm": "Uncoarsening with Refinement (V-cycle):\nProjects and refines partition through coarsening hierarchy:\n1. Start with partition on coarsest graph G_L\n2. For each level l = L-1 down to 0:\n   a. Project partition: P(v) = P(coarse_map[v]) for fine v\n   b. Apply waterdance refinement (FM + QP)\n   c. Continue to next finer level\n3. Return refined partition on original graph G_0", "math": "Partition inheritance via matching map:\nIf vertex v in G_i was matched to form supervertex S in G_{i+1}:\nP_i(v) = P_{i+1}(S)\n\nCut can only improve during refinement (waterdance finds better cuts).", "complexity": "O(|E|) per level for projection + O(|E|) for refinement.\nTotal: O(|E| log |V|) across all levels.", "see": ["Mongoose_Coarsening.hpp for forward coarsening phase", "Mongoose_Waterdance.hpp for partition improvement at each level"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPGradProj.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPGradProj.hpp", "filename": "Mongoose_QPGradProj.hpp", "file": "Mongoose_QPGradProj.hpp", "brief": "Projected gradient descent for QP partition optimization\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPGradProj implements gradient projection for bound-constrained QP:\nminimizes quadratic cut objective subject to box constraints [0,1]\nand balance constraint (lo <= a'x <= hi). Projects gradient onto\nfeasible region, iterates until convergence or iteration limit.", "algorithm": "Gradient Projection for Bound-Constrained QP:\nProjected gradient descent with active set management:\n1. Compute gradient: g = Lx (graph Laplacian times current x)\n2. Identify active constraints (x_i = 0 or x_i = 1)\n3. Project gradient onto feasible region (box + balance)\n4. Line search along projected direction\n5. Update x, repeat until ||projected_grad|| < tolerance", "math": "Gradient projection iteration:\n$$x_{k+1} = P_C(x_k - \\alpha_k \\nabla f(x_k))$$\nwhere P_C projects onto feasible set C = {x : 0≤x≤1, lo≤a'x≤hi}.\n\nBalance constraint projection uses continuous knapsack solver.", "complexity": "O(|E|) per iteration for gradient Lx.\nTypically O(10-100) iterations for convergence.", "ref": ["Hager & Zhang (2006). \"Algorithm 851: CG_DESCENT, a conjugate\n  gradient method with guaranteed descent\". ACM TOMS 32(1):113-137."], "see": ["Mongoose_QPDelta.hpp for QP state (x, gradient, free set)", "Mongoose_QPNapsack.hpp for napsack subproblem solver"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPMaxHeap.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPMaxHeap.hpp", "filename": "Mongoose_QPMaxHeap.hpp", "file": "Mongoose_QPMaxHeap.hpp", "brief": "Max-heap for QP napsack breakpoint processing\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPMaxHeap provides max-heap operations (build, delete, add, heapify)\nfor efficient breakpoint processing in napsack solver. Extracts\nbreakpoints in descending order, complementing min-heap for\nbidirectional lambda search.", "algorithm": "Binary Max-Heap for Breakpoint Processing:\nClassic binary heap with maximum element at root:\n\nOperations (symmetric to min-heap):\n- build: Floyd's bottom-up O(n) heap construction\n- delete: Remove maximum, bubble down replacement, O(log n)\n- add: Insert at end, bubble up, O(log n)\n- heapify: Restore heap property after modification, O(log n)", "math": "Heap property for indices:\nheap[i] ≥ heap[2i] and heap[i] ≥ heap[2i+1]\nIndices stored in heap, values in external array x.\n\nUsed in QPNapDown: extract breakpoints in decreasing λ order.", "complexity": "O(n) build, O(log n) per operation.\nTotal for napsack: O(n log n) worst case.", "see": ["Mongoose_QPNapsack.hpp for napsack solver using heaps", "Mongoose_QPMinHeap.hpp for complementary min-heap"], "has_pass2": true}, "Mongoose/Include/Mongoose_ImproveQP.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_ImproveQP.hpp", "filename": "Mongoose_ImproveQP.hpp", "file": "Mongoose_ImproveQP.hpp", "brief": "Quadratic programming partition improvement via continuous relaxation\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQP improvement relaxes discrete partition to continuous [0,1] variables,\noptimizes via gradient projection with balance constraints, then rounds\nto discrete partition. Complements FM by exploring continuous solution\nspace; combined in waterdance for best results.", "algorithm": "Quadratic Programming Partition Refinement:\nContinuous relaxation approach for edge cut minimization:\n1. Relax discrete x_i ∈ {0,1} to continuous x_i ∈ [0,1]\n2. Formulate QP: min ½xᵀLx s.t. ∑w_i·x_i = W/2, 0≤x≤1\n3. Solve via gradient projection (projected gradient descent)\n4. Round continuous solution to discrete partition\n5. Use rounded solution to warm-start FM", "math": "Graph Laplacian QP formulation:\n$$\\min_{x} \\frac{1}{2} x^T L x = \\frac{1}{4} \\sum_{(i,j)\\in E} w_{ij}(x_i - x_j)^2$$\nwhere L = D - A is the graph Laplacian (D = degree matrix, A = adjacency).\n\nBalance constraint as equality: ∑ w_i·x_i = W·target_split\nBox constraints: 0 ≤ x_i ≤ 1 (relaxation of binary)", "complexity": "O(k·|E|) where k = gradient projection iterations.\nMatrix-vector product Lx costs O(|E|).", "ref": ["Hager et al. (2018). \"A graph partitioning algorithm based on\n  continuous quadratic programming\". SIAM J. Sci. Comput."], "see": ["Mongoose_QPGradProj.hpp for gradient projection solver", "Mongoose_ImproveFM.hpp for discrete FM alternative"], "has_pass2": true}, "Mongoose/Include/Mongoose_ImproveFM.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_ImproveFM.hpp", "filename": "Mongoose_ImproveFM.hpp", "file": "Mongoose_ImproveFM.hpp", "brief": "Fiduccia-Mattheyses partition refinement algorithm\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nFM algorithm improves partitions via gain-based vertex swapping.\nSwapCandidate tracks vertex, partition side, weight, gain, and heap\nposition. Algorithm maintains boundary heaps, selects highest-gain\nboundary vertex, swaps, and updates neighbor gains. Allows non-positive\nmoves within search_depth to escape local minima.", "algorithm": "Fiduccia-Mattheyses (FM) Partition Refinement:\nLocal search improving edge cut via gain-based vertex moves:\n1. Initialize boundary heaps (vertices with cross-partition edges)\n2. Select vertex v with maximum gain = external_deg - internal_deg\n3. Move v to other partition, lock it (can't move again this pass)\n4. Update gains of v's neighbors\n5. Repeat until all vertices locked\n6. Roll back to best partition seen during pass\n7. Iterate until no improvement or max_refinements reached", "math": "Gain of moving vertex v from partition A to B:\n$$gain(v) = \\sum_{u \\in N(v) \\cap B} w_{uv} - \\sum_{u \\in N(v) \\cap A} w_{uv}$$\n= (edges to B) - (edges to A) = benefit of moving v.\n\nNon-positive moves allowed within search_depth to escape local minima.\nBalance constraint: |W_A - W_B| ≤ tolerance enforced during moves.", "complexity": "O(|E|) per pass: each vertex moved once, gains updated in O(deg).\nTypically 2-5 passes needed for convergence.", "ref": ["Fiduccia & Mattheyses (1982). \"A Linear-Time Heuristic for Improving\n  Network Partitions\". DAC '82, pp. 175-181. [Original FM algorithm]"], "see": ["Mongoose_BoundaryHeap.hpp for boundary vertex management", "Mongoose_ImproveQP.hpp for QP-based alternative refinement"], "has_pass2": true}, "Mongoose/Include/Mongoose_Internal.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Internal.hpp", "filename": "Mongoose_Internal.hpp", "file": "Mongoose_Internal.hpp", "brief": "Internal type definitions and enumerations for Mongoose\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nDefines Int type (int64_t), matching strategies (Random, HEM, HEMSR,\nHEMSRdeg), initial cut types (QP, Random, NaturalOrder), and match\ntypes (Orphan, Standard, Brotherly, Community) used throughout Mongoose.", "algorithm": "Enumeration Definitions:\n\nMatchingStrategy - How to pair vertices during coarsening:\n- Random: Random neighbor selection, O(1) per vertex\n- HEM: Heavy Edge Matching, O(degree) per vertex\n- HEMSR: HEM with Stall Ratio, retries on mismatch\n- HEMSRdeg: HEMSR with degree ordering\n\nInitialEdgeCutType - How to partition coarsest graph:\n- QP: Quadratic programming relaxation (best quality)\n- Random: Random assignment (fastest)\n- NaturalOrder: First half vs second half\n\nMatchType - Classification of matched vertices:\n- Orphan: Unmatched (creates singleton coarse vertex)\n- Standard: Paired with single neighbor\n- Brotherly: 3-way match forming cycle\n- Community: 4-way match forming clique", "see": ["Mongoose_Matching.hpp for matching algorithm implementations"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPMinHeap.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPMinHeap.hpp", "filename": "Mongoose_QPMinHeap.hpp", "file": "Mongoose_QPMinHeap.hpp", "brief": "Min-heap for QP napsack breakpoint processing\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPMinHeap provides min-heap operations (build, delete, add, heapify)\nfor efficient breakpoint processing in napsack solver. Extracts\nbreakpoints in ascending order to find optimal lambda for balance\nconstraint satisfaction.", "algorithm": "Binary Min-Heap for Breakpoint Processing:\nClassic binary heap with minimum element at root:\n\nOperations:\n- build: Floyd's bottom-up O(n) heap construction\n- delete: Remove minimum, bubble down replacement, O(log n)\n- add: Insert at end, bubble up, O(log n)\n- heapify: Restore heap property after modification, O(log n)", "math": "Heap property for indices:\nheap[i] ≤ heap[2i] and heap[i] ≤ heap[2i+1]\nIndices stored in heap, values in external array x.\n\nUsed in QPNapUp: extract breakpoints in increasing λ order.", "complexity": "O(n) build, O(log n) per operation.\nTotal for napsack: O(n log n) worst case.", "see": ["Mongoose_QPNapsack.hpp for napsack solver using heaps", "Mongoose_QPMaxHeap.hpp for complementary max-heap"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPLinks.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPLinks.hpp", "filename": "Mongoose_QPLinks.hpp", "file": "Mongoose_QPLinks.hpp", "brief": "QP free set rounding and partition conversion\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPLinks converts continuous QP solution to discrete partition by\nrounding fractional variables and updating free set. Handles the\ninterface between continuous relaxation and discrete partition\nrepresentation in the waterdance refinement cycle.", "algorithm": "QP Solution Rounding:\nConverts continuous x ∈ [0,1]^n to discrete partition {0,1}^n:\n1. For each vertex v:\n   - If x_v already 0 or 1: keep as is\n   - If x_v ∈ (0,1): round to nearest (0 if x_v < 0.5, else 1)\n2. Update partition arrays and cut cost\n3. Identify new boundary vertices (neighbors across cut)\n4. Update free set status for subsequent QP iterations", "math": "Rounding preserves cut quality intuition:\nQP minimizes ½x'Lx which penalizes edges (i,j) by w_ij·(x_i - x_j)².\nFractional values indicate \"uncertain\" assignment; rounding to\nnearest integer preserves the QP's preference.", "complexity": "O(|E|) to scan edges and update cut cost.", "see": ["Mongoose_QPDelta.hpp for QP state representation", "Mongoose_ImproveQP.hpp for QP improvement workflow"], "has_pass2": true}, "Mongoose/Include/Mongoose_Sanitize.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Sanitize.hpp", "filename": "Mongoose_Sanitize.hpp", "file": "Mongoose_Sanitize.hpp", "brief": "Matrix preprocessing for graph partitioning\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nMatrix sanitization prepares input for partitioning: sanitizeMatrix\nhandles symmetry and binary weights, removeDiagonal strips self-loops,\nmirrorTriangular expands triangular to symmetric. Ensures valid\nundirected graph representation for algorithms.", "algorithm": "Graph Preprocessing Pipeline:\nTransforms arbitrary sparse matrix into valid undirected graph:\n\nsanitizeMatrix steps:\n1. Remove diagonal entries (self-loops invalid for partitioning)\n2. If triangular: mirror to create symmetric matrix A + A^T\n3. If binary requested: set all edge weights to 1\n4. Return cleaned CSC matrix", "math": "Symmetry requirement:\nGraph partitioning requires undirected graphs: A = A^T.\nIf input is lower triangular L, compute A = L + L^T.\nDiagonal removal: A = A - diag(A).\n\nBinary weights useful when only topology matters, not edge strengths.", "complexity": "O(n + nz) for all operations.\nmirrorTriangular doubles storage to 2×nz.", "see": ["Mongoose_IO.hpp for file input using sanitization", "Mongoose_CSparse.hpp for cs matrix operations"], "has_pass2": true}, "Mongoose/Include/Mongoose_EdgeCut.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_EdgeCut.hpp", "filename": "Mongoose_EdgeCut.hpp", "file": "Mongoose_EdgeCut.hpp", "brief": "Edge cut result structure and partitioning entry points\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nEdgeCut struct holds partitioning results: boolean partition array,\ncut_cost (edge weight sum), cut_size (edge count), partition weights\n(w0, w1), and imbalance metric. edge_cut() functions are main entry\npoints for computing graph partitions.", "algorithm": "Edge Cut Computation Entry Point:\nMain driver executing multilevel partitioning pipeline:\n1. Build coarsening hierarchy (Mongoose_Coarsening)\n2. Compute initial partition on coarsest graph\n3. Uncoarsen with FM/QP refinement at each level\n4. Return EdgeCut with partition[], cut_cost, imbalance", "math": "Cut quality metrics:\n- cut_cost = ∑_{(i,j)∈cut} w_{ij} (minimize this)\n- imbalance = |0.5 - W₀/W| where W = W₀ + W₁\n- Target: imbalance ≤ soft_split_tolerance", "see": ["Mongoose_EdgeCutOptions.hpp for algorithm parameters", "Mongoose_EdgeCutProblem.hpp for internal problem representation"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPBoundary.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPBoundary.hpp", "filename": "Mongoose_QPBoundary.hpp", "file": "Mongoose_QPBoundary.hpp", "brief": "QP boundary initialization from graph partition\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPBoundary initializes QP state from discrete partition, setting\nx values based on partition assignment and identifying boundary\nvertices (those with neighbors in opposite partition) as the\nactive set for optimization.", "algorithm": "QP Initialization from Discrete Partition:\nConverts binary partition to QP starting point:\n1. Set x_v = 0 for vertices in partition A, x_v = 1 for partition B\n2. Compute initial gradient g = Lx (graph Laplacian times x)\n3. Identify boundary vertices: those with edges crossing cut\n4. Add boundary vertices to free set (can move in [0,1])\n5. Mark interior vertices as bound (fixed at 0 or 1)", "math": "Boundary-only optimization:\nInterior vertices have all neighbors in same partition, so\nmoving them always increases cut. Only boundary vertices can\nimprove cut, so we optimize only over them (free set).\n\nReduces effective problem size from n to |boundary| << n.", "complexity": "O(|E|) to identify boundary vertices via edge scan.", "see": ["Mongoose_QPDelta.hpp for QP state structure", "Mongoose_QPGradProj.hpp for subsequent optimization"], "has_pass2": true}, "Mongoose/Include/Mongoose.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose.hpp", "filename": "Mongoose.hpp", "file": "Mongoose.hpp", "brief": "Main public API for Mongoose graph partitioning library\nCopyright (C) 2017-2018, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nMongoose: High-quality graph partitioning via multilevel coarsening\nwith Fiduccia-Mattheyses and quadratic programming refinement.\nPublic interface includes Graph, EdgeCut, EdgeCut_Options classes\nand read_graph/edge_cut functions for partitioning workflows.", "algorithm": "Multilevel Graph Partitioning (Mongoose):\nPartitions graph into two balanced sets minimizing edge cut:\n1. COARSEN: Repeatedly match vertices (HEM) and contract\n2. INITIAL CUT: Partition coarsest graph (QP or random)\n3. UNCOARSEN: Project partition up, refine at each level\n4. REFINE: Waterdance interleaving FM and QP improvement", "math": "Edge cut minimization with balance constraint:\n$$\\min \\sum_{(i,j)\\in E} w_{ij} \\cdot |x_i - x_j|$$\nsubject to: $$|W_0 - W_1| \\leq \\epsilon \\cdot W$$\nwhere x_i ∈ {0,1} is partition assignment, W_k = ∑_{x_i=k} w_i.", "complexity": "O(|E| log |V|) for multilevel algorithm.\nCoarsening: O(|E|) per level, O(log |V|) levels.\nFM refinement: O(|E|) per pass.", "ref": ["Davis et al. (2020). \"Algorithm 1003: Mongoose, a graph coarsening\n  and partitioning library\". ACM Trans. Math. Software 46(1):7.", "Karypis & Kumar (1998). \"A fast and high quality multilevel scheme\n  for partitioning irregular graphs\". SIAM J. Sci. Comput. 20(1):359-392."], "see": ["Mongoose_EdgeCut.hpp for partition result structure", "Mongoose_EdgeCutOptions.hpp for algorithm configuration"], "param": ["filename the filename or path to the Matrix Market File.", "filename the filename or path to the Matrix Market File."], "has_pass2": true}, "Mongoose/Include/Mongoose_Logger.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Logger.hpp", "filename": "Mongoose_Logger.hpp", "file": "Mongoose_Logger.hpp", "brief": "Debug logging and performance timing utilities\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nLogger class provides runtime debug level control and tic/toc timing\nfor algorithm phases: Matching, Coarsening, Refinement, FM, QP, IO.\nLogError/Warn/Info/Test macros for conditional output. Timing tracks\ncumulative time per phase for profiling.", "see": ["Mongoose_Debug.hpp for debug assertions and macros"], "param": ["timingType The portion of the library being timed (MatchingTiming,\n  CoarseningTiming, RefinementTiming, FMTiming, QPTiming, or IOTiming).", "timingType The portion of the library being timed (MatchingTiming,\n  CoarseningTiming, RefinementTiming, FMTiming, QPTiming, or IOTiming).", "timingType The portion of the library being timed (MatchingTiming,\n  CoarseningTiming, RefinementTiming, FMTiming, QPTiming, or IOTiming)."], "has_pass2": false}, "Mongoose/Include/Mongoose_IO.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_IO.hpp", "filename": "Mongoose_IO.hpp", "file": "Mongoose_IO.hpp", "brief": "Matrix Market file I/O for graphs and matrices\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nFile I/O for Mongoose: read_graph creates Graph from Matrix Market file,\nread_matrix creates cs struct. Handles symmetrization (A+A')/2 for\nasymmetric matrices, extracts largest connected component, removes\ndiagonal. Accepts C string or std::string filenames.", "algorithm": "Matrix Market Import Pipeline:\nReads standard sparse matrix format into Mongoose graph:\n\n1. Parse Matrix Market header (format, type, symmetry)\n2. Read coordinate entries into triplet form\n3. Convert triplet to CSC via cs_compress\n4. Symmetrize if needed: A = (A + A^T)/2\n5. Remove diagonal (self-loops)\n6. Extract largest connected component (via BFS)\n7. Return Graph or cs struct", "math": "Connected component extraction:\nBFS from arbitrary vertex marks reachable set.\nIf not all vertices reached, restart BFS from unmarked vertex.\nKeep only largest component for partitioning.", "complexity": "O(n + nz) for parsing and preprocessing.", "see": ["Mongoose_Graph.hpp for Graph class", "Mongoose_CSparse.hpp for cs matrix struct"], "param": ["filename the filename or path to the Matrix Market File.", "filename the filename or path to the Matrix Market File.", "matcode the four character Matrix Market type code.", "filename the filename or path to the Matrix Market File.", "filename the filename or path to the Matrix Market File.", "matcode the four character Matrix Market type code."], "has_pass2": true}, "Mongoose/Include/Mongoose_Graph.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Graph.hpp", "filename": "Mongoose_Graph.hpp", "file": "Mongoose_Graph.hpp", "brief": "Graph data structure for Mongoose partitioning\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nGraph class stores adjacency in CSC format (p, i arrays) with optional\nedge weights (x) and vertex weights (w). Factory methods create from\nraw arrays or CSparse matrices. Shallow copy flags track ownership.", "algorithm": "Compressed Sparse Column (CSC) Graph Representation:\nStandard sparse matrix format optimized for column access:\n\nArrays:\n- p[n+1]: Column pointers. Neighbors of vertex j are at i[p[j]..p[j+1]-1]\n- i[nz]: Row indices (neighbor IDs for each edge)\n- x[nz]: Edge weights (optional, default 1)\n- w[n]: Vertex weights (optional, default 1)", "math": "CSC interpretation for graphs:\nFor undirected graph, store symmetric adjacency matrix A:\nA[i,j] = edge weight between i and j (0 if no edge).\np[j+1] - p[j] = degree of vertex j.\n\nMemory ownership: shallow_* flags indicate borrowed vs owned arrays.\nFactory methods from CSparse matrices share data when possible.", "complexity": "O(n + nz) storage.\nNeighbor iteration: O(degree) per vertex.", "see": ["Mongoose_EdgeCutProblem.hpp for extended graph with partition state", "Mongoose_CSparse.hpp for CSparse matrix format"], "has_pass2": true}, "Mongoose/Include/Mongoose_CSparse.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_CSparse.hpp", "filename": "Mongoose_CSparse.hpp", "file": "Mongoose_CSparse.hpp", "brief": "Sparse matrix operations subset from CSparse library\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nProvides CSparse subset for sparse matrix operations: cs struct\n(compressed column/triplet format), cs_add (matrix addition),\ncs_transpose, cs_compress (triplet to CSC), and allocation.\nUses int64_t (csi) matching Mongoose's Int type.", "algorithm": "CSparse Subset for Graph I/O:\nMinimal CSparse operations needed for graph construction:\n\nData structure (cs):\n- CSC mode (nz == -1): p[n+1] column pointers, i[nzmax] row indices\n- Triplet mode (nz >= 0): p[nzmax] col indices, i[nzmax] row indices\n\nOperations:\n- cs_compress: Convert triplet to CSC, O(n + nz)\n- cs_transpose: Compute A^T, O(n + nz)\n- cs_add: Compute αA + βB, O(nnz(A) + nnz(B))", "math": "Triplet to CSC conversion:\n1. Count column lengths: O(nz)\n2. Cumulative sum → column pointers: O(n)\n3. Scatter entries into CSC: O(nz)", "complexity": "All operations O(n + nz) time and space.", "ref": ["Davis (2006). \"Direct Methods for Sparse Linear Systems\".\n  SIAM, Chapters 1-2 for CSparse design."], "see": ["Mongoose_Graph.hpp for graph representation using cs format", "Mongoose_IO.hpp for reading matrices from files"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPDelta.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPDelta.hpp", "filename": "Mongoose_QPDelta.hpp", "file": "Mongoose_QPDelta.hpp", "brief": "QP solver state: solution, gradient, free set, and workspace\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPDelta stores iterative QP solver state: current solution x, gradient,\nfree set (variables not at bounds), balance constraint bounds (lo/hi),\nLagrange multiplier lambda, and workspace arrays. FreeSet_status tracks\nwhether each x_i is at 0, 1, or strictly between (free).", "algorithm": "Active Set Representation for Bound-Constrained QP:\nMaintains partition of variables into three sets:\n- FreeSet (status = 0): 0 < x_i < 1, can move freely\n- LowerBound (status = -1): x_i = 0, held at lower bound\n- UpperBound (status = +1): x_i = 1, held at upper bound\n\nKey invariants:\n- Gradient and x always consistent: g = Lx\n- Balance constraint tracked: b = a'x ∈ [lo, hi]\n- Free set list enables O(|F|) iteration over free variables", "math": "Optimality via KKT conditions:\nAt solution: g_i = λa_i for free variables (gradient aligned with constraint).\nFor bound variables: sign of (g_i - λa_i) confirms bound is correct.", "complexity": "O(n) storage for solution, gradient, and status arrays.", "see": ["Mongoose_QPGradProj.hpp for gradient projection using this state", "Mongoose_QPLinks.hpp for free set management"], "has_pass2": true}, "Mongoose/Include/Mongoose_Debug.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Debug.hpp", "filename": "Mongoose_Debug.hpp", "file": "Mongoose_Debug.hpp", "brief": "Debug macros, assertions, and diagnostic print functions\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nDebug infrastructure: ASSERT/DEBUG macros (disabled by NDEBUG),\nPR macro for printf debugging, IMPLIES/IFF logic macros. Print\nfunctions for cs matrices, EdgeCutProblem graphs, and QP state.\nEnable debugging by uncommenting #undef NDEBUG (very slow).", "see": ["Mongoose_Logger.hpp for runtime-configurable logging"], "has_pass2": false}, "Mongoose/Include/Mongoose_CutCost.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_CutCost.hpp", "filename": "Mongoose_CutCost.hpp", "file": "Mongoose_CutCost.hpp", "brief": "Partition quality metrics structure\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nCutCost struct holds partition quality metrics: heuCost (cut + balance\npenalty), cutCost (edge weight sum), W[2] (partition weights), and\nimbalance (deviation from target split). Used internally to evaluate\nand compare partitions during refinement.", "algorithm": "Partition Quality Evaluation:\nTracks two competing objectives during partitioning:\n\nFields:\n- cutCost: Σ w_ij for edges (i,j) crossing the partition\n- W[0], W[1]: Total vertex weight in each partition\n- imbalance: target_split - W[0]/W (deviation from ideal)\n- heuCost: Combined cost = cutCost + penalty(imbalance)", "math": "Heuristic cost with balance penalty:\n$$heuCost = cutCost + \\lambda \\cdot (imbalance)^2$$\n\nPenalty term ensures algorithm finds balanced partitions.\nλ trades off cut quality vs balance: higher λ → more balanced.", "complexity": "O(1) to store and compare; O(|E|) to recompute from scratch.\nIncremental updates during FM: O(degree) per vertex move.", "see": ["Mongoose_EdgeCut.hpp for public partition result", "Mongoose_ImproveFM.hpp for gain calculations"], "has_pass2": true}, "Mongoose/Include/Mongoose_GuessCut.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_GuessCut.hpp", "filename": "Mongoose_GuessCut.hpp", "file": "Mongoose_GuessCut.hpp", "brief": "Initial partition generation at coarsest level\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nGuessCut creates initial partition for coarsest graph before refinement\nbegins. Strategies include QP relaxation, random assignment, or natural\nvertex order. Quality of initial guess affects final partition quality\ndespite refinement. Selected via initial_cut_type option.", "algorithm": "Initial Partition Strategies:\nCreates first partition at coarsest level in multilevel hierarchy:\n\n1. QP (InitialEdgeCut_QP): Solve QP relaxation, round to {0,1}\n   - Best quality but highest cost\n   - Recommended for small coarse graphs (<100 vertices)\n\n2. Random (InitialEdgeCut_Random): Random assignment respecting balance\n   - Fast but may need many FM passes to reach good cut\n\n3. Natural (InitialEdgeCut_Natural): First half vs second half\n   - Fastest, works well if input has locality", "math": "Quality impact on final result:\nGood initial cut reduces FM iterations needed during uncoarsening.\nWith multilevel: even random starts converge to similar quality,\nbut better starts reduce total work.", "complexity": "O(n²) for QP, O(n) for random/natural on coarse graph.\nCoarse graph typically has O(√n) vertices, so even QP is cheap.", "see": ["Mongoose_EdgeCutOptions.hpp for InitialEdgeCutType selection", "Mongoose_ImproveQP.hpp for QP-based initial cut"], "has_pass2": true}, "Mongoose/Include/Mongoose_BoundaryHeap.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_BoundaryHeap.hpp", "filename": "Mongoose_BoundaryHeap.hpp", "file": "Mongoose_BoundaryHeap.hpp", "brief": "Boundary vertex heap for FM partition refinement\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nManages heaps of boundary vertices (those with edges crossing cut)\nfor FM algorithm. Two heaps (one per partition) ordered by vertex\ngain. Operations: load, clear, insert, remove, heapifyUp/Down.\nCritical for efficient O(n log n) FM refinement.", "algorithm": "Boundary Heap for FM Refinement:\nMax-heap of boundary vertices ordered by move gain:\n\nStructure: Two heaps, one per partition (A and B).\n- Only boundary vertices (with edges to other partition) are in heap\n- Interior vertices excluded (moving them always hurts cut)\n\nOperations:\n- bhLoad: Initialize heaps with all boundary vertices, O(n)\n- bhInsert: Add vertex to appropriate heap, O(log n)\n- bhRemove: Remove vertex (after move), O(log n)\n- heapifyUp/Down: Restore heap property after gain update, O(log n)", "math": "Gain ordering enables greedy selection:\nFM always moves vertex with maximum gain (heap top).\ngain(v) = external_edges(v) - internal_edges(v)\nPositive gain → moving v reduces cut.", "complexity": "Each FM pass: O(n log n) for n heap operations.\nHeap property ensures O(1) access to best move.", "see": ["Mongoose_ImproveFM.hpp for FM algorithm using boundary heaps", "Mongoose_EdgeCutProblem.hpp for bhHeap storage"], "has_pass2": true}, "Mongoose/Include/Mongoose_EdgeCutOptions.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_EdgeCutOptions.hpp", "filename": "Mongoose_EdgeCutOptions.hpp", "file": "Mongoose_EdgeCutOptions.hpp", "brief": "Configuration options for edge cut partitioning algorithms\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nEdgeCut_Options controls all algorithm parameters: coarsening (limit,\nmatching strategy, community detection), initial cut type (QP/random),\nFiduccia-Mattheyses (search depth, refinement count), QP gradient\nprojection (tolerance, iteration limit), and partition targets\n(split ratio, balance tolerance).", "algorithm": "Configuration Parameter Groups:\nControls algorithm behavior at each phase:\n\nCoarsening phase:\n- coarsen_limit: Stop coarsening at this size (default ~64)\n- matching_strategy: HEM, SHEM, or random (HEM usually best)\n- do_community_matching: Enable 3-4 vertex community detection\n\nInitial partition:\n- initial_cut_type: QP (best quality) or Random (fast)\n\nRefinement phase:\n- num_dances: FM/QP alternation count (1-3 typical)\n- FM_search_depth: Non-improving moves before stopping\n- FM_max_num_refinements: FM passes per level\n- gradproj_tolerance: QP convergence threshold\n- gradproj_iteration_limit: Max QP iterations\n\nTarget metrics:\n- target_split: Desired partition ratio (0.5 = balanced)\n- soft_split_tolerance: Acceptable imbalance range", "complexity": "O(1) to create/access options.", "see": ["Mongoose_ImproveFM.hpp for FM algorithm", "Mongoose_ImproveQP.hpp for QP refinement"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPNapDown.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPNapDown.hpp", "filename": "Mongoose_QPNapDown.hpp", "file": "Mongoose_QPNapDown.hpp", "brief": "Downward lambda search in napsack solver\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPNapDown searches for lambda in decreasing direction when current\nsolution violates lower balance bound (b < lo). Processes breakpoints\nvia heaps until constraint is satisfied, returning optimal lambda.", "algorithm": "Downward Breakpoint Search:\nDecreases λ to increase a'x (currently b < lo, need b ≥ lo):\n1. Build heaps of breakpoints: λ values where variables hit bounds\n   - bound_heap: λ where free x_i hits 1 (upper bound)\n   - free_heap: λ where bound x_i = 0 becomes free\n2. Process breakpoints in decreasing λ order:\n   - At each breakpoint, variable changes status (free↔bound)\n   - Update b = a'x incrementally\n   - Stop when b ≥ lo\n3. Interpolate final λ if stopping between breakpoints", "math": "Symmetric to QPNapUp but decreasing λ:\nFor free variable i: Δx_i = -a_i·δ (δ < 0) → Δb > 0 for a_i > 0\nDecreasing λ increases x values, increasing a'x.", "complexity": "O(n log n) for heap operations over n variables.", "see": ["Mongoose_QPNapsack.hpp for main napsack solver", "Mongoose_QPNapUp.hpp for upward search"], "has_pass2": true}, "Mongoose/Include/Mongoose_EdgeCutProblem.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_EdgeCutProblem.hpp", "filename": "Mongoose_EdgeCutProblem.hpp", "file": "Mongoose_EdgeCutProblem.hpp", "brief": "Extended graph with matching and partition state for algorithms\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nEdgeCutProblem extends Graph with algorithm state: partition array,\nvertex gains, external degrees, boundary heaps, cut metrics. Also\nstores matching data (matchmap, invmatchmap, matchtype) for multilevel\nhierarchy. Provides inline helpers for boundary heap and mark array.", "algorithm": "Multilevel Algorithm State Management:\nEncapsulates all mutable state for partition algorithms:\n\nPartition state:\n- partition[v]: Boolean, which side vertex v is on\n- vertexGains[v]: Current FM gain for moving v\n- externalDegree[v]: # edges crossing cut from v\n- bhHeap[2], bhSize[2]: Boundary heaps for each partition\n\nMatching state (for coarsening):\n- matchmap[v]: Coarse vertex ID that v maps to\n- invmatchmap[c]: Fine vertex that coarse c came from\n- matchtype[v]: How v was matched (standard, brotherly, community)", "math": "Mark array with O(1) amortized clear:\nInstead of clearing n elements, increment markValue.\nmark(v): markArray[v] = markValue\nisMarked(v): markArray[v] == markValue\nAmortized O(1) clear by incrementing markValue.", "complexity": "O(n) storage for all per-vertex arrays.\nTotal memory: ~10n integers + 5n doubles for n-vertex graph.", "see": ["Mongoose_Graph.hpp for base graph structure", "Mongoose_BoundaryHeap.hpp for heap operations"], "has_pass2": true}, "Mongoose/Include/Mongoose_Waterdance.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Waterdance.hpp", "filename": "Mongoose_Waterdance.hpp", "file": "Mongoose_Waterdance.hpp", "brief": "Alternating FM/QP refinement passes for partition improvement\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nWaterdance alternates between FM (discrete swaps) and QP (continuous\noptimization) refinement passes. The interplay (\"dance\") between methods\nescapes local minima that either method alone would get stuck in.\nNumber of dances controlled by num_dances option.", "algorithm": "Waterdance (Hybrid FM/QP Refinement):\nAlternates discrete and continuous optimization to escape local minima:\nfor i = 1 to num_dances:\n  1. Run QP gradient projection (continuous relaxation)\n  2. Round QP solution to discrete partition\n  3. Run FM passes (discrete vertex swaps)\n  4. If no improvement in both, terminate early", "math": "Intuition for hybrid approach:\n- FM: fast local moves, explores discrete neighborhood\n- QP: smooth objective, can \"jump over\" discrete barriers\n- Alternating: FM finds local minimum → QP perturbs → FM refines\n\nThe \"waterdance\" metaphor: like water finding lowest point via\ndifferent flow patterns (discrete drops vs continuous flow).", "complexity": "O(num_dances × |E|) per coarsening level.\nDefault num_dances = 1-2; more for difficult graphs.", "see": ["Mongoose_ImproveFM.hpp for Fiduccia-Mattheyses refinement", "Mongoose_ImproveQP.hpp for quadratic programming refinement"], "has_pass2": true}, "Mongoose/Include/Mongoose_Coarsening.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Coarsening.hpp", "filename": "Mongoose_Coarsening.hpp", "file": "Mongoose_Coarsening.hpp", "brief": "Graph coarsening via vertex matching for multilevel partitioning\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nCoarsening reduces graph size while preserving structure by contracting\nmatched vertices. Given a matching (HEM, random, etc.), creates coarser\ngraph with merged vertices and aggregated edge weights. Essential for\nmultilevel partitioning to handle large graphs efficiently.", "algorithm": "Graph Coarsening (Vertex Contraction):\nCreates smaller graph G' from G by merging matched vertex pairs:\n1. Use matching (match[i] = j means vertices i,j are paired)\n2. Create supervertex for each matched pair (or singleton)\n3. Aggregate vertex weights: w'_k = w_i + w_j\n4. Combine edges: e'(k,l) = e(i,l) + e(j,l) for matched pair (i,j)\n5. Remove self-loops (edges within matched pairs become internal)", "math": "Coarsening preserves cut structure:\nFor partition P of G inducing P' on G':\ncut(P) = cut(P') + internal edges (contracted)\nSince internal edges can't be cut, optimizing on G' approximates G.", "complexity": "O(|E|) per coarsening level.\nTotal levels: O(log |V|) until |V'| < coarsen_limit.", "see": ["Mongoose_Matching.hpp for vertex matching algorithms", "Mongoose_Refinement.hpp for uncoarsening/projection"], "has_pass2": true}, "Mongoose/Include/Mongoose_Matching.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Matching.hpp", "filename": "Mongoose_Matching.hpp", "file": "Mongoose_Matching.hpp", "brief": "Vertex matching algorithms for graph coarsening\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nImplements matching strategies: Random (random neighbor), HEM (heavy\nedge matching), SR/SRdeg (sorted/degree-sorted heavy edge). Matching\npairs vertices for coarsening; heavier edges are preferred to preserve\ngraph structure. Cleanup handles unmatched (orphan) vertices.", "algorithm": "Heavy Edge Matching (HEM):\nPairs vertices connected by heaviest edges to preserve structure:\n1. Visit vertices in random order (for randomization)\n2. For unmatched vertex v, find heaviest edge to unmatched neighbor u\n3. Match v with u: match[v] = u, match[u] = v\n4. Continue until all vertices matched or isolated\n5. Cleanup: handle orphan vertices (no valid matches)", "math": "Heavy edge principle: prefer matching (i,j) if w_ij is large.\nRationale: heavy edges likely internal to optimal partition,\nso collapsing them preserves cut structure on coarser graph.\n\nVariants:\n- Random: match with random neighbor (fastest)\n- HEM: match with heaviest-edge neighbor\n- HEMSR: sorted random ordering for HEM\n- HEMSRdeg: degree-sorted ordering (low-degree first)", "complexity": "O(|E|) for all variants (single pass through edges).", "see": ["Mongoose_Coarsening.hpp for graph contraction using matches", "Mongoose_Internal.hpp for MatchingStrategy enum"], "has_pass2": true}, "Mongoose/Include/Mongoose_Random.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_Random.hpp", "filename": "Mongoose_Random.hpp", "file": "Mongoose_Random.hpp", "brief": "Random number generation for Mongoose algorithms\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nSimple random number interface: random() returns Int, setRandomSeed()\ninitializes generator. Used by random matching, random initial cuts,\nand tie-breaking in algorithms. Seed set via EdgeCut_Options::random_seed.", "algorithm": "Reproducible Randomization:\nProvides deterministic pseudo-random sequence for algorithm reproducibility:\n\nUsage in Mongoose:\n- Random matching: select random neighbor when ties occur\n- Random initial cut: assign vertices randomly at coarsest level\n- Tie-breaking: deterministic choice when gains are equal\n\nReproducibility: Same seed → same random sequence → identical partitions.\nSet via EdgeCut_Options::random_seed to enable reproducible experiments.", "complexity": "O(1) per random number generation.", "see": ["Mongoose_EdgeCutOptions.hpp for random_seed option", "Mongoose_Matching.hpp for random matching strategy"], "has_pass2": true}, "Mongoose/Include/Mongoose_QPNapsack.hpp": {"path": "layer-0/SuiteSparse/Mongoose/Include/Mongoose_QPNapsack.hpp", "filename": "Mongoose_QPNapsack.hpp", "file": "Mongoose_QPNapsack.hpp", "brief": "Napsack subproblem solver for QP balance constraint\nCopyright (C) 2017-2023, Scott P. Kolodziej, Nuri S. Yeralan,\nTimothy A. Davis, William W. Hager. GPL-3.0-only license.\n\nQPNapsack solves the napsack subproblem: find lambda such that the\nprojected solution satisfies balance constraint lo <= a'x <= hi.\nUses breakpoint method with heaps to efficiently find optimal lambda.\nCore subroutine in QP gradient projection.", "algorithm": "Continuous Knapsack (Breakpoint Search):\nFinds Lagrange multiplier λ for balance constraint a'x ∈ [lo, hi]:\n1. Initialize: compute a'y for unconstrained projection y\n2. If a'y ∈ [lo, hi]: done, λ = 0\n3. Otherwise: binary/heap search for breakpoint where constraint binds\n   - Breakpoints: λ values where some x_i hits 0 or 1\n   - Track a'x as λ changes (piecewise linear in λ)\n4. Return λ such that a'x = lo (or hi)", "math": "Projection with balance constraint:\n$$x_i = \\Pi_{[0,1]}(y_i - \\lambda a_i)$$\nwhere λ is chosen so $\\sum_i a_i x_i \\in [lo, hi]$.\n\nAs λ increases: x_i with positive a_i decrease → a'x decreases.\nAs λ decreases: x_i with positive a_i increase → a'x increases.", "complexity": "O(n log n) due to heap-based breakpoint enumeration.\nDominates each QP iteration when balance constraint is tight.", "ref": ["Kiwiel (2008). \"Breakpoint searching algorithms for the\n  continuous quadratic knapsack problem\". Math Programming 112(2)."], "see": ["Mongoose_QPNapUp.hpp for upward lambda search", "Mongoose_QPNapDown.hpp for downward lambda search"], "has_pass2": true}, "SPQR/Include/SuiteSparseQR.hpp": {"path": "layer-0/SuiteSparse/SPQR/Include/SuiteSparseQR.hpp", "filename": "SuiteSparseQR.hpp", "file": "SuiteSparseQR.hpp", "brief": "User C++ API for sparse multifrontal QR factorization\nCopyright (c) 2008-2023, Timothy A Davis. GPL-2.0+ license.\n\nMain user interface for SPQR: SuiteSparseQR overloads for [Q,R,E]=qr(A),\nX=A\\B, qmult. Structures: spqr_symbolic (pattern analysis), spqr_numeric\n(R values, Householder H), spqr_gpu (GPU staging). Expert functions:\nSuiteSparseQR_factorize, _solve, _min2norm, _symbolic, _numeric for\nfactorization reuse. Supports real/complex, int32/int64.", "algorithm": "Sparse Multifrontal QR Factorization:\nComputes A·P = Q·R where P is fill-reducing permutation:\n\n1. Symbolic Analysis (spqr_symbolic):\n   - Compute column elimination tree of A'A\n   - Find supernodal structure (frontal matrices)\n   - Allocate Householder vector storage\n\n2. Numeric Factorization (spqr_numeric):\n   - Process fronts bottom-up in elimination tree\n   - Each front: dense QR via Householder reflections\n   - Assemble contribution blocks from children\n   - Store R factor and optionally H vectors\n\n3. Solve (optional):\n   - Q'b via applying H vectors\n   - R\\(Q'b) via back-substitution", "math": "Householder QR at each front:\nFor front F = [A_rows; C_children], compute F = Q_F · [R_F; 0]\nusing Householder reflections H_i = I - τ_i v_i v_i'.", "complexity": "O(nnz(R)²/n) for sparse QR, where nnz(R) depends on fill-in.\nMultifrontal organization enables parallelism across independent fronts.", "ref": ["Davis (2011). \"Algorithm 915: SuiteSparseQR, a multifrontal\n  multithreaded sparse QR factorization package\". ACM TOMS 38(1)."], "see": ["spqr.hpp for internal implementation"], "has_pass2": true}, "SPQR/Include/spqr.hpp": {"path": "layer-0/SuiteSparse/SPQR/Include/spqr.hpp", "filename": "spqr.hpp", "file": "spqr.hpp", "brief": "Internal SPQR implementation functions and data structures\nCopyright (c) 2008-2023, Timothy A Davis. GPL-2.0+ license.\n\nNon-user-callable routines: spqr_analyze (symbolic), spqr_factorize (numeric),\nspqr_kernel (parallel front factorization), spqr_assemble/cpack/rhpack (front\nassembly). Support: spqr_tol, stranspose1/2, larftb (block reflectors),\nhapply, panel, 1colamd, 1fixed. Helper structs: spqr_work, spqr_blob.\nMacros: FLIP/UNFLIP for marking, INDEX for column-major.", "algorithm": "Multifrontal QR Implementation:\nInternal routines for sparse QR factorization:\n\nSymbolic phase (spqr_analyze):\n- Build S = A(P,Q) in row-oriented form\n- Compute supernodal structure: fronts, parent/child\n- Allocate task graph for parallel numeric phase\n\nNumeric phase (spqr_factorize → spqr_kernel):\n- Process fronts in parallel via task graph\n- spqr_assemble: gather rows and child contributions into front\n- spqr_front: dense QR via blocked Householder (LAPACK-style)\n- spqr_cpack/rhpack: pack contribution block and R+H for storage", "math": "Block Householder transformations:\nApply panels of k reflectors using WY form: Q = I - V·T·V'\nwhere T is k×k upper triangular. Enables BLAS-3 performance.\n\nspqr_larftb: Apply block reflector to trailing submatrix.\nspqr_happly: Apply stored H vectors to dense matrix X.", "complexity": "O(flops) where flops dominated by dense BLAS-3 in fronts.\nParallel speedup from independent subtrees in elimination tree.", "see": ["SuiteSparseQR.hpp for public user API"], "has_pass2": true}, "SPQR/Include/spqr_cholmod_wrappers.hpp": {"path": "layer-0/SuiteSparse/SPQR/Include/spqr_cholmod_wrappers.hpp", "filename": "spqr_cholmod_wrappers.hpp", "file": "spqr_cholmod_wrappers.hpp", "brief": "Template wrappers for CHOLMOD functions with int32/int64 support\nCopyright (c) 2008-2023, Timothy A Davis. GPL-2.0+ license.\n\nTemplate wrappers enabling SPQR to use CHOLMOD with either int32_t or int64_t\nindices. Functions: spqr_start/finish (init), memory (malloc/calloc/free/realloc),\nsparse matrix ops (allocate_sparse, free_sparse, transpose, copy), dense\n(allocate_dense, zeros, ones), ordering (amd, metis, colamd, postorder),\nutility (norm_sparse/dense, sdmult, ssmult, ssadd). Each has explicit\ntemplate specializations for int32_t and int64_t.", "see": ["spqr.hpp includes this header"], "has_pass2": false}, "SPQR/Include/spqrgpu.hpp": {"path": "layer-0/SuiteSparse/SPQR/Include/spqrgpu.hpp", "filename": "spqrgpu.hpp", "file": "spqrgpu.hpp", "brief": "GPU kernel interfaces for SPQR sparse QR factorization\nCopyright (c) 2008-2023, Timothy A Davis. GPL-2.0+ license.\n\nGPU acceleration entry points: spqrgpu_kernel (processes spqr_blob on GPU),\nspqrgpu_computeFrontStaging (plans front staging by GPU memory capacity),\nspqrgpu_buildAssemblyMaps (constructs Rimap/Rjmap for contribution assembly).\nTemplates for double/Complex Entry types. Requires GPUQREngine_SuiteSparse.hpp.", "see": ["GPUQREngine_SuiteSparse.hpp for underlying GPU engine", "spqr.hpp for spqr_blob structure"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_SEntry.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_SEntry.hpp", "filename": "GPUQREngine_SEntry.hpp", "file": "GPUQREngine_SEntry.hpp", "brief": "Sparse entry tuple for GPU front assembly\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nSEntry struct: (findex, value) tuple for placing sparse matrix values\ninto dense frontal matrices. findex is linear index into front, value\nis numeric entry. Used in S assembly phase (ASSEMBLE_S state) to transfer\ninput matrix to GPU fronts via cpuS/gpuS arrays in SparseMeta.", "see": ["GPUQREngine_SparseMeta.hpp uses SEntry arrays"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_Stats.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_Stats.hpp", "filename": "GPUQREngine_Stats.hpp", "file": "GPUQREngine_Stats.hpp", "brief": "Performance statistics for GPU QR factorization\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nQREngineStats template struct: kernelTime (total GPU kernel time across\nlaunches), numLaunches (kernel invocation count), flopsActual (total\nfloating-point operations). Optional output parameter for GPUQREngine_Internal.", "see": ["GPUQREngine_Internal.hpp uses stats parameter"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_BucketList.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_BucketList.hpp", "filename": "GPUQREngine_BucketList.hpp", "file": "GPUQREngine_BucketList.hpp", "brief": "Tile bucket management and GPU task generation\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nBucketList manages LLBundles in doubly-linked lists organized by column\nbucket. Tracks idle tiles (head/next/prev), generates Factorize/Apply\ntasks. Wavefront advances through buckets. VT block allocation via\nwsMongoVT workspace. Methods: Initialize, AdvanceBundles, CreateBundles,\nFillWorkQueue. Supports staircase exploitation for sparse factorization.", "see": ["GPUQREngine_LLBundle.hpp for bundle structure", "GPUQREngine_TaskDescriptor.hpp for generated tasks"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_Front.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_Front.hpp", "filename": "GPUQREngine_Front.hpp", "file": "GPUQREngine_Front.hpp", "brief": "Frontal matrix class for GPU QR factorization\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nFront class encapsulates frontal matrix metadata: dimensions (fm×fn),\nCPU/GPU pointers (F, gpuF, cpuR), factorization state, staircase for\nexploiting block zeros. SparseMeta member extends for multifrontal\nsparse factorization. State machine tracks: ALLOCATE_WAIT → FACTORIZE → DONE.", "see": ["GPUQREngine_Scheduler.hpp for front coordination", "GPUQREngine_FrontState.hpp for state machine definition"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_GraphVizHelper.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_GraphVizHelper.hpp", "filename": "GPUQREngine_GraphVizHelper.hpp", "file": "GPUQREngine_GraphVizHelper.hpp", "brief": "Debug visualization for bucket list state\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nGPUQREngine_RenderBuckets function generates GraphViz output for\nBucketList visualization. Only compiled when GPUQRENGINE_RENDER defined.\nUsed for debugging factorization scheduling and tile progression.", "see": ["GPUQREngine_BucketList.hpp for rendered data structure"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine.hpp", "filename": "GPUQREngine.hpp", "file": "GPUQREngine.hpp", "brief": "Version information for GPUQREngine CUDA library\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nVersion macros for GPUQREngine GPU-accelerated QR factorization.\nCurrent version 4.3.4 (June 2024). Part of SuiteSparse SPQR.", "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_LLBundle.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_LLBundle.hpp", "filename": "GPUQREngine_LLBundle.hpp", "file": "GPUQREngine_LLBundle.hpp", "brief": "Bundle of row tiles for GPU factorization tasks\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nLLBundle groups row tiles for Factorize/Apply tasks. Tracks: First (smallest\nrowtile, made triangular), Shadow (memento of factorized First), Last,\nDelta (pipelining), Max. VT[2] pointers for Householder vectors. Methods:\nAddTileToSlots, Advance, gpuPack. CurrentTask indicates GenericFactorize/Apply.\nManaged by BucketList in doubly-linked structure.", "see": ["GPUQREngine_BucketList.hpp for bundle management"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_TaskDescriptor.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_TaskDescriptor.hpp", "filename": "GPUQREngine_TaskDescriptor.hpp", "file": "GPUQREngine_TaskDescriptor.hpp", "brief": "GPU task types and metadata for QR kernel dispatch\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nTaskType enum: Factorize variants (3x1, 2x1, 1x1, edge cases), Apply\nvariants (Apply3/2/1), Assembly (SAssembly, PackAssembly). TaskDescriptor\nstruct contains F pointer, AuxAddress[4] (VT blocks, maps), dimensions,\nextra[10] (tile indices, ranges). Used by UberKernel for dispatch.\ngetFlops/getWeightedFlops for work queue balancing.", "see": ["GPUQREngine_Internal.hpp for UberKernel declaration"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_Common.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_Common.hpp", "filename": "GPUQREngine_Common.hpp", "file": "GPUQREngine_Common.hpp", "brief": "GPU thread geometry, tile constants, and common macros\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nCore constants: TILESIZE=32 (tile dimension), PANELSIZE=3 (tiles per\npanel), NUMTHREADS=384 (threads per kernel), PADDING=1 (bank conflicts).\nCommon macros: CEIL, MIN, MAX, EMPTY sentinel. Optional GPUQRENGINE_RENDER\nfor GraphViz visualization, experimental GPUQRENGINE_PIPELINING.", "see": ["GPUQREngine_BucketList.hpp uses these constants"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_FrontState.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_FrontState.hpp", "filename": "GPUQREngine_FrontState.hpp", "file": "GPUQREngine_FrontState.hpp", "brief": "Finite state machine for front factorization lifecycle\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nFrontState enum defines 9-state FSM: ALLOCATE_WAIT (0) → ASSEMBLE_S (1) →\nCHILD_WAIT (2) → FACTORIZE (3) → FACTORIZE_COMPLETE (4) → PARENT_WAIT (5) →\nPUSH_ASSEMBLE (6) → CLEANUP (7) → DONE (8). Transitions driven by\nFillWorkQueue and PostProcessing. Scheduler uses states to coordinate work.", "see": ["GPUQREngine_Front.hpp uses FrontState", "GPUQREngine_Scheduler.hpp manages state transitions"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_Scheduler.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_Scheduler.hpp", "filename": "GPUQREngine_Scheduler.hpp", "file": "GPUQREngine_Scheduler.hpp", "brief": "Central coordinator for GPU QR factorization tasks\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nScheduler manages fronts and GPU resources: frontList with numFronts,\nbucketLists for task generation, workQueues (double-buffered), CUDA\nstreams (kernelStreams[2], memoryStreamH2D/D2H). Main loop: fillWorkQueue →\nlaunchKernel → postProcess. Tracks completion via FrontDataPulled events.", "see": ["GPUQREngine_Front.hpp for front structure", "GPUQREngine_BucketList.hpp for task generation"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_SparseMeta.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_SparseMeta.hpp", "filename": "GPUQREngine_SparseMeta.hpp", "file": "GPUQREngine_SparseMeta.hpp", "brief": "Sparse multifrontal factorization metadata\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nSparseMeta extends Front for sparse factorization: fp (pivotal columns),\nnc (remaining children), isStaged/pushOnly flags for staging. S assembly\nmetadata (cpuS/gpuS SEntry arrays, Scount). Pack assembly: contribution\nblock dimensions (cm×cn), parent info (pn, gpuP), row/col maps (gpuRimap/gpuRjmap).", "see": ["GPUQREngine_Front.hpp for containing class", "GPUQREngine_SEntry.hpp for SEntry structure"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_SuiteSparse.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_SuiteSparse.hpp", "filename": "GPUQREngine_SuiteSparse.hpp", "file": "GPUQREngine_SuiteSparse.hpp", "brief": "Public SuiteSparse API for GPU QR factorization\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nMain entry point for SPQR GPU support. QREngineResultCode enum: SUCCESS,\nOUTOFMEMORY, GPUERROR. GPUQREngine template functions: dense (fronts only)\nand sparse (with Parent/Childp/Child tree). GPUQREngine_FindStaircase\ncomputes staircase for block zero exploitation.", "see": ["GPUQREngine_Internal.hpp for implementation details"], "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_Timing.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_Timing.hpp", "filename": "GPUQREngine_Timing.hpp", "file": "GPUQREngine_Timing.hpp", "brief": "CUDA event-based timing macros for kernel profiling\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nTiming macros enabled by TIMING define: TIMER_INIT (create cudaEvents),\nTIMER_START (record start), TIMER_STOP (synchronize and get elapsed),\nTIMER_FINISH (destroy events). Compiles to no-ops when TIMING undefined.\nUsed for kernel performance measurement and optimization.", "has_pass2": false}, "SPQR/GPUQREngine/Include/GPUQREngine_Internal.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/GPUQREngine_Internal.hpp", "filename": "GPUQREngine_Internal.hpp", "file": "GPUQREngine_Internal.hpp", "brief": "Internal API and UberKernel declaration\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nInternal includes (Common, TaskDescriptor, Front, Stats, SuiteSparse).\nGPUQREngine_UberKernel: launches GPU kernel on stream with work queue.\nGPUQREngine_Internal: main factorization entry with Parent/Child arrays\nfor multifrontal tree traversal. Returns QREngineResultCode.", "see": ["GPUQREngine_SuiteSparse.hpp for public API"], "has_pass2": false}, "SPQR/GPUQREngine/Include/Kernel/sharedMemory.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/Kernel/sharedMemory.hpp", "filename": "sharedMemory.hpp", "file": "Kernel/sharedMemory.hpp", "brief": "GPU shared memory layout for QR kernel operations\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nSharedMemory union overlays three kernel types: factorize (A, T, Z tiles\nfor Householder), apply (V, C matrices for block update), packassemble\n(Rimap/Rjmap for contribution assembly). Global __shared__ shMemory,\nmyTask (current task), IsApplyFactorize flag. Sized for PANELSIZE×TILESIZE.", "see": ["GPUQREngine_Common.hpp for TILESIZE, PANELSIZE constants"], "has_pass2": false}, "SPQR/GPUQREngine/Include/Kernel/Apply/params_apply.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPUQREngine/Include/Kernel/Apply/params_apply.hpp", "filename": "params_apply.hpp", "file": "Kernel/Apply/params_apply.hpp", "brief": "Apply kernel parameters, macros, and function declarations\nCopyright (c) 2013, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nConstants: MAX_ROW_TILES=3, MAX_COL_TILES=2. Shared memory accessors:\nshV, shC, SHV(t,i,j), SHA(i,j), ST(i,j). Global memory: GLVT, GLF,\nIFRONT. Thread indexing: iv, jv, VCHUNKSIZE, NVCHUNKS. Device functions:\nblock_apply_3/2/1, block_apply_3_by_1/2_by_1/1_by_1 variants.", "see": ["Kernel/sharedMemory.hpp for shared memory union"], "has_pass2": false}, "SPQR/GPURuntime/Include/SuiteSparse_GPURuntime.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPURuntime/Include/SuiteSparse_GPURuntime.hpp", "filename": "SuiteSparse_GPURuntime.hpp", "file": "SuiteSparse_GPURuntime.hpp", "brief": "Version information for SuiteSparse GPU runtime library\nCopyright (c) 2013-2016, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nVersion macros for SuiteSparse_GPURuntime shared GPU infrastructure.\nCurrent version 4.3.4 (June 2024). Provides common GPU utilities used\nby GPUQREngine and other SuiteSparse GPU components.", "has_pass2": false}, "SPQR/GPURuntime/Include/SuiteSparseGPU_workspace_macros.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPURuntime/Include/SuiteSparseGPU_workspace_macros.hpp", "filename": "SuiteSparseGPU_workspace_macros.hpp", "file": "SuiteSparseGPU_workspace_macros.hpp", "brief": "Convenience macros for accessing Workspace CPU/GPU pointers\nCopyright (c) 2013-2016, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nGPU_REFERENCE(ws, TYPE): safely get GPU pointer from Workspace with NULL check.\nCPU_REFERENCE(ws, TYPE): safely get CPU pointer from Workspace with NULL check.\nBoth cast to TYPE and handle NULL workspace gracefully.", "see": ["SuiteSparseGPU_Workspace.hpp for Workspace class"], "has_pass2": false}, "SPQR/GPURuntime/Include/SuiteSparseGPU_Workspace.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPURuntime/Include/SuiteSparseGPU_Workspace.hpp", "filename": "SuiteSparseGPU_Workspace.hpp", "file": "SuiteSparseGPU_Workspace.hpp", "brief": "Unified CPU/GPU memory workspace management class\nCopyright (c) 2013-2016, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nWorkspace class manages paired CPU/GPU memory allocations: nitems × size_of_item\nbytes on both devices. Static methods: cpu_malloc/calloc/free, gpu_malloc/calloc/free.\nInstance methods: allocate (with cpu/gpu/pageLocked flags), destroy, transfer\n(cudaMemcpyKind with sync option). Accessors: cpu(), gpu(), getCount(), getStride().", "see": ["GPUQREngine_BucketList.hpp uses Workspace for VT blocks"], "has_pass2": false}, "SPQR/GPURuntime/Include/SuiteSparseGPU_internal.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPURuntime/Include/SuiteSparseGPU_internal.hpp", "filename": "SuiteSparseGPU_internal.hpp", "file": "SuiteSparseGPU_internal.hpp", "brief": "Internal includes for SuiteSparse GPU runtime\nCopyright (c) 2013-2016, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nConditionally includes CUDA runtime when SPQR_HAS_CUDA defined. Forward\ndeclares Workspace class. Includes SuiteSparse_config.h, macros, and\nWorkspace header. Also includes version header unconditionally.", "see": ["SuiteSparseGPU_Workspace.hpp for Workspace class definition"], "has_pass2": false}, "SPQR/GPURuntime/Include/SuiteSparseGPU_macros.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPURuntime/Include/SuiteSparseGPU_macros.hpp", "filename": "SuiteSparseGPU_macros.hpp", "file": "SuiteSparseGPU_macros.hpp", "brief": "Common macros for SuiteSparse GPU components\nCopyright (c) 2013-2016, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nIMPLIES(p,q) logical implication macro: true unless p is true and q is false.\nIncludes debug level macros and workspace accessor macros. Used throughout\nGPURuntime and GPUQREngine.", "see": ["SuiteSparseGPU_debug.hpp for debug level definitions"], "has_pass2": false}, "SPQR/GPURuntime/Include/SuiteSparseGPU_debug.hpp": {"path": "layer-0/SuiteSparse/SPQR/GPURuntime/Include/SuiteSparseGPU_debug.hpp", "filename": "SuiteSparseGPU_debug.hpp", "file": "SuiteSparseGPU_debug.hpp", "brief": "Debug verbosity levels for SuiteSparse GPU runtime\nCopyright (c) 2013-2016, Timothy A Davis, Sencer Nuri Yeralan,\nand Sanjay Ranka. GPL-2.0+ license.\n\nDebug levels: OFF (0), ERRORONLY (1), CASUAL (2), VERBOSE (3), EXTREME (4).\nGPURUNTIME_DLEVEL set based on NDEBUG: CASUAL when debugging, OFF otherwise.\nConvenience macros: DEBUG_ATLEAST_ERRORONLY/CASUAL/VERBOSE/EXTREME.\nGPURUNTIME_LOGFILE_PATH defaults to \"SuiteSparse_GPURuntime-logfile.txt\".", "has_pass2": false}, "ParU/Tcov/paru_cov.hpp": {"path": "layer-0/SuiteSparse/ParU/Tcov/paru_cov.hpp", "filename": "paru_cov.hpp", "file": "paru_cov.hpp", "brief": "Test coverage utilities for ParU sparse LU\nCopyright (c) 2022-2025, Mohsen Aznaveh and Timothy A. Davis.\nGPL-3.0-or-later license.\n\nTesting infrastructure: TEST_PASSES (success exit), TEST_ASSERT/TEST_ASSERT_INFO\n(abort on failure). paru_backward for residual computation. BRUTAL_ALLOC_TEST\nmacro: iteratively runs method with increasing malloc failure counts to test\nall allocation paths. Enabled by PARU_ALLOC_TESTING define.", "has_pass2": false}, "ParU/Source/paru_omp.hpp": {"path": "layer-0/SuiteSparse/ParU/Source/paru_omp.hpp", "filename": "paru_omp.hpp", "file": "paru_omp.hpp", "brief": "OpenMP abstraction layer for ParU parallel LU\nCopyright (c) 2022-2025, Mohsen Aznaveh and Timothy A. Davis.\nGPL-3.0-or-later license.\n\nPortable OpenMP wrappers: PARU_omp_get_wtime, get_max_threads, get_num_threads,\nset_num_threads, get/set_dynamic, get_active_level, get_max_active_levels,\nget_thread_num. When _OPENMP undefined, provides stub implementations returning\nsequential defaults (1 thread, 0 wtime). Defines PARU_1TASK when no OpenMP.", "see": ["paru_internal.hpp includes this for parallel factorization"], "has_pass2": false}, "ParU/Source/paru_internal.hpp": {"path": "layer-0/SuiteSparse/ParU/Source/paru_internal.hpp", "filename": "paru_internal.hpp", "file": "paru_internal.hpp", "brief": "Internal data structures and functions for ParU sparse LU\nCopyright (c) 2022-2025, Mohsen Aznaveh and Timothy A. Davis.\nGPL-3.0-or-later license.\n\nCore ParU structures: ParU_Symbolic_struct (row-form S, singletons, fronts,\ntask tree), ParU_Numeric_struct (LU factors, permutations, scaling),\nParU_Control_struct (tolerances, threading). paru_element (contribution\nblock), paru_work (workspace), paru_tuple (element lists). Internal\nfunctions: front assembly/factorization, heap management, BLAS threading.\nIncludes UMFPACK SymbolicType/SWType for singleton detection.", "algorithm": "Parallel Unsymmetric Sparse LU Factorization:\nMultifrontal LU with task-based parallelism:\n\n1. Symbolic Analysis (uses UMFPACK):\n   - Detect singletons (rows/cols with single nonzero)\n   - Build elimination tree for remaining submatrix S\n   - Create task tree for parallel numeric phase\n\n2. Numeric Factorization:\n   - Process fronts in parallel via task tree\n   - Each front: dense partial pivoting LU\n   - Assemble contribution blocks from children\n   - Store L and U factors separately", "math": "Frontal matrix factorization:\nFront F = [pivots | non-pivots] factored as F = P·L·U\nPartial pivoting within front for numerical stability.\nContribution block C = Schur complement passed to parent.\n\nSingleton handling: Extract trivial 1×1 pivots first,\nreducing problem size before expensive multifrontal phase.", "complexity": "O(flops) dominated by dense BLAS-3 in fronts.\nParallel speedup via task tree scheduling with OpenMP.", "ref": ["Aznaveh & Davis (2024). \"ParU: A Parallel Unsymmetric\n  Multifrontal Sparse LU Factorization Method\"."], "see": ["ParU.h for public API", "paru_omp.hpp for OpenMP abstraction"], "has_pass2": true}, "GraphBLAS/CUDA/GB_cuda_apply.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/GB_cuda_apply.hpp", "filename": "GB_cuda_apply.hpp", "file": "GB_cuda_apply.hpp", "brief": "CUDA JIT kernel launchers for GraphBLAS apply operations\nSPDX-License-Identifier: Apache-2.0\n\nGPU apply operations: GB_cuda_apply_unop_jit (unary operator with optional\nflipij), GB_cuda_apply_bind1st_jit (binary op with scalar bound to first),\nGB_cuda_apply_bind2nd_jit (scalar bound to second). All take stream, grid\nand block size parameters for kernel launch.", "see": ["GB_cuda.hpp for common CUDA utilities"], "has_pass2": false}, "GraphBLAS/CUDA/GB_cuda_ewise.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/GB_cuda_ewise.hpp", "filename": "GB_cuda_ewise.hpp", "file": "GB_cuda_ewise.hpp", "brief": "CUDA JIT kernel launchers for GraphBLAS element-wise operations\nSPDX-License-Identifier: Apache-2.0\n\nGPU element-wise scaling: GB_cuda_rowscale_jit (C = D.*B row scaling),\nGB_cuda_colscale_jit (C = A.*D column scaling). Both support flipxy for\noperand order and take stream/grid/block parameters.", "see": ["GB_cuda.hpp for common CUDA utilities"], "has_pass2": false}, "GraphBLAS/CUDA/GB_cuda_AxB.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/GB_cuda_AxB.hpp", "filename": "GB_cuda_AxB.hpp", "file": "GB_cuda_AxB.hpp", "brief": "CUDA JIT kernel launcher for GraphBLAS matrix multiplication\nCopyright (c) 2017-2025, Timothy A. Davis. Apache-2.0 license.\n\nGB_cuda_AxB_dot3_jit: GPU sparse matrix-matrix multiply C=A*B using dot\nproduct method (dot3). Supports masked operation (M, Mask_struct), semiring\nselection, and flipxy. Takes device and number_of_sms for launch config.", "see": ["GB_cuda.hpp for common CUDA utilities"], "has_pass2": false}, "GraphBLAS/CUDA/GB_cuda_reduce.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/GB_cuda_reduce.hpp", "filename": "GB_cuda_reduce.hpp", "file": "GB_cuda_reduce.hpp", "brief": "CUDA JIT kernel launcher for GraphBLAS reductions\nCopyright (c) 2017-2025, Timothy A. Davis. Apache-2.0 license.\n\nGB_cuda_reduce_to_scalar_jit: GPU reduction of matrix A to scalar z using\nmonoid. Output to z (scalar) or V (1×1 matrix) based on has_cheeseburger\nflag. Takes stream/grid/block parameters for kernel launch.", "see": ["GB_cuda.hpp for common CUDA utilities"], "has_pass2": false}, "GraphBLAS/CUDA/GB_cuda.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/GB_cuda.hpp", "filename": "GB_cuda.hpp", "file": "GB_cuda.hpp", "brief": "Host-side CUDA utilities for GraphBLAS GPU operations\nCopyright (c) 2017-2025, Timothy A. Davis. Apache-2.0 license.\nNVIDIA CORPORATION contributions (c) 2024-2025.\n\nHost CUDA includes and utilities (not for JIT kernels). Matrix prefetch\nfunctions with component flags (P/H/Y/B/I/X for pointers/hyperstart/y/bitmap/\nindices/values). GB_cuda_upscale_identity for monoid identity. Stream pool\nacquire/release for CUDA stream management.", "see": ["GB_cuda_apply.hpp, GB_cuda_ewise.hpp for operation-specific headers"], "has_pass2": false}, "GraphBLAS/CUDA/GB_cuda_select.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/GB_cuda_select.hpp", "filename": "GB_cuda_select.hpp", "file": "GB_cuda_select.hpp", "brief": "CUDA JIT kernel launchers for GraphBLAS select operations\nSPDX-License-Identifier: Apache-2.0\n\nGPU select operations: GB_cuda_select_bitmap_jit (bitmap format input),\nGB_cuda_select_sparse_jit (sparse format input). Both apply IndexUnaryOp\nwith optional flipij and ythunk scalar. Returns selected entries in C.", "see": ["GB_cuda.hpp for common CUDA utilities", "GB_select_iso.h for iso matrix handling"], "has_pass2": false}, "GraphBLAS/rmm_wrap/rmm_wrap.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/rmm_wrap/rmm_wrap.hpp", "filename": "rmm_wrap.hpp", "file": "rmm_wrap.hpp", "brief": "RMM (RAPIDS Memory Manager) wrapper types for GraphBLAS GPU memory\nSPDX-License-Identifier: Apache-2.0\n\nType aliases for RMM memory resources: host_mr (new/delete), pinned_mr\n(pinned host), device_mr (CUDA malloc), managed_mr (unified memory).\nPool variants: host_pool_mr, host_pinned_pool_mr, device_pool_mr,\nmanaged_pool_mr. Stream types: cuda_stream_pool, cuda_stream_view.", "see": ["GB_cuda.hpp uses RMM for GPU memory management"], "has_pass2": false}, "GraphBLAS/CUDA/include/GB_cuda_error.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/include/GB_cuda_error.hpp", "filename": "GB_cuda_error.hpp", "file": "GB_cuda_error.hpp", "brief": "CUDA error checking macro for GraphBLAS GPU operations\nCopyright (c) 2017-2025, Timothy A. Davis. Apache-2.0 license.\n\nCUDA_OK macro: wraps CUDA API calls with error checking. On failure,\nmaps cudaErrorMemoryAllocation to GrB_OUT_OF_MEMORY, other errors to\nGxB_GPU_ERROR. Prints error info via printf and GBURBLE, calls GB_FREE_ALL.", "see": ["GB_cuda.hpp for CUDA utilities using this macro"], "has_pass2": false}, "GraphBLAS/CUDA/include/GB_cuda_timer.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/include/GB_cuda_timer.hpp", "filename": "GB_cuda_timer.hpp", "file": "GB_cuda_timer.hpp", "brief": "CUDA event-based GPU timer class\nCopyright (c) 2017-2025, Timothy A. Davis. Apache-2.0 license.\nNVIDIA CORPORATION contributions (c) 2024-2025.\n\nGpuTimer class: Start() records start event, Stop() records stop event,\nElapsed() synchronizes and returns milliseconds between. Uses cudaEvent_t\nfor precise GPU timing. Useful for kernel performance measurement.", "has_pass2": false}, "GraphBLAS/CUDA/include/GB_cuda_geometry.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/include/GB_cuda_geometry.hpp", "filename": "GB_cuda_geometry.hpp", "file": "GB_cuda_geometry.hpp", "brief": "CUDA kernel launch geometry constants\nCopyright (c) 2017-2025, Timothy A. Davis.\n\nBlock and chunk size constants for CUDA kernels, used by host and JIT.\nSelect sparse: BLOCKDIM1=512, CHUNKSIZE1=4096; BLOCKDIM2=256, CHUNKSIZE2=1024.\nSelect bitmap: BLOCKDIM=512. Includes log2 variants for bit shifts.", "see": ["GB_cuda_select.hpp uses these constants"], "has_pass2": false}, "GraphBLAS/CUDA/include/GraphBLAS_cuda.hpp": {"path": "layer-0/SuiteSparse/GraphBLAS/CUDA/include/GraphBLAS_cuda.hpp", "filename": "GraphBLAS_cuda.hpp", "file": "GraphBLAS_cuda.hpp", "brief": "Main GraphBLAS CUDA include with C++ complex type definitions\nCopyright (c) 2017-2025, Timothy A. Davis. Apache-2.0 license.\n\nCUDA-compatible GraphBLAS header. Defines C++ complex types: GxB_FC32_t\n(std::complex<float>), GxB_FC64_t (std::complex<double>). Macros GxB_CMPLXF,\nGxB_CMPLX for construction. Includes GB_cuda_geometry.hpp. Used by JIT\nkernels requiring complex arithmetic.", "see": ["GB_cuda.hpp for host-side CUDA utilities"], "has_pass2": false}, "SuiteSparse_config/SuiteSparse_config.h": {"path": "layer-0/SuiteSparse/SuiteSparse_config/SuiteSparse_config.h", "filename": "SuiteSparse_config.h", "file": "SuiteSparse_config.h", "brief": "Central configuration and utility header for all SuiteSparse libraries\n\nThis file provides the shared foundation for the entire SuiteSparse\ncollection of sparse matrix algorithms. Key features include:\n\n- Compiler and platform detection (GCC, Clang, MSVC, ICC, NVCC)\n- Configurable memory management with replaceable malloc/calloc/realloc/free\n- Portable BLAS/LAPACK interface handling Fortran name mangling and integer sizes\n- OpenMP detection and thread-safe timing utilities\n- Complex number type definitions for C/C++ interoperability\n\n@note Applications can customize memory allocation by calling the\n      SuiteSparse_config_*_set functions before using any SuiteSparse library.", "see": ["amd.h, colamd.h, cholmod.h, umfpack.h, klu.h for library-specific headers", "SuiteSparse_finish()", "SuiteSparse_start()", "SuiteSparse_calloc(), SuiteSparse_free()", "SuiteSparse_malloc(), SuiteSparse_free()", "SuiteSparse_malloc(), SuiteSparse_free()", "SuiteSparse_malloc(), SuiteSparse_calloc()", "SuiteSparse_toc(), SuiteSparse_time()", "SuiteSparse_tic()"], "param": ["nitems Number of items to allocate (enforced >= 1)", "size_of_item Size in bytes of each item (typically sizeof(type))", "nitems Number of items to allocate (enforced >= 1)", "size_of_item Size in bytes of each item (typically sizeof(type))", "nitems_new New number of items desired", "nitems_old Current number of items (for size calculation)", "size_of_item Size in bytes of each item", "p Pointer to existing allocation (may be NULL for initial alloc)", "ok Output: set to 1 on success, 0 on failure", "p Pointer to memory block to free (NULL is safe)", "tic Output array of 2 doubles to store timer state", "tic Timer state from previous SuiteSparse_tic call", "x First value", "y Second value", "ar Real part of numerator a", "ai Imaginary part of numerator a", "br Real part of denominator b", "bi Imaginary part of denominator b", "cr Output: real part of result c", "ci Output: imaginary part of result c", "version Optional output array for version components:\n               version[0] = major, version[1] = minor, version[2] = patch.\n               May be NULL if only the combined version is needed."], "return": "Pointer to allocated memory, or NULL on failure", "math": "Implements the hypot function: sqrt(x^2 + y^2) using a numerically\n      stable algorithm that avoids intermediate overflow/underflow.", "has_pass2": true}, "AMD/Include/amd.h": {"path": "layer-0/SuiteSparse/AMD/Include/amd.h", "filename": "amd.h", "file": "amd.h", "brief": "Approximate Minimum Degree ordering for sparse matrix factorization\n\nAMD computes a fill-reducing permutation P for sparse Cholesky or LU\nfactorization. Given a symmetric matrix A (or A+A' if A is unsymmetric),\nAMD finds P such that P*A*P' has fewer nonzeros in its Cholesky factor\nthan A would.", "algorithm": "Approximate Minimum Degree (AMD)\n  Repeatedly eliminates the node with minimum approximate external degree:\n  1. Select pivot i with minimum approximate degree d̃(i)\n  2. Form element e from eliminated node and merge with adjacent elements\n  3. Update approximate degrees for remaining nodes\n  4. Apply aggressive absorption: absorb element e into element f if\n     adj(e) ⊆ adj(f), even when f is not adjacent to current pivot", "math": "Approximate external degree:\n  d̃(i) ≈ |adj(i) ∪ (∪ adj(e) : e ∈ elements adjacent to i)| - |absorbed|\n  The true minimum degree would require d(i) = |L_{*i}| - 1 (column count\n  in Cholesky factor), but this is expensive to compute exactly.", "complexity": "Time: O(nnz(A)·α(n)) average case with aggressive absorption,\n  where α is the inverse Ackermann function. Worst case O(n·nnz).\n  Space: O(nnz(A) + n) for the quotient graph representation.", "ref": ["Amestoy, Davis, Duff (1996). \"An Approximate Minimum Degree Ordering\n     Algorithm\". SIAM J. Matrix Analysis and Applications 17(4):886-905.", "George, Liu (1989). \"The Evolution of the Minimum Degree Ordering\n     Algorithm\". SIAM Review 31(1):1-19.\n\nKey features:\n- Aggressive absorption for better approximate degrees\n- Dense row/column detection and deferral\n- Post-ordering of the elimination tree\n- Both int32_t and int64_t versions available"], "see": ["colamd.h for column ordering of unsymmetric matrices", "cholmod.h for sparse Cholesky factorization using AMD", "amd_l_order() for 64-bit integer version", "amd_defaults() to initialize Control array", "amd_order() for full parameter documentation", "amd_order()", "amd_valid() for full documentation", "amd_defaults()", "amd_control()", "amd_info()"], "param": ["n Matrix dimension (A is n-by-n). Must be >= 0.", "Ap Column pointers array of size n+1. Ap[j] gives the start of column j.", "Ai Row indices array of size Ap[n]. Contains row indices for each column.", "P Output permutation array of size n. P[k]=i means row i is the k-th pivot.", "Control Optional control parameters of size AMD_CONTROL. NULL uses defaults.", "Info Optional output statistics of size AMD_INFO. May be NULL.", "n_row Number of rows in the matrix", "n_col Number of columns in the matrix", "Ap Column pointers array of size n_col+1", "Ai Row indices array of size Ap[n_col]", "Control Output array of size AMD_CONTROL to initialize", "Control Control array of size AMD_CONTROL (NULL prints defaults)", "Info Statistics array of size AMD_INFO from amd_order", "version Output array filled with [major, minor, patch] version"], "return": "AMD_OK on success, AMD_OK_BUT_JUMBLED if input had unsorted/duplicate\n        entries, AMD_INVALID for bad input, AMD_OUT_OF_MEMORY on allocation failure.", "has_pass2": true}, "AMD/Include/amd_internal.h": {"path": "layer-0/SuiteSparse/AMD/Include/amd_internal.h", "filename": "amd_internal.h", "file": "amd_internal.h", "brief": "Internal definitions for AMD (Approximate Minimum Degree) ordering\nCopyright (c) 1996-2023, Timothy A. Davis, Patrick R. Amestoy,\nand Iain S. Duff. BSD-3-clause license.\n\nAMD computes fill-reducing orderings for sparse matrix factorization.\nUses approximate minimum degree heuristic with quotient graph representation\nfor efficient O(|A|) ordering of symmetric matrices (or A'A for unsymmetric).", "algorithm": "Approximate Minimum Degree (AMD):\nFill-reducing ordering using quotient graph compression:\n1. Build quotient graph G representing remaining elimination graph\n2. For i = 1 to n:\n   a. Find vertex v with approximate minimum degree\n   b. Eliminate v: add v to ordering, update quotient graph\n   c. Absorb indistinguishable nodes (mass elimination)\n   d. Supervariable detection for compression", "math": "Quotient graph compression:\nAfter eliminating vertex v, neighbors become clique (fill-in).\nInstead of storing clique explicitly, store \"element\" e_v.\nadj(u) = original_adj(u) ∪ ⋃{elements containing u}.\nDegree approximation: |adj(u)| ≤ |original_adj(u)| + Σ|element sizes|.\n\nSupervariables: vertices with identical adjacency merged for O(1) updates.", "complexity": "O(n·m) worst case, O(n·m/log(n)) typical for sparse matrices.\nNear-optimal fill for many practical matrices (within 5-20% of MMD).", "ref": ["Amestoy, Davis & Duff (1996). \"An Approximate Minimum Degree Ordering\n  Algorithm\". SIAM J. Matrix Anal. Appl. 17(4):886-905.", "Amestoy, Davis & Duff (2004). \"Algorithm 837: AMD, An Approximate\n  Minimum Degree Ordering Algorithm\". ACM TOMS 30(3):381-388."], "see": ["COLAMD for column ordering of unsymmetric matrices", "CHOLMOD for sparse Cholesky using AMD ordering"], "has_pass2": true}, "RBio/Include/RBio.h": {"path": "layer-0/SuiteSparse/RBio/Include/RBio.h", "filename": "RBio.h", "file": "RBio.h", "brief": "Rutherford-Boeing sparse matrix I/O library\nCopyright (c) 2009-2023, Timothy A. Davis. GPL-2.0+ license.\n\nRBio reads and writes sparse matrices in Rutherford-Boeing format,\na standard format for exchanging sparse matrices. Supports real, complex,\ninteger, and pattern-only matrices in assembled or elemental forms.", "algorithm": "Rutherford-Boeing I/O:\nParse and write standardized sparse matrix format:\n\nREAD OPERATION:\n1. Parse 4-line header: title, key, dimensions, format specifiers\n2. Read column pointers (Fortran-style Harwell format)\n3. Read row indices for each column\n4. Read numerical values (if present)\n5. Convert to CSC (0-indexed C arrays)\n\nWRITE OPERATION:\n1. Generate header with matrix properties (type: RUA/RSA/PSA/etc.)\n2. Write column pointers in fixed-width Fortran format\n3. Write row indices\n4. Write values (with appropriate precision)", "math": "Matrix types in RB format:\nFirst 3 characters encode: R=real, C=complex, P=pattern, I=integer\n                          U=unsymmetric, S=symmetric, H=Hermitian\n                          A=assembled, E=elemental (FEM)\nExample: RUA = Real Unsymmetric Assembled", "complexity": "O(nnz) for reading/writing, where nnz = number of nonzeros.", "ref": ["Duff, Grimes & Lewis (1992). \"Users' Guide for the Harwell-Boeing\n  Sparse Matrix Collection\". Technical Report RAL-92-086."], "see": ["SuiteSparse Matrix Collection (https://sparse.tamu.edu)", "CHOLMOD for using matrices read via RBio"], "has_pass2": true}, "CSparse/Include/cs.h": {"path": "layer-0/SuiteSparse/CSparse/Include/cs.h", "filename": "cs.h", "file": "cs.h", "brief": "Concise Sparse matrix library - teaching implementation of sparse algorithms\n\nCSparse provides a minimal, readable implementation of core sparse matrix\noperations. It serves as both a standalone library and educational reference\nfor sparse linear algebra algorithms.\n\nKey features:\n- Sparse matrix in triplet or compressed-column (CSC) format\n- Sparse Cholesky (cs_chol), LU (cs_lu), and QR (cs_qr) factorization\n- Fill-reducing orderings via AMD\n- Direct solvers: cs_cholsol, cs_lusol, cs_qrsol\n- Dulmage-Mendelsohn decomposition (cs_dmperm)", "algorithm": "Sparse Matrix Operations:\n  - Compressed-column (CSC): column pointers + row indices + values\n  - cs_compress: O(nnz) triplet to CSC conversion\n  - cs_multiply: sparse matrix-matrix product using symbolic + numeric phases\n  - cs_chol/cs_lu/cs_qr: left-looking factorization algorithms", "math": "Sparse formats:\n  CSC: A stored as (p, i, x) where p[j]..p[j+1]-1 index column j entries\n  Triplet (COO): (row[k], col[k], val[k]) for k = 0..nz-1", "complexity": "Space: O(nnz + n) for CSC format\n  cs_compress: O(nnz) time for triplet → CSC\n  cs_multiply: O(flops) where flops depends on sparsity pattern\n  cs_chol/cs_lu: O(nnz(L)²/n) typical for sparse factors", "ref": ["Davis (2006). \"Direct Methods for Sparse Linear Systems\".\n     SIAM. ISBN: 978-0-898716-13-9"], "see": ["CXSparse for extended version with complex numbers", "CHOLMOD, UMFPACK for production-quality implementations", "cs_compress() to convert triplet to CSC", "CS_CSC(), CS_TRIPLET() macros to check format", "cs_schol() for symbolic Cholesky analysis", "cs_sqr() for symbolic QR analysis", "cs_chol() for numeric Cholesky", "cs_lu() for numeric LU", "cs_qr() for numeric QR", "cs_dmperm() for computing this decomposition", "cs_scc() for strongly connected components"], "param": ["order 0=natural, 1=AMD", "order ordering,", "tol pivot tolerance", "order 0=natural, 1-3=AMD variants", "values 1=copy values, 0=pattern only"], "has_pass2": true}, "CAMD/Include/camd.h": {"path": "layer-0/SuiteSparse/CAMD/Include/camd.h", "filename": "camd.h", "file": "camd.h", "brief": "Constrained Approximate Minimum Degree ordering for symmetric matrices\nCopyright (c) 1996-2024, Timothy A. Davis, Yanqing Chen,\nPatrick R. Amestoy, and Iain S. Duff. BSD-3-clause license.\n\nCAMD computes fill-reducing orderings for sparse Cholesky factorization\nwith user-specified constraints. Extends AMD to support constraint sets\nthat force certain nodes to be ordered before others.", "algorithm": "Constrained Approximate Minimum Degree (CAMD):\nAMD ordering with constraint satisfaction for symmetric matrices:\n1. CONSTRAINT PROCESSING:\n   - User specifies C[i] = k meaning vertex i in constraint set k\n   - All vertices in set k ordered before any vertex in set k+1\n   - Within each set, AMD heuristic orders vertices freely\n2. AMD ORDERING (per constraint set):\n   - Quotient graph maintains elimination graph implicitly\n   - Select vertex with approximate minimum degree\n   - Eliminate, update quotient graph, absorb indistinguishable vertices\n3. OUTPUT: Permutation P such that P·A·P' has reduced fill", "math": "Constrained ordering for Cholesky:\nL·L' = P·A·P' where P respects constraints C[·].\nFill(L) depends on elimination order of A+A' (symmetrized pattern).\nConstraint sets useful for:\n- Nested dissection separators (must be ordered last)\n- Known dense rows/columns (order last to limit fill)\n- Hierarchical structures", "complexity": "O(n·m) worst case, typically O(n·m/log(n)) for sparse matrices.\nConstraint overhead: O(n) additional work.", "ref": ["Amestoy, Davis & Duff (1996). \"An Approximate Minimum Degree Ordering\n  Algorithm\". SIAM J. Matrix Anal. Appl. 17(4):886-905."], "see": ["AMD for unconstrained approximate minimum degree", "CHOLMOD for sparse Cholesky using CAMD ordering"], "has_pass2": true}, "CCOLAMD/Include/ccolamd.h": {"path": "layer-0/SuiteSparse/CCOLAMD/Include/ccolamd.h", "filename": "ccolamd.h", "file": "ccolamd.h", "brief": "Constrained Column Approximate Minimum Degree ordering\nCopyright (c) 1996-2024, Timothy A. Davis, Sivasankaran Rajamanickam,\nand Stefan Larimore. BSD-3-clause license.\n\nCCOLAMD computes column orderings for sparse QR and LU factorization with\nuser-specified constraints. Extends COLAMD with constraint sets that force\ncertain columns to appear before or after others in the ordering.", "algorithm": "Constrained Column AMD (CCOLAMD):\nCOLAMD with constraint satisfaction for column ordering:\n1. CONSTRAINT PROCESSING:\n   - User specifies groups: columns in group k ordered before group k+1\n   - Columns within same group ordered freely by COLAMD heuristic\n2. COLAMD WITHIN GROUPS:\n   - Column elimination on A'A pattern (without forming A'A)\n   - Approximate minimum degree with aggressive absorption\n   - Supercolumn detection for efficiency\n3. POSTORDERING: Optional elimination tree postorder for supernodes", "math": "Fill reduction for unsymmetric matrices:\nFor A ∈ ℝ^(m×n), ordering columns of A affects fill in A'A\n(Cholesky factor of normal equations) or R factor (QR of A).\nDegree of column j in A'A graph = |{i : A_ij ≠ 0}| - 1 + indegree.\n\nConstraint sets: cmember[j] = k means column j is in group k.\nAll group-0 columns ordered first, then group-1, etc.", "complexity": "O(n + nz) for ordering, where nz = nnz(A).", "ref": ["Davis, Gilbert, Larimore, Ng (2004). \"A Column Approximate Minimum\n  Degree Ordering Algorithm\". ACM TOMS 30(3):353-376."], "see": ["COLAMD for unconstrained column ordering", "SPQR, UMFPACK for sparse factorization using CCOLAMD"], "has_pass2": true}, "LAGraph/include/LAGraphX.h": {"path": "layer-0/SuiteSparse/LAGraph/include/LAGraphX.h", "filename": "LAGraphX.h", "return": "Any GraphBLAS errors that may have been encountered", "has_pass2": false}, "UMFPACK/Include/umfpack.h": {"path": "layer-0/SuiteSparse/UMFPACK/Include/umfpack.h", "filename": "umfpack.h", "file": "umfpack.h", "brief": "Multifrontal sparse LU factorization for unsymmetric matrices\n\nUMFPACK computes a sparse LU factorization of a general (unsymmetric)\nsquare matrix A:\n  P*R*A*Q = L*U\nwhere P and Q are permutation matrices, R is diagonal scaling, L is\nunit lower triangular, and U is upper triangular.\n\nKey features:\n- Multifrontal algorithm with BLAS-3 dense kernels\n- Automatic strategy selection (symmetric vs unsymmetric)\n- Fill-reducing orderings: AMD (symmetric), COLAMD (unsymmetric)\n- Real and complex matrices (double precision)\n- Row scaling for numerical stability\n\nTypical workflow:\n1. umfpack_di_symbolic: Symbolic analysis (ordering, memory estimates)\n2. umfpack_di_numeric: Numerical LU factorization\n3. umfpack_di_solve: Solve Ax = b, A'x = b, etc.\n4. umfpack_di_free_symbolic, umfpack_di_free_numeric: Free memory", "algorithm": "Multifrontal LU with threshold partial pivoting:\n  1. Symbolic analysis: compute column elimination tree and frontal matrices\n     - COLAMD ordering for unsymmetric, AMD for symmetric patterns\n     - Identify supernodes: columns with identical nonzero patterns\n  2. Numerical factorization (multifrontal):\n     - Process columns in elimination tree postorder\n     - For each frontal matrix: dense partial LU with threshold pivoting\n     - Assemble child contributions (extend-add operation)\n     - Extract L and U columns, pass remainder to parent front\n  3. Solve phase: forward/back substitution through frontal matrices", "math": "Factorization: P·R·A·Q = L·U where:\n  - P, Q are permutation matrices (row/column reordering)\n  - R is diagonal scaling matrix (row equilibration)\n  - L is unit lower triangular, U is upper triangular\n  Threshold pivoting: select pivot if |a_kk| ≥ τ·max_i|a_ik| (τ ≈ 0.1)", "complexity": "Time: O(nnz(L+U)·f̄) where f̄ is average front size\n  Typically O(n^1.5) to O(n^2) for 2D problems, O(n^2) for 3D\n  Space: O(nnz(L+U) + front_stack) where front_stack depends on ordering", "ref": ["Davis (2004). \"Algorithm 832: UMFPACK V4.3 - An unsymmetric-pattern\n     multifrontal method\". ACM Trans. Math. Software 30(2):196-199.", "Davis (2004). \"A column pre-ordering strategy for the unsymmetric-pattern\n     multifrontal method\". ACM Trans. Math. Software 30(2):165-195.", "Duff, Reid (1983). \"The Multifrontal Solution of Indefinite Sparse\n     Symmetric Linear Equations\". ACM Trans. Math. Software 9(3):302-325."], "see": ["klu.h for circuit simulation matrices (often faster for this case)", "cholmod.h for symmetric positive definite matrices", "umfpack_di_numeric() for numerical factorization", "umfpack_di_free_symbolic() to free returned object", "umfpack_di_symbolic() for symbolic analysis", "umfpack_di_solve() to solve using the factorization", "umfpack_di_numeric() for computing the factorization", "UMFPACK_CONTROL_* defines for parameter indices", "cs_compress in CSparse for similar functionality"], "param": ["n_row Number of rows in A", "n_col Number of columns in A", "Ap Column pointers (size n_col+1)", "Ai Row indices (size Ap[n_col])", "Ax Numerical values (may be NULL for pattern-only analysis)", "Symbolic Output: opaque symbolic object", "Control Control parameters (NULL for defaults)", "Info Output: statistics (NULL to ignore)", "Ap Column pointers (size n_col+1)", "Ai Row indices (size Ap[n_col])", "Ax Numerical values (size Ap[n_col])", "Symbolic Symbolic analysis from umfpack_di_symbolic", "Numeric Output: opaque numeric factorization object", "Control Control parameters (NULL for defaults)", "Info Output: statistics (NULL to ignore)", "sys System to solve: UMFPACK_A, UMFPACK_At, UMFPACK_Aat, etc.", "Ap Column pointers (used for iterative refinement, or NULL)", "Ai Row indices (used for iterative refinement, or NULL)", "Ax Numerical values (used for iterative refinement, or NULL)", "X Output: solution vector (size n)", "B Input: right-hand side vector (size n)", "Numeric LU factorization from umfpack_di_numeric", "Control Control parameters (NULL for defaults)", "Info Output: statistics (NULL to ignore)", "Symbolic Pointer to Symbolic handle (set to NULL on output)", "Numeric Pointer to Numeric handle (set to NULL on output)", "Control Output: array of size UMFPACK_CONTROL with defaults", "n_row Number of rows", "n_col Number of columns", "nz Number of triplet entries", "Ti,Tj,Tx Row indices, column indices, values (triplet input)", "Ap,Ai,Ax Column pointers, row indices, values (CSC output)", "Map Optional: Map[k] gives destination index for triplet k", "lnz Output: # nonzeros in L", "unz Output: # nonzeros in U", "n_row,n_col Output: matrix dimensions", "nz_udiag Output: # nonzeros on diagonal of U", "Numeric Input: Numeric object from umfpack_*_numeric"], "return": "UMFPACK_OK on success, error code otherwise", "has_pass2": true}, "UMFPACK/Source/umf_internal.h": {"path": "layer-0/SuiteSparse/UMFPACK/Source/umf_internal.h", "filename": "umf_internal.h", "file": "umf_internal.h", "brief": "Internal definitions for UMFPACK sparse LU factorization\nCopyright (c) 2005-2023, Timothy A. Davis. GPL-2.0+ license.\n\nUMFPACK is an unsymmetric multifrontal sparse LU factorization package.\nComputes P·A·Q = L·U via supernodal factorization with partial pivoting.\nHandles real and complex matrices in single and double precision.", "algorithm": "Unsymmetric Multifrontal LU (UMFPACK):\nSparse LU with supernodal assembly tree:\n1. SYMBOLIC ANALYSIS:\n   - Column preordering via COLAMD or AMD(A'A)\n   - Build column elimination tree\n   - Compute upper bounds on L and U nonzero counts\n2. NUMERIC FACTORIZATION:\n   - Process columns in elimination tree order\n   - Frontal matrices: dense submatrices during elimination\n   - Partial pivoting with threshold (for numerical stability)\n   - Extend-add operation for child contributions\n3. SOLVE: Forward/backward substitution with L, U factors", "math": "Frontal matrix factorization:\nEach front F ∈ ℝ^(m×n) corresponds to columns with similar structure.\nF = L_F · U_F via dense partial-pivoted LU (BLAS-3 for performance).\nSchur complement S = F_22 - L_21·U_12 assembled to parent front.\n\nKey optimizations:\n- Supernodes: columns with identical structure factored together\n- BLAS-3: dense operations within frontal matrices\n- Symbolic reuse: same pattern → reuse symbolic analysis", "complexity": "O(flops) for numeric factorization.\nFlop count highly dependent on sparsity pattern and ordering quality.", "ref": ["Davis (2004). \"A column pre-ordering strategy for the unsymmetric-\n  pattern multifrontal method\". ACM TOMS 30(2):165-195.", "Davis (2004). \"Algorithm 832: UMFPACK V4.3—an unsymmetric-pattern\n  multifrontal method\". ACM TOMS 30(2):196-199."], "see": ["COLAMD for column ordering (used by default)", "KLU for circuit matrices (alternative with BTF)"], "has_pass2": true}, "BTF/Include/btf.h": {"path": "layer-0/SuiteSparse/BTF/Include/btf.h", "filename": "btf.h", "file": "btf.h", "brief": "Block Triangular Form permutation for sparse matrices\n\nBTF computes permutations to transform a sparse matrix into block upper\ntriangular form (BTF). This decomposes the matrix into independent blocks\nthat can be processed separately, improving efficiency for factorization.\n\nThree main routines:\n- btf_maxtrans: Maximum transversal (zero-free diagonal matching)\n- btf_strongcomp: Strongly connected components (block decomposition)\n- btf_order: Combined BTF ordering (calls both above)", "algorithm": "Tarjan's Strongly Connected Components:\n  Find permutation P such that P*A*P' is block upper triangular.\n  1. Build directed graph: edge i→j if A(i,j) ≠ 0 and i ≠ j\n  2. Find SCCs using depth-first search with low-link values\n  3. Order SCCs in reverse topological order", "math": "Structural rank: max # nonzeros achievable on diagonal = sprank(A)\n  If sprank(A) < n, matrix is structurally singular.\n  BTF form: P*A*Q = | B_11  B_12 ... | with square diagonal blocks B_ii\n                    |  0   B_22 ... |", "complexity": "Maximum transversal: O(nnz + n) time and O(n) space\n  SCC decomposition: O(nnz + n) time (single DFS pass)\n  Total BTF ordering: O(nnz + n) time and space", "ref": ["Duff (1981). \"On Algorithms for Obtaining a Maximum Transversal\".\n     ACM Trans. Math. Software 7(3):315-330.", "Tarjan (1972). \"Depth-first search and linear graph algorithms\".\n     SIAM J. Computing 1(2):146-160."], "see": ["klu.h for sparse LU factorization using BTF", "btf_order() for combined BTF ordering", "btf_maxtrans()", "btf_order() for combined BTF ordering", "btf_strongcomp()", "btf_maxtrans(), btf_strongcomp()", "btf_order()"], "param": ["nrow Number of rows in A", "ncol Number of columns in A", "Ap Column pointers array of size ncol+1", "Ai Row indices array of size Ap[ncol]", "maxwork Max work limit as multiple of nnz(A); <= 0 for no limit", "work Output: actual work done, or -1 if limit reached", "Match Output: Match[i]=j if row i matched with column j, -1 if unmatched", "Work Workspace array of size 5*ncol", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "Q Optional input column permutation (may be NULL). Modified on output.", "P Output: row/col permutation. P[k]=j means j is the k-th row/col.", "R Output: block boundaries. Block b is rows/cols R[b] to R[b+1]-1.", "Work Workspace array of size 4*n", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "maxwork Max work limit for maxtrans; <= 0 for no limit", "work Output: work done in maxtrans, -1 if limit reached", "P Output: row permutation of size n", "Q Output: column permutation of size n (may be flagged negative)", "R Output: block boundaries of size n+1", "nmatch Output: number of nonzeros on diagonal of P*A*Q", "Work Workspace array of size 5*n", "version Output array filled with [major, minor, patch]"], "return": "Number of columns matched (structural rank)", "has_pass2": true}, "BTF/Include/btf_internal.h": {"path": "layer-0/SuiteSparse/BTF/Include/btf_internal.h", "filename": "btf_internal.h", "file": "btf_internal.h", "brief": "Internal definitions for BTF (Block Triangular Form) permutation\nCopyright (c) 2004-2023, University of Florida. LGPL-2.1+ license.\n\nBTF finds permutation matrices P and Q such that P·A·Q has block upper\ntriangular form with square diagonal blocks that are irreducible\n(strongly connected components). Essential preprocessing for sparse LU.", "algorithm": "Block Triangular Form (BTF/Dulmage-Mendelsohn):\nDecomposes sparse matrix into block upper triangular structure:\n1. MAXIMUM TRANSVERSAL: Find maximum matching in bipartite graph\n   - Row i matched to column j if A(i,j) nonzero and selected\n   - Hopcroft-Karp or DFS-based augmenting path algorithm\n2. STRONGLY CONNECTED COMPONENTS:\n   - Build directed graph from A with matched edges\n   - Tarjan's algorithm finds SCCs (diagonal blocks)\n3. TOPOLOGICAL SORT: Order SCCs so dependencies flow downward", "math": "Dulmage-Mendelsohn decomposition:\nP·A·Q = [A_11  A_12  ...  A_1k]\n        [0     A_22  ...  A_2k]\n        [⋮      ⋱    ⋱    ⋮  ]\n        [0     0    ...  A_kk]\nEach A_ii is square and structurally nonsingular (irreducible).\nSingular matrices have zero diagonal blocks (structural rank deficiency).", "complexity": "O(n + nz) for finding BTF permutation.\nMatching: O(n·√nz) using Hopcroft-Karp.", "ref": ["Duff & Reid (1978). \"Algorithm 529: Permutations to Block Triangular\n  Form\". ACM TOMS 4(2):189-192."], "see": ["KLU for sparse LU using BTF preprocessing", "AMD for ordering within diagonal blocks"], "has_pass2": true}, "KLU/Include/klu.h": {"path": "layer-0/SuiteSparse/KLU/Include/klu.h", "filename": "klu.h", "file": "klu.h", "brief": "Sparse LU factorization optimized for circuit simulation matrices\n\nKLU computes a sparse LU factorization of a square matrix A:\n  P*A*Q = L*U\nwhere P and Q are permutation matrices, L is unit lower triangular,\nand U is upper triangular.\n\nKLU is specifically designed for matrices arising from circuit simulation,\nwhich tend to be sparse and nearly block-triangular. The factorization\nproceeds in three phases:\n1. klu_analyze: BTF pre-ordering + fill-reducing ordering (AMD/COLAMD)\n2. klu_factor: Numerical LU factorization (left-looking, column-by-column)\n3. klu_solve: Forward/back substitution to solve Ax = b", "algorithm": "Left-looking LU with BTF preprocessing and partial pivoting:\n  1. BTF pre-ordering (Tarjan's algorithm):\n     - Find strongly connected components of digraph(A)\n     - Permute to block upper triangular form (BTF)\n     - Each diagonal block factored independently\n  2. Fill-reducing ordering within blocks:\n     - AMD for symmetric pattern blocks, COLAMD for unsymmetric\n  3. Left-looking column-by-column LU:\n     - For column k: solve L_{1:k-1, 1:k-1} · x = A_{1:k-1, k}\n     - Apply partial pivoting within column\n     - Extract L_{k+1:n, k} and U_{1:k, k}", "math": "Block Triangular Form: P·A·Q has structure\n  | B_11  B_12  ...  B_1m |\n  |   0   B_22  ...  B_2m |\n  |   ⋮     ⋱    ⋱    ⋮  |\n  |   0    0   ...  B_mm |\n  Each B_ii is factored as P_i·B_ii = L_i·U_i independently.\n  Off-diagonal blocks solved via triangular solves.", "complexity": "Time: O(Σ nnz(L_i)·nnz(U_i)/n_i + off-diagonal work)\n  For circuit matrices: typically O(n) to O(n log n) due to BTF structure\n  Much faster than UMFPACK for matrices with many small diagonal blocks.\n  Space: O(nnz(L+U)) for factors, O(maxblock) workspace per block.", "ref": ["Davis, Palamadai (2010). \"KLU: A Direct Sparse Solver for Circuit\n     Simulation Problems\". ACM Trans. Math. Software.", "Duff, Reid (1978). \"An Implementation of Tarjan's Algorithm for the\n     Block Triangularization of a Matrix\". ACM Trans. Math. Software 4(2)."], "see": ["btf.h for BTF decomposition", "amd.h, colamd.h for fill-reducing orderings", "umfpack.h for general-purpose multifrontal LU", "klu_analyze() to create this object", "klu_factor() to use this for numerical factorization", "klu_factor() to create this object", "klu_solve() to use for solving linear systems", "klu_refactor() to update with new numerical values", "klu_defaults() to initialize", "klu_defaults()", "klu_factor() to perform numerical factorization", "klu_free_symbolic() to free the returned object", "klu_analyze()", "klu_analyze() for automatic ordering", "klu_analyze_given()", "klu_solve() to solve linear systems using the factorization", "klu_refactor() to update factorization with new values", "klu_factor()", "klu_factor()", "klu_factor()", "klu_tsolve() for transpose solve A'x = b", "klu_solve()", "klu_solve()", "klu_solve()", "klu_solve() for standard solve Ax = b", "klu_free_symbolic()", "klu_condest() for a more accurate (but slower) condition estimate"], "param": ["Common Control/statistics structure to initialize", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "Common Control parameters and output statistics", "n Matrix dimension (A is n-by-n)", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "P User's row permutation (size n, NULL for identity)", "Q User's column permutation (size n, NULL for identity)", "Common Control parameters and output statistics", "Ap Column pointers array of size n+1", "Ai Row indices array of size Ap[n]", "Ax Numerical values array of size Ap[n]", "Symbolic Symbolic analysis from klu_analyze", "Common Control parameters and output statistics", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "ldim Leading dimension of B (>= n for column-major storage)", "nrhs Number of right-hand sides", "B Input: right-hand side(s). Output: solution(s). Size ldim*nrhs", "Common Control parameters", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "ldim Leading dimension of B", "nrhs Number of right-hand sides", "B Input: right-hand side(s). Output: solution(s)", "Common Control parameters", "Ap Column pointers (must match original pattern)", "Ai Row indices (must match original pattern)", "Ax New numerical values", "Symbolic Symbolic analysis from klu_analyze", "Numeric Existing numeric object (modified in place)", "Common Control parameters", "Symbolic Pointer to Symbolic object pointer (set to NULL on return)", "Common Control parameters", "Numeric Pointer to Numeric object pointer (set to NULL on return)", "Common Control parameters", "Ap Column pointers array", "Ax Numerical values array", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "Common Result stored in Common->condest", "Symbolic Symbolic analysis from klu_analyze", "Numeric LU factors from klu_factor", "Common Result stored in Common->rcond", "version Output array filled with [major, minor, patch]"], "return": "TRUE on success, FALSE if Common is NULL", "has_pass2": true}, "KLU/Include/klu_internal.h": {"path": "layer-0/SuiteSparse/KLU/Include/klu_internal.h", "filename": "klu_internal.h", "file": "klu_internal.h", "brief": "Internal definitions for KLU sparse LU factorization\nCopyright (c) 2004-2023, University of Florida. LGPL-2.1+ license.\n\nKLU is a sparse LU factorization package designed for circuit simulation\nmatrices, which are typically highly sparse with near-diagonal structure.\nUses BTF (Block Triangular Form) permutation to exploit structure.", "algorithm": "KLU Sparse LU Factorization:\nEfficient sparse LU for circuit matrices via structure exploitation:\n1. BTF PERMUTATION: Find block triangular form P·A·Q = [B_11 B_12 ...]\n   - Diagonal blocks factored independently\n   - Reduces fill-in by isolating strongly connected components\n2. SYMBOLIC ANALYSIS (per block):\n   - AMD ordering within each block\n   - Compute elimination tree and column counts\n   - Allocate storage for L and U factors\n3. NUMERIC FACTORIZATION:\n   - Gilbert-Peierls left-looking algorithm\n   - Partial pivoting within supernodes\n   - Off-diagonal blocks solved by triangular solves", "math": "Block triangular form (BTF):\nP·A·Q has diagonal blocks that are irreducible (strongly connected).\nEach diagonal block B_kk factored as L_k·U_k = B_kk.\nOff-diagonal blocks solved: B_ij·x_j = b_i - L_i·U_i·...", "complexity": "O(|L| + |U|) for factorization, where |L|, |U| are\nnonzero counts in factors. Circuit matrices typically yield sparse factors.", "ref": ["Davis & Palamadai (2010). \"A column pre-ordering strategy for the\n  unsymmetric-pattern multifrontal method\". ACM TOMS 30(2):165-195."], "see": ["BTF library for block triangular form permutation", "AMD library for fill-reducing ordering"], "has_pass2": true}, "CHOLMOD/Include/cholmod_internal.h": {"path": "layer-0/SuiteSparse/CHOLMOD/Include/cholmod_internal.h", "filename": "cholmod_internal.h", "file": "cholmod_internal.h", "brief": "Internal definitions for CHOLMOD sparse Cholesky factorization\nCopyright (C) 2005-2023, Timothy A. Davis. Apache-2.0 license.\n\nCHOLMOD is a comprehensive sparse Cholesky factorization package supporting\nsupernodal and simplicial methods, update/downdate, and multiple orderings.\nHandles symmetric positive definite systems A·x = b via L·L' = A.", "algorithm": "CHOLMOD Sparse Cholesky Factorization:\nSupernodal and simplicial methods for symmetric positive definite matrices:\n\nSUPERNODAL (default for large matrices):\n1. SYMBOLIC ANALYSIS: Compute elimination tree, supernodes, nonzero counts\n2. NUMERIC: For each supernode (group of consecutive pivots):\n   - Assemble frontal matrix from children and original entries\n   - Factor dense submatrix via LAPACK (BLAS-3 for performance)\n   - Store L columns in packed supernodal form\n\nSIMPLICIAL (for very sparse or ill-conditioned):\n1. Process columns left-to-right\n2. Column j: L_j = (A_j - ∑_{k<j} L_jk·L_k) / sqrt(A_jj - ∑L_jk²)\n3. LDL' variant available for indefinite matrices", "math": "Cholesky decomposition:\nA = L·L' where L is lower triangular with positive diagonal.\nFor each column j: L_j = (A_j - L·L_j) / L_jj\nSupernodes: columns j...j+k with identical nonzero patterns in L.\n\nUpdate/downdate: rank-1 modifications L·L' ± w·w' in O(|L|) time.", "complexity": "O(flops) for factorization, typically O(n^1.5) for 2D, O(n²) for 3D.\nSupernodal: better BLAS utilization, ~2-10x faster than simplicial.", "ref": ["Chen, Davis, Hager & Rajamanickam (2008). \"Algorithm 887: CHOLMOD,\n  Supernodal Sparse Cholesky Factorization and Update/Downdate\". ACM TOMS."], "see": ["AMD, CAMD, METIS for fill-reducing orderings", "UMFPACK, KLU for unsymmetric systems"], "has_pass2": true}, "CHOLMOD/Include/cholmod.h": {"path": "layer-0/SuiteSparse/CHOLMOD/Include/cholmod.h", "filename": "cholmod.h", "file": "cholmod.h", "brief": "Comprehensive sparse Cholesky factorization library\n\nCHOLMOD provides high-performance sparse Cholesky factorization for\nsymmetric positive definite (SPD) and symmetric positive semi-definite\nmatrices. It supports:\n\nKey features:\n- Supernodal and simplicial Cholesky (LL' and LDL')\n- Fill-reducing orderings: AMD, COLAMD, METIS, CAMD\n- Real, complex, and pattern-only matrices\n- Single and double precision (float/double)\n- Row/column updates and downdates\n- GPU acceleration (NVIDIA CUDA)\n\nTypical workflow:\n1. cholmod_start: Initialize Common workspace\n2. cholmod_analyze: Symbolic analysis (fill-reducing ordering)\n3. cholmod_factorize: Numerical Cholesky factorization\n4. cholmod_solve: Solve Ax = b using the factors\n5. cholmod_finish: Free workspace", "algorithm": "Update/Downdate (rank-k modification):\n  Given L·L' = A, compute L̃ such that L̃·L̃' = A ± C·C':\n  - Uses Givens rotations for numerical stability\n  - O(k × nnz(L)) complexity for rank-k modification", "complexity": "Factorization: O(nnz(L)²/m) with supernodal method\n  Solve: O(nnz(L)) per right-hand side\n  Fill-in (nnz(L)) depends on ordering quality", "math": "For A = L·L': L(j,j) = √(A(j,j) - Σ L(j,k)²)\n  L(i,j) = (A(i,j) - Σ L(i,k)·L(j,k)) / L(j,j) for i > j", "ref": ["Davis (2006). \"Direct Methods for Sparse Linear Systems\".\n     SIAM. ISBN: 978-0-898716-13-9. Chapter 4-8.", "Chen, Davis, Hager, Rajamanickam (2008). \"Algorithm 887: CHOLMOD,\n     Supernodal Sparse Cholesky Factorization and Update/Downdate\".\n     ACM Trans. Math. Software 35(3)."], "see": ["ldl.h for simple LDL' factorization", "amd.h, colamd.h for fill-reducing orderings", "cholmod_finish() to free all workspace at end", "cholmod_start()", "cholmod_finish()", "cholmod_allocate_sparse() to create", "cholmod_triplet for triplet (COO) format", "cholmod_analyze() to create symbolic factor", "cholmod_factorize() to compute numeric factor", "cholmod_solve() to solve using the factor", "cholmod_allocate_dense() to create", "cholmod_solve() to solve with dense right-hand sides", "cholmod_factorize() to compute numerical factorization", "cholmod_analyze_p() to provide a user permutation", "cholmod_analyze()", "cholmod_analyze() for symbolic analysis", "cholmod_solve() to solve using the factorization", "cholmod_factorize()", "cholmod_factorize() to compute the factorization", "cholmod_solve2() for reusable workspace version", "cholmod_solve()"], "param": ["Common Workspace object to initialize", "Common Workspace to free", "A Sparse matrix to analyze (symmetric or rectangular)", "Common Control parameters and workspace", "A Sparse matrix to factorize (must match pattern from analyze)", "L Factor object from cholmod_analyze (updated with numeric values)", "Common Control parameters and workspace", "sys System to solve (see CHOLMOD_A, CHOLMOD_L, etc.)", "L Cholesky factorization from cholmod_factorize", "B Dense right-hand side matrix", "Common Control parameters and workspace"], "return": "TRUE on success, FALSE on failure", "has_pass2": true}, "LDL/Include/ldl.h": {"path": "layer-0/SuiteSparse/LDL/Include/ldl.h", "filename": "ldl.h", "file": "ldl.h", "brief": "Simple sparse LDL' factorization for symmetric matrices\n\nLDL computes a sparse LDL' factorization of a symmetric matrix A:\n  A = L * D * L'\nwhere L is unit lower triangular and D is diagonal. This factorization\nworks for symmetric indefinite matrices (D may have negative entries).\n\nThe factorization is performed in two phases:\n1. ldl_symbolic: Compute elimination tree and allocate storage\n2. ldl_numeric: Compute numerical values of L and D\n\nTriangular solves (ldl_lsolve, ldl_dsolve, ldl_ltsolve) complete the\nsolution of Ax = b.", "algorithm": "Sparse LDL' factorization (up-looking):\n  1. Symbolic: compute elimination tree and nonzero pattern of L\n     - Parent[j] = min{i > j : L(i,j) ≠ 0} defines elimination tree\n     - Lnz[j] = # nonzeros in column j of L (excluding diagonal)\n  2. Numeric: compute L and D column by column\n     - For each column j: gather contributions from columns k < j\n     - where k has nonzero in row j (i.e., L(j,k) ≠ 0)\n     - D(j,j) = A(j,j) - Σ L(j,k)² D(k,k)\n     - L(i,j) = (A(i,j) - Σ L(i,k) L(j,k) D(k,k)) / D(j,j)", "math": "A = L * D * L' where L is unit lower triangular, D is diagonal.\n  Unlike Cholesky (A = L*L'), LDL' handles indefinite matrices.\n  If A is positive definite, D has all positive entries.\n  D may have negative entries for indefinite matrices.", "complexity": "Time: O(nnz(L)²/n) average, O(nnz(L)·nnz(A)) worst case\n  Space: O(nnz(L) + n) for factors\n  Simple implementation - for production use CHOLMOD instead.", "ref": ["Davis (2005). \"Algorithm 849: A Concise Sparse Cholesky Factorization\n     Package\". ACM Trans. Math. Software 31(4):587-591."], "see": ["cholmod.h for more sophisticated Cholesky implementation", "amd.h for fill-reducing orderings to use with LDL"], "param": ["n Matrix dimension", "Ap Column pointers for A (size n+1)", "Ai Row indices for A", "Lp Output: column pointers for L (size n+1)", "Parent Output: elimination tree (size n)", "Lnz Output: # nonzeros in each column of L (size n)", "Flag Workspace (size n)", "P Fill-reducing permutation (size n, or NULL for natural order)", "Pinv Inverse permutation (size n, or NULL)", "n Matrix dimension", "Ap,Ai,Ax Sparse matrix A in CSC format", "Lp,Parent,Lnz From ldl_symbolic", "Li,Lx Output: row indices and values of L", "D Output: diagonal matrix D (size n)", "Y,Pattern,Flag Workspace arrays (size n each)", "P,Pinv Permutation and inverse (or NULL)"], "return": "n if successful, k if D[k] is zero (matrix singular at column k)", "has_pass2": true}, "COLAMD/Include/colamd.h": {"path": "layer-0/SuiteSparse/COLAMD/Include/colamd.h", "filename": "colamd.h", "file": "colamd.h", "brief": "Column Approximate Minimum Degree ordering for sparse LU factorization\n\nCOLAMD computes a column permutation Q that reduces fill-in during LU\nfactorization of an unsymmetric matrix A. The ordering minimizes the\nfill-in of A*Q when factored as LU.\n\nSYMAMD computes a symmetric ordering for a symmetric matrix, using COLAMD\non the matrix's structure. Both are related to the minimum degree family\nof algorithms.", "algorithm": "SYMAMD (Symmetric AMD using COLAMD):\n  For symmetric A, apply COLAMD to A+A' structure to find fill-reducing order.", "math": "Approximate column degree for LU:\n  deg(j) ≈ # rows in current column's nonzero pattern after elimination\n  Aggressive absorption reduces degree overestimates from element merging.", "complexity": "Time: O(nnz(A)·α(n)) average with aggressive absorption\n  Space: O(nnz(A) + n) for quotient graph representation\n  Slightly slower than AMD but better for unsymmetric patterns.", "ref": ["Davis, Gilbert, Larimore, Ng (2004). \"A Column Approximate Minimum\n     Degree Ordering Algorithm\". ACM Trans. Math. Software 30(3):353-376."], "see": ["amd.h for symmetric matrix ordering", "umfpack.h for sparse LU factorization using COLAMD", "colamd()", "colamd_recommended()", "colamd_set_defaults()", "colamd_recommended(), colamd_set_defaults()", "colamd()", "amd_order() for alternative symmetric ordering", "symamd()", "colamd_report()", "symamd_report()"], "param": ["nnz Number of nonzeros in the matrix", "n_row Number of rows in the matrix", "n_col Number of columns in the matrix", "knobs Output array of size COLAMD_KNOBS to initialize", "n_row Number of rows in A", "n_col Number of columns in A", "Alen Size of array A (use colamd_recommended to compute)", "A Input: row indices in CSC format. Output: destroyed, replaced with permutation", "p Input: column pointers. Output: column permutation Q", "knobs Control parameters (NULL uses defaults)", "stats Output statistics and error codes", "n Matrix dimension (A is n-by-n symmetric)", "A Row indices array (CSC format, lower triangle only)", "p Column pointers array of size n+1", "perm Output permutation array of size n+1", "knobs Control parameters (NULL uses defaults)", "stats Output statistics and error codes", "allocate Memory allocator function (calloc signature)", "release Memory release function (free signature)", "stats Statistics array from colamd() call", "stats Statistics array from symamd() call", "version Output array filled with [major, minor, patch]"], "return": "Recommended array size, or 0 if inputs are invalid", "has_pass2": true}, "ParU/Include/ParU.h": {"path": "layer-0/SuiteSparse/ParU/Include/ParU.h", "filename": "ParU.h", "file": "ParU.h", "brief": "Parallel unsymmetric multifrontal sparse LU factorization\nCopyright (c) 2022-2025, Mohsen Aznaveh and Timothy A. Davis. GPL-3.0-or-later.\n\nParU is a parallel sparse direct solver using OpenMP tasking for task-based\nparallelism combined with parallel BLAS (nested parallelism). Solves Ax = b\nfor sparse A via LU factorization with partial pivoting.", "algorithm": "Parallel Multifrontal LU Factorization:\nSparse unsymmetric LU with task-parallel numeric phase:\n1. SYMBOLIC ANALYSIS (via UMFPACK):\n   - Compute fill-reducing permutation (AMD, COLAMD, METIS)\n   - Build elimination/assembly tree\n   - Identify singleton rows/columns for special handling\n2. FRONTAL MATRIX FACTORIZATION:\n   - Process fronts in topological (child-before-parent) order\n   - Each front: dense LU with partial pivoting via BLAS\n   - Assemble contributions from children (extend-add)\n   - Task parallelism: independent fronts processed concurrently\n3. SOLVE PHASE:\n   - Forward solve: L·y = P·b\n   - Backward solve: U·x = y\n   - Apply permutations", "math": "Multifrontal LU factorization:\nP·A·Q = L·U where P,Q are permutation matrices.\nEach frontal matrix F_k ∈ ℝ^(m_k × n_k) factored densely:\nF_k = [L_k 0; L_21 I] · [U_k U_12; 0 S_k]\nSchur complement S_k assembled to parent front.\n\nSingleton handling: rows/columns with single nonzero factored trivially,\nremoved before main factorization for efficiency.", "complexity": "O(n + symbolic) for analysis, O(flops) for factorization.\nFlop count depends on matrix structure and fill-reducing ordering quality.\nTypically O(n^1.5) to O(n^2) for 2D/3D mesh problems.", "ref": ["Aznaveh & Davis (2024). \"ParU: A Parallel Unsymmetric Multifrontal\n  Sparse LU Factorization\". ACM Trans. Math. Software."], "see": ["UMFPACK for sequential sparse LU (used for symbolic analysis)", "CHOLMOD for sparse matrix data structures"], "has_pass2": true}, "GraphBLAS/zstd/zstd_subset/compress/zstd_ldm.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/compress/zstd_ldm.h", "filename": "zstd_ldm.h", "return": "The length of the last literals.\n\nNOTE: The source must be at most the maximum block size, but the predefined\nsequences can be any size, and may be longer than the block. In the case that\nthey are longer than the block, the last sequences may need to be split into\ntwo. We handle that case correctly, and update `rawSeqStore` appropriately.\nNOTE: This function does not return any errors.", "has_pass2": false}, "GraphBLAS/zstd/zstd_subset/compress/zstd_compress_internal.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/compress/zstd_compress_internal.h", "filename": "zstd_compress_internal.h", "return": ": 0 on success or error code", "has_pass2": false}, "GraphBLAS/zstd/zstd_subset/common/huf.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/common/huf.h", "filename": "huf.h", "return": ": 0==HUF_decompress4X1, 1==HUF_decompress4X2 .\n Assumption : 0 < dstSize <= 128 KB", "has_pass2": false}, "GraphBLAS/zstd/zstd_subset/common/zstd_trace.h": {"path": "layer-0/SuiteSparse/GraphBLAS/zstd/zstd_subset/common/zstd_trace.h", "filename": "zstd_trace.h", "param": ["cctx The dctx pointer for the compression.\n            It can be used as a key to map begin() to end().", "ctx The return value of ZSTD_trace_compress_begin().", "trace The zstd tracing info.", "dctx The dctx pointer for the decompression.\n            It can be used as a key to map begin() to end().", "ctx The return value of ZSTD_trace_decompress_begin().", "trace The zstd tracing info."], "has_pass2": false}}}}}, "layer-1": {"name": "layer-1", "library_count": 4, "libraries": {"Clp": {"name": "Clp", "file_count": 87, "pass2_count": 77, "files": {"src/AbcDualRowSteepest.hpp": {"path": "layer-1/Clp/src/AbcDualRowSteepest.hpp", "filename": "AbcDualRowSteepest.hpp", "file": "AbcDualRowSteepest.hpp", "brief": "Steepest edge pivot selection for ABC dual simplex", "author": "John Forrest (FasterCoin, 2012)\n\nImplements steepest edge algorithm (Forrest-Goldfarb) for choosing the\nleaving variable in dual simplex. Normalizes infeasibilities by the\nsquared norm of the tableau row to select the direction of steepest\nimprovement.", "algorithm": "Steepest Edge for Dual Simplex Row Selection:\n  Choose leaving variable r that maximizes |d_r|/||e_r·B⁻¹·A||:\n  1. Maintain weights w_i = ||e_i·B⁻¹·A||² for each basic variable\n  2. Select r = argmax_i (d_i²/w_i) where d_i is dual infeasibility\n  3. Update weights: w_i ← w_i - 2·(α_i/α_r)·⟨u,a_i⟩ + (α_i/α_r)²·w_r\n     where α_i is i-th element of pivot column, u is BTRAN result", "math": "Weight update formula (Forrest-Goldfarb):\n  Let B be current basis, α = B⁻¹·a_s (pivot column), τ = B⁻ᵀ·e_r (row)\n  After pivot, new weights: w_i' = w_i + (α_i/α_r)²·w_r - 2·(α_i/α_r)·γ_i\n  where γ_i = ⟨row i of B⁻¹·A, τ⟩. Special case: w_r' = w_s/α_r².", "complexity": "O(m) per pivot for weight updates when sparse.\n  Full weight recomputation: O(m²) but rarely needed.\n  Reduces total iterations by 30-50% vs Dantzig on hard problems.", "ref": ["Forrest & Goldfarb (1992). \"Steepest-edge simplex algorithms for\n     linear programming\". Mathematical Programming 57:341-374.\n\nModes (controlled by constructor parameter):\n- 0: Uninitialized weights\n- 1: Full steepest edge with maintained weights\n- 2: Partial uninitialized (only scan some infeasible rows)\n- 3: Starts partial, may switch to full (default, recommended)\n\nThe ABC version optimizes weight storage and updates for cache efficiency."], "see": ["AbcDualRowPivot for the base interface", "AbcDualRowDantzig for simpler but often slower alternative", "ClpDualRowSteepest for the standard (non-ABC) implementation"], "has_pass2": true}, "src/ClpModelParameters.hpp": {"path": "layer-1/Clp/src/ClpModelParameters.hpp", "filename": "ClpModelParameters.hpp", "file": "ClpModelParameters.hpp", "brief": "Enumeration types for ClpModel parameter access\n\nDefines typed parameter indices for ClpModel's get/set methods:\n\nClpIntParam - Integer parameters:\n- ClpMaxNumIteration: Maximum simplex iterations\n- ClpMaxNumIterationHotStart: Hot start iteration limit\n- ClpNameDiscipline: Row/column naming mode (0=auto, 1=lazy, 2=full)\n\nClpDblParam - Double parameters:\n- ClpDualObjectiveLimit: Stop when dual objective exceeds\n- ClpPrimalObjectiveLimit: Stop when primal objective exceeds\n- ClpDualTolerance: Dual feasibility tolerance\n- ClpPrimalTolerance: Primal feasibility tolerance\n- ClpObjOffset: Constant term in objective\n- ClpMaxSeconds/ClpMaxWallSeconds: Time limits\n- ClpPresolveTolerance: Presolve zero tolerance\n\nClpStrParam - String parameters:\n- ClpProbName: Problem name from MPS file\n\nUtility templates for array operations:\n- ClpDisjointCopyN(), ClpFillN(), ClpCopyOfArray(): Non-COIN versions\n\nClpTrustedData: Opaque structure for passing user data to trusted code.", "see": ["ClpModel for parameter getter/setter methods"], "has_pass2": false}, "src/ClpGubMatrix.hpp": {"path": "layer-1/Clp/src/ClpGubMatrix.hpp", "filename": "ClpGubMatrix.hpp", "file": "ClpGubMatrix.hpp", "brief": "Generalized Upper Bound (GUB) matrix for special LP structure\n\nImplements GUB constraints - sets of variables where exactly one (or at\nmost one) must be in the basis. This structure appears in problems like\nassignment, crew scheduling, and set partitioning.", "algorithm": "GUB Simplex with Implicit Rows:\n  Exploits constraint structure Σ_{j∈S_k} x_j = 1 (or ≤1) for each set S_k:\n  1. **Key variable selection:** For each set, designate one \"key\" variable\n     - If any x_j in set is basic, key is basic; others are at bounds\n     - Key status determines set's contribution to RHS\n  2. **Basis reduction:** k GUB sets reduce basis size by k rows\n     - Factor (m-k)×(m-k) matrix instead of m×m\n     - Pivot operations proportionally cheaper\n  3. **Pricing:** For each set, only key or one bound variable can price\n     - Reduced cost of entering variable adjusts for GUB dual\n  4. **Ratio test:** Standard but respects set membership constraints\n     - Entering non-key forces current key to bound\n  5. **Basis update:** Key may change within set during pivot", "math": "GUB constraint handling:\n  Original: min c'x, Ax = b, Σ_{j∈S_k} x_j = d_k for k=1..K\n  GUB stores: d_k - Σ_{j∈S_k, j≠key} x_j = x_key implicitly\n  Dual multiplier π_k adjusts reduced costs: c̄_j = c_j - π'A_j - π_k\n  Only (m-K) explicit constraints need factorization", "complexity": "Basis factorization: O((m-K)³) vs O(m³) for explicit rows\n  Pricing: O(n) but faster due to set structure exploitation\n  Memory: O(n) for GUB data + standard matrix storage\n  Most beneficial when K/m is large (many GUB sets relative to rows)", "ref": ["Dantzig & Van Slyke (1967). \"Generalized Upper Bounding Techniques\".\n     Journal of Computer and System Sciences 1:213-226.\n\nGUB structure: For each \"set\" of columns, sum(x_j for j in set) = 1 (or ≤1).\nInstead of adding these as explicit rows, GUB exploits the structure to:\n- Reduce basis size (fewer rows to factorize)\n- Speed up pricing (only one variable per set can improve)\n- Handle larger problems efficiently\n\nKey concepts:\n- Each set has a \"key\" variable that's basic if any in set is basic\n- Hidden rows track GUB constraints implicitly\n- Special pivot logic handles set membership constraints"], "see": ["ClpPackedMatrix which this extends", "ClpDynamicMatrix for column generation with GUB structure", "ClpSimplex for the solver that uses this matrix"], "has_pass2": true}, "src/ClpLsqr.hpp": {"path": "layer-1/Clp/src/ClpLsqr.hpp", "filename": "ClpLsqr.hpp", "file": "ClpLsqr.hpp", "brief": "LSQR iterative solver for sparse least-squares problems\n\nImplements the LSQR algorithm of Paige and Saunders (1982) for solving:\n- Ax = b (exact solve)\n- min ||b - Ax||_2 (least squares)\n- min ||(b) - (A    )x||_2 (damped/regularized)\n      ||(0)   (damp*I)  ||", "algorithm": "LSQR (Lanczos Bidiagonalization Least Squares):\n  Iterative method building Krylov subspace via bidiagonalization:\n  1. Initialize: β₁u₁ = b, α₁v₁ = A'u₁\n  2. Bidiagonalization step k:\n     β_{k+1}u_{k+1} = Av_k - α_k u_k\n     α_{k+1}v_{k+1} = A'u_{k+1} - β_{k+1}v_k\n  3. Update QR factorization of bidiagonal matrix B_k\n  4. Update solution: x_k = V_k y_k where B_k y_k ≈ β₁e₁\n  5. Check convergence: ||r_k|| and ||A'r_k|| tolerances", "math": "Bidiagonalization produces:\n  A·V_k = U_{k+1}·B_k where B_k is (k+1)×k bidiagonal\n  Solution minimizes ||B_k y - β₁e₁|| which equals ||Ax - b||\n  Equivalent to CG on normal equations A'Ax = A'b but more stable.", "complexity": "O(nnz(A)) per iteration for matrix-vector products.\n  Convergence: O(κ(A)) iterations for condition number κ.\n  Total: O(nnz(A)·κ(A)) vs O(n³) for direct methods.", "ref": ["Paige & Saunders (1982). \"LSQR: An algorithm for sparse linear equations\n     and sparse least squares\". ACM TOMS 8(1):43-71.", "Saunders (1995). \"Solution of sparse rectangular systems using LSQR\n     and CRAIG\". BIT 35:588-604.\n\nLSQR is a conjugate-gradient-like method based on Lanczos bidiagonalization.\nOnly requires matrix-vector products, not explicit matrix storage.\n\nUsed by PDCO for iterative solution of normal equations."], "see": ["ClpPdco which uses LSQR for linear solves", "ClpPdcoBase for the problem interface"], "has_pass2": true}, "src/ClpSimplexDual.hpp": {"path": "layer-1/Clp/src/ClpSimplexDual.hpp", "filename": "ClpSimplexDual.hpp", "file": "ClpSimplexDual.hpp", "brief": "Dual simplex algorithm implementation", "author": "John Forrest", "algorithm": "Dual Simplex Method:\nSolves LP by maintaining dual feasibility (reduced costs have correct signs)\nwhile iterating toward primal feasibility. Each iteration:\n1. Choose leaving variable (pivot row) - infeasible basic variable\n2. Compute pivot row of tableau: r^T = e_i^T B^{-1} A\n3. Choose entering variable (pivot column) - via ratio test on reduced costs\n4. Update basis: swap entering/leaving variables\n5. Update dual solution and reduced costs\n\nUses single-phase approach with fake bounds (updatedDualBound_) to handle\ndual infeasibility. Steepest edge or Dantzig strategies for pivot selection.\nAnti-degeneracy via cost perturbation (OSL heritage).\n\nThree nested loops: outer handles fake bounds, middle handles refactorization,\ninner performs pivots until optimality or infeasibility detected.", "math": "min c^T x s.t. Ax=b, l≤x≤u (primal)\nDual: max b^T y s.t. A^T y + s = c, s_j≥0 for x_j at lower, s_j≤0 for x_j at upper\nReduced cost: s_j = c_j - a_j^T y where y = B^{-T} c_B\nDual feasible when reduced costs have correct signs for non-basic variables.\nRatio test: θ* = min{s_j/r_j : r_j has correct sign} determines entering var.", "complexity": "O(m^2 n) per iteration typical (dominated by BTRAN/FTRAN).\nIteration count highly problem-dependent. Often faster than primal simplex\nfor problems with many constraints, especially after adding cuts in B&B.", "ref": ["Forrest & Goldfarb, \"Steepest-edge simplex algorithms for LP\",\n     Mathematical Programming 57 (1992) 341-374", "Koberstein, \"Progress in the dual simplex algorithm\",\n     Computers & Operations Research 35 (2008) 2297-2320\n\nImplements the dual simplex method for LP. This is a \"mix-in\" class that\ninherits from ClpSimplex but adds no data - ClpSimplex objects are cast\nto this type when running dual simplex.\n\nThe dual simplex maintains dual feasibility (reduced costs have correct signs)\nwhile iterating toward primal feasibility. It's typically faster than primal\nsimplex for most problems, especially after adding cuts in branch-and-bound.\n\nKey algorithmic features:\n- Single-phase approach with weighted objective for dual feasibility\n- Fake bounds (updatedDualBound_) to handle dual infeasibility\n- Sparse data structures to exploit problem sparsity\n- Steepest edge or Dantzig pivot selection for choosing leaving variable\n- Anti-degeneracy via cost perturbation (from OSL heritage)"], "see": ["ClpSimplex for the base simplex class", "ClpSimplexPrimal for primal simplex variant", "ClpDualRowPivot for pivot row selection strategies", "ClpDualRowSteepest, ClpDualRowDantzig for specific strategies"], "has_pass2": true}, "src/ClpNonLinearCost.hpp": {"path": "layer-1/Clp/src/ClpNonLinearCost.hpp", "filename": "ClpNonLinearCost.hpp", "file": "ClpNonLinearCost.hpp", "brief": "Piecewise linear cost handling and bound infeasibility tracking\n\nManages piecewise linear objective functions and tracks bound violations\nduring primal simplex. When variables move outside their bounds, this class\ncomputes the appropriate infeasibility penalty costs.", "algorithm": "Piecewise Linear Cost Management:\n  Handles non-linear costs and bound infeasibilities in primal simplex:\n  1. **Region tracking:** Each variable has status (below/feasible/above)\n     - CLP_BELOW_LOWER: x_j < l_j, penalty cost applied\n     - CLP_FEASIBLE: l_j ≤ x_j ≤ u_j, true cost used\n     - CLP_ABOVE_UPPER: x_j > u_j, penalty cost applied\n  2. **Cost computation:** At iteration k, effective cost is:\n     - Feasible: c_j (true objective coefficient)\n     - Infeasible: c_j ± M where M = infeasibilityWeight_\n  3. **Breakpoint handling (Method 1):** Full piecewise linear\n     - Store breakpoints lower_[range], costs cost_[range]\n     - Track current range via whichRange_[], offset_[]\n  4. **Big-M handling (Method 2):** Simple penalty\n     - Just add ±infeasibilityWeight_ outside bounds\n     - Faster, sufficient for standard LP", "math": "Two-phase simplex via penalty:\n  Phase I objective: min Σ M·max(l_j - x_j, 0) + Σ M·max(x_j - u_j, 0)\n  Phase II objective: min c'x (original)\n  Combined: min c'x + M·(infeasibility measure)\n  As M → ∞, optimal solution minimizes infeasibility first\n  Clp uses finite M (default 1e10) with dynamic adjustment", "complexity": "O(1) per cost evaluation and region change\n  Method 2 is ~2x faster than Method 1 for standard LP\n  Total overhead: O(n) per iteration for bound checking\n\nThe class supports two methods:\n- Method 1: Full piecewise linear with explicit breakpoints\n- Method 2: Simple big-M penalty for bound violations (faster)\n\nKey functionality:\n- Tracks which variables are below lower, feasible, or above upper bound\n- Computes infeasibility-weighted costs during Phase I\n- Updates costs efficiently as variables move between regions\n\nThis enables the two-phase simplex method where Phase I minimizes\ninfeasibility before Phase II minimizes the true objective.", "see": ["ClpSimplexPrimal for how this is used in primal simplex", "ClpSimplex::infeasibilityCost() for the penalty weight"], "has_pass2": true}, "src/ClpCholeskyPardiso.hpp": {"path": "layer-1/Clp/src/ClpCholeskyPardiso.hpp", "filename": "ClpCholeskyPardiso.hpp", "file": "ClpCholeskyPardiso.hpp", "brief": "Intel MKL Pardiso sparse direct solver for Cholesky factorization\n\nWraps Intel's Pardiso solver from the Math Kernel Library (MKL) for\nCholesky factorization of normal equations in interior point methods.", "algorithm": "Intel MKL Pardiso Sparse Cholesky:\n  Highly optimized parallel sparse factorization A = LL':\n  1. Reordering phase: Nested dissection or minimum degree ordering\n  2. Symbolic factorization: Determine nonzero structure of L\n  3. Numerical factorization: Compute L with OpenMP parallelism\n     - Supernodal blocking for BLAS-3 efficiency\n     - Two-level parallelism: task + loop\n  4. Solve: Forward/backward substitution (parallelized)", "math": "Supernodal factorization:\n  Group consecutive pivots with same sparsity pattern.\n  Supernode operations: dense BLAS-3 on column panels.\n  L_JI = A_JI · L_II⁻ᵀ where I is supernode, J below.", "complexity": "O(nnz(L)·supernode_size) for factorization.\n  Parallel efficiency 70-90% with OpenMP on multi-core.\n  Out-of-core mode enables problems larger than RAM.", "ref": ["Schenk & Gärtner (2004). \"Solving unsymmetric sparse systems of linear\n     equations with PARDISO\". Future Generation Computer Systems 20:475-487.\n\nPardiso provides:\n- Highly optimized sparse direct solving on Intel CPUs\n- Parallel factorization using OpenMP\n- Out-of-core capability for large problems\n\nRequires Intel MKL library and PARDISO_BARRIER compile flag."], "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization"], "has_pass2": true}, "src/AbcSimplex.hpp": {"path": "layer-1/Clp/src/AbcSimplex.hpp", "filename": "AbcSimplex.hpp", "file": "AbcSimplex.hpp", "brief": "AVX/SIMD-optimized simplex solver (\"A Better Clp\")", "author": "John Forrest (FasterCoin, 2012)\n\nAbcSimplex is an optimized reimplementation of ClpSimplex designed for\nmodern CPU architectures. Key optimizations include:\n\n- Cache-friendly data layout: arrays organized as [rows|columns] contiguously\n- SIMD/AVX vectorization support for matrix operations\n- Optional parallelization via pthreads or Intel Cilk Plus\n- Partitioned vectors for parallel-safe sparse operations\n\nBuild modes (CLP_HAS_ABC):\n- 1: Serial Abc, no inheritance\n- 2: Serial Abc with inheritance (ClpSimplex can use Abc internally)\n- 3: Cilk parallel, no inheritance\n- 4: Cilk parallel with inheritance\n\nThe class inherits from ClpSimplex for API compatibility but maintains\nits own internal data structures (abcSolution_, abcDj_, abcLower_, etc.)\nthat are better suited for vectorized operations.", "see": ["ClpSimplex for the standard (non-AVX) implementation", "AbcSimplexDual for AVX-optimized dual simplex", "AbcSimplexPrimal for AVX-optimized primal simplex", "AbcMatrix for the optimized matrix class"], "algorithm": "Data Layout Optimization:\n      All arrays use [structural|slack] layout instead of separate storage.\n      This improves cache locality during basis updates. Key arrays:\n      - abcSolution_[0..n+m]: primal solution values\n      - abcDj_[0..n+m]: reduced costs (dual solution)\n      - abcLower_/abcUpper_[0..n+m]: variable bounds", "complexity": "Same asymptotic complexity as ClpSimplex:\n      Per-iteration: O(m²) average for basis update\n      Practical speedup: 2-4x from SIMD, additional from parallelism", "ref": ["Forrest, J.J. and Goldfarb, D. (1992). \"Steepest-edge simplex\n      algorithms for linear programming\". Math. Programming 57:341-374."], "param": ["indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints"], "has_pass2": true}, "src/ClpLinearObjective.hpp": {"path": "layer-1/Clp/src/ClpLinearObjective.hpp", "filename": "ClpLinearObjective.hpp", "file": "ClpLinearObjective.hpp", "brief": "Standard linear objective function (c'x)", "algorithm": "Linear Objective c'x with Gradient Access\nImplements the standard LP objective: minimize c'x. This is a thin wrapper\naround a coefficient vector that implements the ClpObjective interface.\n\nFor quadratic objectives (x'Qx/2 + c'x), see ClpQuadraticObjective.", "see": ["ClpObjective for the abstract objective interface", "ClpQuadraticObjective for convex QP objectives", "ClpModel for how objectives are stored in the model"], "has_pass2": true}, "src/ClpNode.hpp": {"path": "layer-1/Clp/src/ClpNode.hpp", "filename": "ClpNode.hpp", "file": "ClpNode.hpp", "brief": "Node representation for branch-and-bound fathoming\n\nSupport classes for Clp's fathom capability - solving subproblems\nin a branch-and-bound tree. Used when Clp is embedded in CBC.", "algorithm": "Pseudocost Branching:\nLearn branching effectiveness from history:\n\n  pseudocost_down[j] = avg(LP_degradation / fraction_cut) when branching down\n  pseudocost_up[j] = avg(LP_degradation / fraction_raised) when branching up\n\nVariable selection heuristics:\n  - Strong branching: actually solve children, use real degradation\n  - Pseudocost: use predicted degradation from history\n  - Hybrid: strong branching initially, pseudocosts after \"trust\" threshold\n\nClasses:\n- ClpNode: State of a B&B node (bounds, basis, factorization, pseudocosts)\n- ClpNodeStuff: Shared data across nodes (pseudocosts, solver options)\n- ClpHashValue: Hash table for deduplicating values\n\nKey methods:\n- applyNode(): Restore node state to model\n- chooseVariable(): Select branching variable\n- fixOnReducedCosts(): Tighten bounds using reduced cost fixing", "math": "For minimization with best incumbent z_best:\n  If x_j at lower bound with reduced cost d_j > 0:\n    Gap to force x_j=1 exceeds (z_best - z_LP) / d_j\n    If this exceeds (u_j - l_j), can fix x_j = l_j permanently\n  Similarly for variables at upper bound with d_j < 0", "see": ["Cbc for the full branch-and-bound solver", "ClpSimplex which solves individual nodes"], "has_pass2": true}, "src/AbcMatrix.hpp": {"path": "layer-1/Clp/src/AbcMatrix.hpp", "filename": "AbcMatrix.hpp", "file": "AbcMatrix.hpp", "brief": "Cache-optimized matrix for ABC simplex with multiple copies", "author": "John Forrest (FasterCoin, 2012)\n\nImplements a scaled matrix wrapper optimized for ABC's data layout.\nMay maintain up to THREE copies for different access patterns.", "algorithm": "Row Copy Organization:\nFor dual simplex ratio test (dualColumn1):\n  1. Compute π = c_B'·B⁻¹ (BTRAN)\n  2. For each nonbasic j: compute π·A_j using row copy\n  3. Partitioned row copy skips basic columns", "see": ["AbcSimplex which uses this matrix", "ClpPackedMatrix for the standard (non-ABC) matrix", "CoinPackedMatrix in CoinUtils for underlying storage"], "has_pass2": true}, "src/ClpCholeskyUfl.hpp": {"path": "layer-1/Clp/src/ClpCholeskyUfl.hpp", "filename": "ClpCholeskyUfl.hpp", "file": "ClpCholeskyUfl.hpp", "brief": "SuiteSparse CHOLMOD interface for Cholesky factorization\n\nWraps the CHOLMOD library from SuiteSparse (University of Florida) for\nCholesky factorization of normal equations in interior point methods.", "algorithm": "CHOLMOD Supernodal Sparse Cholesky:\n  Hybrid supernodal/simplicial factorization A = LL':\n  1. Ordering: Fill-reducing permutation via AMD, COLAMD, or METIS\n  2. Symbolic: Analyze sparsity of L, allocate memory\n  3. Numeric: Factor using supernodal method (large fronts) or\n     simplicial (small problems) - automatically selected\n  4. Solve: Forward L·z = b, backward L'·x = z", "math": "Supernodal vs simplicial:\n  Supernodal: Groups consecutive pivots with same sparsity pattern\n  Uses dense BLAS-3 on column panels for cache efficiency\n  Simplicial: Classical left-looking algorithm for small/sparse problems\n  CHOLMOD auto-selects based on estimated flop count", "complexity": "Supernodal: O(nnz(L)·f̄) where f̄ = average supernode size\n  Typically 5-10x faster than simplicial for large problems\n  Memory: O(nnz(L)) with efficient compressed storage", "ref": ["Davis & Hager (2009). \"Dynamic Supernodes in Sparse Cholesky Update/\n     Downdate and Triangular Solves\". ACM TOMS 35:27.\n\nCHOLMOD provides:\n- Supernodal and simplicial Cholesky modes\n- Automatic fill-reducing ordering (AMD, METIS)\n- Efficient memory management for sparse factors\n\nThis is the most portable high-quality option across platforms.\nRequires SuiteSparse/CHOLMOD library."], "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization", "http://www.cise.ufl.edu/research/sparse/cholmod"], "has_pass2": true}, "src/ClpDualRowSteepest.hpp": {"path": "layer-1/Clp/src/ClpDualRowSteepest.hpp", "filename": "ClpDualRowSteepest.hpp", "file": "ClpDualRowSteepest.hpp", "brief": "Steepest edge pivot selection for dual simplex", "algorithm": "Steepest Edge Dual Pivot Selection:\nSelects leaving variable (pivot row) by normalizing infeasibility by edge weight.\nInstead of choosing most infeasible basic variable (Dantzig), computes:\n  score_i = (infeasibility_i)² / weight_i\nwhere weight_i = ||r_i||² = ||e_i^T B^{-1}||² is squared norm of i-th row of B^{-1}.\n\nWeight update after pivot in row p with column entering:\n  weight_i := weight_i - 2*(r_ip/r_pp)*v_i + (r_ip/r_pp)²*weight_p\nwhere r is pivot row and v is BTRAN of pivot column. This preserves sparsity.\n\nThree modes: uninitialized (1s), full (exact weights), partial (sparse scan).\nMaintains infeasibility list for efficient scanning of only violated rows.", "math": "Edge weight: γ_i = ||B^{-T}e_i||² = (i-th row of B^{-1})^T(i-th row of B^{-1})\nNormalized ratio: d_i/γ_i where d_i is dual infeasibility of row i.\nUpdate formula from BTRAN column: v = B^{-T}a_j gives γ_i' = γ_i - 2α_i v_i + α_i² γ_p\nwhere α_i = r_ip/r_pp is multiplier for row i in elimination.", "complexity": "O(m) per pivot for weight updates (sparse operations).\nFull recomputation O(m²) but rare. Typically 30-50% fewer iterations than Dantzig.", "ref": ["Forrest & Goldfarb, \"Steepest-edge simplex algorithms for LP\",\n     Mathematical Programming 57 (1992) 341-374", "Harris, \"Pivot selection methods of the Devex LP code\",\n     Mathematical Programming 5 (1973) 1-28\n\nImplements the steepest edge algorithm for choosing the leaving variable\nin dual simplex. Instead of choosing the most infeasible row (Dantzig),\nsteepest edge normalizes by the squared norm of the tableau row, selecting\nthe direction of steepest improvement in the dual objective.\n\nThis is the recommended pivot strategy - it typically requires fewer\niterations than Dantzig, especially on degenerate problems."], "see": ["\"Implementing the Dantzig-Wolfe decomposition\" by Forrest & Goldfarb\n     for the steepest edge algorithm", "ClpDualRowPivot for the base interface", "ClpDualRowDantzig for simpler but often slower alternative", "ClpSimplexDual for the dual simplex algorithm"], "has_pass2": true}, "src/AbcSimplexPrimal.hpp": {"path": "layer-1/Clp/src/AbcSimplexPrimal.hpp", "filename": "AbcSimplexPrimal.hpp", "file": "AbcSimplexPrimal.hpp", "brief": "AVX-optimized primal simplex algorithm", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the primal simplex method using ABC's optimized data structures.\nLike ClpSimplexPrimal, this is a \"mix-in\" class - AbcSimplex objects are\ncast to this type at runtime when running primal simplex.", "algorithm": "AVX-Optimized Primal Simplex:\n  Vectorized primal simplex with partitioned sparse operations:\n  1. **Pricing (primalColumn):** Scan reduced costs for entering variable\n     - AVX-parallel: Process 4 doubles simultaneously\n     - Partitioned: CoinPartitionedVector enables cache-local scans\n     - Early termination: Stop when \"good enough\" candidate found\n  2. **FTRAN:** Compute pivot column B⁻¹·a_j\n     - Uses AbcSimplex's optimized factorization\n     - Exploits sparsity through CoinIndexedVector\n  3. **Ratio test (primalRow):** Select leaving variable\n     - Harris two-pass: First find θ_max, then select leaving\n     - Vectorized bound comparisons\n  4. **Update:** Modify basis and solution\n     - Minor iterations: Multiple pivots per factorization check\n     - Partial BTRAN for cost updates", "math": "Primal simplex optimality and feasibility:\n  Maintain: x_B = B⁻¹b - B⁻¹N·x_N (primal feasible if x ≥ 0)\n  Reduced costs: c̄_N = c_N - N'·π where π = B⁻ᵀc_B\n  Optimal when: c̄_N ≥ 0 (minimization) and x_B ≥ 0\n  Step: θ* = min{x_Bi/α_i : α_i > 0} (ratio test)", "complexity": "Per iteration:\n  Pricing: O(n) but ~n/4 with AVX vectorization\n  FTRAN: O(nnz(L) + nnz(U)) with sparse factorization\n  Ratio test: O(m) worst case, sparse typically\n  Minor iterations reduce factorization overhead by k× for k pivots\n\nKey optimizations over ClpSimplexPrimal:\n- Vectorized column selection in primalColumn()\n- Optimized ratio test in primalRow()\n- Better cache utilization through partitioned vectors\n- Support for minor iterations (multiple pivots per factorization check)\n\nThe algorithm structure mirrors ClpSimplexPrimal:\n- Choose incoming column (pivot column)\n- Compute pivot column in tableau\n- Choose outgoing row via ratio test\n- Update solution and basis\n\nSpecial features:\n- alwaysOptimal(): Don't change infeasibility cost, always report optimal\n- exactOutgoing(): Prevent variables from going slightly negative\n- lexSolve(): Lexicographic resolution for degeneracy", "see": ["AbcSimplex for the base optimized simplex class", "AbcSimplexDual for optimized dual simplex", "ClpSimplexPrimal for the standard (non-AVX) implementation"], "has_pass2": true}, "src/ClpNetworkMatrix.hpp": {"path": "layer-1/Clp/src/ClpNetworkMatrix.hpp", "filename": "ClpNetworkMatrix.hpp", "file": "ClpNetworkMatrix.hpp", "brief": "Specialized matrix for pure network LP problems\n\nImplements efficient storage for network flow problems where each column\n(arc) has exactly two nonzeros: +1 at the head node and -1 at the tail node.\nThis representation requires only O(n) storage for row indices vs O(2n) for\na general sparse matrix.", "algorithm": "Network Matrix Operations:\n  Exploits +1/-1 structure for O(1) element access:\n  1. Storage: indices_[2j] = tail, indices_[2j+1] = head for arc j\n  2. Column j: A[tail,j] = -1, A[head,j] = +1 (flow conservation)\n  3. Matrix-vector: y = Ax requires only n additions (no multiplies)\n     For each arc j: y[head] += x[j], y[tail] -= x[j]\n  4. Transpose: x = A'y similarly O(n) additions", "math": "Network flow structure:\n  Column j represents arc from node tail to node head\n  A_j = e_{head} - e_{tail} (unit vectors)\n  For min-cost flow: min c'x s.t. Ax = b, 0 ≤ x ≤ u\n  Basis is spanning tree → O(n) pivot operations", "complexity": "Storage: O(2n) integers vs O(nnz) for general sparse\n  Matrix-vector: O(n) vs O(nnz) for general (no multiplications)\n  Enables network simplex with O(n) pivots vs O(n²) general", "ref": ["Ahuja, Magnanti, Orlin (1993). \"Network Flows: Theory, Algorithms,\n     and Applications\". Prentice Hall, Ch. 11.\n\nNetwork problems have special structure that allows very fast simplex pivots\nand specialized algorithms. This class exploits that structure while\npresenting the standard ClpMatrixBase interface.\n\nStorage: indices_ stores pairs of row indices (tail, head) for each column.\nA row index of -1 indicates a slack arc with only one endpoint."], "see": ["ClpMatrixBase for the abstract matrix interface", "ClpPackedMatrix for general sparse matrix storage", "CoinPackedMatrix in CoinUtils for underlying sparse format"], "has_pass2": true}, "src/ClpConstraintLinear.hpp": {"path": "layer-1/Clp/src/ClpConstraintLinear.hpp", "filename": "ClpConstraintLinear.hpp", "file": "ClpConstraintLinear.hpp", "brief": "Linear constraint implementation for nonlinear extensions\n\nImplements ClpConstraint for a linear constraint: sum(a_j * x_j) = b.\nUsed with ClpSimplexNonlinear when linear constraints appear alongside\nnonlinear objective or other nonlinear constraints.", "algorithm": "Use in SLP (Sequential Linear Programming):\nFor mixed linear/nonlinear problems, linear constraints provide exact\nlinearization (no approximation error) at any point. This allows SLP\nto handle them without trust region restrictions.\n\nStorage is sparse: parallel column_ and coefficient_ arrays store\nonly non-zero coefficients. This is efficient when linear constraints\nare embedded within larger nonlinear models.", "math": "CONSTRAINT FORM:\n  g(x) = Σⱼ aⱼxⱼ - b = 0 (equality) or ≤ 0 (inequality)", "see": ["ClpConstraint for the abstract interface", "ClpConstraintQuadratic for quadratic constraints", "ClpSimplexNonlinear for the nonlinear solver"], "has_pass2": true}, "src/ClpFactorization.hpp": {"path": "layer-1/Clp/src/ClpFactorization.hpp", "filename": "ClpFactorization.hpp", "file": "ClpFactorization.hpp", "brief": "Wrapper around CoinFactorization for use within Clp simplex", "algorithm": "LU Factorization with Forrest-Tomlin Updates:\nMaintains factorization B = LU of the basis matrix for simplex operations.\nCore operations:\n- factorize(): Compute B = LU from scratch using Markowitz pivoting\n- FTRAN: Solve Bx = a via Ly = a then Ux = y (forward transformation)\n- BTRAN: Solve B^T x = c via U^T y = c then L^T x = y (backward transformation)\n- replaceColumn(): Update LU when basis changes via Forrest-Tomlin method\n\nForrest-Tomlin update maintains L fixed, updates U with spike column.\nAlternative Product Form of Inverse (PFI) available but less stable.\nRefactorization triggered when fill-in exceeds threshold or numerical error grows.", "math": "Basis matrix B ∈ ℝ^{m×m} where columns are basic column vectors from [A|I].\nLU factorization: PBQ = LU where P,Q are permutation matrices.\nFTRAN solves: d = B^{-1}a_j (pivot column for primal simplex)\nBTRAN solves: π^T = c_B^T B^{-1} (dual prices for reduced cost calculation)\nUpdate: B_new = B_old + (a_j - Be_i)e_i^T for basis change at position i.", "complexity": "Factorization: O(m^2) to O(m^3) depending on sparsity and fill-in.\nFTRAN/BTRAN: O(nnz(L) + nnz(U)) per solve, typically O(m) for sparse problems.\nForrest-Tomlin update: O(m) average, refactorization every 100-200 pivots.", "ref": ["Forrest & Tomlin, \"Updated triangular factors of the basis to maintain\n     sparsity in the product form simplex method\", Mathematical Programming 2 (1972)", "Suhl & Suhl, \"Computing sparse LU factorizations for large-scale LP bases\",\n     ORSA Journal on Computing 2 (1990)\n\nClpFactorization adapts CoinFactorization to the Clp solver context,\nhandling the integration between the simplex algorithm and the LU\nfactorization of the basis matrix. It also supports special cases\nlike network bases (ClpNetworkBasis) and dense factorizations.\n\nKey responsibilities:\n- Factorize the basis matrix given ClpSimplex model and basic variable info\n- Provide FTRAN (forward solve) and BTRAN (backward solve) operations\n- Handle basis updates via replaceColumn (Forrest-Tomlin update)\n- Switch between factorization types (sparse LU, dense, network)"], "see": ["CoinFactorization in CoinUtils for the underlying LU implementation", "ClpNetworkBasis for special network structure handling", "ClpSimplex for how factorization integrates with simplex iterations"], "has_pass2": true}, "src/ClpPEPrimalColumnDantzig.hpp": {"path": "layer-1/Clp/src/ClpPEPrimalColumnDantzig.hpp", "filename": "ClpPEPrimalColumnDantzig.hpp", "file": "ClpPEPrimalColumnDantzig.hpp", "brief": "Positive Edge enhanced Dantzig pricing for primal simplex", "author": "Jeremy Omer, Mehdi Towhidi\n\nCombines classic Dantzig pricing (most negative reduced cost) with\nPositive Edge compatibility checking. Prioritizes compatible columns\nthat can make real progress on degenerate problems.", "algorithm": "Bi-Dimensional Pricing:\nScore(j) = |c̄_j|^(1-ψ) × Compat(j)^ψ\n\nWhere ψ ∈ [0,1] controls priority (default 0.5):\n  ψ = 0: Pure Dantzig (ignore compatibility)\n  ψ = 1: Pure compatibility (ignore reduced cost)\n  ψ = 0.5: Balanced - good default", "math": "DEGENERACY PROBLEM:\nWhen basic variables are at bounds, ratio test gives θ = 0.\nPivot changes basis but x, z unchanged → no progress.\nDegenerate pivots can cycle or stall for many iterations.", "complexity": "O(n) per pricing like Dantzig, but with compatibility\ncheck overhead. Compatibility set updated periodically, not every iter.", "ref": ["Towhidi, Desrosiers, Soumis (2014): \"The positive edge criterion\n     within COIN-OR's CLP\""], "see": ["ClpPESimplex for the compatibility framework", "ClpPrimalColumnDantzig for the base pricing rule", "ClpPEPrimalColumnSteepest for steepest edge variant"], "has_pass2": true}, "src/MyEventHandler.hpp": {"path": "layer-1/Clp/src/MyEventHandler.hpp", "filename": "MyEventHandler.hpp", "file": "MyEventHandler.hpp", "brief": "Example event handler demonstrating callback customization\n\nSample implementation of ClpEventHandler showing how users can\nintercept simplex events for custom processing. Used in unit tests.\n\nTo create your own handler:\n1. Inherit from ClpEventHandler\n2. Override event() method\n3. Return 0 to continue, non-zero to stop\n4. Attach via ClpSimplex::passInEventHandler()\n\nAvailable events (see ClpEventHandler::Event):\n- endOfIteration: After each pivot\n- endOfFactorization: After basis refactorization\n- endOfValuesPass: After crash/values pass\n- node: During B&B (if used with Cbc)\n\nThe model_ pointer gives access to full solver state including\ncurrent solution, basis, and objective value.", "see": ["ClpEventHandler for the event types and interface", "ClpSimplex::passInEventHandler() to install"], "has_pass2": false}, "src/ClpParamUtils.hpp": {"path": "layer-1/Clp/src/ClpParamUtils.hpp", "filename": "ClpParamUtils.hpp", "file": "ClpParamUtils.hpp", "brief": "Parameter handler callback functions for ClpParam", "author": "Lou Hafer\n\nImplements the push/pull functions that transfer values between\nClpParam objects and ClpSimplex solver state. These callbacks are\ninvoked when parameters are modified via the command-line interface.\n\nAction handlers (doXxxParam):\n- doDebugParam(): Enable debug output\n- doExitParam(): Terminate solver\n- doHelpParam(): Display help text\n- doImportParam(): Read MPS/LP file\n- doPrintMaskParam(): Set print filter pattern\n- doSolutionParam(): Write solution file\n- doVersionParam(): Print version info\n\nPush handlers (pushClpXxxParam):\nTransfer parameter values from ClpParam to ClpSimplex:\n- pushClpDblParam(): Double parameters\n- pushClpIntParam(): Integer parameters\n- pushClpKwdParam(): Keyword (enum) parameters\n- pushClpStrParam(): String parameters\n- pushClpBoolParam(): Boolean parameters\n\nSolution I/O:\n- saveSolution()/restoreSolution(): Serialize solution state", "see": ["ClpParam for the parameter definition class", "ClpParameters for the parameter container"], "has_pass2": false}, "src/ClpPrimalColumnSteepest.hpp": {"path": "layer-1/Clp/src/ClpPrimalColumnSteepest.hpp", "filename": "ClpPrimalColumnSteepest.hpp", "file": "ClpPrimalColumnSteepest.hpp", "brief": "Steepest edge and Devex pivot selection for primal simplex", "algorithm": "Steepest Edge and Devex Primal Pivot Selection:\nSelects entering variable (pivot column) by normalizing reduced cost by weight.\nInstead of choosing most negative reduced cost (Dantzig), computes:\n  score_j = (reduced_cost_j)² / weight_j\nSteepest edge: weight_j = ||d_j||² = ||B^{-1}a_j||² (exact column norm)\nDevex: approximate weights, reference framework tracks recent pivots.\n\nModes: 0=exact Devex, 1=full steepest, 2=partial Devex (sparse scan),\n3=auto-switch 0/2, 4=partial Dantzig then switch. Mode 3 is default.\n\nWeight update after pivot: uses FTRAN result d = B^{-1}a_j directly.\nSteepest: weight_k := ||d_k||² computed from tableau column.\nDevex: weight_k := max(weight_k, (d_k/d_p)² * weight_p) for columns in ref set.", "math": "Column weight: γ_j = ||B^{-1}a_j||² = d_j^T d_j where d_j is FTRAN of a_j.\nNormalized ratio: |c̄_j|/√γ_j gives steepest descent direction in primal space.\nDevex approximation: track reference set R, update γ_j ≈ max over iterations.", "complexity": "Steepest: O(m²) per iteration (FTRAN for each candidate column).\nDevex: O(m) per pivot with reference framework. Typically 30-50% fewer\niterations than Dantzig at modest extra cost per iteration.", "ref": ["Forrest & Goldfarb, \"Steepest-edge simplex algorithms for LP\",\n     Mathematical Programming 57 (1992) 341-374", "Harris, \"Pivot selection methods of the Devex LP code\",\n     Mathematical Programming 5 (1973) 1-28\n\nImplements advanced pivot column selection strategies for primal simplex:\n- Steepest Edge: normalizes reduced costs by column norms for faster convergence\n- Devex: approximate steepest edge with cheaper weight updates\n\nThe mode parameter controls behavior:\n- 0: Exact Devex\n- 1: Full steepest edge\n- 2: Partial exact Devex (only scans some nonbasics)\n- 3: Switches between 0 and 2 based on factorization (default)\n- 4: Starts partial Dantzig/Devex, may switch to 0 or 2"], "see": ["\"Implementing the Dantzig-Wolfe decomposition\" by Forrest & Goldfarb", "ClpPrimalColumnPivot for the base interface", "ClpPrimalColumnDantzig for simpler Dantzig rule", "ClpSimplexPrimal for the primal simplex algorithm"], "has_pass2": true}, "src/ClpDummyMatrix.hpp": {"path": "layer-1/Clp/src/ClpDummyMatrix.hpp", "filename": "ClpDummyMatrix.hpp", "file": "ClpDummyMatrix.hpp", "brief": "Placeholder matrix with dimensions but no data\n\nImplements ClpMatrixBase with only dimensions (rows, columns, elements)\nbut no actual matrix data. Used primarily with ClpPdco where the user\nprovides custom matrix-vector products via callbacks.", "algorithm": "Matrix-Free Optimization:\nFor problems where A is too large to store or has special structure\n(e.g., discretizations, FFT-based), represent matrix implicitly.\n\nINTERFACE PATTERN:\n  User provides: matVecMult(mode, x, y)\n    mode=1: y := y + A·x\n    mode=2: x := x + A'·y\n\n  ClpDummyMatrix provides: dimensions for size checking\n    getNumRows(), getNumCols(), getNumElements()\n\nAll matrix operations return empty/zero results:\n  times()/transposeTimes(): No-op (matrix is empty)\n  getElements()/getIndices(): Return NULL\n  unpack(): Produces empty sparse vector\n\nUSE CASES:\n  - PDCO interior point: User implements matrix products in ClpPdcoBase\n  - Large-scale problems: Matrix arises from operator, not explicit data\n  - Testing: Mock matrix object for algorithm development", "see": ["ClpPdco for the interior point method using this", "ClpPdcoBase for user-defined objective/matrix interface", "ClpMatrixBase for the full matrix interface"], "has_pass2": true}, "src/ClpSimplex.hpp": {"path": "layer-1/Clp/src/ClpSimplex.hpp", "filename": "ClpSimplex.hpp", "file": "ClpSimplex.hpp", "brief": "Main simplex solver class - orchestrates primal and dual simplex algorithms", "author": "John Forrest\n\nClpSimplex is the core solver class that implements the simplex method for\nlinear programming. It inherits problem data from ClpModel and adds all the\nalgorithm machinery: basis handling, factorization management, pivot selection,\nand the main iteration loops.\n\nThe actual algorithm implementations are split into derived mix-in classes:\n- ClpSimplexDual: dual simplex algorithm (typically faster, default choice)\n- ClpSimplexPrimal: primal simplex algorithm\n- ClpSimplexOther: auxiliary methods (crossover, parametric, etc.)\n\nKey concepts:\n- Status array: tracks whether each variable is basic, at bound, or free\n- Factorization: LU factors of the basis matrix (via ClpFactorization)\n- Pivot selection: choosing entering/leaving variables (pluggable strategies)", "algorithm": "Simplex Method for Linear Programming:\n  Maintains a basic feasible solution (BFS) at a vertex of the polytope.\n  Each iteration moves along an edge to an adjacent vertex with better objective.\n  - Primal simplex: maintains primal feasibility, achieves dual feasibility\n  - Dual simplex: maintains dual feasibility, achieves primal feasibility\n  Terminates when both primal and dual feasibility achieved (optimality).", "complexity": "O(2^n) worst-case (Klee-Minty), but typically polynomial in practice.\n  Per-iteration cost: O(m²) for basis update + O(mn) for pricing.\n  Iteration count: typically O(m) to O(3m) for practical problems.", "ref": ["Dantzig, G.B. (1963). \"Linear Programming and Extensions\".\n  Princeton University Press. [Original simplex method]", "Forrest, J.J. and Goldfarb, D. (1992). \"Steepest-edge simplex algorithms\n  for linear programming\". Math. Programming 57:341-374. [Steepest edge pricing]\n\n@note The algorithm_ member indicates variant: positive=primal, negative=dual", "Wright, S.J. (1997). \"Primal-Dual Interior-Point Methods\". SIAM."], "see": ["ClpModel for problem data storage", "ClpSimplexDual for dual simplex implementation", "ClpSimplexPrimal for primal simplex implementation", "ClpFactorization for basis factorization", "ClpDualRowPivot, ClpPrimalColumnPivot for pivot selection strategies", "CoinFactorization in CoinUtils for the underlying LU factorization"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the constraints whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints"], "has_pass2": true}, "src/ClpPEPrimalColumnSteepest.hpp": {"path": "layer-1/Clp/src/ClpPEPrimalColumnSteepest.hpp", "filename": "ClpPEPrimalColumnSteepest.hpp", "file": "ClpPEPrimalColumnSteepest.hpp", "brief": "Positive Edge enhanced steepest edge for primal simplex", "author": "Jeremy Omer, Mehdi Towhidi\n\nCombines steepest edge pricing (Forrest-Goldfarb) with Positive Edge\ncompatibility checking. The most effective anti-degeneracy variant\nfor primal simplex.", "algorithm": "Positive Edge Primal Steepest Edge:\n  Enhanced column selection combining steepest edge with compatibility:\n  1. Compute steepest edge scores: |d_j|²/w_j for attractive columns\n  2. Identify compatible columns via ClpPESimplex::isCompatibleCol()\n  3. Apply bi-dimensional pricing: prefer compatibles unless much worse\n  4. Update compatibility set when basic variables change bounds", "math": "Column selection with compatibility weight:\n  Select s = argmax_j { |d_j|²/w_j · (1 + (1-ψ)·c_j) }\n  where c_j = 1 if compatible, 0 otherwise\n  Compatible columns move dual degenerates off zero reduced cost.", "complexity": "Same as ClpPrimalColumnSteepest plus O(n) compatibility check.\n  Reduces degenerate iterations by 20-50% on difficult LPs.", "ref": ["Towhidi & Orban (2014). \"Customizing the solution process of COIN-OR's\n     linear solvers with Python\". Math. Prog. Computation 6:247-282.\n\nModes: 0=exact devex, 1=full steepest, 2=partial exact devex,\n       3=adaptive (switches based on factorization), 4=partial Dantzig start\n\nUses bi-dimensional pricing: candidates scored by steepest edge weight\nand compatibility, weighted by psi parameter."], "see": ["ClpPESimplex for the compatibility framework", "ClpPrimalColumnSteepest for the base pricing rule", "ClpPEPrimalColumnDantzig for Dantzig variant"], "has_pass2": true}, "src/ClpSolve.hpp": {"path": "layer-1/Clp/src/ClpSolve.hpp", "filename": "ClpSolve.hpp", "file": "ClpSolve.hpp", "brief": "Algorithm selection and configuration for ClpSimplex::initialSolve()", "author": "John Forrest\n\nClpSolve is a parameter object that controls how ClpSimplex solves an LP.\nIt specifies the algorithm (dual/primal simplex, barrier), presolve settings,\ncrash procedures, and various solver options.", "algorithm": "Presolve Options:\nControls problem reduction before solving:\n  - Singleton: Remove rows/cols with single nonzero\n  - Doubleton: Substitute variables appearing in 2 rows\n  - Tripleton: Handle 3-row patterns\n  - Forcing: Detect implied bound constraints\n  - ImpliedFree: Find unbounded variables that are implicitly bounded\n  - Dupcol: Remove duplicate columns\n  - Duprow: Remove duplicate rows\n  - Tighten: Strengthen variable bounds\n\nnumberPasses controls how many presolve passes to run.\n\nAlso contains ClpSimplexProgress for detecting cycling during iteration.", "see": ["ClpSimplex::initialSolve() which uses this class", "ClpInterior for barrier/interior point implementation", "Idiot.hpp for the Idiot crash procedure", "ClpPresolve for the presolve implementation"], "has_pass2": true}, "src/AbcWarmStart.hpp": {"path": "layer-1/Clp/src/AbcWarmStart.hpp", "filename": "AbcWarmStart.hpp", "file": "AbcWarmStart.hpp", "brief": "Extended warm start with factorization caching for ABC", "author": "John Forrest (FasterCoin, 2012)", "algorithm": "Extended Warm Start with Cached Factorization\nExtends CoinWarmStartBasis to store additional information for efficient\nrestart of ABC simplex. Beyond basic/nonbasic status, can store:\n\nType encoding:\n- 0: Standard basis (as CoinWarmStartBasis)\n- 1,2: Plus factor order as shorts/ints (high bit = column)\n- 3,4: Plus compact saved factorization\n- +8: Steepest edge weights stored (as floats)\n\nAlso defines AbcWarmStartOrganizer for managing multiple warm starts\nin tree search (branch and bound) scenarios.\n\nKey features:\n- deleteRows()/deleteColumns(): Maintain valid basis after problem changes\n- createBasis0/12/34(): Create warm starts of different types\n- Support for incremental factorization updates", "see": ["CoinWarmStartBasis in CoinUtils for the base class", "AbcSimplex which uses warm starts", "ClpSimplex::getWarmStart() for standard warm start creation"], "has_pass2": true}, "src/Idiot.hpp": {"path": "layer-1/Clp/src/Idiot.hpp", "filename": "Idiot.hpp", "file": "Idiot.hpp", "brief": "Heuristic crash procedure for finding initial LP solutions\n\nThe \"Idiot\" algorithm is a simple heuristic that finds approximate primal\nsolutions quickly. Despite its self-deprecating name (which is copylefted!),\nit serves as an effective \"crash\" procedure to warm-start the simplex method.", "algorithm": "Idiot Crash Heuristic:\n  Iterative relaxation method for approximate LP solutions:\n  1. Start with x = 0 or weighted column averages\n  2. For each iteration:\n     a. Compute constraint violations: v = Ax - b\n     b. For each row i with violation: adjust x_j proportionally\n        Δx_j ∝ a_ij · v_i / (Σ_k a_ik² + μ) weighted by reduced cost\n     c. Project x onto bounds [l, u]\n  3. Reduce penalty μ geometrically each major iteration\n  4. Stop when sufficiently feasible or iteration limit reached", "math": "Weighted adjustment formula:\n  Δx_j = -w_j · Σ_i (a_ij · v_i / row_weight_i)\n  where w_j = 1/(c_j + penalty) prioritizes cheap variables\n  Row weights based on ||a_i||² for normalization.", "complexity": "O(nnz(A)) per iteration, typically 50-200 iterations.\n  Much cheaper than simplex iteration but only approximate.\n  Good crash reduces simplex iterations by 30-70%.", "ref": ["Forrest (1992). Internal IBM documentation on crash heuristics.\n\nThe algorithm works well on problems where the volume algorithm succeeds,\noften finding better primal solutions, though it produces no dual solution.\nIt's particularly useful for:\n- Getting a starting basis before simplex (crash mode)\n- Homogeneous problems with similar constraint structure\n\nKey methods:\n- crash(): Lightweight initialization (2-5 passes typical)\n- solve(): Full approximate solution (50-100 iterations)\n- crossOver(): Transition to simplex for exact solution"], "see": ["ClpSimplex::initialSolve() which may call Idiot automatically", "ClpSimplexPrimal for the simplex algorithm that uses the crash solution"], "has_pass2": true}, "src/AbcNonLinearCost.hpp": {"path": "layer-1/Clp/src/AbcNonLinearCost.hpp", "filename": "AbcNonLinearCost.hpp", "file": "AbcNonLinearCost.hpp", "brief": "Piecewise linear cost and infeasibility tracking for ABC", "author": "John Forrest (FasterCoin, 2012)\n\nManages bound violations and infeasibility costs during ABC primal simplex.\nTracks whether each variable is below lower, feasible, or above upper bound,\nand computes appropriate penalty costs.", "algorithm": "Cost Change Computation:\nchangeInCost(i, α): When x_i changes by θ in direction α,\ncost changes by ±w depending on crossing a bound.", "math": "PIECEWISE LINEAR COST:\n\n       cost\n         │      /\n   w·(l-x)     /  w·(x-u)\n         │    /\n         │   /│\n   ──────┼──/─┼────────→ x\n         l    u\n          \\  /\n           \\/  original cost c·x (feasible region)\n\nTotal cost = c'x + w·Σ(violations)", "see": ["AbcSimplexPrimal which uses this for infeasibility tracking", "ClpNonLinearCost for the standard (non-ABC) implementation"], "has_pass2": true}, "src/ClpInterior.hpp": {"path": "layer-1/Clp/src/ClpInterior.hpp", "filename": "ClpInterior.hpp", "file": "ClpInterior.hpp", "brief": "Interior point (barrier) method for LP", "author": "John Tomlin (PDCO), John Forrest (predictor-corrector)", "algorithm": "Interior Point Method (Barrier Method):\nSolves LP by following central path through interior of feasible region.\nMain variant is Mehrotra predictor-corrector:\n1. Affine scaling direction: solve Newton system for complementarity\n2. Centering direction: correct toward central path with adaptive μ\n3. Combined step: predictor + corrector with step length limiting\n4. Update (x, y, s) and reduce barrier parameter μ\n\nPer iteration solves normal equations: (ADA^T)Δy = rhs\nwhere D is diagonal scaling matrix from current iterate.\nCholesky factorization of ADA^T is computational bottleneck.\n\nConverges in O(√n log(1/ε)) iterations (polynomial complexity).\nEach iteration O(m²n + m³) for forming/factoring normal equations.", "math": "Barrier problem: min c^Tx - μ∑log(x_j-l_j) - μ∑log(u_j-x_j)\nKKT conditions: Ax=b, A^Ty+s=c, XSe=μe (complementarity)\nNormal equations: (ADA^T)Δy = A(D*r_c - X^{-1}r_xs) + r_b\nwhere D = X/S diagonal, r_b, r_c, r_xs are residuals.", "complexity": "O(√n log(1/ε)) iterations for ε-optimality.\nPer iteration: O(nnz(A)m) to form ADA^T, O(m³) for dense Cholesky,\nor O(nnz(L)²) for sparse Cholesky. Total typically O(n^{1.5}) to O(n³).", "ref": ["Mehrotra, \"On the implementation of a primal-dual interior point method\",\n     SIAM Journal on Optimization 2 (1992) 575-601", "Andersen & Andersen, \"The MOSEK interior point optimizer for LP\",\n     High Performance Optimization, Kluwer (2000)\n\nImplements interior point methods as an alternative to simplex for LP.\nInterior point follows a central path through the interior of the feasible\nregion, converging to optimality. Often faster than simplex for very large\nor dense problems.\n\nTwo main variants supported:\n- Predictor-Corrector: Standard Mehrotra-style algorithm\n- PDCO: Primal-Dual Barrier method with regularization\n\nRequires Cholesky factorization for the normal equations A*D*A'.\nMultiple Cholesky backends available (dense, MUMPS, TAUCS, etc.)"], "see": ["ClpPredictorCorrector for the predictor-corrector implementation", "ClpPdco for PDCO variant", "ClpCholeskyBase for Cholesky factorization interface", "ClpSimplex for the alternative simplex method"], "has_pass2": true}, "src/ClpCholeskyDense.hpp": {"path": "layer-1/Clp/src/ClpCholeskyDense.hpp", "filename": "ClpCholeskyDense.hpp", "file": "ClpCholeskyDense.hpp", "brief": "Dense Cholesky factorization for interior point methods\n\nImplements Cholesky factorization when A*D*A' becomes effectively dense.\nUses blocked recursive algorithms for cache efficiency and supports\nparallel execution via ClpCholeskySpawn.", "algorithm": "Blocked Dense Cholesky Factorization:\n  Recursive blocked algorithm for cache-efficient A = LL':\n  1. Partition A into blocks: [A₁₁ A₁₂; A₂₁ A₂₂]\n  2. Factor diagonal block: L₁₁ = chol(A₁₁)\n  3. Solve for off-diagonal: L₂₁ = A₂₁ · L₁₁⁻ᵀ\n  4. Update Schur complement: A₂₂' = A₂₂ - L₂₁ · L₂₁'\n  5. Recurse on A₂₂'\n  Block size chosen for L1/L2 cache efficiency (typically 64-256).", "math": "Cholesky factorization of normal equations:\n  For LP: factor M = ADA' where D = X²S⁻² (primal-dual scaling)\n  Solve: M·Δy = r via L·L'·Δy = r\n  Forward: L·z = r, Backward: L'·Δy = z", "complexity": "Factorization: O(n³/3) flops, O(n³/(3p)) with p processors.\n  Solve: O(n²) per right-hand side.\n  Memory: O(n²) for dense storage.", "ref": ["Golub & Van Loan (2013). \"Matrix Computations\", 4th ed., Ch. 4.2.\n\nThis is used as a fallback from sparse Cholesky when fill-in is excessive,\nor for problems with inherently dense normal equations.\n\nKey methods:\n- factorize(): Dense LL' factorization with row dropping for singularity\n- solve(): Forward/backward triangular solves (solveF1/B1 variants)\n\nAlso exports C functions for recursive blocked factorization that can be\nparallelized (ClpCholeskyCfactor, etc.)."], "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses Cholesky for normal equations"], "has_pass2": true}, "src/CoinAbcCommon.hpp": {"path": "layer-1/Clp/src/CoinAbcCommon.hpp", "filename": "CoinAbcCommon.hpp", "file": "CoinAbcCommon.hpp", "brief": "Common definitions for ABC (A Better Coin) optimized simplex", "algorithm": "ABC SIMD Foundation (Types, Tolerances, IEEE Zero Tests)\nCore type definitions and macros for the FasterCoin 2012 SIMD-optimized\nsimplex implementation. Provides:\n- Type aliases: CoinSimplexDouble, CoinSimplexInt\n- Compiler-specific forced inlining (ABC_INLINE)\n- Parallelism control: ABC_PARALLEL (0=none, 1=pthreads, 2=Cilk)\n- LAPACK integration options (ABC_USE_LAPACK, ABC_DENSE_CODE)\n- Low-level zero testing macros using IEEE 754 bit patterns\n- AbcTolerancesEtc class for algorithm tolerance parameters", "see": ["AbcSimplex for the main ABC simplex implementation", "CoinAbcFactorization for the optimized factorization"], "has_pass2": true}, "src/ClpCholeskyTaucs.hpp": {"path": "layer-1/Clp/src/ClpCholeskyTaucs.hpp", "filename": "ClpCholeskyTaucs.hpp", "file": "ClpCholeskyTaucs.hpp", "brief": "TAUCS sparse solver interface for Cholesky factorization\n\nWraps Sivan Toledo's TAUCS library for Cholesky factorization of normal\nequations in interior point methods.", "algorithm": "TAUCS Left-Looking Supernodal Cholesky:\n  Left-looking factorization with supernodal blocking:\n  1. Ordering: AMD, METIS, or GENMMD fill-reducing permutation\n  2. Symbolic: Build elimination tree, identify supernodes\n  3. Numeric left-looking: For each supernode j:\n     - Assemble: Gather column j from A and previous L updates\n     - Factor: Dense Cholesky on diagonal block\n     - Scatter: Distribute updates to later columns\n  4. Store L in CCS format with supernode structure", "math": "Left-looking vs right-looking:\n  Left-looking: L_j computed from A_j and L_{<j} (memory efficient)\n  Right-looking: Updates distributed immediately (more parallelism)\n  TAUCS uses left-looking for lower memory footprint", "complexity": "O(nnz(L)·f̄) where f̄ = average supernode front size\n  Memory: O(nnz(L)) for factor storage\n  Well-suited for medium-size interior point problems", "ref": ["Toledo (2003). \"TAUCS: A Library of Sparse Linear Solvers\".\n     Tel-Aviv University. http://www.tau.ac.il/~stoledo/taucs/\n\nTAUCS provides:\n- Left-looking supernodal factorization\n- Multiple ordering methods (AMD, METIS, GENMMD)\n- Research-quality implementation with educational focus\n\nNote: Requires modifications to taucs.h for C++ linkage and LAPACK\ndpotf2.f for singularity handling (see class documentation)."], "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization", "http://www.tau.ac.il/~stoledo/taucs/"], "has_pass2": true}, "src/ClpDualRowDantzig.hpp": {"path": "layer-1/Clp/src/ClpDualRowDantzig.hpp", "filename": "ClpDualRowDantzig.hpp", "file": "ClpDualRowDantzig.hpp", "brief": "Dantzig's rule for dual simplex pivot selection\n\nImplements the simplest pivot row selection: choose the basic variable\nwith the largest primal infeasibility. Simple and fast per iteration,\nbut may require many more iterations than steepest edge on degenerate\nor difficult problems.\n\nThis is Dantzig's original 1947 rule applied to dual simplex.\nUse ClpDualRowSteepest for better performance on most problems.", "algorithm": "Dantzig's Rule (Dual Simplex Leaving Variable Selection):\nScans all basic variables and selects the one most infeasible w.r.t. bounds.\nFor variable x_B[i] with bounds [l_i, u_i]:\n- If x_B[i] < l_i: infeasibility = l_i - x_B[i] (want to increase)\n- If x_B[i] > u_i: infeasibility = x_B[i] - u_i (want to decrease)\nChoose the row with maximum infeasibility magnitude.\nNo weight storage needed - stateless selection on current solution.", "math": "In dual simplex, select leaving row i* = argmax{|x_B[i] - bound_i|}\nover all basic variables violating their bounds.\nThe direction of movement (toward lower or upper bound) determines\nwhich reduced costs are candidates for the entering variable.\nIf all basic variables feasible, current solution is optimal.", "complexity": "O(m) per iteration to scan all basic variables.\nSame worst-case issues as primal Dantzig (exponential on Klee-Minty\nvariants). Dual steepest edge typically 2-3x fewer iterations.", "ref": ["Lemke, C.E. (1954). \"The dual method of solving the linear programming\n  problem\". Naval Research Logistics Quarterly. [Early dual simplex]", "Dantzig, G.B. (1963). \"Linear Programming and Extensions\".\n  Princeton University Press. [Comprehensive treatment]"], "see": ["ClpDualRowPivot for the base interface", "ClpDualRowSteepest for recommended steepest edge variant", "ClpSimplexDual for the dual simplex algorithm"], "has_pass2": true}, "src/ClpMatrixBase.hpp": {"path": "layer-1/Clp/src/ClpMatrixBase.hpp", "filename": "ClpMatrixBase.hpp", "file": "ClpMatrixBase.hpp", "brief": "Abstract base class for constraint matrices in Clp\n\nDefines the interface that all matrix types must implement for use with\nClp algorithms. The abstraction allows specialized matrix formats that\nexploit structure (network, GUB, ±1 matrices) while providing a uniform\ninterface to the simplex solver.", "algorithm": "Partial Pricing Support:\nFor very large problems, pricing all columns is expensive.\nMatrix provides partial pricing interface:\n  - partialPricing(start, end): Price subset of columns\n  - startFraction_/endFraction_: Current search window\n  - savedBestDj_: Best reduced cost found so far", "complexity": "Matrix-vector products: O(nnz) for general sparse,\n  O(n) for network, O(nnz) but faster constant for ±1.\n\nKey virtual methods:\n- times(), transposeTimes(): Matrix-vector multiplication (y = A*x, y = A'*x)\n- subsetTransposeTimes(): Partial BTRAN for steepest edge pricing\n- getPackedMatrix(): Access underlying CoinPackedMatrix\n- scale(): Apply row/column scaling", "see": ["ClpPackedMatrix for standard sparse matrix (wraps CoinPackedMatrix)", "ClpNetworkMatrix for pure network structure", "ClpPlusMinusOneMatrix for ±1 coefficient matrices", "ClpGubMatrix for generalized upper bound structure", "CoinPackedMatrix in CoinUtils for sparse storage format"], "has_pass2": true}, "src/ClpCholeskyWssmpKKT.hpp": {"path": "layer-1/Clp/src/ClpCholeskyWssmpKKT.hpp", "filename": "ClpCholeskyWssmpKKT.hpp", "file": "ClpCholeskyWssmpKKT.hpp", "brief": "WSSMP solver for KKT system (augmented system) formulation\n\nVariant of ClpCholeskyWssmp that solves the KKT/augmented system directly\ninstead of forming and factoring the normal equations A*D*A'.", "algorithm": "Indefinite Factorization:\nKKT matrix is symmetric indefinite → use LDL' with pivoting.\nWSSMP handles this with Bunch-Kaufman pivoting.", "math": "NORMAL EQUATIONS (Alternative):\n  (A·D⁻¹·A')·Δy = r₂ - A·D⁻¹·r₁", "complexity": "KKT: O((m+n)^α) where α depends on sparsity\nOften better constant than forming A·D·A' explicitly.", "see": ["ClpCholeskyWssmp for the normal equations variant", "ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization"], "has_pass2": true}, "src/ClpCholeskyMumps.hpp": {"path": "layer-1/Clp/src/ClpCholeskyMumps.hpp", "filename": "ClpCholeskyMumps.hpp", "file": "ClpCholeskyMumps.hpp", "brief": "MUMPS sparse direct solver interface for Cholesky factorization\n\nWraps the MUMPS (MUltifrontal Massively Parallel sparse direct Solver)\nlibrary for Cholesky factorization of normal equations in interior point.", "algorithm": "MUMPS Multifrontal Sparse Cholesky:\n  Parallel sparse direct factorization A = LL':\n  1. Analyze phase: Fill-reducing ordering (AMD/METIS/SCOTCH)\n  2. Build elimination tree and allocate frontal matrices\n  3. Factor via multifrontal method:\n     - Assemble contributions into frontal matrix F_k\n     - Factor dense block: F_k = L_k · L_k'\n     - Form update matrix (Schur complement) for parent\n  4. Solve via forward/backward substitution through tree", "math": "Multifrontal factorization:\n  Elimination tree defines partial ordering of pivots.\n  Frontal matrix F_k contains all fill-in for subtree rooted at k.\n  Dense BLAS-3 operations on fronts for efficiency.", "complexity": "O(n·f²) where f = max frontal matrix size.\n  Parallel: O(n·f²/p) with p MPI processes.\n  Memory: O(n·f) for frontal storage.", "ref": ["Amestoy et al. (2001). \"A Fully Asynchronous Multifrontal Solver Using\n     Distributed Dynamic Scheduling\". SIAM J. Matrix Anal. Appl. 23:15-41.\n\nMUMPS provides:\n- Multifrontal factorization with supernodes\n- Distributed memory parallelism (MPI)\n- Dense column handling for improved performance\n\nRequires MUMPS library to be installed and linked. The DMUMPS_STRUC_C\nstructure is forward-declared as void for compilation without MUMPS headers."], "see": ["ClpCholeskyBase for the abstract interface", "ClpInterior which uses this factorization", "http://mumps.enseeiht.fr/ for MUMPS documentation"], "has_pass2": true}, "src/AbcCommon.hpp": {"path": "layer-1/Clp/src/AbcCommon.hpp", "filename": "AbcCommon.hpp", "file": "AbcCommon.hpp", "brief": "Configuration macros for ABC (A Better Clp) build modes", "author": "John Forrest (FasterCoin, 2012)", "algorithm": "ABC Build Configuration (Serial/Parallel/Cilk Modes)\nControls how the ABC optimized simplex code is built and integrated.\n\nCLP_HAS_ABC values:\n- 0: ABC disabled (use standard ClpSimplex only)\n- 1: Serial ABC, standalone (no inheritance into ClpSimplex)\n- 2: Serial ABC with inheritance (ClpSimplex can delegate to ABC)\n- 3: Cilk parallel ABC, standalone\n- 4: Cilk parallel ABC with inheritance\n\nABC_PARALLEL values:\n- 0: No parallelization\n- 1: Use pthreads for parallelization\n- 2: Use Intel Cilk Plus for parallelization\n\nABC_INHERIT: When defined, ClpSimplex::initialSolve() can automatically\nuse AbcSimplex when beneficial.", "see": ["AbcSimplex for the optimized simplex implementation", "ClpSimplex for the standard implementation"], "has_pass2": true}, "src/ClpConstraint.hpp": {"path": "layer-1/Clp/src/ClpConstraint.hpp", "filename": "ClpConstraint.hpp", "file": "ClpConstraint.hpp", "brief": "Abstract base class for nonlinear constraints\n\nDefines the interface for general (potentially nonlinear) constraints\nused in nonlinear programming extensions. The standard LP constraints\n(Ax ≤ b) are handled directly by ClpModel; this class is for more\ngeneral constraint forms g(x) ≤ 0.", "algorithm": "Constraint Linearization (for SQP/LP-based NLP):\n  At current point x̄, linearize g(x) ≈ g(x̄) + ∇g(x̄)'(x - x̄)\n  Constraint g(x) ≤ 0 becomes: ∇g(x̄)'x ≤ -g(x̄) + ∇g(x̄)'x̄\n\nCACHING:\n  - lastGradient_: cached gradient from last evaluation\n  - functionValue_: cached function value\n  - refresh parameter controls whether to recompute\n\nKey methods that derived classes must implement:\n- gradient(): Compute constraint gradient and function value\n- markNonlinear(): Identify which variables appear nonlinearly\n- markNonzero(): Identify sparsity pattern of gradient\n\nThe type_ member indicates constraint type: 0=linear, 1=nonlinear.\nRow number -1 indicates this is an objective function, not a constraint.", "math": "Gradient computation:\n  For constraint g(x) ≤ 0:\n    - functionValue() returns g(x) at current point\n    - gradient() returns ∇g(x) = [∂g/∂x₁, ..., ∂g/∂xₙ]\n\nSPARSITY PATTERN:\n  - markNonzero(): identifies which variables have non-zero gradient entries\n  - markNonlinear(): identifies which variables appear nonlinearly", "see": ["ClpObjective for objective function handling (similar interface)", "ClpModel for standard linear constraint handling"], "has_pass2": true}, "src/ClpPackedMatrix.hpp": {"path": "layer-1/Clp/src/ClpPackedMatrix.hpp", "filename": "ClpPackedMatrix.hpp", "file": "ClpPackedMatrix.hpp", "brief": "Standard sparse matrix implementation for Clp (wraps CoinPackedMatrix)\n\nThe default matrix type for Clp, implementing ClpMatrixBase using\nCoinPackedMatrix for sparse storage. This is appropriate for general\nsparse LP matrices without special structure.", "algorithm": "Scaling Integration:\nRow/column scaling applied on-the-fly in times()/transposeTimes():\n  scaled_A[i,j] = rowScale[i] * A[i,j] * columnScale[j]\nAvoids storing scaled copy.", "see": ["ClpMatrixBase for the abstract interface", "CoinPackedMatrix in CoinUtils for underlying storage", "ClpNetworkMatrix for network-structured problems", "ClpPlusMinusOneMatrix for ±1 matrices"], "has_pass2": true}, "src/ClpPdcoBase.hpp": {"path": "layer-1/Clp/src/ClpPdcoBase.hpp", "filename": "ClpPdcoBase.hpp", "file": "ClpPdcoBase.hpp", "brief": "Abstract base class for PDCO problem customization\n\nStrategy pattern interface for defining custom convex objectives in PDCO.", "algorithm": "Preconditioning:\n  matPrecon(delta, x, y): Apply preconditioner\n  Typically diagonal scaling based on (A·D²·A' + δ²I)\n\nREGULARIZATION PARAMETERS:\n  d1 (primal): Adds ||d₁·x||²/2 to objective, ensures D² > 0\n  d2 (dual): Adds δ²I to (2,2) block of augmented system", "math": "Effect on KKT system:\n  Without regularization: A·(H⁻¹)·A' may be singular if H has zeros\n  With regularization: A·(H + d₁²I)⁻¹·A' + d₂²I always invertible\n\nCommon objective functions that can be implemented:\n- Entropy: φ(x) = Σ xᵢ·log(xᵢ)  → H = diag(1/xᵢ)\n- Barrier: φ(x) = -Σ log(xᵢ)    → H = diag(1/xᵢ²)\n- Quadratic: φ(x) = x'Qx/2      → H = Q (if diagonal)\n- Huber: φ(x) = Σ huber(xᵢ)     → H = piecewise", "see": ["ClpPdco which uses this interface", "ClpLsqr for the iterative solver"], "has_pass2": true}, "src/ClpPresolve.hpp": {"path": "layer-1/Clp/src/ClpPresolve.hpp", "filename": "ClpPresolve.hpp", "file": "ClpPresolve.hpp", "brief": "Clp interface to CoinPresolve for LP preprocessing", "algorithm": "LP Presolve (Preprocessing):\nApplies reversible transformations to simplify LP before solving:\n- Singleton rows/columns: substitute and eliminate\n- Doubleton: eliminate free column in 2-element row\n- Forcing/dominated constraints: fix variables\n- Duplicate rows/columns: merge or eliminate\n- Implied free: detect columns with implied bounds from constraints\n- Bound tightening: propagate bounds through constraints\n\nTransformations saved as list of CoinPresolveAction objects.\nAfter solving presolved model, postsolve applies actions in reverse\nto recover solution to original model.\n\nMultiple passes until no more reductions. Each transformation must\npreserve optimal solution recovery.", "math": "Presolve operations:\nSingleton row a_j x_j = b: fix x_j = b/a_j, eliminate row\nDoubleton col in a_i x_i + a_j x_j = b with x_j free: x_j = (b - a_i x_i)/a_j\nForcing: if all coeffs same sign and bound known, may fix variables\nImplied free: if x_j bounded by constraints, can treat as free\nBound tightening: l_x = max{l_j : a_ij > 0} helps detect fixed vars", "complexity": "Each pass O(nnz) for most transformations. Total typically\nO(passes × nnz) with 3-10 passes common. Can reduce problem size 50-90%.", "ref": ["Andersen & Andersen, \"Presolving in LP\", Mathematical Programming 71 (1995)", "Gondzio, \"Presolve analysis of LP problems prior to IPM\",\n     Optimization Methods and Software 10 (1997)\n\nWraps the CoinPresolve framework for use with Clp models. Presolve\nsimplifies an LP before solving by applying reversible transformations:\nremoving fixed variables, empty rows/columns, redundant constraints, etc.\n\nThe typical workflow is:\n1. presolvedModel() - create simplified model\n2. Solve the presolved model\n3. postsolve() - recover solution to original model"], "see": ["CoinPresolveMatrix in CoinUtils for the transformation framework", "ClpSimplex::initialSolve() which can invoke presolve automatically"], "has_pass2": true}, "src/ClpObjective.hpp": {"path": "layer-1/Clp/src/ClpObjective.hpp", "filename": "ClpObjective.hpp", "file": "ClpObjective.hpp", "brief": "Abstract base class for objective functions", "algorithm": "Objective Function Interface (Gradient, Reduced Gradient, Step Length)\nDefines the interface for objective functions in Clp. Derived classes\nimplement specific objective types (linear, quadratic, etc.).\n\nKey methods that derived classes must implement:\n- gradient(): Returns objective gradient at current solution\n- reducedGradient(): Gradient accounting for basic/nonbasic structure\n- stepLength(): Optimal step size in a given direction\n- objectiveValue(): Evaluate objective at a point\n\nThe type_ member indicates the objective type: 1=linear, 2=quadratic.", "see": ["ClpLinearObjective for standard LP objective (c'x)", "ClpQuadraticObjective for QP objective (x'Qx/2 + c'x)", "ClpModel for how objectives are used in models"], "has_pass2": true}, "src/ClpPlusMinusOneMatrix.hpp": {"path": "layer-1/Clp/src/ClpPlusMinusOneMatrix.hpp", "filename": "ClpPlusMinusOneMatrix.hpp", "file": "ClpPlusMinusOneMatrix.hpp", "brief": "Specialized matrix where all nonzeros are +1 or -1\n\nEfficient storage for constraint matrices where every coefficient is either\n+1 or -1. Common in set partitioning, covering, and assignment problems.\nNo element values are stored - only row indices, with separate start arrays\nfor positive and negative entries in each column.", "algorithm": "Plus-Minus-One Matrix Operations:\n  Exploits binary coefficient structure for multiplication-free operations:\n  1. **Storage:** Only row indices stored, no element values\n     - startPositive_[j] to startNegative_[j]-1: rows with +1\n     - startNegative_[j] to startPositive_[j+1]-1: rows with -1\n  2. **Column j unpacking:** Direct index lookup, no multiplication\n  3. **Matrix-vector y = Ax:**\n     For each column j with x_j ≠ 0:\n       For i in +1 region: y[i] += x_j\n       For i in -1 region: y[i] -= x_j\n  4. **Transpose x = A'y:** Similar, accumulate ±y[i] into x[j]", "math": "Coefficient structure:\n  A_ij ∈ {-1, 0, +1} for all entries\n  No floating-point multiplications needed in SpMV\n  Perfect for set partitioning: Σ_j x_j = 1 (each row)\n  And covering: Σ_j x_j ≥ 1 (each row)", "complexity": "Storage: O(nnz) integers vs O(2·nnz) for general sparse\n  Matrix-vector: O(nnz) additions only (no multiplications)\n  ~2x faster than general sparse for these problem classes\n\nStorage: For each column, startPositive_[j] to startNegative_[j]-1 are +1s,\nand startNegative_[j] to startPositive_[j+1]-1 are -1s.\n\nAlso defines ClpPoolMatrix (when CLP_POOL_MATRIX is set) for matrices with\nfew distinct coefficient values, using a pool of unique elements.", "see": ["ClpMatrixBase for the abstract matrix interface", "ClpNetworkMatrix for pure network structure (+1/-1 with 2 per column)", "ClpPackedMatrix for general sparse storage"], "has_pass2": true}, "src/ClpHelperFunctions.hpp": {"path": "layer-1/Clp/src/ClpHelperFunctions.hpp", "filename": "ClpHelperFunctions.hpp", "file": "ClpHelperFunctions.hpp", "brief": "BLAS-1 style dense vector operations for Clp", "algorithm": "Dense BLAS-1 Operations (AXPY, Dot, Norms)\nNon-templated dense array operations optimized for LP use. These exist\nseparately from CoinDenseVector to enable architecture-specific optimization\nwithout template overhead (per JJF's design note).\n\nBasic operations (double and CoinWorkDouble variants):\n- maximumAbsElement(): L-infinity norm (max |x_i|)\n- setElements(): Fill array with constant\n- multiplyAdd(): y = a1*x + a2*y (AXPY variant)\n- innerProduct(): x'y dot product\n- getNorms(): Simultaneous L1 and L2 norm computation\n\nExtended precision support (COIN_LONG_WORK):\nWhen enabled, provides long double versions for extra precision in\nnumerically sensitive computations (e.g., basis factorization).\n\nPDCO helper functions (when ClpPdco_H defined):\n- pdxxxmerit(): Merit function for Newton's method\n- pdxxxresid1(): Primal-dual residuals\n- pdxxxresid2(): Complementarity residuals\n- pdxxxstep(): Line search step length\n\nClpTraceDebug: Debug assertion macro (active when NDEBUG not defined).", "see": ["CoinHelperFunctions for general utility functions", "ClpPdco for the PDCO interior point method"], "has_pass2": true}, "src/ClpQuadraticObjective.hpp": {"path": "layer-1/Clp/src/ClpQuadraticObjective.hpp", "filename": "ClpQuadraticObjective.hpp", "file": "ClpQuadraticObjective.hpp", "brief": "Quadratic objective function for convex QP (x'Qx/2 + c'x)\n\nImplements convex quadratic objectives for quadratic programming.\nThe quadratic term is stored as a CoinPackedMatrix Q, supporting\nboth full symmetric and half (lower triangular) storage.", "algorithm": "Line Search for QP:\nGiven direction Δx, find θ* = argmin_{θ≥0} f(x + θΔx):\n  f(x + θΔx) = f(x) + θ·∇f(x)'Δx + (θ²/2)·Δx'Q·Δx\n\nIf Δx'QΔx > 0 (descent in strictly convex direction):\n  θ* = -∇f(x)'Δx / (Δx'QΔx)\nElse (linear along Δx):\n  θ* = maximumTheta (go to bound)\n\nMATRIX STORAGE:\n  fullMatrix_ = false: Lower triangular only (Q_ij stored for i ≥ j)\n  fullMatrix_ = true: Full symmetric (both Q_ij and Q_ji stored)", "math": "PROBLEM FORMULATION:\n  minimize    (1/2)x'Qx + c'x\n  subject to  Ax = b, l ≤ x ≤ u\n\nwhere Q ∈ ℝⁿˣⁿ is symmetric positive semi-definite (PSD).\nQ PSD ⟺ x'Qx ≥ 0 for all x ⟺ all eigenvalues ≥ 0.", "complexity": "Gradient evaluation: O(nnz(Q)) per iteration\n  Line search: O(nnz(Q)) for computing Δx'QΔx", "see": ["ClpObjective for the abstract objective interface", "ClpLinearObjective for linear-only objectives", "ClpSimplexNonlinear for QP simplex variants", "ClpInterior for interior point QP solving"], "has_pass2": true}, "src/ClpEventHandler.hpp": {"path": "layer-1/Clp/src/ClpEventHandler.hpp", "filename": "ClpEventHandler.hpp", "file": "ClpEventHandler.hpp", "brief": "Callback interface for handling solver events during optimization\n\nProvides a mechanism for user code to receive callbacks during the solve\nprocess. Users derive from ClpEventHandler and override event() to handle\nevents like end of iteration, factorization, or presolve stages.", "algorithm": "Disaster Recovery (ClpDisasterHandler):\nFor numerical difficulties that would cause simplex to abort:\n  intoSimplex(): Called at start\n  check(): Returns true if disaster detected\n  saveInfo(): Preserve state for retry\n  typeOfDisaster(): 0=can fix, 1=must abort\n\nCommon uses:\n- Handling Ctrl-C to gracefully stop optimization\n- Logging progress or updating a GUI\n- Implementing custom termination criteria\n- Monitoring solution quality during solve\n- Recovery from numerical instability", "see": ["ClpSimplex::setEventHandler() to install a handler", "ClpModel::secondaryStatus() for event-based status codes"], "has_pass2": true}, "src/ClpPredictorCorrector.hpp": {"path": "layer-1/Clp/src/ClpPredictorCorrector.hpp", "filename": "ClpPredictorCorrector.hpp", "file": "ClpPredictorCorrector.hpp", "brief": "Mehrotra's predictor-corrector interior point algorithm", "author": "John Forrest\n\nImplements the primal-dual interior point method for LP/QP using\nMehrotra's predictor-corrector with Gondzio's multiple centrality corrections.", "algorithm": "Gondzio's Multiple Centrality Corrections:\nAfter corrector, additional corrections keep iterates well-centered:\n  - If any (x+Δx)(s+Δs) too small: push toward center\n  - If any (x+Δx)(s+Δs) too large: no correction needed\n  - Typically 1-3 corrections improve robustness", "math": "KKT CONDITIONS for LP (primal min c'x s.t. Ax=b, x≥0):\n  Primal feasibility:   Ax = b\n  Dual feasibility:     A'y + s = c\n  Complementarity:      X·S·e = 0 (component-wise x_i·s_i = 0)\n  Non-negativity:       x ≥ 0, s ≥ 0", "complexity": "Per iteration: O(m²n + m³) dominated by Cholesky factorization\n  of m×m normal equations (m=rows, n=cols).\n  Total iterations: O(√n·log(1/ε)) for ε-optimality (polynomial).", "ref": ["Mehrotra (1992). \"On the implementation of a primal-dual interior\n  point method\". SIAM J. Optimization 2(4):575-601.", "Gondzio (1996). \"Multiple centrality corrections in a primal-dual\n  method for LP\". Comput. Optim. Appl. 6:137-156.", "Wright (1997). \"Primal-Dual Interior-Point Methods\". SIAM.\n\nKey methods:\n- solve(): Main predictor-corrector iteration loop\n- findStepLength(): Compute max step maintaining positivity\n- complementarityGap(): Measure of optimality (should → 0)"], "see": ["ClpInterior for the base interior point class", "ClpCholeskyBase for the linear system solver", "ClpPdco for PDCO alternative algorithm"], "has_pass2": true}, "src/ClpSimplexPrimal.hpp": {"path": "layer-1/Clp/src/ClpSimplexPrimal.hpp", "filename": "ClpSimplexPrimal.hpp", "file": "ClpSimplexPrimal.hpp", "brief": "Primal simplex algorithm implementation", "author": "John Forrest", "algorithm": "Primal Simplex Method:\nSolves LP by maintaining primal feasibility (variables within bounds)\nwhile iterating toward dual feasibility (optimality). Each iteration:\n1. Choose entering variable (pivot column) - with negative reduced cost\n2. Compute pivot column of tableau: d = B^{-1} a_j (via FTRAN)\n3. Choose leaving variable (pivot row) - via ratio test on bounds\n4. Update basis: swap entering/leaving variables\n5. Update primal solution and reduced costs\n\nUses single-phase approach with infeasibilityCost_ weighting for handling\ninfeasible starting points. Steepest edge or Dantzig strategies for pivot\nselection. Anti-degeneracy via cost perturbation (OSL heritage).\n\nThree nested loops: outer handles fake bounds, middle handles refactorization,\ninner performs pivots until optimality or unboundedness detected.", "math": "min c^T x s.t. Ax=b, l≤x≤u\nBasic solution: x_B = B^{-1}b - B^{-1}N x_N, with x_N at bounds.\nReduced cost: s_j = c_j - c_B^T B^{-1} a_j (computed via BTRAN then dot product)\nRatio test: θ* = min{(x_B[i]-l_i)/d_i, (u_i-x_B[i])/(-d_i)} for feasible step.\nPrimal feasible when l ≤ x ≤ u. Optimal when also s_j ≥ 0 for vars at lower bound.", "complexity": "O(m^2 n) per iteration typical (dominated by FTRAN/BTRAN).\nIteration count varies widely. Often slower than dual simplex but useful\nwhen starting from feasible solution or for problems with few constraints.", "ref": ["Dantzig, \"Linear Programming and Extensions\", Princeton (1963)", "Forrest & Goldfarb, \"Steepest-edge simplex algorithms for LP\",\n     Mathematical Programming 57 (1992) 341-374\n\nImplements the primal simplex method for LP. This is a \"mix-in\" class that\ninherits from ClpSimplex but adds no data - ClpSimplex objects are cast\nto this type when running primal simplex.\n\nThe primal simplex maintains primal feasibility (variables within bounds)\nwhile iterating toward dual feasibility (optimality). Useful when starting\nfrom a feasible solution or when dual simplex struggles.\n\nKey algorithmic features:\n- Single-phase approach with infeasibilityCost_ weighting\n- Explicit bounds on reduced costs for feasibility handling\n- Sparse data structures exploiting problem sparsity\n- Steepest edge or Dantzig pivot selection for entering variable\n- Anti-degeneracy via cost perturbation\n- Supports nonlinear costs (though not heavily tested)"], "see": ["ClpSimplex for the base simplex class", "ClpSimplexDual for dual simplex variant (often faster)", "ClpPrimalColumnPivot for pivot column selection strategies", "ClpPrimalColumnSteepest, ClpPrimalColumnDantzig for specific strategies"], "has_pass2": true}, "src/ClpPdco.hpp": {"path": "layer-1/Clp/src/ClpPdco.hpp", "filename": "ClpPdco.hpp", "file": "ClpPdco.hpp", "brief": "PDCO (Primal-Dual interior point for Convex Objectives) algorithm", "author": "John Tomlin\n\nImplements the PDCO algorithm for convex optimization problems, an\nalternative to Mehrotra's predictor-corrector method.", "algorithm": "Preconditioning:\nDiagonal preconditioner improves LSQR convergence:\n  M = diag(A·D²·A' + d₂²I)^{-1/2}", "math": "Matrix-Vector Products:\n  mode=1: y = y + A·x   (multiply by A)\n  mode=2: x = x + A'·y  (multiply by A transpose)", "complexity": "LSQR per iteration: O(nnz(A)) for sparse A.\n  Number of LSQR iterations depends on condition number.\n  Total: O(IPM_iters × LSQR_iters × nnz(A))\n\nAdvantages over Mehrotra predictor-corrector:\n- Better for ill-conditioned normal equations\n- Handles dense columns without fill-in\n- Natural for separable convex objectives\n\nThis is a mix-in class - ClpInterior objects are cast to this type at\nalgorithm time. No additional data is stored.", "ref": ["Saunders (2003). \"PDCO: A primal-dual interior method for convex\n  objectives\". http://stanford.edu/group/SOL/software/pdco/"], "see": ["ClpPdcoBase for user-customizable objective/Hessian", "ClpLsqr for the iterative linear solver", "ClpPredictorCorrector for the alternative direct method"], "has_pass2": true}, "src/AbcSimplexDual.hpp": {"path": "layer-1/Clp/src/AbcSimplexDual.hpp", "filename": "AbcSimplexDual.hpp", "file": "AbcSimplexDual.hpp", "brief": "AVX-optimized dual simplex algorithm", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the dual simplex method using ABC's optimized data structures.\nLike ClpSimplexDual, this is a \"mix-in\" class - AbcSimplex objects are\ncast to this type at runtime when running dual simplex.", "algorithm": "Dual Simplex Flow (unchanged from Clp):\n```\nwhile (not finished) {\n  while (not clean) { Factorize, flip for dual feasibility }\n  while (iterating) {\n    Choose leaving variable (most infeasible)\n    Get pivot row (BTRAN + matrix multiply)\n    Choose entering variable (ratio test) ← VECTORIZED\n    Update basis (FTRAN + rank-1 updates)\n  }\n}\n```", "complexity": "Same O(m²n) worst case as standard dual simplex,\nbut ~2-4× faster constant factor from SIMD and cache optimization.", "see": ["AbcSimplex for the base optimized simplex class", "AbcSimplexPrimal for optimized primal simplex", "ClpSimplexDual for the standard (non-AVX) implementation"], "has_pass2": true}, "src/ClpSolver.hpp": {"path": "layer-1/Clp/src/ClpSolver.hpp", "filename": "ClpSolver.hpp", "file": "ClpSolver.hpp", "brief": "Standalone Clp solver driver and command-line interface", "algorithm": "Clp Main Driver with ABC Dispatch\nProvides the main entry points for running Clp as a standalone solver:\n- ClpMain0(): Initialize model with default settings\n- ClpMain1(): Process command queue or command-line arguments\n\nFeatures:\n- Signal handler: Ctrl+C sets iteration limit to 0 for graceful stop\n- BLAS threading control: Defaults to single-thread (openblas_set_num_threads)\n- AMPL interface: clpReadAmpl() reads .nl files\n- FOREIGN_BARRIER: Detects availability of external Cholesky solvers\n\nABC_INHERIT mode: When enabled, uses AbcSimplex instead of ClpSimplex\nfor SIMD-optimized solving.\n\nUser-driven extensions (CLP_USER_DRIVEN1):\n- userChoiceValid1(): Validate leaving variable choice\n- userChoiceValid2(): Validate in/out pair\n- userChoiceWasGood(): Post-pivot callback", "see": ["ClpParameters for parameter handling", "AbcSimplex for the ABC optimized variant", "Clp_C_Interface.h for C language bindings"], "has_pass2": true}, "src/AbcPrimalColumnDantzig.hpp": {"path": "layer-1/Clp/src/AbcPrimalColumnDantzig.hpp", "filename": "AbcPrimalColumnDantzig.hpp", "file": "AbcPrimalColumnDantzig.hpp", "brief": "Dantzig's rule for ABC primal simplex pivot selection", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the simplest pivot selection: choose the nonbasic variable\nwith most negative reduced cost. This is Dantzig's original 1947 rule\nadapted for the ABC optimized simplex framework.", "algorithm": "FULL PRICING:\nMust examine all n-m nonbasic variables to find minimum.\n\"Lumbers over all columns\" - no sophisticated screening.", "math": "REDUCED COST:\nc̄_j = c_j - π'A_j  where π = c_B'·B⁻¹ are dual variables\n\nc̄_j < 0 means increasing x_j will decrease objective.\nChoose most negative = steepest descent in reduced cost space.", "complexity": "O(n) per pricing iteration.\nSimple but can be slow on large problems.\n\nWHEN TO USE:\n- Debugging: Simplest rule, easiest to verify\n- Small problems: Overhead of steepest edge not worth it\n- Dense problems: Steepest edge weight updates expensive\n\nWHEN TO AVOID:\n- Large sparse problems: Use partial pricing or steepest edge\n- Degenerate problems: Dantzig can cycle; steepest edge more stable\n\nABC optimization: Uses CoinPartitionedVector for cache-friendly\nscanning of reduced costs.", "see": ["AbcPrimalColumnPivot for the base interface", "AbcPrimalColumnSteepest for recommended steepest edge variant", "ClpPrimalColumnDantzig for the standard (non-ABC) implementation"], "has_pass2": true}, "src/ClpSimplexOther.hpp": {"path": "layer-1/Clp/src/ClpSimplexOther.hpp", "filename": "ClpSimplexOther.hpp", "file": "ClpSimplexOther.hpp", "brief": "Auxiliary simplex operations: ranging, parametrics, and utilities", "author": "John Forrest\n\nMix-in class providing operations beyond basic primal/dual simplex:", "algorithm": "Parametric Programming:\n  Solve family of LPs as bounds/costs vary: min c(θ)'x s.t. l(θ) ≤ x ≤ u(θ)\n  1. Start with optimal basis at θ = θ_start\n  2. Increase θ continuously, monitoring optimality/feasibility\n  3. When condition violated: perform ratio test to find θ* and pivot\n  4. Continue until θ = θ_end or infeasible/unbounded\n  Reports solution at each breakpoint where basis changes", "math": "Parametric optimality:\n  Bounds: l(θ) = l₀ + θ·∆l, u(θ) = u₀ + θ·∆u\n  Costs: c(θ) = c₀ + θ·∆c\n  Optimal basis changes when: x_B(θ) hits bound or c̄_j(θ) changes sign", "complexity": "Ranging: O(m·n) per variable queried\n  Parametric: O(k·(m²+mn)) where k = number of basis changes\n  Each breakpoint requires refactorization\n\nSensitivity Analysis (Ranging):\n- dualRanging(): How much can objective coefficients change?\n- primalRanging(): How much can variable bounds change?\n\nParametric Programming:\n- parametrics(): Solve family of LPs as parameters vary continuously\n- Trace optimal solution as bounds/costs change from start to end theta\n\nModel Manipulation:\n- dualOfModel(): Create dual formulation of the LP\n- crunch(): Quick presolve for subproblem extraction\n- gubVersion(): Convert to GUB-structured model\n\nI/O:\n- writeBasis()/readBasis(): MPS format basis files\n\nLike ClpSimplexDual/Primal, this is a mix-in - objects are cast at runtime.", "see": ["ClpSimplex which this extends", "ClpSimplexDual for dual simplex algorithm", "ClpSimplexPrimal for primal simplex algorithm"], "has_pass2": true}, "src/AbcPrimalColumnSteepest.hpp": {"path": "layer-1/Clp/src/AbcPrimalColumnSteepest.hpp", "filename": "AbcPrimalColumnSteepest.hpp", "file": "AbcPrimalColumnSteepest.hpp", "brief": "Steepest edge and Devex for ABC primal simplex", "author": "John Forrest (FasterCoin, 2012)\n\nImplements advanced pivot column selection for ABC primal simplex:\n- Steepest Edge: Normalizes reduced costs by column norms\n- Devex: Approximate steepest edge with cheaper updates", "algorithm": "Devex (Approximate Steepest Edge):\n  Cheaper alternative maintaining reference framework weights:\n  1. Initialize w_j = 1 for all nonbasic columns\n  2. After pivot, update only: w_s' = ||B⁻¹·a_s||², w_j' = max(w_j, w_j_hat²)\n     where w_j_hat is contribution from pivot column\n  3. Reset all weights to 1 periodically (every ~factorization)\n  Typically 80-90% as effective as full steepest edge at lower cost.", "math": "Steepest edge weight update (Harris, 1973):\n  Let α = B⁻¹·a_s (pivot column). After pivot where row r leaves:\n  w_j' = w_j - 2·α_j·(a_j^T·τ) + α_j²·w_s where τ = (B⁻ᵀ·e_r)/α_r\n  Incoming column: w_s' = 1/α_r² (becomes basic, new row weight)", "complexity": "Steepest edge: O(m) per pivot for weight update.\n  Devex: O(nnz(pivot_column)) per pivot, much sparser.\n  Both reduce iteration count significantly vs Dantzig.", "ref": ["Harris (1973). \"Pivot selection methods of the Devex LP code\".\n     Mathematical Programming 5:1-28.\n\nModes (controlled by constructor parameter):\n- 0: Exact Devex\n- 1: Full steepest edge\n- 2: Partial exact Devex (scan subset of nonbasics)\n- 3: Switches between 0/2 based on factorization (default)\n- 4: Starts as partial Dantzig/Devex, may switch\n- 5: Always partial Dantzig\n- >=10: Mini-sprint mode\n\nKey optimizations over ClpPrimalColumnSteepest:\n- Uses CoinPartitionedVector for parallel column blocks\n- Vectorized weight updates in djsAndDevex()\n- Efficient partial pricing with partialPricing()"], "see": ["AbcPrimalColumnPivot for the base interface", "AbcPrimalColumnDantzig for simpler alternative", "ClpPrimalColumnSteepest for the standard (non-ABC) implementation"], "has_pass2": true}, "src/ClpParam.hpp": {"path": "layer-1/Clp/src/ClpParam.hpp", "filename": "ClpParam.hpp", "file": "ClpParam.hpp", "brief": "Individual parameter definitions for Clp control", "author": "Lou Hafer\n\nExtends CoinParam with Clp-specific parameter codes and types.\nClpParamCode enumeration defines all configurable options:\n\n- Action params: DUALSIMPLEX, PRIMALSIMPLEX, BARRIER, etc.\n- Boolean params: AUTOSCALE, PERTURBATION, SPARSEFACTOR\n- Keyword params: CHOLESKY, CRASH, PRESOLVE, SCALING\n- Integer params: MAXITERATION, IDIOT, SPRINT, THREADS\n- Double params: DUALTOLERANCE, PRIMALTOLERANCE, TIMELIMIT\n\nEach parameter has push/pull functions for value validation\nand transfer between parameter objects and solver state.", "see": ["ClpParameters for the parameter container", "CoinParam for the base parameter class"], "has_pass2": false}, "src/CbcOrClpParam.hpp": {"path": "layer-1/Clp/src/CbcOrClpParam.hpp", "filename": "CbcOrClpParam.hpp", "file": "CbcOrClpParam.hpp", "brief": "Shared parameter codes for Cbc and Clp solvers\n\nLegacy parameter handling shared between Cbc (branch-and-cut) and Clp\n(linear programming). This file is intentionally duplicated between\nCbc/Test and Clp/Test for simplicity.\n\nParameter code ranges (CbcOrClpParameterType):\n- 1-100: Double parameters (CLP_PARAM_DBL_*, CBC_PARAM_DBL_*)\n- 101-200: Integer parameters (CLP_PARAM_INT_*, CBC_PARAM_INT_*)\n- 201-300: Clp string parameters\n- 301-400: Cbc string parameters\n- 401-500: Clp actions (mostly)\n- 501-600: Cbc actions (mostly)\n\nConditional compilation:\n- COIN_HAS_CBC: Include Cbc-specific parameters\n- COIN_HAS_CLP: Include Clp-specific parameters\n\nNote: Being superseded by ClpParam/CbcParam for cleaner separation.", "see": ["ClpParam for modern Clp parameter handling", "ClpParameters for parameter container\n@deprecated Use ClpParam for new code"], "has_pass2": false}, "src/ClpSimplexNonlinear.hpp": {"path": "layer-1/Clp/src/ClpSimplexNonlinear.hpp", "filename": "ClpSimplexNonlinear.hpp", "file": "ClpSimplexNonlinear.hpp", "brief": "Nonlinear LP solver using reduced gradient and SLP methods", "author": "John Forrest\n\nExtends ClpSimplexPrimal to handle nonlinear objectives and constraints.", "algorithm": "Pivot Modes:\n  mode=0: Consider all dual infeasible variables\n  mode=1: Select only largest reduced cost\n  mode≥10: Startup phase (finding initial feasible point)\n\nRETURN CODES from pivotColumn:\n  0: Normal iteration (basis change)\n  1: No basis change (line search)\n  2: Singleton pivot\n  3: Refactorization needed\n\nImplementation note: This class has no data members - it's a \"behavior\"\nclass that is cast from ClpSimplexPrimal at algorithm time.", "math": "PROBLEM: minimize f(x) subject to g(x) ≤ 0, Ax = b\n\nITERATION:\n  1. At current point x̄, linearize constraints:\n     g(x) ≈ g(x̄) + ∇g(x̄)'(x - x̄)\n\n  2. Solve LP subproblem with trust region:\n     minimize ∇f(x̄)'x\n     subject to ∇g(x̄)'x ≤ -g(x̄) + ∇g(x̄)'x̄\n                |x - x̄| ≤ Δ (trust region)\n\n  3. Update: x̄ ← solution of LP\n  4. Adjust trust region Δ based on actual vs predicted improvement", "see": ["ClpSimplexPrimal for the base primal simplex", "ClpConstraint for nonlinear constraint interface", "ClpQuadraticObjective for quadratic objectives"], "has_pass2": true}, "src/AbcDualRowDantzig.hpp": {"path": "layer-1/Clp/src/AbcDualRowDantzig.hpp", "filename": "AbcDualRowDantzig.hpp", "file": "AbcDualRowDantzig.hpp", "brief": "Dantzig's rule for ABC dual simplex pivot selection", "author": "John Forrest (FasterCoin, 2012)\n\nImplements the simplest pivot selection: choose the basic variable with\nlargest primal infeasibility. This is Dantzig's original 1947 rule\nadapted for the ABC optimized simplex framework.", "algorithm": "Weight Updates (trivial):\nDantzig rule has no weights to maintain.\nupdateWeights() just tracks which rows become feasible/infeasible.\n\nFast per iteration but may require more iterations than steepest edge.\nUse AbcDualRowSteepest for better performance on most problems.", "math": "PRIMAL INFEASIBILITY:\nviolation_i = max(l_i - x_Bi, 0) + max(x_Bi - u_i, 0)", "complexity": "O(k) where k = number of infeasible rows.\nBest case O(1) when few infeasibilities, worst case O(m).", "see": ["AbcDualRowPivot for the base interface", "AbcDualRowSteepest for recommended steepest edge variant", "ClpDualRowDantzig for the standard (non-ABC) implementation"], "has_pass2": true}, "src/ClpParameters.hpp": {"path": "layer-1/Clp/src/ClpParameters.hpp", "filename": "ClpParameters.hpp", "file": "ClpParameters.hpp", "brief": "Command-line and runtime parameter management", "author": "Lou Hafer\n\nContainer for Clp's configurable parameters, supporting:\n- Command-line parsing (standalone clp executable)\n- Runtime parameter access (embedded use)\n- Integration with CBC mode\n\nParameter types: string, int, double, boolean, keyword, file paths.\nOrganized into categories: strategy, display, algorithm control.", "see": ["ClpParam for individual parameter definitions", "CoinParam for the underlying parameter framework"], "has_pass2": false}, "src/ClpNetworkBasis.hpp": {"path": "layer-1/Clp/src/ClpNetworkBasis.hpp", "filename": "ClpNetworkBasis.hpp", "file": "ClpNetworkBasis.hpp", "brief": "Specialized factorization for pure network problems", "author": "John Forrest\n\nExploits network structure for O(n) factorization instead of O(n^2-3).\nNetwork LPs have constraint matrices that are node-arc incidence matrices\nof directed graphs - each column has exactly one +1 and one -1.", "algorithm": "Basis Update (pivot operation):\nWhen arc (u,v) enters basis replacing arc (p,q):\n  - Create cycle by adding (u,v) to tree\n  - Remove (p,q) from cycle → new tree\n  - Update parent/child/sibling pointers along affected path", "math": "Constraint matrix A is node-arc incidence of digraph G=(V,E):\n  A[i,j] = +1 if arc j leaves node i\n  A[i,j] = -1 if arc j enters node i\n  A[i,j] = 0 otherwise\n\nEach column (arc) has exactly one +1 and one -1.\nExamples: transportation, assignment, shortest path, max flow problems.", "complexity": "Factorization: O(m) vs O(m²) for general LU\n  FTRAN/BTRAN: O(m) vs O(m²) for general\n  Update: O(path length) vs O(m²) for general\n\nUses spanning tree representation:\n- parent/descendant/sibling arrays encode tree structure\n- FTRAN/BTRAN reduce to tree traversal operations\n- Updates are simple tree modifications\n\nRequires ClpNetworkMatrix to detect network structure.", "see": ["ClpNetworkMatrix for the network matrix representation", "ClpFactorization for the general factorization interface"], "has_pass2": true}, "src/ClpModel.hpp": {"path": "layer-1/Clp/src/ClpModel.hpp", "filename": "ClpModel.hpp", "file": "ClpModel.hpp", "brief": "Base class for LP/QP models - stores problem data without algorithm logic\n\nClpModel holds the complete representation of a linear or quadratic program:\nconstraint matrix, variable bounds, objective coefficients, and solution vectors.\nThis is the base class inherited by ClpSimplex and ClpInterior - it knows about\nproblem data but nothing about solution algorithms.", "algorithm": "SCALING MODES (scalingMode parameter):\n  0: Off - no scaling\n  1: Equilibrium - scale so max |a_ij| = 1 in each row/col\n  2: Geometric - row×col scaling to balance row/col norms\n  3: Auto - Clp chooses based on matrix properties\n  4: Auto for B&B - like 3 but preserves integrality", "math": "EQUILIBRIUM SCALING:\n  r_i = 1 / max_j |a_ij|  (row scales)\n  c_j = 1 / max_i |a_ij|  (column scales)\n  Scaled problem: min c̃'x̃  s.t. Ãx̃ = b̃\n  where: ã_ij = r_i · a_ij · c_j\n\nScaling improves:\n  - Basis condition number → fewer numerical errors\n  - Reduced cost computation accuracy\n  - Interior point convergence\n\nKey responsibilities:\n- Problem loading from various formats (MPS, GMPL, CoinModel, raw arrays)\n- Row/column manipulation (add, delete, modify bounds)\n- Matrix scaling for numerical stability\n- Solution and status storage\n- Name management for rows and columns", "see": ["ClpSimplex for simplex algorithm implementation", "ClpInterior for interior point algorithm", "ClpMatrixBase for the constraint matrix interface", "ClpObjective for objective function handling", "CoinPackedMatrix in CoinUtils for sparse matrix storage"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the variables whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n            end of the array of the indices of the constraints whose\n        <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints"], "has_pass2": true}, "src/ClpPrimalQuadraticDantzig.hpp": {"path": "layer-1/Clp/src/ClpPrimalQuadraticDantzig.hpp", "filename": "ClpPrimalQuadraticDantzig.hpp", "file": "ClpPrimalQuadraticDantzig.hpp", "brief": "Dantzig-style pricing for quadratic programming\n\nExtends ClpPrimalColumnPivot for QP problems where the reduced cost\ndepends on the current solution (due to the quadratic objective).", "algorithm": "Selection Rule:\nChoose entering variable j* = argmax |c̄_j| among:\n  - Variables at lower bound with c̄_j < 0 (can increase)\n  - Variables at upper bound with c̄_j > 0 (can decrease)", "math": "REDUCED COST FOR QP:\nFor LP: c̄_j = c_j - π'A_j (constant during iteration)\nFor QP: c̄_j = c_j + (Qx)_j - π'A_j (changes with x!)", "complexity": "O(n) per pricing: examines all nonbasic variables.\nNo sophisticated structures like steepest edge or Devex.\n\nWHY REDUCED COST CHANGES IN QP:\n  ∇f(x) = c + Qx\n  When basic variables change → x changes → Qx changes → c̄ changes\n  Must recompute all reduced costs each iteration (expensive!)\n\nquadraticInfo_ stores:\n  - Current Qx product\n  - Solution state for gradient updates\n\nUse when: Starting out with QP, need robust method.\nAvoid when: Large problems (use steepest edge variants).", "see": ["ClpPrimalColumnPivot for the abstract pivot interface", "ClpQuadraticObjective for the QP objective representation", "ClpSimplexPrimalQuadratic for the QP simplex solver"], "has_pass2": true}, "src/AbcSimplexFactorization.hpp": {"path": "layer-1/Clp/src/AbcSimplexFactorization.hpp", "filename": "AbcSimplexFactorization.hpp", "file": "AbcSimplexFactorization.hpp", "brief": "LU factorization wrapper for ABC simplex", "author": "John Forrest (FasterCoin, 2012)\n\nWraps CoinAbcAnyFactorization (or CoinFactorization) for use with\nAbcSimplex. Handles basis factorization and FTRAN/BTRAN operations\nwith ABC's optimized data structures.", "algorithm": "Factorization Type Selection:\n  - Dense (goDenseThreshold_): For small m, use LAPACK-style\n  - Small (goSmallThreshold_): Optimized for cache efficiency\n  - Standard: General sparse LU\n  - Long (goLongThreshold_): Ordered for very large problems", "complexity": "Factorization: O(nnz(L)·nnz(U)) depends on fill-in\nFTRAN/BTRAN: O(nnz(L)+nnz(U)) per solve\nUpdate: O(nnz(eta)) per rank-1 update", "see": ["AbcSimplex which uses this factorization", "ClpFactorization for the standard (non-ABC) wrapper", "CoinAbcFactorization in CoinUtils for ABC factorization"], "has_pass2": true}, "src/AbcDualRowPivot.hpp": {"path": "layer-1/Clp/src/AbcDualRowPivot.hpp", "filename": "AbcDualRowPivot.hpp", "file": "AbcDualRowPivot.hpp", "brief": "Abstract base class for dual pivot row selection in ABC", "author": "John Forrest (FasterCoin, 2012)\n\nDefines the interface for choosing which row (basic variable) should\nleave the basis in dual simplex iterations within the ABC framework.", "algorithm": "ABC Optimizations:\n- CoinIndexedVector references (not pointers) for cache locality\n- Weights computed in chunks for vectorization\n- FT update integrated with weight maintenance", "see": ["AbcDualRowDantzig for simple largest-infeasibility selection", "AbcDualRowSteepest for steepest edge variant", "ClpDualRowPivot for the standard (non-ABC) interface"], "has_pass2": true}, "src/ClpMessage.hpp": {"path": "layer-1/Clp/src/ClpMessage.hpp", "filename": "ClpMessage.hpp", "file": "ClpMessage.hpp", "brief": "Message codes and localization for Clp solver output\n\nDefines the CLP_Message enum with all status and diagnostic message codes\nused by Clp during optimization. Messages are organized by solver phase:\n- CLP_SIMPLEX_*: General simplex status (finished, infeasible, etc.)\n- CLP_DUAL_*: Dual simplex specific messages\n- CLP_PRIMAL_*: Primal simplex specific messages\n- CLP_BARRIER_*: Interior point method messages\n\nClpMessage inherits from CoinMessages to provide localized message text.\nControl output verbosity via ClpSimplex::setLogLevel().", "see": ["CoinMessageHandler in CoinUtils for the messaging infrastructure", "ClpSimplex::setLogLevel() to control output verbosity"], "has_pass2": false}, "src/ClpConstraintQuadratic.hpp": {"path": "layer-1/Clp/src/ClpConstraintQuadratic.hpp", "filename": "ClpConstraintQuadratic.hpp", "file": "ClpConstraintQuadratic.hpp", "brief": "Quadratic constraint implementation: x'Qx + c'x ≤ b\n\nImplements ClpConstraint for quadratic constraints. The constraint\nfunction is: 0.5 * sum_{ij}(Q_ij * x_i * x_j) + sum_j(c_j * x_j) ≤ b", "algorithm": "Use in SLP (Sequential Linear Programming):\nAt point x̄, linearize: g(x) ≈ g(x̄) + ∇g(x̄)'(x - x̄)\nConstraint becomes: ∇g(x̄)'x ≤ -g(x̄) + ∇g(x̄)'x̄\n\nTrust region limits step size to control linearization error.\nFor convex constraints (Q PSD), linearization underestimates\ntrue constraint, maintaining feasibility.", "math": "CONSTRAINT FORM:\n  g(x) = (1/2)x'Qx + c'x - b ≤ 0\n\nwhere Q is symmetric (but not necessarily PSD for constraints).", "see": ["ClpConstraint for the abstract interface", "ClpConstraintLinear for purely linear constraints", "ClpSimplexNonlinear for the SLP solver"], "has_pass2": true}, "src/ClpPrimalColumnPivot.hpp": {"path": "layer-1/Clp/src/ClpPrimalColumnPivot.hpp", "filename": "ClpPrimalColumnPivot.hpp", "file": "ClpPrimalColumnPivot.hpp", "brief": "Abstract base class for primal simplex pivot column selection\n\nIn primal simplex, the pivot column (entering variable) is chosen based on\nreduced cost. This class defines the interface for different selection\nstrategies. Derived classes implement specific rules.\n\nKey methods:\n- pivotColumn(): Select which column (variable) enters the basis\n- updateWeights(): Maintain pricing information after pivots\n- saveWeights(): Preserve weights across refactorizations", "algorithm": "Primal Simplex Pivot Column Selection (Strategy Pattern):\nChooses which nonbasic variable enters the basis in primal simplex iteration.\nThe entering variable has a favorable reduced cost for improving the objective.\nCommon strategies:\n- Dantzig: Choose most negative reduced cost (simple, fast per iteration)\n- Steepest edge: Normalize by ||B^{-1}A_j|| (fewer iterations overall)\n- Devex: Approximate steepest edge with less overhead\n- Partial pricing: Only scan subset of columns (for large problems)", "math": "In primal simplex, we maintain primal feasibility and iterate toward\ndual feasibility. Reduced cost d_j = c_j - c_B^T B^{-1} A_j.\n- Dantzig: Select j with min{d_j | d_j < 0}\n- Steepest: Select j with min{d_j / ||B^{-1}A_j||} among d_j < 0\nThe ||B^{-1}A_j|| norms are maintained incrementally across iterations.", "complexity": "Dantzig: O(n) scan of nonbasic variables per iteration.\nSteepest edge: O(n) scan + O(m*nnz) weight updates per iteration.\nSteepest edge typically reduces total iteration count by 2-3x,\noutweighing higher per-iteration cost for most problems.", "ref": ["Dantzig, G.B. (1963). \"Linear Programming and Extensions\".\n  Princeton University Press. [Original Dantzig rule]", "Goldfarb, D. & Reid, J.K. (1977). \"A practicable steepest-edge simplex\n  algorithm\". Mathematical Programming 12:361-371."], "see": ["ClpPrimalColumnDantzig for simple most-negative reduced cost", "ClpPrimalColumnSteepest for steepest edge pricing (recommended)", "ClpSimplexPrimal for the primal simplex algorithm", "ClpDualRowPivot for the dual simplex equivalent"], "has_pass2": true}, "src/CoinAbcHelperFunctions.hpp": {"path": "layer-1/Clp/src/CoinAbcHelperFunctions.hpp", "filename": "CoinAbcHelperFunctions.hpp", "file": "CoinAbcHelperFunctions.hpp", "brief": "SIMD-optimized scatter/gather operations for ABC factorization\n\nHigh-performance kernels for sparse matrix operations using:\n- Manual loop unrolling (UNROLL_SCATTER, UNROLL_GATHER)\n- AVX/AVX2 intrinsics when available\n- Cilk parallel loops with configurable grainsize\n- Prefetching hints (coin_prefetch macros)\n\nKey operations:\n- CoinAbcScatterUpdate: region[index[j]] -= value[j] * pivot\n- CoinAbcGatherUpdate: dot product over sparse indices\n- Memory utilities: CoinAbcMemcpyLong, CoinAbcMemset0Long\n\nIncludes specialized ScatterUpdateN functions for N=1..8 and 4N variants\nusing function pointer dispatch (scatterStruct) for optimal performance.", "algorithm": "Function Pointer Dispatch (scatterStruct):\n  Precomputed table of specialized ScatterUpdate{1..8,4N} functions\n  avoids branch misprediction in small-column cases.", "complexity": "O(nnz) per scatter/gather operation\n  With SIMD: ~4x speedup on vectorizable sections\n  With Cilk: ~Kx speedup on K cores for large columns", "see": ["CoinAbcFactorization which uses these kernels", "AbcSimplex which uses these for FTRAN/BTRAN"], "has_pass2": true}, "src/MyMessageHandler.hpp": {"path": "layer-1/Clp/src/MyMessageHandler.hpp", "filename": "MyMessageHandler.hpp", "file": "MyMessageHandler.hpp", "brief": "Example message handler demonstrating custom logging\n\nSample implementation of CoinMessageHandler showing how users can\ncustomize message output. Used in unit tests to demonstrate\ncapturing solver progress and collecting feasible extreme points.\n\nFeatures demonstrated:\n- Override print() to intercept all solver messages\n- Access to model_ for querying current solution\n- Collection of feasible extreme points during solve\n- Custom file output via FILE* pointer\n\nUse case - Feasible point enumeration:\nDuring optimization, each time the solver finds a new feasible\nextreme point (basic feasible solution), the handler captures it.\nUseful for problems where alternative optima matter.\n\nTo create your own handler:\n1. Inherit from CoinMessageHandler\n2. Override print() method\n3. Attach via ClpSimplex::passInMessageHandler()", "see": ["CoinMessageHandler for the base interface", "ClpMessage for Clp message definitions", "ClpSimplex::passInMessageHandler() to install"], "has_pass2": false}, "src/ClpDualRowPivot.hpp": {"path": "layer-1/Clp/src/ClpDualRowPivot.hpp", "filename": "ClpDualRowPivot.hpp", "file": "ClpDualRowPivot.hpp", "brief": "Abstract base class for dual simplex pivot row selection\n\nIn dual simplex, the pivot row (leaving variable) is chosen based on\nprimal infeasibility. This class defines the interface for different\nselection strategies. Derived classes implement specific rules.\n\nKey methods:\n- pivotRow(): Select which row (basic variable) leaves the basis\n- updateWeights(): Maintain pricing information after pivots\n- updatePrimalSolution(): Update solution after basis change", "algorithm": "Dual Simplex Pivot Row Selection (Strategy Pattern):\nChooses which basic variable leaves the basis in dual simplex iteration.\nThe leaving variable is primal infeasible (violating bounds).\nCommon strategies:\n- Dantzig: Choose most infeasible variable (simple, fast per iteration)\n- Steepest edge: Minimize ||B^{-1}e_i|| weighted infeasibility (fewer iterations)\n- Partial pricing: Only scan subset of rows (for large problems)", "math": "In dual simplex, we maintain dual feasibility (reduced costs correct signs)\nand iterate toward primal feasibility. Leaving variable selection:\n- Find i where x_B[i] < l_i or x_B[i] > u_i (primal infeasibility)\n- Dantzig: max{|x_B[i] - bound_i|}\n- Steepest: max{|x_B[i] - bound_i| / ||B^{-1}e_i||}\nThe ||B^{-1}e_i|| weights are maintained incrementally across iterations.", "complexity": "Dantzig: O(m) scan of basic variables per iteration.\nSteepest edge: O(m) scan + O(nnz) weight updates per iteration.\nSteepest edge typically reduces total iteration count by 2-3x,\noutweighing higher per-iteration cost for most problems.", "ref": ["Goldfarb, D. & Reid, J.K. (1977). \"A practicable steepest-edge simplex\n  algorithm\". Mathematical Programming 12:361-371.", "Forrest, J.J. & Goldfarb, D. (1992). \"Steepest-edge simplex algorithms\n  for linear programming\". Mathematical Programming 57:341-374."], "see": ["ClpDualRowDantzig for simple most-infeasible selection", "ClpDualRowSteepest for steepest edge pricing (recommended)", "ClpSimplexDual for the dual simplex algorithm", "ClpPrimalColumnPivot for the primal simplex equivalent"], "has_pass2": true}, "src/ClpPrimalColumnDantzig.hpp": {"path": "layer-1/Clp/src/ClpPrimalColumnDantzig.hpp", "filename": "ClpPrimalColumnDantzig.hpp", "file": "ClpPrimalColumnDantzig.hpp", "brief": "Dantzig's rule for primal simplex pivot selection\n\nImplements the simplest pivot column selection: choose the nonbasic variable\nwith the most negative reduced cost (for minimization). This is Dantzig's\noriginal 1947 rule.\n\nSimple and fast per iteration, but typically requires more iterations than\nsteepest edge methods on degenerate or difficult problems. Use\nClpPrimalColumnSteepest for better performance on most problems.", "algorithm": "Dantzig's Rule (Primal Simplex Entering Variable Selection):\nScans all nonbasic variables and selects the one with most negative reduced\ncost. This greedy choice maximizes rate of improvement per unit movement.\nNo weight storage needed - stateless selection based purely on current\nreduced costs. Fast per iteration but may cycle on degenerate problems\nwithout anti-cycling rules (handled elsewhere in ClpSimplexPrimal).", "math": "For minimization, select entering column j* = argmin{d_j | d_j < 0}\nwhere d_j = c_j - c_B^T B^{-1} A_j is the reduced cost of nonbasic column j.\nIf d_j >= 0 for all j, current solution is optimal.\nThe most negative d_j gives steepest descent in objective space,\nthough not necessarily fewest iterations to optimality.", "complexity": "O(n) per iteration to scan all nonbasic reduced costs.\nTotal work depends heavily on problem structure - may require\nO(2^n) iterations worst case (Klee-Minty), but polynomial expected\nfor random problems. Steepest edge typically 2-3x fewer iterations.", "ref": ["Dantzig, G.B. (1947). \"Maximization of a linear function of variables\n  subject to linear inequalities\". Activity Analysis of Production and\n  Allocation. [Original simplex paper]", "Klee, V. & Minty, G.J. (1972). \"How good is the simplex algorithm?\".\n  Inequalities III. [Worst-case exponential example]"], "see": ["ClpPrimalColumnPivot for the base interface", "ClpPrimalColumnSteepest for recommended steepest edge variant", "ClpSimplexPrimal for the primal simplex algorithm"], "has_pass2": true}, "src/CoinAbcDenseFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcDenseFactorization.hpp", "filename": "CoinAbcDenseFactorization.hpp", "file": "CoinAbcDenseFactorization.hpp", "brief": "Abstract base class for ABC factorization and dense submatrix handling", "author": "John Forrest\n\nDefines CoinAbcAnyFactorization, the abstract interface for all ABC\nfactorization variants, plus CoinAbcDenseFactorization for small/dense bases.", "algorithm": "When Dense Beats Sparse:\n- Small m (< 200): BLAS-3 efficiency dominates\n- Nearly dense basis: Sparse overhead not worth it\n- Numerical difficulty: Dense more stable", "math": "LU WITH PARTIAL PIVOTING:\nFactor B = PLU where:\n  P = permutation matrix (row interchanges for stability)\n  L = unit lower triangular\n  U = upper triangular", "complexity": "Dense factorization: O(m³)\nDense FTRAN/BTRAN: O(m²)", "see": ["CoinAbcBaseFactorization for sparse LU implementation", "CoinAbcFactorization for template-instantiated variants"], "has_pass2": true}, "src/ClpCholeskyBase.hpp": {"path": "layer-1/Clp/src/ClpCholeskyBase.hpp", "filename": "ClpCholeskyBase.hpp", "file": "ClpCholeskyBase.hpp", "brief": "Cholesky factorization base class for interior point methods\n\nProvides Cholesky factorization of the normal equations matrix A*D*A'\nused in predictor-corrector interior point methods. The factorization\nuses AMD ordering to reduce fill-in.\n\nThe base class provides a simple sparse Cholesky implementation with\nsupernodal dense blocks. Derived classes can interface to more\nsophisticated factorizations (MUMPS, Pardiso, TAUCS, etc.).\n\nKey methods:\n- order(): Compute fill-reducing ordering (AMD by default)\n- symbolic(): Set up sparsity structure of factor\n- factorize(): Numeric factorization of A*D*A'\n- solve(): Solve system using computed factors", "algorithm": "Sparse Cholesky Factorization for Interior Point Methods:\nComputes A*D*A' = L*L' where L is lower triangular. Used to solve the\nnormal equations in each IPM iteration. Key algorithmic features:\n1. AMD ordering to minimize fill-in (sparse L has fewer nonzeros)\n2. Symbolic factorization: predict sparsity structure before numeric work\n3. Supernodal blocking: dense submatrices within sparse structure\n4. Row dropping: handle rank deficiency by removing dependent rows", "math": "Normal equations: A*D*A' * dy = r where D = X^{-1}*S (diagonal scaling).\nFactorization: LL' = P(ADA')P' where P is permutation from AMD ordering.\nSolution: dy = P' * L'^{-1} * L^{-1} * P * r (forward/back substitution).\nCondition number tracked via choleskyCondition_ for numerical stability.", "complexity": "Symbolic: O(nnz(L)) to compute structure.\nNumeric factorization: O(nnz(L)^2 / n) with supernodal, O(n^3) dense worst case.\nForward/backward solve: O(nnz(L)) per iteration.\nWith AMD ordering, nnz(L) << n^2 for sparse problems.", "ref": ["George & Liu (1981). \"Computer Solution of Large Sparse Positive\n  Definite Systems\". Prentice-Hall. [Sparse Cholesky foundations]", "Amestoy et al. (1996). \"An Approximate Minimum Degree Ordering Algorithm\".\n  SIAM J. Matrix Anal. Appl. [AMD ordering]"], "see": ["ClpInterior for the interior point algorithm", "ClpCholeskyDense for dense Cholesky when A*D*A' becomes dense", "ClpCholeskyMumps, ClpCholeskyPardiso for advanced factorizations", "CoinFactorization in CoinUtils for simplex LU factorization"], "has_pass2": true}, "src/ClpPEDualRowDantzig.hpp": {"path": "layer-1/Clp/src/ClpPEDualRowDantzig.hpp", "filename": "ClpPEDualRowDantzig.hpp", "file": "ClpPEDualRowDantzig.hpp", "brief": "Positive Edge enhanced Dantzig pricing for dual simplex", "author": "Jeremy Omer\n\nCombines classic dual Dantzig pricing (most infeasible row) with\nPositive Edge compatibility checking. Prioritizes compatible rows\nthat can make real progress on degenerate problems.", "algorithm": "Bi-Dimensional Dual Pricing:\nScore(i) = |violation_i|^(1-ψ) × Compat(i)^ψ\n\nWhere violation_i = max(l_i - x_Bi, x_Bi - u_i, 0) measures\nhow far basic variable i is from feasibility.\n\nψ parameter (default 0.5):\n  ψ = 0: Pure Dantzig (most infeasible)\n  ψ = 1: Pure compatibility\n  ψ = 0.5: Balanced", "math": "DUAL DEGENERACY:\nOccurs when nonbasic variables have c̄_j = 0 (multiple optimal bases).\nRatio test may give θ = 0 → basis change but no dual improvement.", "complexity": "O(m) per selection like standard Dantzig.\nCompatibility updates amortized over multiple iterations.", "ref": ["Towhidi, Desrosiers, Soumis (2014): \"The positive edge criterion\""], "see": ["ClpPESimplex for the compatibility framework", "ClpDualRowDantzig for the base pricing rule", "ClpPEDualRowSteepest for steepest edge variant"], "has_pass2": true}, "src/ClpGubDynamicMatrix.hpp": {"path": "layer-1/Clp/src/ClpGubDynamicMatrix.hpp", "filename": "ClpGubDynamicMatrix.hpp", "file": "ClpGubDynamicMatrix.hpp", "brief": "Dynamic column generation with Generalized Upper Bound structure\n\nCombines GUB constraints with dynamic column generation. Columns are\npartitioned into GUB sets where at most one column per set can be basic.", "algorithm": "Effective RHS:\nInstead of explicit GUB rows, track effective RHS implicitly:\n  effectiveRHS_k = u_k - Σ_{j∈Sₖ, j nonbasic} x_j\nThis avoids adding m_gub rows to the constraint matrix.\n\nAPPLICATIONS:\n- Cutting stock: patterns as columns, roll types as GUB sets\n- Vehicle routing: routes as columns, vehicles as GUB sets\n- Crew scheduling: pairings as columns, days as GUB sets", "math": "GUB STRUCTURE:\n  Variables partitioned into sets S₁, S₂, ..., Sₖ\n  GUB constraint for set k: Σ_{j∈Sₖ} xⱼ ≤ uₖ (or = uₖ)\n\nKey property: At most ONE variable from each GUB set can be basic.\nThis reduces effective basis size and enables implicit handling.", "see": ["ClpGubMatrix for static GUB handling", "ClpDynamicMatrix for non-GUB dynamic columns", "ClpDynamicExampleMatrix for column generation example"], "has_pass2": true}, "src/ClpCholeskyWssmp.hpp": {"path": "layer-1/Clp/src/ClpCholeskyWssmp.hpp", "filename": "ClpCholeskyWssmp.hpp", "file": "ClpCholeskyWssmp.hpp", "brief": "WSSMP sparse direct solver interface for Cholesky factorization\n\nWraps IBM's Watson Sparse Matrix Package (WSSMP) for Cholesky factorization\nof normal equations in interior point methods.", "algorithm": "WSSMP Features:\n- Multifrontal sparse Cholesky: O(n^1.5 to n^2) for 2D/3D problems\n- Shared memory parallelism (OpenMP)\n- Dense column handling: columns with >threshold nonzeros\n  treated as dense for cache efficiency\n- Symbolic factorization reuse: factor pattern once, update numerics", "math": "NORMAL EQUATIONS:\nGiven step direction system:  [D  A'] [Δx]   [r₁]\n                              [A  0 ] [Δy] = [r₂]\n\nEliminate Δx: (A·D⁻¹·A')·Δy = r₂ - A·D⁻¹·r₁\n              M·Δy = rhs  where M = A·D⁻¹·A'\n\nThen: Δx = D⁻¹·(r₁ - A'·Δy)", "complexity": "Symbolic: O(nnz²/n) typical for sparse\nNumeric: O(nnz(L)·n_ops) where L is Cholesky factor\nSolve: O(nnz(L)) triangular solves\n\nRequires WSSMP library to be installed and linked.", "see": ["ClpCholeskyBase for the abstract interface", "ClpCholeskyWssmpKKT for KKT system variant", "ClpInterior which uses this factorization"], "has_pass2": true}, "src/ClpPEDualRowSteepest.hpp": {"path": "layer-1/Clp/src/ClpPEDualRowSteepest.hpp", "filename": "ClpPEDualRowSteepest.hpp", "file": "ClpPEDualRowSteepest.hpp", "brief": "Positive Edge enhanced steepest edge for dual simplex", "author": "Jeremy Omer\n\nCombines dual steepest edge pricing with Positive Edge compatibility\nchecking. The most effective anti-degeneracy variant for dual simplex.", "algorithm": "Positive Edge Dual Steepest Edge:\n  Enhanced row selection combining steepest edge with compatibility:\n  1. Compute steepest edge scores: |d_i|²/w_i for infeasible rows\n  2. Identify compatible rows via ClpPESimplex::isCompatibleRow()\n  3. Apply bi-dimensional pricing: prefer compatibles unless much worse\n  4. Track degeneracy statistics for adaptive mode switching", "math": "Row selection with compatibility weight:\n  Select r = argmax_i { |d_i|²/w_i · (1 + (1-ψ)·c_i) }\n  where c_i = 1 if compatible, 0 otherwise\n  Compatible rows make positive progress on primal degenerates.", "complexity": "Same as ClpDualRowSteepest plus O(m) compatibility check.\n  Reduces degenerate iterations by 20-50% on difficult LPs.", "ref": ["Towhidi & Orban (2014). \"Customizing the solution process of COIN-OR's\n     linear solvers with Python\". Math. Prog. Computation 6:247-282.\n\nModes: 0=uninitialized, 1=full, 2=partial uninitialized,\n       3=adaptive (starts partial, may switch to full)\n\nUses bi-dimensional pricing: candidates scored by steepest edge weight\nand compatibility, weighted by psi parameter."], "see": ["ClpPESimplex for the compatibility framework", "ClpDualRowSteepest for the base pricing rule", "ClpPEDualRowDantzig for Dantzig variant"], "has_pass2": true}, "src/CoinAbcFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcFactorization.hpp", "filename": "CoinAbcFactorization.hpp", "file": "CoinAbcFactorization.hpp", "brief": "ABC optimized LU factorization variants", "author": "John Forrest\n\nTemplate-instantiation header that creates multiple factorization variants\nfrom CoinAbcBaseFactorization using preprocessor macros.", "algorithm": "SIMD Optimizations (all variants):\n- Vectorized scatter/gather for L and U updates\n- Aligned memory for AVX operations\n- Parallel factorization via ABC_PARALLEL", "see": ["CoinAbcBaseFactorization for the actual implementation", "CoinAbcHelperFunctions for SIMD kernels"], "has_pass2": true}, "src/ClpPESimplex.hpp": {"path": "layer-1/Clp/src/ClpPESimplex.hpp", "filename": "ClpPESimplex.hpp", "file": "ClpPESimplex.hpp", "brief": "Positive Edge anti-degeneracy framework for simplex", "author": "Jeremy Omer\n\nCore infrastructure for the Positive Edge method that reduces degenerate\npivots in simplex. Identifies \"compatible\" variables that can make real\nprogress toward optimality.", "algorithm": "Positive Edge Compatibility Detection:\n  Reduces degenerate pivots by identifying \"compatible\" variables:\n  1. Compute reference direction: d = B⁻¹·a_j for candidate column j\n  2. Variable i is compatible if ⟨d, e_i⟩ > tolerance\n     (i.e., pivot would cause positive movement on variable i)\n  3. Prioritize compatible variables in pricing to avoid stalling\n  4. Fall back to standard pricing when no compatibles exist", "math": "Bi-dimensional pricing with compatibility:\n  For primal: score(j) = |d_j| / (ψ · w_j + (1-ψ) · c_j)\n  where d_j = reduced cost, w_j = steepest edge weight,\n        c_j = compatibility score (0 or 1)\n  For dual: analogous scoring for row selection\n  Parameter ψ ∈ [0,1] balances steepest edge vs compatibility.", "complexity": "Compatibility update: O(nnz(pivot_column)) per iteration.\n  Tracking primal/dual degenerates: O(m) per factorization.\n  Reduces iteration count significantly on highly degenerate LPs.", "ref": ["Towhidi & Orban (2014). \"Customizing the solution process of COIN-OR's\n     linear solvers with Python\". Math. Prog. Computation 6:247-282.\n\nKey concepts:\n- Primal degenerates: basic variables at bound (zero pivot impact)\n- Dual degenerates: non-basic variables with zero reduced cost\n- Compatible columns/rows: can make positive improvement\n- Bi-dimensional pricing: balances reduced cost vs. compatibility\n\nThe psi parameter (default 0.5) controls priority given to compatibles.\nSmaller psi = more priority to compatible variables."], "see": ["ClpPEPrimalColumnSteepest, ClpPEDualRowSteepest for pivot implementations"], "has_pass2": true}, "src/AbcPrimalColumnPivot.hpp": {"path": "layer-1/Clp/src/AbcPrimalColumnPivot.hpp", "filename": "AbcPrimalColumnPivot.hpp", "file": "AbcPrimalColumnPivot.hpp", "brief": "Abstract base class for primal pivot column selection in ABC", "author": "John Forrest (FasterCoin, 2012)\n\nDefines the interface for choosing which column (nonbasic variable)\nshould enter the basis in primal simplex iterations within ABC.", "algorithm": "Weight Update Protocol:\n  updateWeights(): After pivot, update ||ā_j||² estimates\n  saveWeights(): Preserve weights across refactorizations", "see": ["AbcPrimalColumnDantzig for simple most-negative reduced cost", "AbcPrimalColumnSteepest for steepest edge/Devex variants", "ClpPrimalColumnPivot for the standard (non-ABC) interface"], "has_pass2": true}, "src/CoinAbcBaseFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcBaseFactorization.hpp", "filename": "CoinAbcBaseFactorization.hpp", "file": "CoinAbcBaseFactorization.hpp", "brief": "Core ABC SIMD-optimized LU factorization implementation", "author": "John Forrest (FasterCoin 2012)\n\nImplements CoinAbcTypeFactorization (type varies by #define), the main\nsparse LU factorization with SIMD optimizations. This file is included\nmultiple times with different macro settings to generate variants:\n\n- CoinAbcFactorization: Standard double precision\n- CoinAbcLongFactorization: Long double (COIN_BIG_DOUBLE=1)\n- CoinAbcSmallFactorization: Optimized for small matrices (ABC_SMALL=4)\n- CoinAbcOrderedFactorization: With ordered elimination\n\nKey optimizations over standard CoinFactorization:\n- SIMD scatter/gather operations (AVX when available)\n- Cache-aligned memory layout (BLOCKING8)\n- Cilk parallel factorization (ABC_PARALLEL=2)\n- Optimized pivot search (Markowitz with tie-breaking)\n\nStorage format:\n- L stored by rows (startRowL_) and columns (startColumnL_)\n- U stored in column-major with explicit row indices\n- Pivot region separate for fast diagonal access", "algorithm": "ABC (Alternative Basis Code) LU Factorization:\n  Enhanced sparse LU with modern CPU optimizations:\n  1. Markowitz pivot selection with singleton detection\n  2. SIMD-vectorized scatter/gather for column updates\n  3. Cache-blocking (BLOCKING8) for L2/L3 efficiency\n  4. Parallel elimination using Cilk (when ABC_PARALLEL=2)\n  5. Hyper-sparse mode for very sparse intermediate results", "complexity": "Factorization: O(nnz × fill-in), highly structure-dependent\n  FTRAN/BTRAN: O(nnz(L) + nnz(U)) with SIMD acceleration\n  SIMD speedup: 2-4x for dense column operations", "math": "B = L·U with row/column permutations P, Q:\n  P·B·Q = L·U, where L unit lower triangular, U upper triangular\n  Markowitz criterion: select pivot minimizing (r_i - 1)(c_j - 1)\n\nNote: 32 bits assumed sufficient for row/column counts,\nbut CoinBigIndex can be 64-bit for element indices.", "see": ["CoinAbcFactorization.hpp for instantiation", "CoinAbcHelperFunctions.hpp for SIMD kernels"], "has_pass2": true}, "src/ClpDynamicMatrix.hpp": {"path": "layer-1/Clp/src/ClpDynamicMatrix.hpp", "filename": "ClpDynamicMatrix.hpp", "file": "ClpDynamicMatrix.hpp", "brief": "Dynamic column generation matrix with GUB structure\n\nSupports column generation by maintaining a pool of potential columns\nand dynamically adding promising ones to the active working matrix.\nBuilt on GUB structure where each \"set\" can generate multiple columns.", "algorithm": "Column Generation with Dynamic Matrix:\n  Delayed column generation for large-scale LPs:\n  1. **Restricted Master:** Solve LP with subset of columns\n  2. **Pricing:** Use duals π to find column with c̄_j = c_j - π'A_j < 0\n     - createVariable() generates and adds promising column\n  3. **Pool Management:** Columns enter \"small\" working matrix from pool\n     - If working matrix too large: compress (remove non-basic columns)\n  4. **Re-optimize:** Solve updated master problem\n  5. **Termination:** No more negative reduced cost columns", "math": "Reduced cost pricing:\n  For column j in pool: c̄_j = c_j - Σ_i π_i · a_ij\n  If c̄_j < 0 for minimization: column can improve objective\n  GUB structure: Σ_{j∈S_k} x_j = 1 adds dual π_k to pricing", "complexity": "Per pricing: O(pool_size · column_length) to evaluate all\n  Compression: O(working_matrix_nnz) when triggered\n  Total: O(iterations · pricing) + O(simplex_pivots)\n\nColumn generation workflow:\n1. Solve restricted master problem with current columns\n2. Use dual prices to identify promising new columns (pricing subproblem)\n3. Add columns with negative reduced cost via createVariable()\n4. Re-optimize and repeat until no improving columns exist\n\nKey features:\n- Maintains both \"small\" working matrix and full column pool\n- Columns move between pool and working matrix as needed\n- Automatic compression when working matrix gets too large\n- Integrates with GUB structure for efficient handling", "see": ["ClpGubMatrix for the underlying GUB structure", "ClpPackedMatrix which this extends", "ClpSimplexPrimal which drives the column generation"], "has_pass2": true}, "src/CoinAbcCommonFactorization.hpp": {"path": "layer-1/Clp/src/CoinAbcCommonFactorization.hpp", "filename": "CoinAbcCommonFactorization.hpp", "file": "CoinAbcCommonFactorization.hpp", "brief": "Common infrastructure for ABC SIMD-optimized factorization", "algorithm": "ABC Factorization Infrastructure (LAPACK LU, Blocking)\nShared definitions, statistics tracking, and LAPACK interfaces for\nABC factorization variants. Provides:\n\n- ABC_SMALL modes: Controls memory copies for factorization\n  - -1: Force copies (no tests)\n  - 0: Force copy of U\n  - 2: Force no copies (no tests)\n\n- CoinAbcStatistics: Tracks element counts through L, R, U phases\n- CoinAbcStack: Stack structure for depth-first traversal\n- CoinAbcDgetrf/Dgetrs: LAPACK-compatible LU routines (homegrown or external)\n\nBlocking parameters for cache-friendly factorization:\n- BLOCKING8: 8-element blocks for SIMD alignment\n- SWAP_FACTOR: Controls swap threshold", "see": ["CoinAbcBaseFactorization for the main implementation", "CoinAbcDenseFactorization for dense submatrix handling"], "has_pass2": true}, "src/OsiClp/OsiClpSolverInterface.hpp": {"path": "layer-1/Clp/src/OsiClp/OsiClpSolverInterface.hpp", "filename": "OsiClpSolverInterface.hpp", "file": "OsiClpSolverInterface.hpp", "brief": "Osi interface for CLP simplex solver\nCopyright (C) 2000, International Business Machines Corporation.\nEPL-1.0 license.\n\nOsiClpSolverInterface wraps ClpSimplex via the Osi abstraction, enabling\nClp use within CBC and with Cgl cut generators. Key features: hot-starting\nfor strong branching (markHotStart/solveFromHotStart), simplex tableau\naccess (getBInvARow/getBInvACol), disaster recovery for numerical issues.", "see": ["ClpSimplex for underlying solver implementation", "OsiSolverInterface for base abstraction class"], "algorithm": "Hot-Start Optimization for Strong Branching:\n  1. markHotStart(): Save current basis, factorization, solution\n  2. Modify bounds (single variable typically)\n  3. solveFromHotStart(): Resolve with warm start (few iterations)\n  4. Repeat steps 2-3 for each branching candidate\n  5. unmarkHotStart(): Release saved state\n  This avoids full refactorization for each strong branching LP.", "complexity": "Most operations O(1) delegations to ClpSimplex\n  initialSolve/resolve: O(m·n·iterations) for underlying simplex\n  Hot-start resolve: O(k·m) where k = iterations to re-optimize (typically small)\n  getBInvARow/getBInvACol: O(nnz(B^{-1})) per call\n\n@invariant basis_ synchronized with modelPtr_->status arrays after each solve\n@invariant fakeMinInSimplex_ == true implies linearObjective_ is negated copy\n@invariant smallModel_ != NULL implies hot-start state is active", "ref": ["Lougee-Heimer, R., et al. (2003). \"The Common Optimization INterface for\n  Operations Research: Promoting open-source software in the operations\n  research community\". IBM J. Res. Dev. 47(1):57-66."], "param": ["indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": true}, "src/Attic/ClpParam.hpp": {"path": "layer-1/Clp/src/Attic/ClpParam.hpp", "filename": "ClpParam.hpp", "file": "ClpParam.hpp", "brief": "Parameter type enums for Clp and CBC command-line interface (legacy)\nCopyright (C) 2002, International Business Machines Corporation.\nEPL-1.0 license.\n\nClpParameterType enum: ranges 1-100 (double), 101-200 (int), 201-300\n(Clp string), 301-400 (Cbc string), 401-500 (Clp actions), 501-600 (Cbc\nactions). Parameters for tolerances, limits, scaling, algorithms, output.\nThis file is in Attic (deprecated) - see ClpParameters.hpp for current version.\n\n@deprecated Replaced by ClpParameters.hpp", "has_pass2": false}, "src/Clp_C_Interface.h": {"path": "layer-1/Clp/src/Clp_C_Interface.h", "filename": "Clp_C_Interface.h", "file": "Clp_C_Interface.h", "brief": "C language interface to Clp solver\n\nPure C API for embedding Clp in C programs or creating language bindings.\nDesign follows OSL V3 conventions for familiarity.\n\nOpaque handles:\n- Clp_Simplex: Pointer to internal ClpSimplex object\n- Clp_Solve: Pointer to ClpSolve options object\n\nNaming convention: C++ method foo() becomes Clp_foo(model, ...)\nwhere model is the first parameter.\n\nKey function groups:\n- Construction: Clp_newModel(), Clp_deleteModel()\n- Problem setup: Clp_loadProblem(), Clp_readMps()\n- Solving: Clp_dual(), Clp_primal(), Clp_initialSolve()\n- Solution access: Clp_getColSolution(), Clp_getRowActivity()\n- Parameters: Clp_setLogLevel(), Clp_setMaximumIterations()\n\nCallback support: clp_callback typedef for user message handling.\n\nThread safety: Each Clp_Simplex is independent; do not share across threads.", "algorithm": "Warm-Starting Protocol:\n  1. Solve initial LP: Clp_initialSolve()\n  2. Modify problem (bounds, RHS, etc.)\n  3. Re-solve from basis: Clp_dual() or Clp_primal()\n  Warm start typically requires O(k) iterations where k << m+n.", "complexity": "Same as underlying ClpSimplex methods:\n  Per-iteration: O(m²) average, O(m³) worst-case for factorization\n  Total: O(m·n·iterations) for complete solve", "see": ["ClpSimplex for the C++ class being wrapped", "ClpSolve for solve options"], "return": "infeasibility ray, or NULL returned if none/wrong.", "has_pass2": true}, "src/ClpConfig.h": {"path": "layer-1/Clp/src/ClpConfig.h", "filename": "ClpConfig.h", "file": "ClpConfig.h", "brief": "Clp configuration and platform detection\n\nHandles build configuration for cross-platform compatibility:\n- HAVE_CONFIG_H: Uses autoconf-generated config.h\n- CLPLIB_BUILD: Building the library vs. using it\n- DLL_EXPORT: Windows DLL export declarations\n- GCC visibility: Hidden visibility with explicit exports\n\nDefines CLPLIB_EXPORT macro for public API symbols.\n\nConfiguration precedence:\n1. config.h (autoconf) when HAVE_CONFIG_H defined\n2. config_clp.h for configured installations\n3. config_clp_default.h for manual builds", "has_pass2": false}}}, "CppAD": {"name": "CppAD", "file_count": 310, "pass2_count": 7, "files": {"val_graph/atomic_xam.hpp": {"path": "layer-1/CppAD/val_graph/atomic_xam.hpp", "filename": "atomic_xam.hpp", "file": "atomic_xam.hpp", "brief": "CppAD: atomic xam", "has_pass2": false}, "introduction/exp_eps.hpp": {"path": "layer-1/CppAD/introduction/exp_eps.hpp", "filename": "exp_eps.hpp", "file": "exp_eps.hpp", "brief": "CppAD: exp eps", "has_pass2": false}, "introduction/exp_2.hpp": {"path": "layer-1/CppAD/introduction/exp_2.hpp", "filename": "exp_2.hpp", "file": "exp_2.hpp", "brief": "CppAD: exp 2", "has_pass2": false}, "cppad_ipopt/src/cppad_ipopt_nlp.hpp": {"path": "layer-1/CppAD/cppad_ipopt/src/cppad_ipopt_nlp.hpp", "filename": "cppad_ipopt_nlp.hpp", "file": "cppad_ipopt_nlp.hpp", "brief": "Ipopt interface: cppad_ipopt_nlp", "has_pass2": false}, "include/cppad/utility.hpp": {"path": "layer-1/CppAD/include/cppad/utility.hpp", "filename": "utility.hpp", "file": "utility.hpp", "brief": "CppAD utility: utility", "has_pass2": false}, "include/cppad/cppad.hpp": {"path": "layer-1/CppAD/include/cppad/cppad.hpp", "filename": "cppad.hpp", "file": "cppad.hpp", "brief": "Contains all variables and functions defined by CppAD package", "algorithm": "Operator overloading records a \"tape\" of operations.\n           Forward mode: propagates derivatives from inputs to outputs.\n           Reverse mode: propagates derivatives from outputs to inputs.\n           Reverse mode is more efficient for many inputs, few outputs.", "ref": ["Griewank, Walther (2008). \"Evaluating Derivatives: Principles and\n     Techniques of Algorithmic Differentiation\", 2nd ed. SIAM."], "see": ["AD for the automatic differentiation scalar type", "ADFun for the recorded function object", "Ipopt for nonlinear optimization using CppAD derivatives\n\n@namespace CppAD"], "has_pass2": true}, "include/cppad/base_require.hpp": {"path": "layer-1/CppAD/include/cppad/base_require.hpp", "filename": "base_require.hpp", "file": "base_require.hpp", "brief": "CppAD: base require", "has_pass2": false}, "include/cppad/wno_conversion.hpp": {"path": "layer-1/CppAD/include/cppad/wno_conversion.hpp", "filename": "wno_conversion.hpp", "file": "wno_conversion.hpp", "brief": "CppAD: wno conversion", "has_pass2": false}, "include/cppad/ipopt/solve.hpp": {"path": "layer-1/CppAD/include/cppad/ipopt/solve.hpp", "filename": "solve.hpp", "file": "solve.hpp", "brief": "Ipopt interface: solve", "has_pass2": false}, "include/cppad/local/ad_tape.hpp": {"path": "layer-1/CppAD/include/cppad/local/ad_tape.hpp", "filename": "ad_tape.hpp", "file": "ad_tape.hpp", "brief": "Internal implementation: ad_tape", "has_pass2": false}, "include/cppad/local/op_code_dyn.hpp": {"path": "layer-1/CppAD/include/cppad/local/op_code_dyn.hpp", "filename": "op_code_dyn.hpp", "file": "op_code_dyn.hpp", "brief": "Internal implementation: op_code_dyn", "has_pass2": false}, "include/cppad/local/atomic_index.hpp": {"path": "layer-1/CppAD/include/cppad/local/atomic_index.hpp", "filename": "atomic_index.hpp", "file": "atomic_index.hpp", "brief": "Internal implementation: atomic_index", "has_pass2": false}, "include/cppad/local/op_code_var.hpp": {"path": "layer-1/CppAD/include/cppad/local/op_code_var.hpp", "filename": "op_code_var.hpp", "file": "op_code_var.hpp", "brief": "Internal implementation: op_code_var", "has_pass2": false}, "include/cppad/local/is_pod.hpp": {"path": "layer-1/CppAD/include/cppad/local/is_pod.hpp", "filename": "is_pod.hpp", "file": "is_pod.hpp", "brief": "Internal implementation: is_pod", "has_pass2": false}, "include/cppad/local/set_get_in_parallel.hpp": {"path": "layer-1/CppAD/include/cppad/local/set_get_in_parallel.hpp", "filename": "set_get_in_parallel.hpp", "file": "set_get_in_parallel.hpp", "brief": "Internal implementation: set_get_in_parallel", "has_pass2": false}, "include/cppad/local/atom_state.hpp": {"path": "layer-1/CppAD/include/cppad/local/atom_state.hpp", "filename": "atom_state.hpp", "file": "atom_state.hpp", "brief": "Internal implementation: atom_state", "has_pass2": false}, "include/cppad/local/temp_file.hpp": {"path": "layer-1/CppAD/include/cppad/local/temp_file.hpp", "filename": "temp_file.hpp", "file": "temp_file.hpp", "brief": "Internal implementation: temp_file", "has_pass2": false}, "include/cppad/local/pod_vector.hpp": {"path": "layer-1/CppAD/include/cppad/local/pod_vector.hpp", "filename": "pod_vector.hpp", "file": "pod_vector.hpp", "brief": "Internal implementation: pod_vector", "has_pass2": false}, "include/cppad/core/sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/sparse_jac.hpp", "filename": "sparse_jac.hpp", "file": "sparse_jac.hpp", "brief": "Sparse Jacobian computation using graph coloring", "algorithm": "Reverse Mode Sparse Jacobian (sparse_jac_rev):\nCompute multiple rows per sweep using row coloring.\n\n  For each color c:\n    1. Set adjoint seed: u_i = 1 if color[i]==c, else 0\n    2. Reverse sweep: v = J'·u\n    3. Extract: J_{ij} = v_j for rows i with color c\n\n  CHOICE:\n    sparse_jac_for better when n << m (tall Jacobian)\n    sparse_jac_rev better when m << n (wide Jacobian)", "math": "Jacobian: J_{ij} = ∂F_i/∂x_j\n\n  KEY INSIGHT:\n    If columns j and k have disjoint row sparsity patterns,\n    they can be computed simultaneously in one forward sweep.\n\n  GRAPH COLORING:\n    Column intersection graph: edge (j,k) if some row i has\n    both J_{ij} ≠ 0 and J_{ik} ≠ 0\n    Color columns so adjacent columns get different colors\n    #forward_sweeps = #colors (much less than n for sparse J)", "complexity": "Graph coloring: O(nnz + n) for greedy\n  Jacobian: O(colors × tape_ops)\n  For many NLP problems: colors ≈ 5-20 regardless of n", "see": ["sparse_hes.hpp for sparse Hessian", "coloring algorithms in local/color_*.hpp"], "has_pass2": true}, "include/cppad/core/sparse.hpp": {"path": "layer-1/CppAD/include/cppad/core/sparse.hpp", "filename": "sparse.hpp", "file": "sparse.hpp", "brief": "Core AD functionality: sparse", "has_pass2": false}, "include/cppad/core/dependent.hpp": {"path": "layer-1/CppAD/include/cppad/core/dependent.hpp", "filename": "dependent.hpp", "file": "dependent.hpp", "brief": "Core AD functionality: dependent", "has_pass2": false}, "include/cppad/core/optimize.hpp": {"path": "layer-1/CppAD/include/cppad/core/optimize.hpp", "filename": "optimize.hpp", "file": "optimize.hpp", "brief": "Core AD functionality: optimize", "has_pass2": false}, "include/cppad/core/bool_fun.hpp": {"path": "layer-1/CppAD/include/cppad/core/bool_fun.hpp", "filename": "bool_fun.hpp", "file": "bool_fun.hpp", "brief": "Core AD functionality: bool fun", "has_pass2": false}, "include/cppad/core/sparse_jacobian.hpp": {"path": "layer-1/CppAD/include/cppad/core/sparse_jacobian.hpp", "filename": "sparse_jacobian.hpp", "file": "sparse_jacobian.hpp", "brief": "Core AD functionality: sparse jacobian", "has_pass2": false}, "include/cppad/core/user_ad.hpp": {"path": "layer-1/CppAD/include/cppad/core/user_ad.hpp", "filename": "user_ad.hpp", "file": "user_ad.hpp", "brief": "Core AD functionality: user ad", "has_pass2": false}, "include/cppad/core/rev_jac_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/rev_jac_sparsity.hpp", "filename": "rev_jac_sparsity.hpp", "file": "rev_jac_sparsity.hpp", "brief": "Core AD functionality: rev jac sparsity", "has_pass2": false}, "include/cppad/core/unary_plus.hpp": {"path": "layer-1/CppAD/include/cppad/core/unary_plus.hpp", "filename": "unary_plus.hpp", "file": "unary_plus.hpp", "brief": "Core AD functionality: unary plus", "has_pass2": false}, "include/cppad/core/con_dyn_var.hpp": {"path": "layer-1/CppAD/include/cppad/core/con_dyn_var.hpp", "filename": "con_dyn_var.hpp", "file": "con_dyn_var.hpp", "brief": "Core AD functionality: con dyn var", "has_pass2": false}, "include/cppad/core/for_jac_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/for_jac_sparsity.hpp", "filename": "for_jac_sparsity.hpp", "file": "for_jac_sparsity.hpp", "brief": "Core AD functionality: for jac sparsity", "has_pass2": false}, "include/cppad/core/reverse.hpp": {"path": "layer-1/CppAD/include/cppad/core/reverse.hpp", "filename": "reverse.hpp", "file": "core/reverse.hpp", "brief": "Reverse mode automatic differentiation", "algorithm": "Reverse Mode AD (Backpropagation):\nComputes gradients by traversing the computation graph backwards.\nKey advantage: gradient of scalar function w.r.t. all inputs in O(ops).\n\nFORWARD SWEEP (already done):\n  Compute y = f(x) while recording operation tape\n  Store intermediate values at each node\n\nREVERSE SWEEP:\n  Initialize: ȳ = 1 (adjoint of output)\n  For each operation v = op(u₁, u₂, ...) in reverse order:\n    Compute: ūᵢ += ȳ · ∂v/∂uᵢ (chain rule)\n  Final: x̄ = gradient ∂y/∂x\n\nHIGHER ORDER REVERSE:\n  With q-th order forward coefficients stored:\n  Computes ∂W/∂x where W = Σₖ wₖ · yₖ\n  Returns q Taylor coefficient derivatives per variable\n\nADJOINT EQUATIONS:\n  For v = u₁ + u₂:  ū₁ += v̄, ū₂ += v̄\n  For v = u₁ · u₂:  ū₁ += v̄·u₂, ū₂ += v̄·u₁\n  For v = sin(u):   ū += v̄·cos(u)\n  For v = exp(u):   ū += v̄·v", "math": "Complexity comparison for f: Rⁿ → Rᵐ:\n  Forward mode: O(n·ops) for full Jacobian (n forward sweeps)\n  Reverse mode: O(m·ops) for full Jacobian (m reverse sweeps)\n  For gradient (m=1): reverse is O(ops), forward is O(n·ops)\n\nMemory: O(ops) to store forward sweep values for reverse", "complexity": "- Single reverse sweep: O(ops) time, O(ops) space\n- Gradient of scalar function: O(ops) regardless of input dimension\n- Full Jacobian: O(min(n,m)·ops) using appropriate mode", "ref": ["Griewank & Walther (2008). \"Evaluating Derivatives: Principles and\n  Techniques of Algorithmic Differentiation\". 2nd ed., SIAM."], "see": ["cppad/core/forward.hpp for forward mode", "cppad/core/sparse_hes.hpp for sparse Hessian via reverse-on-forward"], "has_pass2": true}, "include/cppad/core/convert.hpp": {"path": "layer-1/CppAD/include/cppad/core/convert.hpp", "filename": "convert.hpp", "file": "convert.hpp", "brief": "Core AD functionality: convert", "has_pass2": false}, "include/cppad/core/sub_eq.hpp": {"path": "layer-1/CppAD/include/cppad/core/sub_eq.hpp", "filename": "sub_eq.hpp", "file": "sub_eq.hpp", "brief": "Core AD functionality: sub eq", "has_pass2": false}, "include/cppad/core/fun_eval.hpp": {"path": "layer-1/CppAD/include/cppad/core/fun_eval.hpp", "filename": "fun_eval.hpp", "file": "fun_eval.hpp", "brief": "Core AD functionality: fun eval", "has_pass2": false}, "include/cppad/core/sign.hpp": {"path": "layer-1/CppAD/include/cppad/core/sign.hpp", "filename": "sign.hpp", "file": "sign.hpp", "brief": "Core AD functionality: sign", "has_pass2": false}, "include/cppad/core/subgraph_jac_rev.hpp": {"path": "layer-1/CppAD/include/cppad/core/subgraph_jac_rev.hpp", "filename": "subgraph_jac_rev.hpp", "file": "subgraph_jac_rev.hpp", "brief": "Core AD functionality: subgraph jac rev", "has_pass2": false}, "include/cppad/core/epsilon.hpp": {"path": "layer-1/CppAD/include/cppad/core/epsilon.hpp", "filename": "epsilon.hpp", "file": "epsilon.hpp", "brief": "Core AD functionality: epsilon", "has_pass2": false}, "include/cppad/core/ad_valued.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_valued.hpp", "filename": "ad_valued.hpp", "file": "ad_valued.hpp", "brief": "Core AD functionality: ad valued", "has_pass2": false}, "include/cppad/core/drivers.hpp": {"path": "layer-1/CppAD/include/cppad/core/drivers.hpp", "filename": "drivers.hpp", "file": "drivers.hpp", "brief": "Core AD functionality: drivers", "has_pass2": false}, "include/cppad/core/abort_recording.hpp": {"path": "layer-1/CppAD/include/cppad/core/abort_recording.hpp", "filename": "abort_recording.hpp", "file": "abort_recording.hpp", "brief": "Core AD functionality: abort recording", "has_pass2": false}, "include/cppad/core/for_one.hpp": {"path": "layer-1/CppAD/include/cppad/core/for_one.hpp", "filename": "for_one.hpp", "file": "for_one.hpp", "brief": "Core AD functionality: for one", "has_pass2": false}, "include/cppad/core/div.hpp": {"path": "layer-1/CppAD/include/cppad/core/div.hpp", "filename": "div.hpp", "file": "div.hpp", "brief": "Core AD functionality: div", "has_pass2": false}, "include/cppad/core/mul_eq.hpp": {"path": "layer-1/CppAD/include/cppad/core/mul_eq.hpp", "filename": "mul_eq.hpp", "file": "mul_eq.hpp", "brief": "Core AD functionality: mul eq", "has_pass2": false}, "include/cppad/core/opt_val_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/opt_val_hes.hpp", "filename": "opt_val_hes.hpp", "file": "opt_val_hes.hpp", "brief": "Core AD functionality: opt val hes", "has_pass2": false}, "include/cppad/core/compound_assign.hpp": {"path": "layer-1/CppAD/include/cppad/core/compound_assign.hpp", "filename": "compound_assign.hpp", "file": "compound_assign.hpp", "brief": "Core AD functionality: compound assign", "has_pass2": false}, "include/cppad/core/integer.hpp": {"path": "layer-1/CppAD/include/cppad/core/integer.hpp", "filename": "integer.hpp", "file": "integer.hpp", "brief": "Core AD functionality: integer", "has_pass2": false}, "include/cppad/core/compare.hpp": {"path": "layer-1/CppAD/include/cppad/core/compare.hpp", "filename": "compare.hpp", "file": "compare.hpp", "brief": "Core AD functionality: compare", "has_pass2": false}, "include/cppad/core/ad_fun.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_fun.hpp", "filename": "ad_fun.hpp", "file": "ad_fun.hpp", "brief": "ADFun class - function object storing recorded AD operations\n\nADFun<Base> stores a recorded sequence of operations and provides\nmethods to evaluate the function and its derivatives.\n\nKey methods:\n- **Forward(p, x)**: p-th order forward mode Taylor coefficient\n- **Reverse(q, w)**: q-th order reverse mode derivative\n- **Jacobian(x)**: Full Jacobian matrix\n- **Hessian(x, w)**: Weighted Hessian matrix\n- **SparseJacobian/SparseHessian**: Sparse derivative computation", "algorithm": "Forward mode computes derivatives in the direction of inputs.\n           Reverse mode computes derivatives from outputs backward.\n           Forward: O(n) per direction, Reverse: O(m) per output.\n           Use forward when n < m, reverse when n > m.", "see": ["AD<Base> for the scalar type that records operations"], "tparam": ["Base Underlying numeric type (typically double)", "RecBase Type used during recording (usually same as Base)\n\nADFun stores a \"tape\" of operations recorded using AD<Base> scalars.\nThis tape can be used to efficiently compute:\n- Function values at new points\n- Jacobians (first derivatives)\n- Hessians (second derivatives)\n- Higher-order Taylor coefficients\n- Sparsity patterns\n\nThe recorded operations can be optimized to remove redundancies.\n\n@invariant Once constructed, the function signature (n inputs, m outputs) is fixed"], "has_pass2": true}, "include/cppad/core/value.hpp": {"path": "layer-1/CppAD/include/cppad/core/value.hpp", "filename": "value.hpp", "file": "value.hpp", "brief": "Core AD functionality: value", "has_pass2": false}, "include/cppad/core/unary_minus.hpp": {"path": "layer-1/CppAD/include/cppad/core/unary_minus.hpp", "filename": "unary_minus.hpp", "file": "unary_minus.hpp", "brief": "Core AD functionality: unary minus", "has_pass2": false}, "include/cppad/core/ad_io.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_io.hpp", "filename": "ad_io.hpp", "file": "ad_io.hpp", "brief": "Core AD functionality: ad io", "has_pass2": false}, "include/cppad/core/arithmetic.hpp": {"path": "layer-1/CppAD/include/cppad/core/arithmetic.hpp", "filename": "arithmetic.hpp", "file": "arithmetic.hpp", "brief": "Core AD functionality: arithmetic", "has_pass2": false}, "include/cppad/core/subgraph_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/subgraph_sparsity.hpp", "filename": "subgraph_sparsity.hpp", "file": "subgraph_sparsity.hpp", "brief": "Core AD functionality: subgraph sparsity", "has_pass2": false}, "include/cppad/core/pow.hpp": {"path": "layer-1/CppAD/include/cppad/core/pow.hpp", "filename": "pow.hpp", "file": "pow.hpp", "brief": "Core AD functionality: pow", "has_pass2": false}, "include/cppad/core/num_skip.hpp": {"path": "layer-1/CppAD/include/cppad/core/num_skip.hpp", "filename": "num_skip.hpp", "file": "num_skip.hpp", "brief": "Core AD functionality: num skip", "has_pass2": false}, "include/cppad/core/base_double.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_double.hpp", "filename": "base_double.hpp", "file": "base_double.hpp", "brief": "Core AD functionality: base double", "has_pass2": false}, "include/cppad/core/ad_to_string.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_to_string.hpp", "filename": "ad_to_string.hpp", "file": "ad_to_string.hpp", "brief": "Core AD functionality: ad to string", "has_pass2": false}, "include/cppad/core/sparse_hessian.hpp": {"path": "layer-1/CppAD/include/cppad/core/sparse_hessian.hpp", "filename": "sparse_hessian.hpp", "file": "sparse_hessian.hpp", "brief": "Core AD functionality: sparse hessian", "has_pass2": false}, "include/cppad/core/for_two.hpp": {"path": "layer-1/CppAD/include/cppad/core/for_two.hpp", "filename": "for_two.hpp", "file": "for_two.hpp", "brief": "Core AD functionality: for two", "has_pass2": false}, "include/cppad/core/ad.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad.hpp", "filename": "ad.hpp", "file": "ad.hpp", "brief": "Core AD<Base> automatic differentiation scalar type\n\nAD<Base> is the fundamental type for automatic differentiation.\nOperations on AD<Base> values are recorded on a \"tape\" which can\nlater be used to compute derivatives.", "algorithm": "AD Type Classification:\nThree types of AD values with different recording behavior.\n\n  CONSTANT (ad_type_ = constant_enum):\n    - Value known at tape creation time\n    - Not recorded (optimizes tape size)\n    - Example: AD<double> c = 3.14;\n\n  PARAMETER (ad_type_ = dynamic_enum):\n    - Value can change between derivative computations\n    - Recorded as \"dynamic parameter\"\n    - Useful for problem data that varies\n\n  VARIABLE (ad_type_ = variable_enum):\n    - Depends on independent variables\n    - Always recorded on tape\n    - Derivatives computed w.r.t. these\n\nAD types:\n- **Constant**: Value known, not recorded on tape\n- **Parameter**: Value known, may be recorded for dynamic parameters\n- **Variable**: Value depends on independent variables, recorded on tape", "complexity": "Recording: O(1) per operation\n  Memory: O(tape_length × coefficient_order)", "see": ["Independent() to declare independent variables", "ADFun to create a function from recorded operations"], "tparam": ["Base Underlying numeric type (typically double)\n\nAD<Base> overloads arithmetic operators to record operations\non a tape for later derivative computation.\n\nUsage:\n@code\nvector< AD<double> > x(n);\nIndependent(x);           // Mark as independent variables\nAD<double> y = f(x);      // Operations are recorded\nADFun<double> fun(x, y);  // Create function object\n@endcode\n\nMember data:\n- value_: Current numerical value (Base type)\n- tape_id_: ID of tape recording this variable\n- taddr_: Address in the tape\n- ad_type_: Whether constant/parameter/variable"], "has_pass2": true}, "include/cppad/core/sub.hpp": {"path": "layer-1/CppAD/include/cppad/core/sub.hpp", "filename": "sub.hpp", "file": "sub.hpp", "brief": "Core AD functionality: sub", "has_pass2": false}, "include/cppad/core/for_sparse_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/for_sparse_hes.hpp", "filename": "for_sparse_hes.hpp", "file": "for_sparse_hes.hpp", "brief": "Core AD functionality: for sparse hes", "has_pass2": false}, "include/cppad/core/fun_construct.hpp": {"path": "layer-1/CppAD/include/cppad/core/fun_construct.hpp", "filename": "fun_construct.hpp", "file": "fun_construct.hpp", "brief": "Core AD functionality: fun construct", "has_pass2": false}, "include/cppad/core/capacity_order.hpp": {"path": "layer-1/CppAD/include/cppad/core/capacity_order.hpp", "filename": "capacity_order.hpp", "file": "capacity_order.hpp", "brief": "Core AD functionality: capacity order", "has_pass2": false}, "include/cppad/core/mul.hpp": {"path": "layer-1/CppAD/include/cppad/core/mul.hpp", "filename": "mul.hpp", "file": "mul.hpp", "brief": "Core AD functionality: mul", "has_pass2": false}, "include/cppad/core/std_math_11.hpp": {"path": "layer-1/CppAD/include/cppad/core/std_math_11.hpp", "filename": "std_math_11.hpp", "file": "std_math_11.hpp", "brief": "Core AD functionality: std math 11", "has_pass2": false}, "include/cppad/core/new_dynamic.hpp": {"path": "layer-1/CppAD/include/cppad/core/new_dynamic.hpp", "filename": "new_dynamic.hpp", "file": "new_dynamic.hpp", "brief": "Core AD functionality: new dynamic", "has_pass2": false}, "include/cppad/core/base_std_math.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_std_math.hpp", "filename": "base_std_math.hpp", "file": "base_std_math.hpp", "brief": "Core AD functionality: base std math", "has_pass2": false}, "include/cppad/core/parallel_ad.hpp": {"path": "layer-1/CppAD/include/cppad/core/parallel_ad.hpp", "filename": "parallel_ad.hpp", "file": "parallel_ad.hpp", "brief": "Core AD functionality: parallel ad", "has_pass2": false}, "include/cppad/core/base_hash.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_hash.hpp", "filename": "base_hash.hpp", "file": "base_hash.hpp", "brief": "Core AD functionality: base hash", "has_pass2": false}, "include/cppad/core/rev_hes_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/rev_hes_sparsity.hpp", "filename": "rev_hes_sparsity.hpp", "file": "rev_hes_sparsity.hpp", "brief": "Core AD functionality: rev hes sparsity", "has_pass2": false}, "include/cppad/core/rev_one.hpp": {"path": "layer-1/CppAD/include/cppad/core/rev_one.hpp", "filename": "rev_one.hpp", "file": "rev_one.hpp", "brief": "Core AD functionality: rev one", "has_pass2": false}, "include/cppad/core/var2par.hpp": {"path": "layer-1/CppAD/include/cppad/core/var2par.hpp", "filename": "var2par.hpp", "file": "var2par.hpp", "brief": "Core AD functionality: var2par", "has_pass2": false}, "include/cppad/core/abs.hpp": {"path": "layer-1/CppAD/include/cppad/core/abs.hpp", "filename": "abs.hpp", "file": "abs.hpp", "brief": "Core AD functionality: abs", "has_pass2": false}, "include/cppad/core/numeric_limits.hpp": {"path": "layer-1/CppAD/include/cppad/core/numeric_limits.hpp", "filename": "numeric_limits.hpp", "file": "numeric_limits.hpp", "brief": "Core AD functionality: numeric limits", "has_pass2": false}, "include/cppad/core/print_for.hpp": {"path": "layer-1/CppAD/include/cppad/core/print_for.hpp", "filename": "print_for.hpp", "file": "print_for.hpp", "brief": "Core AD functionality: print for", "has_pass2": false}, "include/cppad/core/base_float.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_float.hpp", "filename": "base_float.hpp", "file": "base_float.hpp", "brief": "Core AD functionality: base float", "has_pass2": false}, "include/cppad/core/undef.hpp": {"path": "layer-1/CppAD/include/cppad/core/undef.hpp", "filename": "undef.hpp", "file": "undef.hpp", "brief": "Core AD functionality: undef", "has_pass2": false}, "include/cppad/core/cond_exp.hpp": {"path": "layer-1/CppAD/include/cppad/core/cond_exp.hpp", "filename": "cond_exp.hpp", "file": "cond_exp.hpp", "brief": "Core AD functionality: cond exp", "has_pass2": false}, "include/cppad/core/jacobian.hpp": {"path": "layer-1/CppAD/include/cppad/core/jacobian.hpp", "filename": "jacobian.hpp", "file": "jacobian.hpp", "brief": "Core AD functionality: jacobian", "has_pass2": false}, "include/cppad/core/bender_quad.hpp": {"path": "layer-1/CppAD/include/cppad/core/bender_quad.hpp", "filename": "bender_quad.hpp", "file": "bender_quad.hpp", "brief": "Core AD functionality: bender quad", "has_pass2": false}, "include/cppad/core/fun_check.hpp": {"path": "layer-1/CppAD/include/cppad/core/fun_check.hpp", "filename": "fun_check.hpp", "file": "fun_check.hpp", "brief": "Core AD functionality: fun check", "has_pass2": false}, "include/cppad/core/rev_two.hpp": {"path": "layer-1/CppAD/include/cppad/core/rev_two.hpp", "filename": "rev_two.hpp", "file": "rev_two.hpp", "brief": "Core AD functionality: rev two", "has_pass2": false}, "include/cppad/core/bool_valued.hpp": {"path": "layer-1/CppAD/include/cppad/core/bool_valued.hpp", "filename": "bool_valued.hpp", "file": "bool_valued.hpp", "brief": "Core AD functionality: bool valued", "has_pass2": false}, "include/cppad/core/standard_math.hpp": {"path": "layer-1/CppAD/include/cppad/core/standard_math.hpp", "filename": "standard_math.hpp", "file": "standard_math.hpp", "brief": "Core AD functionality: standard math", "has_pass2": false}, "include/cppad/core/rev_sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/rev_sparse_jac.hpp", "filename": "rev_sparse_jac.hpp", "file": "rev_sparse_jac.hpp", "brief": "Core AD functionality: rev sparse jac", "has_pass2": false}, "include/cppad/core/add.hpp": {"path": "layer-1/CppAD/include/cppad/core/add.hpp", "filename": "add.hpp", "file": "add.hpp", "brief": "Core AD functionality: add", "has_pass2": false}, "include/cppad/core/sparse_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/sparse_hes.hpp", "filename": "sparse_hes.hpp", "file": "sparse_hes.hpp", "brief": "Sparse Hessian computation using automatic differentiation", "algorithm": "Sparse Hessian Computation:\nEfficiently compute Hessian exploiting sparsity structure via\ngraph coloring and reverse-on-forward AD.\n\nHESSIAN STRUCTURE:\n  H(x) = ∇²(w'F(x)) = Σᵢ wᵢ·∇²Fᵢ(x)\n  Only compute nonzero entries H[i,j] identified by sparsity pattern\n\nGRAPH COLORING:\n  Group columns by color c₁, c₂, ... where columns in same group\n  share no row position (can be computed simultaneously)\n  Direction d^(k) = Σⱼ∈cₖ eⱼ (sum of unit vectors in color k)\n\nCOMPRESSED COMPUTATION:\n  For each color k:\n    1. Forward sweep: compute v = ∇F(x)·d^(k)\n    2. Reverse sweep: compute ∇(w'v) = H·d^(k)\n  Extract individual H[:,j] from compressed result\n\nREVERSE-ON-FORWARD (second-order):\n  Forward mode: propagate directional derivative\n  Reverse mode: compute gradient of directional derivative\n  Combined: extracts Hessian-vector product", "math": "Full Hessian: O(n²) entries\n  Sparse Hessian: O(nnz) entries to compute\n  With p colors (from coloring): p forward + p reverse sweeps\n  Each sweep: O(ops) operations\n\nColoring algorithms:\n- \"cppad.general\": Row determination (conservative)\n- \"cppad.symmetric\": Exploits H = H' for half the work", "complexity": "- Sparsity detection: O(ops·nnz_pattern)\n- Graph coloring: O(n² + nnz) for greedy coloring\n- Hessian computation: O(p·ops) where p = chromatic number\n- p ≤ nnz_per_row + 1 typically", "ref": ["Coleman & Moré (1984). \"Estimation of Sparse Hessian Matrices\n  and Graph Coloring Problems\". Mathematical Programming 28:243-270."], "see": ["sparse_jac.hpp for sparse Jacobian", "rev_hes_sparsity.hpp for Hessian sparsity patterns"], "has_pass2": true}, "include/cppad/core/equal_op_seq.hpp": {"path": "layer-1/CppAD/include/cppad/core/equal_op_seq.hpp", "filename": "equal_op_seq.hpp", "file": "equal_op_seq.hpp", "brief": "Core AD functionality: equal op seq", "has_pass2": false}, "include/cppad/core/base_complex.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_complex.hpp", "filename": "base_complex.hpp", "file": "base_complex.hpp", "brief": "Core AD functionality: base complex", "has_pass2": false}, "include/cppad/core/lu_ratio.hpp": {"path": "layer-1/CppAD/include/cppad/core/lu_ratio.hpp", "filename": "lu_ratio.hpp", "file": "lu_ratio.hpp", "brief": "Core AD functionality: lu ratio", "has_pass2": false}, "include/cppad/core/add_eq.hpp": {"path": "layer-1/CppAD/include/cppad/core/add_eq.hpp", "filename": "add_eq.hpp", "file": "add_eq.hpp", "brief": "Core AD functionality: add eq", "has_pass2": false}, "include/cppad/core/near_equal_ext.hpp": {"path": "layer-1/CppAD/include/cppad/core/near_equal_ext.hpp", "filename": "near_equal_ext.hpp", "file": "near_equal_ext.hpp", "brief": "Core AD functionality: near equal ext", "has_pass2": false}, "include/cppad/core/for_sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/for_sparse_jac.hpp", "filename": "for_sparse_jac.hpp", "file": "for_sparse_jac.hpp", "brief": "Core AD functionality: for sparse jac", "has_pass2": false}, "include/cppad/core/check_for_nan.hpp": {"path": "layer-1/CppAD/include/cppad/core/check_for_nan.hpp", "filename": "check_for_nan.hpp", "file": "check_for_nan.hpp", "brief": "Core AD functionality: check for nan", "has_pass2": false}, "include/cppad/core/base_cond_exp.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_cond_exp.hpp", "filename": "base_cond_exp.hpp", "file": "base_cond_exp.hpp", "brief": "Core AD functionality: base cond exp", "has_pass2": false}, "include/cppad/core/ad_binary.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_binary.hpp", "filename": "ad_binary.hpp", "file": "ad_binary.hpp", "brief": "Core AD functionality: ad binary", "has_pass2": false}, "include/cppad/core/base_to_string.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_to_string.hpp", "filename": "base_to_string.hpp", "file": "base_to_string.hpp", "brief": "Core AD functionality: base to string", "has_pass2": false}, "include/cppad/core/hessian.hpp": {"path": "layer-1/CppAD/include/cppad/core/hessian.hpp", "filename": "hessian.hpp", "file": "hessian.hpp", "brief": "Core AD functionality: hessian", "has_pass2": false}, "include/cppad/core/rev_sparse_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/rev_sparse_hes.hpp", "filename": "rev_sparse_hes.hpp", "file": "rev_sparse_hes.hpp", "brief": "Core AD functionality: rev sparse hes", "has_pass2": false}, "include/cppad/core/zdouble.hpp": {"path": "layer-1/CppAD/include/cppad/core/zdouble.hpp", "filename": "zdouble.hpp", "file": "zdouble.hpp", "brief": "Core AD functionality: zdouble", "has_pass2": false}, "include/cppad/core/abs_normal_fun.hpp": {"path": "layer-1/CppAD/include/cppad/core/abs_normal_fun.hpp", "filename": "abs_normal_fun.hpp", "file": "abs_normal_fun.hpp", "brief": "Core AD functionality: abs normal fun", "has_pass2": false}, "include/cppad/core/azmul.hpp": {"path": "layer-1/CppAD/include/cppad/core/azmul.hpp", "filename": "azmul.hpp", "file": "azmul.hpp", "brief": "Core AD functionality: azmul", "has_pass2": false}, "include/cppad/core/base_limits.hpp": {"path": "layer-1/CppAD/include/cppad/core/base_limits.hpp", "filename": "base_limits.hpp", "file": "base_limits.hpp", "brief": "Core AD functionality: base limits", "has_pass2": false}, "include/cppad/core/for_hes_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/for_hes_sparsity.hpp", "filename": "for_hes_sparsity.hpp", "file": "for_hes_sparsity.hpp", "brief": "Core AD functionality: for hes sparsity", "has_pass2": false}, "include/cppad/core/atan2.hpp": {"path": "layer-1/CppAD/include/cppad/core/atan2.hpp", "filename": "atan2.hpp", "file": "atan2.hpp", "brief": "Core AD functionality: atan2", "has_pass2": false}, "include/cppad/core/omp_max_thread.hpp": {"path": "layer-1/CppAD/include/cppad/core/omp_max_thread.hpp", "filename": "omp_max_thread.hpp", "file": "omp_max_thread.hpp", "brief": "Core AD functionality: omp max thread", "has_pass2": false}, "include/cppad/core/ad_type.hpp": {"path": "layer-1/CppAD/include/cppad/core/ad_type.hpp", "filename": "ad_type.hpp", "file": "ad_type.hpp", "brief": "Core AD functionality: ad type", "has_pass2": false}, "include/cppad/core/subgraph_reverse.hpp": {"path": "layer-1/CppAD/include/cppad/core/subgraph_reverse.hpp", "filename": "subgraph_reverse.hpp", "file": "subgraph_reverse.hpp", "brief": "Core AD functionality: subgraph reverse", "has_pass2": false}, "include/cppad/core/div_eq.hpp": {"path": "layer-1/CppAD/include/cppad/core/div_eq.hpp", "filename": "div_eq.hpp", "file": "div_eq.hpp", "brief": "Core AD functionality: div eq", "has_pass2": false}, "include/cppad/core/to_csrc.hpp": {"path": "layer-1/CppAD/include/cppad/core/to_csrc.hpp", "filename": "to_csrc.hpp", "file": "to_csrc.hpp", "brief": "Core AD functionality: to csrc", "has_pass2": false}, "include/cppad/speed/det_by_minor.hpp": {"path": "layer-1/CppAD/include/cppad/speed/det_by_minor.hpp", "filename": "det_by_minor.hpp", "file": "det_by_minor.hpp", "brief": "Speed testing: det_by_minor", "has_pass2": false}, "include/cppad/speed/sparse_jac_fun.hpp": {"path": "layer-1/CppAD/include/cppad/speed/sparse_jac_fun.hpp", "filename": "sparse_jac_fun.hpp", "file": "sparse_jac_fun.hpp", "brief": "Speed testing: sparse_jac_fun", "has_pass2": false}, "include/cppad/speed/mat_sum_sq.hpp": {"path": "layer-1/CppAD/include/cppad/speed/mat_sum_sq.hpp", "filename": "mat_sum_sq.hpp", "file": "mat_sum_sq.hpp", "brief": "Speed testing: mat_sum_sq", "has_pass2": false}, "include/cppad/speed/ode_evaluate.hpp": {"path": "layer-1/CppAD/include/cppad/speed/ode_evaluate.hpp", "filename": "ode_evaluate.hpp", "file": "ode_evaluate.hpp", "brief": "Speed testing: ode_evaluate", "has_pass2": false}, "include/cppad/speed/uniform_01.hpp": {"path": "layer-1/CppAD/include/cppad/speed/uniform_01.hpp", "filename": "uniform_01.hpp", "file": "uniform_01.hpp", "brief": "Speed testing: uniform_01", "has_pass2": false}, "include/cppad/speed/det_by_lu.hpp": {"path": "layer-1/CppAD/include/cppad/speed/det_by_lu.hpp", "filename": "det_by_lu.hpp", "file": "det_by_lu.hpp", "brief": "Speed testing: det_by_lu", "has_pass2": false}, "include/cppad/speed/det_grad_33.hpp": {"path": "layer-1/CppAD/include/cppad/speed/det_grad_33.hpp", "filename": "det_grad_33.hpp", "file": "det_grad_33.hpp", "brief": "Speed testing: det_grad_33", "has_pass2": false}, "include/cppad/speed/det_of_minor.hpp": {"path": "layer-1/CppAD/include/cppad/speed/det_of_minor.hpp", "filename": "det_of_minor.hpp", "file": "det_of_minor.hpp", "brief": "Speed testing: det_of_minor", "has_pass2": false}, "include/cppad/speed/det_33.hpp": {"path": "layer-1/CppAD/include/cppad/speed/det_33.hpp", "filename": "det_33.hpp", "file": "det_33.hpp", "brief": "Speed testing: det_33", "has_pass2": false}, "include/cppad/speed/sparse_hes_fun.hpp": {"path": "layer-1/CppAD/include/cppad/speed/sparse_hes_fun.hpp", "filename": "sparse_hes_fun.hpp", "file": "sparse_hes_fun.hpp", "brief": "Speed testing: sparse_hes_fun", "has_pass2": false}, "include/cppad/utility/memory_leak.hpp": {"path": "layer-1/CppAD/include/cppad/utility/memory_leak.hpp", "filename": "memory_leak.hpp", "file": "memory_leak.hpp", "brief": "CppAD utility: memory leak", "has_pass2": false}, "include/cppad/utility/rosen_34.hpp": {"path": "layer-1/CppAD/include/cppad/utility/rosen_34.hpp", "filename": "rosen_34.hpp", "file": "rosen_34.hpp", "brief": "CppAD utility: rosen 34", "has_pass2": false}, "include/cppad/utility/ode_gear_control.hpp": {"path": "layer-1/CppAD/include/cppad/utility/ode_gear_control.hpp", "filename": "ode_gear_control.hpp", "file": "ode_gear_control.hpp", "brief": "CppAD utility: ode gear control", "has_pass2": false}, "include/cppad/utility/ode_err_control.hpp": {"path": "layer-1/CppAD/include/cppad/utility/ode_err_control.hpp", "filename": "ode_err_control.hpp", "file": "ode_err_control.hpp", "brief": "CppAD utility: ode err control", "has_pass2": false}, "include/cppad/utility/near_equal.hpp": {"path": "layer-1/CppAD/include/cppad/utility/near_equal.hpp", "filename": "near_equal.hpp", "file": "near_equal.hpp", "brief": "CppAD utility: near equal", "has_pass2": false}, "include/cppad/utility/ode_gear.hpp": {"path": "layer-1/CppAD/include/cppad/utility/ode_gear.hpp", "filename": "ode_gear.hpp", "file": "ode_gear.hpp", "brief": "CppAD utility: ode gear", "has_pass2": false}, "include/cppad/utility/pow_int.hpp": {"path": "layer-1/CppAD/include/cppad/utility/pow_int.hpp", "filename": "pow_int.hpp", "file": "pow_int.hpp", "brief": "CppAD utility: pow int", "has_pass2": false}, "include/cppad/utility/runge_45.hpp": {"path": "layer-1/CppAD/include/cppad/utility/runge_45.hpp", "filename": "runge_45.hpp", "file": "runge_45.hpp", "brief": "CppAD utility: runge 45", "has_pass2": false}, "include/cppad/utility/poly.hpp": {"path": "layer-1/CppAD/include/cppad/utility/poly.hpp", "filename": "poly.hpp", "file": "poly.hpp", "brief": "CppAD utility: poly", "has_pass2": false}, "include/cppad/utility/lu_solve.hpp": {"path": "layer-1/CppAD/include/cppad/utility/lu_solve.hpp", "filename": "lu_solve.hpp", "file": "lu_solve.hpp", "brief": "CppAD utility: lu solve", "has_pass2": false}, "include/cppad/utility/vector.hpp": {"path": "layer-1/CppAD/include/cppad/utility/vector.hpp", "filename": "vector.hpp", "file": "vector.hpp", "brief": "CppAD utility: vector", "has_pass2": false}, "include/cppad/utility/track_new_del.hpp": {"path": "layer-1/CppAD/include/cppad/utility/track_new_del.hpp", "filename": "track_new_del.hpp", "file": "track_new_del.hpp", "brief": "CppAD utility: track new del", "has_pass2": false}, "include/cppad/utility/lu_factor.hpp": {"path": "layer-1/CppAD/include/cppad/utility/lu_factor.hpp", "filename": "lu_factor.hpp", "file": "lu_factor.hpp", "brief": "CppAD utility: lu factor", "has_pass2": false}, "include/cppad/utility/omp_alloc.hpp": {"path": "layer-1/CppAD/include/cppad/utility/omp_alloc.hpp", "filename": "omp_alloc.hpp", "file": "omp_alloc.hpp", "brief": "CppAD utility: omp alloc", "has_pass2": false}, "include/cppad/utility/nan.hpp": {"path": "layer-1/CppAD/include/cppad/utility/nan.hpp", "filename": "nan.hpp", "file": "nan.hpp", "brief": "CppAD utility: nan", "has_pass2": false}, "include/cppad/utility/set_union.hpp": {"path": "layer-1/CppAD/include/cppad/utility/set_union.hpp", "filename": "set_union.hpp", "file": "set_union.hpp", "brief": "CppAD utility: set union", "has_pass2": false}, "include/cppad/utility/sparse2eigen.hpp": {"path": "layer-1/CppAD/include/cppad/utility/sparse2eigen.hpp", "filename": "sparse2eigen.hpp", "file": "sparse2eigen.hpp", "brief": "CppAD utility: sparse2eigen", "has_pass2": false}, "include/cppad/utility/error_handler.hpp": {"path": "layer-1/CppAD/include/cppad/utility/error_handler.hpp", "filename": "error_handler.hpp", "file": "error_handler.hpp", "brief": "CppAD utility: error handler", "has_pass2": false}, "include/cppad/utility/link_dll_lib.hpp": {"path": "layer-1/CppAD/include/cppad/utility/link_dll_lib.hpp", "filename": "link_dll_lib.hpp", "file": "link_dll_lib.hpp", "brief": "CppAD utility: link dll lib", "has_pass2": false}, "include/cppad/utility/to_string.hpp": {"path": "layer-1/CppAD/include/cppad/utility/to_string.hpp", "filename": "to_string.hpp", "file": "to_string.hpp", "brief": "CppAD utility: to string", "has_pass2": false}, "include/cppad/utility/lu_invert.hpp": {"path": "layer-1/CppAD/include/cppad/utility/lu_invert.hpp", "filename": "lu_invert.hpp", "file": "lu_invert.hpp", "brief": "CppAD utility: lu invert", "has_pass2": false}, "include/cppad/utility/create_dll_lib.hpp": {"path": "layer-1/CppAD/include/cppad/utility/create_dll_lib.hpp", "filename": "create_dll_lib.hpp", "file": "create_dll_lib.hpp", "brief": "CppAD utility: create dll lib", "has_pass2": false}, "include/cppad/utility/check_simple_vector.hpp": {"path": "layer-1/CppAD/include/cppad/utility/check_simple_vector.hpp", "filename": "check_simple_vector.hpp", "file": "check_simple_vector.hpp", "brief": "CppAD utility: check simple vector", "has_pass2": false}, "include/cppad/utility/sparse_rc.hpp": {"path": "layer-1/CppAD/include/cppad/utility/sparse_rc.hpp", "filename": "sparse_rc.hpp", "file": "sparse_rc.hpp", "brief": "CppAD utility: sparse rc", "has_pass2": false}, "include/cppad/utility/elapsed_seconds.hpp": {"path": "layer-1/CppAD/include/cppad/utility/elapsed_seconds.hpp", "filename": "elapsed_seconds.hpp", "file": "elapsed_seconds.hpp", "brief": "CppAD utility: elapsed seconds", "has_pass2": false}, "include/cppad/utility/romberg_one.hpp": {"path": "layer-1/CppAD/include/cppad/utility/romberg_one.hpp", "filename": "romberg_one.hpp", "file": "romberg_one.hpp", "brief": "CppAD utility: romberg one", "has_pass2": false}, "include/cppad/utility/check_numeric_type.hpp": {"path": "layer-1/CppAD/include/cppad/utility/check_numeric_type.hpp", "filename": "check_numeric_type.hpp", "file": "check_numeric_type.hpp", "brief": "CppAD utility: check numeric type", "has_pass2": false}, "include/cppad/utility/sparse_rcv.hpp": {"path": "layer-1/CppAD/include/cppad/utility/sparse_rcv.hpp", "filename": "sparse_rcv.hpp", "file": "sparse_rcv.hpp", "brief": "CppAD utility: sparse rcv", "has_pass2": false}, "include/cppad/utility/vector_bool.hpp": {"path": "layer-1/CppAD/include/cppad/utility/vector_bool.hpp", "filename": "vector_bool.hpp", "file": "vector_bool.hpp", "brief": "CppAD utility: vector bool", "has_pass2": false}, "include/cppad/utility/romberg_mul.hpp": {"path": "layer-1/CppAD/include/cppad/utility/romberg_mul.hpp", "filename": "romberg_mul.hpp", "file": "romberg_mul.hpp", "brief": "CppAD utility: romberg mul", "has_pass2": false}, "include/cppad/local/sparse/pack_setvec.hpp": {"path": "layer-1/CppAD/include/cppad/local/sparse/pack_setvec.hpp", "filename": "pack_setvec.hpp", "file": "pack_setvec.hpp", "brief": "Internal implementation: pack_setvec", "has_pass2": false}, "include/cppad/local/sparse/list_setvec.hpp": {"path": "layer-1/CppAD/include/cppad/local/sparse/list_setvec.hpp", "filename": "list_setvec.hpp", "file": "list_setvec.hpp", "brief": "Internal implementation: list_setvec", "has_pass2": false}, "include/cppad/local/sparse/size_setvec.hpp": {"path": "layer-1/CppAD/include/cppad/local/sparse/size_setvec.hpp", "filename": "size_setvec.hpp", "file": "size_setvec.hpp", "brief": "Internal implementation: size_setvec", "has_pass2": false}, "include/cppad/local/val_graph/val_type.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/val_type.hpp", "filename": "val_type.hpp", "file": "val_type.hpp", "brief": "Value graph: val_type", "has_pass2": false}, "include/cppad/local/val_graph/record_new.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/record_new.hpp", "filename": "record_new.hpp", "file": "record_new.hpp", "brief": "Value graph: record_new", "has_pass2": false}, "include/cppad/local/val_graph/rev_depend.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/rev_depend.hpp", "filename": "rev_depend.hpp", "file": "rev_depend.hpp", "brief": "Value graph: rev_depend", "has_pass2": false}, "include/cppad/local/val_graph/renumber.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/renumber.hpp", "filename": "renumber.hpp", "file": "renumber.hpp", "brief": "Value graph: renumber", "has_pass2": false}, "include/cppad/local/val_graph/vector_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/vector_op.hpp", "filename": "vector_op.hpp", "file": "vector_op.hpp", "brief": "Value graph: vector_op", "has_pass2": false}, "include/cppad/local/val_graph/op2arg_index.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/op2arg_index.hpp", "filename": "op2arg_index.hpp", "file": "op2arg_index.hpp", "brief": "Value graph: op2arg_index", "has_pass2": false}, "include/cppad/local/val_graph/compress.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/compress.hpp", "filename": "compress.hpp", "file": "compress.hpp", "brief": "Value graph: compress", "has_pass2": false}, "include/cppad/local/val_graph/val2fun.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/val2fun.hpp", "filename": "val2fun.hpp", "file": "val2fun.hpp", "brief": "Value graph: val2fun", "has_pass2": false}, "include/cppad/local/val_graph/op_hash_table.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/op_hash_table.hpp", "filename": "op_hash_table.hpp", "file": "op_hash_table.hpp", "brief": "Value graph: op_hash_table", "has_pass2": false}, "include/cppad/local/val_graph/val_optimize.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/val_optimize.hpp", "filename": "val_optimize.hpp", "file": "val_optimize.hpp", "brief": "Value graph: val_optimize", "has_pass2": false}, "include/cppad/local/val_graph/cumulative.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/cumulative.hpp", "filename": "cumulative.hpp", "file": "cumulative.hpp", "brief": "Value graph: cumulative", "has_pass2": false}, "include/cppad/local/val_graph/enable_parallel.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/enable_parallel.hpp", "filename": "enable_parallel.hpp", "file": "enable_parallel.hpp", "brief": "Value graph: enable_parallel", "has_pass2": false}, "include/cppad/local/val_graph/pri_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/pri_op.hpp", "filename": "pri_op.hpp", "file": "pri_op.hpp", "brief": "Value graph: pri_op", "has_pass2": false}, "include/cppad/local/val_graph/option.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/option.hpp", "filename": "option.hpp", "file": "option.hpp", "brief": "Value graph: option", "has_pass2": false}, "include/cppad/local/val_graph/csum_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/csum_op.hpp", "filename": "csum_op.hpp", "file": "csum_op.hpp", "brief": "Value graph: csum_op", "has_pass2": false}, "include/cppad/local/val_graph/print_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/print_op.hpp", "filename": "print_op.hpp", "file": "print_op.hpp", "brief": "Value graph: print_op", "has_pass2": false}, "include/cppad/local/val_graph/record.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/record.hpp", "filename": "record.hpp", "file": "record.hpp", "brief": "Value graph: record", "has_pass2": false}, "include/cppad/local/val_graph/op_iterator.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/op_iterator.hpp", "filename": "op_iterator.hpp", "file": "op_iterator.hpp", "brief": "Value graph: op_iterator", "has_pass2": false}, "include/cppad/local/val_graph/dead_code.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/dead_code.hpp", "filename": "dead_code.hpp", "file": "dead_code.hpp", "brief": "Value graph: dead_code", "has_pass2": false}, "include/cppad/local/val_graph/tape.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/tape.hpp", "filename": "tape.hpp", "file": "tape.hpp", "brief": "Value graph: tape", "has_pass2": false}, "include/cppad/local/val_graph/unary_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/unary_op.hpp", "filename": "unary_op.hpp", "file": "unary_op.hpp", "brief": "Value graph: unary_op", "has_pass2": false}, "include/cppad/local/val_graph/call_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/call_op.hpp", "filename": "call_op.hpp", "file": "call_op.hpp", "brief": "Value graph: call_op", "has_pass2": false}, "include/cppad/local/val_graph/con_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/con_op.hpp", "filename": "con_op.hpp", "file": "con_op.hpp", "brief": "Value graph: con_op", "has_pass2": false}, "include/cppad/local/val_graph/base_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/base_op.hpp", "filename": "base_op.hpp", "file": "base_op.hpp", "brief": "Value graph: base_op", "has_pass2": false}, "include/cppad/local/val_graph/fun2val.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/fun2val.hpp", "filename": "fun2val.hpp", "file": "fun2val.hpp", "brief": "Value graph: fun2val", "has_pass2": false}, "include/cppad/local/val_graph/comp_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/comp_op.hpp", "filename": "comp_op.hpp", "file": "comp_op.hpp", "brief": "Value graph: comp_op", "has_pass2": false}, "include/cppad/local/val_graph/op_enum2class.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/op_enum2class.hpp", "filename": "op_enum2class.hpp", "file": "op_enum2class.hpp", "brief": "Value graph: op_enum2class", "has_pass2": false}, "include/cppad/local/val_graph/summation.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/summation.hpp", "filename": "summation.hpp", "file": "summation.hpp", "brief": "Value graph: summation", "has_pass2": false}, "include/cppad/local/val_graph/binary_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/binary_op.hpp", "filename": "binary_op.hpp", "file": "binary_op.hpp", "brief": "Value graph: binary_op", "has_pass2": false}, "include/cppad/local/val_graph/cexp_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/cexp_op.hpp", "filename": "cexp_op.hpp", "file": "cexp_op.hpp", "brief": "Value graph: cexp_op", "has_pass2": false}, "include/cppad/local/val_graph/call_atomic.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/call_atomic.hpp", "filename": "call_atomic.hpp", "file": "call_atomic.hpp", "brief": "Value graph: call_atomic", "has_pass2": false}, "include/cppad/local/val_graph/fold_con.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/fold_con.hpp", "filename": "fold_con.hpp", "file": "fold_con.hpp", "brief": "Value graph: fold_con", "has_pass2": false}, "include/cppad/local/val_graph/var_type.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/var_type.hpp", "filename": "var_type.hpp", "file": "var_type.hpp", "brief": "Value graph: var_type", "has_pass2": false}, "include/cppad/local/val_graph/dis_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/dis_op.hpp", "filename": "dis_op.hpp", "file": "dis_op.hpp", "brief": "Value graph: dis_op", "has_pass2": false}, "include/cppad/local/val_graph/dyn_type.hpp": {"path": "layer-1/CppAD/include/cppad/local/val_graph/dyn_type.hpp", "filename": "dyn_type.hpp", "file": "dyn_type.hpp", "brief": "Value graph: dyn_type", "has_pass2": false}, "include/cppad/local/var_op/cskip_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/cskip_op.hpp", "filename": "cskip_op.hpp", "file": "cskip_op.hpp", "brief": "Internal implementation: cskip_op", "has_pass2": false}, "include/cppad/local/var_op/add_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/add_op.hpp", "filename": "add_op.hpp", "file": "add_op.hpp", "brief": "Internal implementation: add_op", "has_pass2": false}, "include/cppad/local/var_op/two_var.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/two_var.hpp", "filename": "two_var.hpp", "file": "two_var.hpp", "brief": "Internal implementation: two_var", "has_pass2": false}, "include/cppad/local/var_op/zmul_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/zmul_op.hpp", "filename": "zmul_op.hpp", "file": "zmul_op.hpp", "brief": "Internal implementation: zmul_op", "has_pass2": false}, "include/cppad/local/var_op/par_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/par_op.hpp", "filename": "par_op.hpp", "file": "par_op.hpp", "brief": "Internal implementation: par_op", "has_pass2": false}, "include/cppad/local/var_op/store_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/store_op.hpp", "filename": "store_op.hpp", "file": "store_op.hpp", "brief": "Internal implementation: store_op", "has_pass2": false}, "include/cppad/local/var_op/sqrt_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/sqrt_op.hpp", "filename": "sqrt_op.hpp", "file": "sqrt_op.hpp", "brief": "Internal implementation: sqrt_op", "has_pass2": false}, "include/cppad/local/var_op/acosh_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/acosh_op.hpp", "filename": "acosh_op.hpp", "file": "acosh_op.hpp", "brief": "Internal implementation: acosh_op", "has_pass2": false}, "include/cppad/local/var_op/sinh_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/sinh_op.hpp", "filename": "sinh_op.hpp", "file": "sinh_op.hpp", "brief": "Internal implementation: sinh_op", "has_pass2": false}, "include/cppad/local/var_op/tanh_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/tanh_op.hpp", "filename": "tanh_op.hpp", "file": "tanh_op.hpp", "brief": "Internal implementation: tanh_op", "has_pass2": false}, "include/cppad/local/var_op/sin_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/sin_op.hpp", "filename": "sin_op.hpp", "file": "sin_op.hpp", "brief": "Internal implementation: sin_op", "has_pass2": false}, "include/cppad/local/var_op/var_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/var_op.hpp", "filename": "var_op.hpp", "file": "var_op.hpp", "brief": "Internal implementation: var_op", "has_pass2": false}, "include/cppad/local/var_op/atanh_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/atanh_op.hpp", "filename": "atanh_op.hpp", "file": "atanh_op.hpp", "brief": "Internal implementation: atanh_op", "has_pass2": false}, "include/cppad/local/var_op/cosh_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/cosh_op.hpp", "filename": "cosh_op.hpp", "file": "cosh_op.hpp", "brief": "Internal implementation: cosh_op", "has_pass2": false}, "include/cppad/local/var_op/compare_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/compare_op.hpp", "filename": "compare_op.hpp", "file": "compare_op.hpp", "brief": "Internal implementation: compare_op", "has_pass2": false}, "include/cppad/local/var_op/mul_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/mul_op.hpp", "filename": "mul_op.hpp", "file": "mul_op.hpp", "brief": "Internal implementation: mul_op", "has_pass2": false}, "include/cppad/local/var_op/exp_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/exp_op.hpp", "filename": "exp_op.hpp", "file": "exp_op.hpp", "brief": "Internal implementation: exp_op", "has_pass2": false}, "include/cppad/local/var_op/pri_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/pri_op.hpp", "filename": "pri_op.hpp", "file": "pri_op.hpp", "brief": "Internal implementation: pri_op", "has_pass2": false}, "include/cppad/local/var_op/asinh_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/asinh_op.hpp", "filename": "asinh_op.hpp", "file": "asinh_op.hpp", "brief": "Internal implementation: asinh_op", "has_pass2": false}, "include/cppad/local/var_op/csum_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/csum_op.hpp", "filename": "csum_op.hpp", "file": "csum_op.hpp", "brief": "Internal implementation: csum_op", "has_pass2": false}, "include/cppad/local/var_op/atomic_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/atomic_op.hpp", "filename": "atomic_op.hpp", "file": "atomic_op.hpp", "brief": "Internal implementation: atomic_op", "has_pass2": false}, "include/cppad/local/var_op/sub_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/sub_op.hpp", "filename": "sub_op.hpp", "file": "sub_op.hpp", "brief": "Internal implementation: sub_op", "has_pass2": false}, "include/cppad/local/var_op/log_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/log_op.hpp", "filename": "log_op.hpp", "file": "log_op.hpp", "brief": "Internal implementation: log_op", "has_pass2": false}, "include/cppad/local/var_op/asin_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/asin_op.hpp", "filename": "asin_op.hpp", "file": "asin_op.hpp", "brief": "Internal implementation: asin_op", "has_pass2": false}, "include/cppad/local/var_op/expm1_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/expm1_op.hpp", "filename": "expm1_op.hpp", "file": "expm1_op.hpp", "brief": "Internal implementation: expm1_op", "has_pass2": false}, "include/cppad/local/var_op/tan_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/tan_op.hpp", "filename": "tan_op.hpp", "file": "tan_op.hpp", "brief": "Internal implementation: tan_op", "has_pass2": false}, "include/cppad/local/var_op/load_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/load_op.hpp", "filename": "load_op.hpp", "file": "load_op.hpp", "brief": "Internal implementation: load_op", "has_pass2": false}, "include/cppad/local/var_op/neg_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/neg_op.hpp", "filename": "neg_op.hpp", "file": "neg_op.hpp", "brief": "Internal implementation: neg_op", "has_pass2": false}, "include/cppad/local/var_op/atan_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/atan_op.hpp", "filename": "atan_op.hpp", "file": "atan_op.hpp", "brief": "Internal implementation: atan_op", "has_pass2": false}, "include/cppad/local/var_op/sign_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/sign_op.hpp", "filename": "sign_op.hpp", "file": "sign_op.hpp", "brief": "Internal implementation: sign_op", "has_pass2": false}, "include/cppad/local/var_op/cexp_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/cexp_op.hpp", "filename": "cexp_op.hpp", "file": "cexp_op.hpp", "brief": "Internal implementation: cexp_op", "has_pass2": false}, "include/cppad/local/var_op/log1p_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/log1p_op.hpp", "filename": "log1p_op.hpp", "file": "log1p_op.hpp", "brief": "Internal implementation: log1p_op", "has_pass2": false}, "include/cppad/local/var_op/div_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/div_op.hpp", "filename": "div_op.hpp", "file": "div_op.hpp", "brief": "Internal implementation: div_op", "has_pass2": false}, "include/cppad/local/var_op/dis_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/dis_op.hpp", "filename": "dis_op.hpp", "file": "dis_op.hpp", "brief": "Internal implementation: dis_op", "has_pass2": false}, "include/cppad/local/var_op/one_var.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/one_var.hpp", "filename": "one_var.hpp", "file": "one_var.hpp", "brief": "Internal implementation: one_var", "has_pass2": false}, "include/cppad/local/var_op/cos_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/cos_op.hpp", "filename": "cos_op.hpp", "file": "cos_op.hpp", "brief": "Internal implementation: cos_op", "has_pass2": false}, "include/cppad/local/var_op/abs_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/abs_op.hpp", "filename": "abs_op.hpp", "file": "abs_op.hpp", "brief": "Internal implementation: abs_op", "has_pass2": false}, "include/cppad/local/var_op/acos_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/var_op/acos_op.hpp", "filename": "acos_op.hpp", "file": "acos_op.hpp", "brief": "Internal implementation: acos_op", "has_pass2": false}, "include/cppad/local/optimize/get_op_usage.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/get_op_usage.hpp", "filename": "get_op_usage.hpp", "file": "get_op_usage.hpp", "brief": "Tape optimization: get_op_usage", "has_pass2": false}, "include/cppad/local/optimize/usage.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/usage.hpp", "filename": "usage.hpp", "file": "usage.hpp", "brief": "Tape optimization: usage", "has_pass2": false}, "include/cppad/local/optimize/record_csum.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/record_csum.hpp", "filename": "record_csum.hpp", "file": "record_csum.hpp", "brief": "Tape optimization: record_csum", "has_pass2": false}, "include/cppad/local/optimize/get_op_previous.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/get_op_previous.hpp", "filename": "get_op_previous.hpp", "file": "get_op_previous.hpp", "brief": "Tape optimization: get_op_previous", "has_pass2": false}, "include/cppad/local/optimize/match_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/match_op.hpp", "filename": "match_op.hpp", "file": "match_op.hpp", "brief": "Tape optimization: match_op", "has_pass2": false}, "include/cppad/local/optimize/optimize_run.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/optimize_run.hpp", "filename": "optimize_run.hpp", "file": "optimize_run.hpp", "brief": "Tape optimization: optimize_run", "has_pass2": false}, "include/cppad/local/optimize/cexp_info.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/cexp_info.hpp", "filename": "cexp_info.hpp", "file": "cexp_info.hpp", "brief": "Tape optimization: cexp_info", "has_pass2": false}, "include/cppad/local/optimize/extract_option.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/extract_option.hpp", "filename": "extract_option.hpp", "file": "extract_option.hpp", "brief": "Tape optimization: extract_option", "has_pass2": false}, "include/cppad/local/optimize/get_cexp_info.hpp": {"path": "layer-1/CppAD/include/cppad/local/optimize/get_cexp_info.hpp", "filename": "get_cexp_info.hpp", "file": "get_cexp_info.hpp", "brief": "Tape optimization: get_cexp_info", "has_pass2": false}, "include/cppad/local/play/dyn_player.hpp": {"path": "layer-1/CppAD/include/cppad/local/play/dyn_player.hpp", "filename": "dyn_player.hpp", "file": "dyn_player.hpp", "brief": "Tape playback: dyn_player", "has_pass2": false}, "include/cppad/local/utility/cppad_vector_itr.hpp": {"path": "layer-1/CppAD/include/cppad/local/utility/cppad_vector_itr.hpp", "filename": "cppad_vector_itr.hpp", "file": "cppad_vector_itr.hpp", "brief": "CppAD utility: cppad vector itr", "has_pass2": false}, "include/cppad/local/utility/vector_bool.hpp": {"path": "layer-1/CppAD/include/cppad/local/utility/vector_bool.hpp", "filename": "vector_bool.hpp", "file": "vector_bool.hpp", "brief": "CppAD utility: vector bool", "has_pass2": false}, "include/cppad/local/record/recorder.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/recorder.hpp", "filename": "recorder.hpp", "file": "recorder.hpp", "brief": "Operation recording: recorder", "has_pass2": false}, "include/cppad/local/record/put_var_atomic.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/put_var_atomic.hpp", "filename": "put_var_atomic.hpp", "file": "put_var_atomic.hpp", "brief": "Operation recording: put_var_atomic", "has_pass2": false}, "include/cppad/local/record/cond_exp.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/cond_exp.hpp", "filename": "cond_exp.hpp", "file": "cond_exp.hpp", "brief": "Operation recording: cond_exp", "has_pass2": false}, "include/cppad/local/record/put_var_vecad.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/put_var_vecad.hpp", "filename": "put_var_vecad.hpp", "file": "put_var_vecad.hpp", "brief": "Operation recording: put_var_vecad", "has_pass2": false}, "include/cppad/local/record/comp_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/comp_op.hpp", "filename": "comp_op.hpp", "file": "comp_op.hpp", "brief": "Operation recording: comp_op", "has_pass2": false}, "include/cppad/local/record/put_dyn_atomic.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/put_dyn_atomic.hpp", "filename": "put_dyn_atomic.hpp", "file": "put_dyn_atomic.hpp", "brief": "Operation recording: put_dyn_atomic", "has_pass2": false}, "include/cppad/local/record/dyn_recorder.hpp": {"path": "layer-1/CppAD/include/cppad/local/record/dyn_recorder.hpp", "filename": "dyn_recorder.hpp", "file": "dyn_recorder.hpp", "brief": "Operation recording: dyn_recorder", "has_pass2": false}, "include/cppad/local/graph/csrc_writer.hpp": {"path": "layer-1/CppAD/include/cppad/local/graph/csrc_writer.hpp", "filename": "csrc_writer.hpp", "file": "csrc_writer.hpp", "brief": "Computational graph: csrc_writer", "has_pass2": false}, "include/cppad/local/graph/cpp_graph_op.hpp": {"path": "layer-1/CppAD/include/cppad/local/graph/cpp_graph_op.hpp", "filename": "cpp_graph_op.hpp", "file": "cpp_graph_op.hpp", "brief": "Computational graph: cpp_graph_op", "has_pass2": false}, "include/cppad/local/graph/cpp_graph_itr.hpp": {"path": "layer-1/CppAD/include/cppad/local/graph/cpp_graph_itr.hpp", "filename": "cpp_graph_itr.hpp", "file": "cpp_graph_itr.hpp", "brief": "Computational graph: cpp_graph_itr", "has_pass2": false}, "include/cppad/local/graph/json_writer.hpp": {"path": "layer-1/CppAD/include/cppad/local/graph/json_writer.hpp", "filename": "json_writer.hpp", "file": "json_writer.hpp", "brief": "Computational graph: json_writer", "has_pass2": false}, "include/cppad/local/graph/json_parser.hpp": {"path": "layer-1/CppAD/include/cppad/local/graph/json_parser.hpp", "filename": "json_parser.hpp", "file": "json_parser.hpp", "brief": "Computational graph: json_parser", "has_pass2": false}, "include/cppad/local/graph/json_lexer.hpp": {"path": "layer-1/CppAD/include/cppad/local/graph/json_lexer.hpp", "filename": "json_lexer.hpp", "file": "json_lexer.hpp", "brief": "Computational graph: json_lexer", "has_pass2": false}, "include/cppad/local/sweep/forward_any.hpp": {"path": "layer-1/CppAD/include/cppad/local/sweep/forward_any.hpp", "filename": "forward_any.hpp", "file": "forward_any.hpp", "brief": "Internal sweep implementation: forward_any", "has_pass2": false}, "include/cppad/local/sweep/forward_0.hpp": {"path": "layer-1/CppAD/include/cppad/local/sweep/forward_0.hpp", "filename": "forward_0.hpp", "file": "forward_0.hpp", "brief": "Internal sweep implementation: forward_0", "has_pass2": false}, "include/cppad/local/sweep/for_hes.hpp": {"path": "layer-1/CppAD/include/cppad/local/sweep/for_hes.hpp", "filename": "for_hes.hpp", "file": "for_hes.hpp", "brief": "Internal sweep implementation: for_hes", "has_pass2": false}, "include/cppad/local/sweep/forward_dir.hpp": {"path": "layer-1/CppAD/include/cppad/local/sweep/forward_dir.hpp", "filename": "forward_dir.hpp", "file": "forward_dir.hpp", "brief": "Internal sweep implementation: forward_dir", "has_pass2": false}, "include/cppad/local/sweep/call_atomic.hpp": {"path": "layer-1/CppAD/include/cppad/local/sweep/call_atomic.hpp", "filename": "call_atomic.hpp", "file": "call_atomic.hpp", "brief": "Internal sweep implementation: call_atomic", "has_pass2": false}, "include/cppad/local/sweep/rev_jac.hpp": {"path": "layer-1/CppAD/include/cppad/local/sweep/rev_jac.hpp", "filename": "rev_jac.hpp", "file": "rev_jac.hpp", "brief": "Internal sweep implementation: rev_jac", "has_pass2": false}, "include/cppad/core/chkpoint_one/set_hes_sparse_bool.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/set_hes_sparse_bool.hpp", "filename": "set_hes_sparse_bool.hpp", "file": "set_hes_sparse_bool.hpp", "brief": "Core AD functionality: set hes sparse bool", "has_pass2": false}, "include/cppad/core/chkpoint_one/reverse.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/reverse.hpp", "filename": "reverse.hpp", "file": "reverse.hpp", "brief": "Core AD functionality: reverse", "has_pass2": false}, "include/cppad/core/chkpoint_one/set_jac_sparse_bool.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/set_jac_sparse_bool.hpp", "filename": "set_jac_sparse_bool.hpp", "file": "set_jac_sparse_bool.hpp", "brief": "Core AD functionality: set jac sparse bool", "has_pass2": false}, "include/cppad/core/chkpoint_one/ctor.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/ctor.hpp", "filename": "ctor.hpp", "file": "ctor.hpp", "brief": "Core AD functionality: ctor", "has_pass2": false}, "include/cppad/core/chkpoint_one/set_hes_sparse_set.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/set_hes_sparse_set.hpp", "filename": "set_hes_sparse_set.hpp", "file": "set_hes_sparse_set.hpp", "brief": "Core AD functionality: set hes sparse set", "has_pass2": false}, "include/cppad/core/chkpoint_one/rev_sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/rev_sparse_jac.hpp", "filename": "rev_sparse_jac.hpp", "file": "rev_sparse_jac.hpp", "brief": "Core AD functionality: rev sparse jac", "has_pass2": false}, "include/cppad/core/chkpoint_one/set_jac_sparse_set.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/set_jac_sparse_set.hpp", "filename": "set_jac_sparse_set.hpp", "file": "set_jac_sparse_set.hpp", "brief": "Core AD functionality: set jac sparse set", "has_pass2": false}, "include/cppad/core/chkpoint_one/for_sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/for_sparse_jac.hpp", "filename": "for_sparse_jac.hpp", "file": "for_sparse_jac.hpp", "brief": "Core AD functionality: for sparse jac", "has_pass2": false}, "include/cppad/core/chkpoint_one/forward.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/forward.hpp", "filename": "forward.hpp", "file": "forward.hpp", "brief": "Core AD functionality: forward", "has_pass2": false}, "include/cppad/core/chkpoint_one/rev_sparse_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_one/rev_sparse_hes.hpp", "filename": "rev_sparse_hes.hpp", "file": "rev_sparse_hes.hpp", "brief": "Core AD functionality: rev sparse hes", "has_pass2": false}, "include/cppad/core/discrete/discrete.hpp": {"path": "layer-1/CppAD/include/cppad/core/discrete/discrete.hpp", "filename": "discrete.hpp", "file": "discrete.hpp", "brief": "Core AD functionality: discrete", "has_pass2": false}, "include/cppad/core/vec_ad/vec_ad.hpp": {"path": "layer-1/CppAD/include/cppad/core/vec_ad/vec_ad.hpp", "filename": "vec_ad.hpp", "file": "vec_ad.hpp", "brief": "Core AD functionality: vec ad", "has_pass2": false}, "include/cppad/core/graph/graph_op_enum.hpp": {"path": "layer-1/CppAD/include/cppad/core/graph/graph_op_enum.hpp", "filename": "graph_op_enum.hpp", "file": "graph_op_enum.hpp", "brief": "Core AD functionality: graph op enum", "has_pass2": false}, "include/cppad/core/graph/to_json.hpp": {"path": "layer-1/CppAD/include/cppad/core/graph/to_json.hpp", "filename": "to_json.hpp", "file": "to_json.hpp", "brief": "Core AD functionality: to json", "has_pass2": false}, "include/cppad/core/graph/to_graph.hpp": {"path": "layer-1/CppAD/include/cppad/core/graph/to_graph.hpp", "filename": "to_graph.hpp", "file": "to_graph.hpp", "brief": "Core AD functionality: to graph", "has_pass2": false}, "include/cppad/core/graph/cpp_graph.hpp": {"path": "layer-1/CppAD/include/cppad/core/graph/cpp_graph.hpp", "filename": "cpp_graph.hpp", "file": "cpp_graph.hpp", "brief": "Core AD functionality: cpp graph", "has_pass2": false}, "include/cppad/core/graph/from_json.hpp": {"path": "layer-1/CppAD/include/cppad/core/graph/from_json.hpp", "filename": "from_json.hpp", "file": "from_json.hpp", "brief": "Core AD functionality: from json", "has_pass2": false}, "include/cppad/core/graph/from_graph.hpp": {"path": "layer-1/CppAD/include/cppad/core/graph/from_graph.hpp", "filename": "from_graph.hpp", "file": "from_graph.hpp", "brief": "Core AD functionality: from graph", "has_pass2": false}, "include/cppad/core/forward/forward.hpp": {"path": "layer-1/CppAD/include/cppad/core/forward/forward.hpp", "filename": "forward.hpp", "file": "forward.hpp", "brief": "Core AD functionality: forward mode differentiation", "algorithm": "Multiple Orders in One Pass:\nCompute Taylor coefficients for orders p through q simultaneously.\n\n  xq INPUT:\n    x^(p), x^(p+1), ..., x^(q) for independent variables\n    Previous orders x^(0),...,x^(p-1) already stored\n\n  yq OUTPUT:\n    y^(p), y^(p+1), ..., y^(q) for dependent variables\n\n  USE CASE:\n    Order 0: function evaluation  y = f(x)\n    Order 1: Jacobian-vector product  y' = f'(x)·v\n    Order 2: Hessian-vector-vector  y'' = v'·f''(x)·v", "math": "Forward mode computes:\n    y^(q) = ∂^q y / ∂t^q |_{t=0}  where x(t) = x^(0) + t·x^(1) + ...\n\n  TAYLOR SERIES:\n    Each variable v stores coefficients: v^(0), v^(1), ..., v^(q)\n    v(t) = v^(0) + t·v^(1) + t²/2!·v^(2) + ... + t^q/q!·v^(q)\n\n  FORWARD SWEEP (order q):\n    For each operation v = op(u, w) in tape order:\n      Compute v^(q) from {u^(0),...,u^(q)} and {w^(0),...,w^(q)}\n\n  EXAMPLE (multiplication v = u * w):\n    v^(q) = Σ_{k=0}^{q} u^(k) · w^(q-k)  (convolution)", "complexity": "O(tape_ops × order) per forward sweep\nCost is similar to function evaluation for order 1", "see": ["reverse.hpp for reverse mode (adjoint)", "sparse_jac.hpp for sparse Jacobian computation"], "has_pass2": true}, "include/cppad/core/chkpoint_two/ctor.hpp": {"path": "layer-1/CppAD/include/cppad/core/chkpoint_two/ctor.hpp", "filename": "ctor.hpp", "file": "ctor.hpp", "brief": "Core AD functionality: ctor", "has_pass2": false}, "include/cppad/core/independent/independent.hpp": {"path": "layer-1/CppAD/include/cppad/core/independent/independent.hpp", "filename": "independent.hpp", "file": "independent.hpp", "brief": "Core AD functionality: independent", "has_pass2": false}, "include/cppad/core/atomic/one/atomic.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/one/atomic.hpp", "filename": "atomic.hpp", "file": "atomic.hpp", "brief": "Core AD functionality: atomic", "has_pass2": false}, "include/cppad/core/atomic/three/reverse.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/reverse.hpp", "filename": "reverse.hpp", "file": "reverse.hpp", "brief": "Core AD functionality: reverse", "has_pass2": false}, "include/cppad/core/atomic/three/rev_depend.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/rev_depend.hpp", "filename": "rev_depend.hpp", "file": "rev_depend.hpp", "brief": "Core AD functionality: rev depend", "has_pass2": false}, "include/cppad/core/atomic/three/ctor.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/ctor.hpp", "filename": "ctor.hpp", "file": "ctor.hpp", "brief": "Core AD functionality: ctor", "has_pass2": false}, "include/cppad/core/atomic/three/atomic.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/atomic.hpp", "filename": "atomic.hpp", "file": "atomic.hpp", "brief": "Core AD functionality: atomic", "has_pass2": false}, "include/cppad/core/atomic/three/for_type.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/for_type.hpp", "filename": "for_type.hpp", "file": "for_type.hpp", "brief": "Core AD functionality: for type", "has_pass2": false}, "include/cppad/core/atomic/three/forward.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/forward.hpp", "filename": "forward.hpp", "file": "forward.hpp", "brief": "Core AD functionality: forward", "has_pass2": false}, "include/cppad/core/atomic/three/hes_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/hes_sparsity.hpp", "filename": "hes_sparsity.hpp", "file": "hes_sparsity.hpp", "brief": "Core AD functionality: hes sparsity", "has_pass2": false}, "include/cppad/core/atomic/three/jac_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/three/jac_sparsity.hpp", "filename": "jac_sparsity.hpp", "file": "jac_sparsity.hpp", "brief": "Core AD functionality: jac sparsity", "has_pass2": false}, "include/cppad/core/atomic/two/reverse.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/reverse.hpp", "filename": "reverse.hpp", "file": "reverse.hpp", "brief": "Core AD functionality: reverse", "has_pass2": false}, "include/cppad/core/atomic/two/for_sparse_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/for_sparse_hes.hpp", "filename": "for_sparse_hes.hpp", "file": "for_sparse_hes.hpp", "brief": "Core AD functionality: for sparse hes", "has_pass2": false}, "include/cppad/core/atomic/two/option.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/option.hpp", "filename": "option.hpp", "file": "option.hpp", "brief": "Core AD functionality: option", "has_pass2": false}, "include/cppad/core/atomic/two/ctor.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/ctor.hpp", "filename": "ctor.hpp", "file": "ctor.hpp", "brief": "Core AD functionality: ctor", "has_pass2": false}, "include/cppad/core/atomic/two/atomic.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/atomic.hpp", "filename": "atomic.hpp", "file": "atomic.hpp", "brief": "Core AD functionality: atomic", "has_pass2": false}, "include/cppad/core/atomic/two/rev_sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/rev_sparse_jac.hpp", "filename": "rev_sparse_jac.hpp", "file": "rev_sparse_jac.hpp", "brief": "Core AD functionality: rev sparse jac", "has_pass2": false}, "include/cppad/core/atomic/two/for_sparse_jac.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/for_sparse_jac.hpp", "filename": "for_sparse_jac.hpp", "file": "for_sparse_jac.hpp", "brief": "Core AD functionality: for sparse jac", "has_pass2": false}, "include/cppad/core/atomic/two/forward.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/forward.hpp", "filename": "forward.hpp", "file": "forward.hpp", "brief": "Core AD functionality: forward", "has_pass2": false}, "include/cppad/core/atomic/two/rev_sparse_hes.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/two/rev_sparse_hes.hpp", "filename": "rev_sparse_hes.hpp", "file": "rev_sparse_hes.hpp", "brief": "Core AD functionality: rev sparse hes", "has_pass2": false}, "include/cppad/core/atomic/four/reverse.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/reverse.hpp", "filename": "reverse.hpp", "file": "reverse.hpp", "brief": "Core AD functionality: reverse", "has_pass2": false}, "include/cppad/core/atomic/four/rev_depend.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/rev_depend.hpp", "filename": "rev_depend.hpp", "file": "rev_depend.hpp", "brief": "Core AD functionality: rev depend", "has_pass2": false}, "include/cppad/core/atomic/four/ctor.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/ctor.hpp", "filename": "ctor.hpp", "file": "ctor.hpp", "brief": "Core AD functionality: ctor", "has_pass2": false}, "include/cppad/core/atomic/four/atomic.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/atomic.hpp", "filename": "atomic.hpp", "file": "atomic.hpp", "brief": "Core AD functionality: atomic", "has_pass2": false}, "include/cppad/core/atomic/four/call.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/call.hpp", "filename": "call.hpp", "file": "call.hpp", "brief": "Core AD functionality: call", "has_pass2": false}, "include/cppad/core/atomic/four/for_type.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/for_type.hpp", "filename": "for_type.hpp", "file": "for_type.hpp", "brief": "Core AD functionality: for type", "has_pass2": false}, "include/cppad/core/atomic/four/forward.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/forward.hpp", "filename": "forward.hpp", "file": "forward.hpp", "brief": "Core AD functionality: forward", "has_pass2": false}, "include/cppad/core/atomic/four/hes_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/hes_sparsity.hpp", "filename": "hes_sparsity.hpp", "file": "hes_sparsity.hpp", "brief": "Core AD functionality: hes sparsity", "has_pass2": false}, "include/cppad/core/atomic/four/jac_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/jac_sparsity.hpp", "filename": "jac_sparsity.hpp", "file": "jac_sparsity.hpp", "brief": "Core AD functionality: jac sparsity", "has_pass2": false}, "include/cppad/core/atomic/four/devel/hes_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/devel/hes_sparsity.hpp", "filename": "hes_sparsity.hpp", "file": "hes_sparsity.hpp", "brief": "Core AD functionality: hes sparsity", "has_pass2": false}, "include/cppad/core/atomic/four/devel/jac_sparsity.hpp": {"path": "layer-1/CppAD/include/cppad/core/atomic/four/devel/jac_sparsity.hpp", "filename": "jac_sparsity.hpp", "file": "jac_sparsity.hpp", "brief": "Core AD functionality: jac sparsity", "has_pass2": false}, "speed/adolc/adolc_alloc_mat.hpp": {"path": "layer-1/CppAD/speed/adolc/adolc_alloc_mat.hpp", "filename": "adolc_alloc_mat.hpp", "file": "adolc_alloc_mat.hpp", "brief": "Speed testing: adolc_alloc_mat", "has_pass2": false}, "speed/src/link_sparse_jacobian.hpp": {"path": "layer-1/CppAD/speed/src/link_sparse_jacobian.hpp", "filename": "link_sparse_jacobian.hpp", "file": "link_sparse_jacobian.hpp", "brief": "Speed testing: link_sparse_jacobian", "has_pass2": false}, "speed/src/link_sparse_hessian.hpp": {"path": "layer-1/CppAD/speed/src/link_sparse_hessian.hpp", "filename": "link_sparse_hessian.hpp", "file": "link_sparse_hessian.hpp", "brief": "Speed testing: link_sparse_hessian", "has_pass2": false}}}, "Osi": {"name": "Osi", "file_count": 20, "pass2_count": 12, "files": {"src/OsiSpx/OsiSpxSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiSpx/OsiSpxSolverInterface.hpp", "filename": "OsiSpxSolverInterface.hpp", "file": "OsiSpxSolverInterface.hpp", "brief": "Osi interface for SoPlex LP solver\nAuthors: Tobias Pfender, Ambros Gleixner, Wei Huang (ZIB Berlin).\nEPL license.\n\nOsiSpxSolverInterface wraps SoPlex >= 1.4.2c via the Osi abstraction.\nImplements initialSolve/resolve/branchAndBound, parameter get/set,\nrow/column access, warm start via CoinWarmStartBasis. Uses soplex::SoPlex\ninternally with DVector/DIdxSet for sparse operations.", "see": ["OsiSolverInterface for base class"], "param": ["<code>[indexfirst,indexLast)</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "indexList the indices of those variables", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast)</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast)</code> contains the indices of\n    \t         the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiGrb/OsiGrbSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiGrb/OsiGrbSolverInterface.hpp", "filename": "OsiGrbSolverInterface.hpp", "file": "OsiGrbSolverInterface.hpp", "brief": "Osi interface for Gurobi optimizer\nAuthor: Stefan Vigerske (HU Berlin), based on Cplex interface by T. Achterberg.\nEPL license.\n\nOsiGrbSolverInterface wraps Gurobi via the Osi abstraction. Uses GRBmodel\nand GRBenv handles for Gurobi C API. Implements initialSolve/resolve/\nbranchAndBound, row/column manipulation, warm starts, and parameter passing.", "see": ["OsiSolverInterface for base class"], "param": ["<code>[indexfirst,indexLast]</code> contains the indices of\n\t the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast]</code> contains the indices of\n\t the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast]</code> contains the indices of\n\t the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiXpr/OsiXprSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiXpr/OsiXprSolverInterface.hpp", "filename": "OsiXprSolverInterface.hpp", "file": "OsiXprSolverInterface.hpp", "brief": "Osi interface for FICO XPRESS-MP solver\nCopyright (C) 2000, International Business Machines Corporation.\nEPL-1.0 license.\n\nOsiXprSolverInterface wraps XPRESS-MP via the Osi abstraction. Uses XPRSprob\nhandle for XPRESS API. Implements initialSolve/resolve/branchAndBound,\nrow/column access, parameter get/set, and problem loading from MPS/LP files.", "see": ["OsiSolverInterface for base class"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the variables whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiGlpk/OsiGlpkSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiGlpk/OsiGlpkSolverInterface.hpp", "filename": "OsiGlpkSolverInterface.hpp", "file": "OsiGlpkSolverInterface.hpp", "brief": "Osi interface for GNU Linear Programming Kit (GLPK)\nCopyright (C) 2001, Vivian De Smedt, Braden Hunsaker. EPL license.\n\nOsiGlpkSolverInterface wraps GLPK via the Osi abstraction. Uses glp_prob\nhandle (LPX typedef for compatibility). Implements initialSolve/resolve/\nbranchAndBound, warm starts via CoinWarmStartBasis, and GLPK-specific\nparameter handling. Supports both simplex and MIP solving.", "see": ["OsiSolverInterface for base class"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the variables whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n\t         end of the array of the indices of the constraints whose\n\t\t <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/OsiCpx/OsiCpxSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiCpx/OsiCpxSolverInterface.hpp", "filename": "OsiCpxSolverInterface.hpp", "file": "OsiCpxSolverInterface.hpp", "brief": "Osi interface for IBM ILOG CPLEX optimizer\nAuthor: Tobias Pfender (ZIB Berlin). EPL license.\n\nOsiCpxSolverInterface wraps CPLEX via the Osi abstraction. Uses CPXLPptr\nand CPXENVptr handles for CPLEX Callable Library. Implements initialSolve/\nresolve/branchAndBound, row/column manipulation (OsiRowCut, OsiColCut),\nwarm starts via CoinWarmStartBasis, and CPLEX-specific parameter handling.", "see": ["OsiSolverInterface for base class"], "param": ["<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/Osi/OsiAuxInfo.hpp": {"path": "layer-1/Osi/src/Osi/OsiAuxInfo.hpp", "filename": "OsiAuxInfo.hpp", "file": "OsiAuxInfo.hpp", "brief": "Auxiliary information for algorithm-specific solver customization", "algorithm": "B&B Solver Customization Interface (Soft/Hard Bounds, Callbacks)\nCopyright (C) 2006, International Business Machines Corporation.\nEPL-1.0 license.\n\nOsiAuxInfo: structured replacement for appData_ pointer, enabling algorithm\ncustomization. OsiBabSolver: derived class for exotic solvers (nonlinear,\nvolume algorithm) in branch-and-bound - tracks solutions, enables\ntryCut/tryBranch callbacks, supports soft/hard bounds and priority-based cuts.", "see": ["OsiSolverInterface for main solver abstraction"], "has_pass2": true}, "src/Osi/OsiRowCut.hpp": {"path": "layer-1/Osi/src/Osi/OsiRowCut.hpp", "filename": "OsiRowCut.hpp", "file": "OsiRowCut.hpp", "brief": "Row-based cutting plane (linear inequality)\n\nRow cuts are the most common form of cutting planes, representing\na linear inequality constraint: lb <= a'x <= ub", "algorithm": "Cutting Planes in MIP:\nLinear inequalities that cut off fractional LP solutions without\nremoving any integer-feasible points.\n\nCUT VALIDITY:\nA cut a'x ≤ b is valid if it satisfies all integer solutions:\n  ∀x ∈ S_I : a'x ≤ b (where S_I is feasible integer set)\n\nCUT EFFECTIVENESS:\n- Violated: a'x* > b for current LP solution x*\n- Depth: (a'x* - b) / ‖a‖ (normalized distance from hyperplane)\n- Efficacy: violation per unit norm, used for ranking cuts\n\nCOMMON CUT FAMILIES:\n- Gomory cuts: From LP simplex tableau non-integrality\n- MIR (Mixed-Integer Rounding): Round coefficients to strengthen\n- Knapsack covers: From single-row knapsack structure\n- Clique/odd-cycle: From conflict graph analysis\n- Flow covers: From network flow structure\n- Lift-and-project: From disjunctive arguments", "math": "Cut inequality:\nlb ≤ Σᵢ aᵢ·xᵢ ≤ ub\n\nViolation at LP solution x*: v = max(lb - a'x*, a'x* - ub, 0)\nEfficacy: e = v / ‖a‖₂", "complexity": "- Violation check: O(nnz) where nnz = nonzeros in cut\n- Add to LP: O(nnz) for coefficient insertion\n- Storage: O(nnz) sparse representation", "ref": ["Nemhauser & Wolsey (1988). \"Integer and Combinatorial Optimization\".\n  Chapter II.1 on valid inequalities."], "see": ["OsiCut for base class", "OsiCuts for cut pool management", "Cgl (Cut Generation Library) for cut generators", "CoinPackedVector for sparse row representation"], "has_pass2": true}, "src/Osi/OsiCollections.hpp": {"path": "layer-1/Osi/src/Osi/OsiCollections.hpp", "filename": "OsiCollections.hpp", "file": "OsiCollections.hpp", "brief": "STL container typedefs for Osi cut and data collections\nCopyright (C) 2000, International Business Machines Corporation.\nEPL-1.0 license.\n\nStandard typedefs: OsiVectorInt, OsiVectorDouble, OsiVectorColCutPtr,\nOsiVectorRowCutPtr, OsiVectorCutPtr. Provides cleaner syntax for\nfrequently-used vector types throughout Osi code.", "has_pass2": false}, "src/Osi/OsiBranchingObject.hpp": {"path": "layer-1/Osi/src/Osi/OsiBranchingObject.hpp", "filename": "OsiBranchingObject.hpp", "file": "OsiBranchingObject.hpp", "brief": "Branch-and-bound objects and branching decisions\n\nThis file defines the object-oriented framework for branching in MIP:\n\n- **OsiObject**: Abstract base for anything that can be branched on\n  (integer variables, SOS constraints, etc.)\n- **OsiBranchingObject**: Describes how to perform a specific branch\n- **OsiBranchingInformation**: Solver state passed to branching decisions", "algorithm": "Two-Way vs N-Way Branching:\nStandard is two-way (down/up), but extensible.\n\n  Two-way on integer x at value x*:\n    Down: x ≤ floor(x*)\n    Up: x ≥ ceil(x*)\n\n  N-way (e.g., for SOS):\n    Multiple children from single branch point", "math": "For integer variable x with value x*:\n  infeasibility = min(x* - floor(x*), ceil(x*) - x*)\n  Maximum infeasibility is 0.5 (exactly midway)\n  Infeasibility 0 means integer-feasible\n\nSELECTION STRATEGIES (use infeasibility):\n  - Most infeasible: Branch on max infeasibility\n  - Most fractional: Same as most infeasible\n  - Priority-weighted: priority × infeasibility", "see": ["OsiSolverInterface::branchAndBound for MIP solving", "Cbc for full branch-and-cut implementation"], "has_pass2": true}, "src/Osi/OsiRowCutDebugger.hpp": {"path": "layer-1/Osi/src/Osi/OsiRowCutDebugger.hpp", "filename": "OsiRowCutDebugger.hpp", "file": "OsiRowCutDebugger.hpp", "brief": "Validate cuts against known optimal solutions", "algorithm": "Cut Validity Checking Against Known Solutions (MIPLIB)", "has_pass2": true}, "src/Osi/OsiFeatures.hpp": {"path": "layer-1/Osi/src/Osi/OsiFeatures.hpp", "filename": "OsiFeatures.hpp", "file": "OsiFeatures.hpp", "brief": "MIP problem features for algorithm selection and parameter tuning", "algorithm": "MIP Feature Extraction for Algorithm Selection (207 Features, O(nz))\nCopyright (C) 2020, COIN-OR Foundation. EPL-1.0 license.\n\nExtracts 207 structural features from MIP models for algorithm recommendation:\n- Matrix statistics: density, nz distribution, coefficient ranges\n- Variable types: binary, general integer, continuous, unbounded counts\n- Constraint patterns: partitioning, packing, covering, cardinality,\n  knapsack (integer, invariant, bin-packing), singleton, aggregation,\n  precedence, variable bound, flow (binary, mixed)\n- Objective/RHS statistics: min, max, avg, stddev, integrality\n\nAll features computed in O(nz) time. Used for ML-based algorithm selection.", "ref": ["Vilas Boas et al. \"Optimal Decision Trees for Algorithm Selection\"\n     Int. Trans. Oper. Res. (2019) DOI:10.1111/itor.12724"], "has_pass2": true}, "src/Osi/OsiSolverParameters.hpp": {"path": "layer-1/Osi/src/Osi/OsiSolverParameters.hpp", "filename": "OsiSolverParameters.hpp", "file": "OsiSolverParameters.hpp", "brief": "Parameter enums for cross-solver configuration in Osi\nCopyright (C) 2000, International Business Machines Corporation.\nEPL-1.0 license.\n\nDefines OsiIntParam (iteration limits, name discipline), OsiDblParam\n(objective limits, tolerances, offset), OsiStrParam (problem/solver names),\nOsiHintParam (crash, presolve, scaling, pivoting hints with OsiHintStrength).\nSolvers implement these via get/setIntParam, get/setDblParam, etc.", "see": ["OsiSolverInterface::setIntParam, setDblParam, setHintParam"], "has_pass2": false}, "src/Osi/OsiChooseVariable.hpp": {"path": "layer-1/Osi/src/Osi/OsiChooseVariable.hpp", "filename": "OsiChooseVariable.hpp", "file": "OsiChooseVariable.hpp", "brief": "Variable selection strategies for branch-and-bound\n\nIn MIP solving, choosing which variable to branch on significantly\naffects tree size and solve time. This file provides:\n\n- OsiChooseVariable: Base class for branching variable selection\n- OsiChooseStrong: Strong branching (evaluates candidates by solving LPs)", "algorithm": "Branching Variable Selection:\nCritical decision in branch-and-bound affecting tree size by orders\nof magnitude.\n\nMOST INFEASIBLE:\n  score(j) = min(fⱼ, 1-fⱼ) where fⱼ = xⱼ - ⌊xⱼ⌋\n  Select j* = argmax{score(j)}\n  Simple but often poor - ignores objective impact\n\nPSEUDO-COST BRANCHING:\n  Estimate objective change from historical data:\n  Δ⁻ⱼ ≈ σ⁻ⱼ · fⱼ,  Δ⁺ⱼ ≈ σ⁺ⱼ · (1-fⱼ)\n  where σ⁻ⱼ, σ⁺ⱼ are per-unit degradation estimates\n  Requires initialization (unreliable for unobserved variables)\n\nSTRONG BRANCHING:\n  Solve LP relaxations for both child problems:\n  - Fix xⱼ ≤ ⌊xⱼ⌋, solve LP → objective zⱼ⁻\n  - Fix xⱼ ≥ ⌈xⱼ⌉, solve LP → objective zⱼ⁺\n  Actual degradation: Δ⁻ⱼ = zⱼ⁻ - z, Δ⁺ⱼ = zⱼ⁺ - z\n  Accurate but expensive (2 LP solves per candidate)\n\nRELIABILITY BRANCHING:\n  Use strong branching until variable has k observations,\n  then switch to pseudo-costs. Typically k = 4-8.\n  Balances accuracy with efficiency.", "math": "Variable selection scoring:\n  Product score: score(j) = max(ε, Δ⁻ⱼ) · max(ε, Δ⁺ⱼ)\n  Weighted score: score(j) = (1-μ)·min(Δ⁻ⱼ,Δ⁺ⱼ) + μ·max(Δ⁻ⱼ,Δ⁺ⱼ)\n  Select j* = argmax{score(j)}\n\nPseudo-cost update: σ⁻ⱼ = (1/k)·Σᵢ(Δᵢ⁻/fᵢ) over k observations", "complexity": "- Most infeasible: O(n) scan of fractional variables\n- Pseudo-cost: O(n) with O(1) per variable\n- Strong branching: O(k·LP_solve) for k candidates\n- Reliability: amortizes strong branching cost over time", "ref": ["Achterberg, Koch & Martin (2005). \"Branching rules revisited\".\n  Operations Research Letters 33(1):42-54."], "see": ["OsiBranchingObject for branch execution", "Cbc for more sophisticated variable selection"], "has_pass2": true}, "src/Osi/OsiSolverBranch.hpp": {"path": "layer-1/Osi/src/Osi/OsiSolverBranch.hpp", "filename": "OsiSolverBranch.hpp", "file": "OsiSolverBranch.hpp", "brief": "Branch information with tighter bounds for both directions", "algorithm": "Floor/Ceiling Branch Storage with Bound Arrays\nCopyright (C) 2005, International Business Machines Corporation.\nEPL-1.0 license.\n\nOsiSolverBranch stores branch decisions as bound changes: addBranch creates\nfloor/ceil branches on integer variables, applyBounds installs bounds for\na given direction (way=-1 first child, +1 second). Compact storage via\nstart_[5] indices and indices_/bound_ arrays. Supports column or row bounds.", "see": ["OsiSolverInterface for applying branches during B&B"], "has_pass2": true}, "src/Osi/OsiPresolve.hpp": {"path": "layer-1/Osi/src/Osi/OsiPresolve.hpp", "filename": "OsiPresolve.hpp", "file": "OsiPresolve.hpp", "brief": "OSI interface to problem simplification (presolve)\n\nPresolve reduces problem size before solving by applying\ntransformations that preserve optimal solutions:\n- Singleton row/column removal\n- Bound tightening\n- Coefficient reduction\n- Duplicate row/column detection", "algorithm": "Presolve Transformations:\nProblem reductions that simplify LP/MIP while preserving optimality.\nEach transform has a postsolve reverse to recover original solution.\n\nSINGLETON ROW (doubleton elimination):\nRow with single variable: aⱼxⱼ = b → fix xⱼ = b/aⱼ\nRemoves row and substitutes variable throughout\n\nSINGLETON COLUMN:\nVariable appears in one constraint only:\nCompute implied bounds from that constraint, fix if possible\n\nFORCED ROW:\nRow bounds force all variables to their bounds:\nIf lb = ub for a constraint, all variables are determined\n\nDOMINATED COLUMN:\nVariable dominated by another with better objective:\nxⱼ can be set to bound if another variable is always better\n\nBOUND TIGHTENING:\nDerive tighter bounds from constraints:\nFrom Σaᵢⱼxⱼ ≤ b with known bounds, improve variable bounds\n\nPROBING:\nTemporarily fix binary to 0/1, propagate implications:\nMay discover fixed variables, tightened bounds, or cuts", "math": "Bound propagation:\nFor constraint Σⱼ aⱼxⱼ ≤ b:\n  xᵢ ≤ (b - Σⱼ≠ᵢ min(aⱼlⱼ, aⱼuⱼ)) / aᵢ when aᵢ > 0\n\nProblem reduction: typically 10-50% smaller after presolve", "complexity": "- Single pass: O(nnz) scan of matrix\n- Full presolve: O(k·nnz) for k iterations until fixpoint\n- Postsolve: O(transforms) reverse order application", "ref": ["Andersen & Andersen (1995). \"Presolving in Linear Programming\".\n  Mathematical Programming 71:221-245.\n\nWorkflow:\n1. presolvedModel() creates simplified problem\n2. Solve the simplified problem\n3. postsolve() transforms solution back to original space"], "see": ["CoinPresolveMatrix for implementation details", "ClpPresolve for Clp-specific presolve"], "has_pass2": true}, "src/Osi/OsiCuts.hpp": {"path": "layer-1/Osi/src/Osi/OsiCuts.hpp", "filename": "OsiCuts.hpp", "file": "OsiCuts.hpp", "brief": "Container for collections of row cuts and column cuts", "algorithm": "Cut Pool Container for B&C\nOsiCuts serves as a cut pool for branch-and-cut algorithms:\n- Stores both row cuts (linear inequalities) and column cuts (bound changes)\n- Provides iteration over all cuts\n- Supports insertion, removal, and transfer of cuts\n\nTypical usage in cut generation:\n1. Cut generator creates OsiCuts object\n2. Generator adds cuts via insert()\n3. Solver applies cuts via OsiSolverInterface::applyCuts()", "see": ["OsiRowCut for linear inequality cuts", "OsiColCut for variable bound cuts", "OsiSolverInterface::applyCuts for adding cuts to solver", "OsiSolverInterface::applyCuts for applying cuts to a model"], "has_pass2": true}, "src/Osi/OsiColCut.hpp": {"path": "layer-1/Osi/src/Osi/OsiColCut.hpp", "filename": "OsiColCut.hpp", "file": "OsiColCut.hpp", "brief": "Column-based cuts for variable bound tightening\n\nColumn cuts represent bound changes on variables rather than\nadding new constraints. They are used for:\n- Bound tightening from probing\n- Implication-based bound strengthening\n- Reduced cost fixing", "algorithm": "Column Cut Application:\nEfficient bound update procedure.\n\n  For each (index, newLb) in lbs_:\n    solver.setColLower(index, max(oldLb, newLb))\n\n  For each (index, newUb) in ubs_:\n    solver.setColUpper(index, min(oldUb, newUb))", "math": "For minimization with UB = cutoff:\n    If r̄_j > 0 and c'x_LP + r̄_j(u_j - x*_j) > UB:\n      Can fix x_j = l_j\n\n  PROBING:\n    Tentatively fix x_j, propagate implications\n    If leads to infeasibility: opposite fixing valid\n    If leads to bound changes: valid column cuts\n\n  IMPLICATION:\n    From logical relationships (e.g., cliques)\n    x_i = 1 implies x_j = 0 → bound tightening", "complexity": "O(nnz in cut) for application\nMore efficient than row cuts when bounds suffice.", "see": ["OsiRowCut for linear inequality cuts", "OsiCut for base class"], "has_pass2": true}, "src/Osi/OsiSolverInterface.hpp": {"path": "layer-1/Osi/src/Osi/OsiSolverInterface.hpp", "filename": "OsiSolverInterface.hpp", "file": "OsiSolverInterface.hpp", "brief": "Abstract base class defining the Open Solver Interface (OSI)\n\nOSI provides a uniform API for accessing different LP/MIP solvers\n(Clp, CPLEX, Gurobi, GLPK, etc.) through a common interface. This\nallows solver-independent application code.\n\nKey capabilities:\n- LP relaxation solving (initialSolve, resolve)\n- Model query (getColLower, getRowUpper, getObjCoefficients)\n- Solution query (getColSolution, getRowPrice, getReducedCost)\n- Problem modification (setColBounds, addRow, addCol)\n- Warm starting (getWarmStart, setWarmStart)\n- Cut management (applyCuts, applyRowCuts)\n- MIP support (setInteger, branchAndBound)\n\nTypical workflow:\n1. Create solver-specific instance (e.g., OsiClpSolverInterface)\n2. Load problem via loadProblem() or readMps()\n3. Call initialSolve() for first LP solution\n4. Call resolve() after modifications\n5. Query solution via getColSolution(), getObjValue()", "algorithm": "Design Pattern: Abstract Factory + Strategy\n  - Factory: concrete implementations (OsiClpSolverInterface, etc.)\n    instantiate solver-specific backends\n  - Strategy: algorithm choice (simplex vs barrier) delegated to\n    underlying solver; OSI provides uniform control interface\n  - Bridge: decouples client code from solver implementation details\n\n@invariant Solution pointers (getColSolution, getRowPrice, etc.) are\n  invalidated by any problem modification or subsequent solve call.\n  Client code must copy data if needed across modifications.\n\n@invariant Warm start objects must be compatible with the solver that\n  created them. Cross-solver warm starting requires basis translation.", "complexity": "All query methods (getColSolution, etc.): O(1) pointer return\n  initialSolve/resolve: delegated to solver, typically O(m·n·iterations)\n  applyCuts: O(cuts × row_length) for constraint matrix update", "ref": ["Lougee-Heimer et al. (2003). \"The Common Optimization INterface\n  for Operations Research\". IBM J. Research & Development 47(1):57-66."], "see": ["OsiClpSolverInterface for Clp implementation", "OsiCuts for cut pool management", "CoinPackedMatrix for constraint matrix format"], "return": "Array of size getNumCols() with solution values", "has_pass2": true}, "src/Osi/OsiCut.hpp": {"path": "layer-1/Osi/src/Osi/OsiCut.hpp", "filename": "OsiCut.hpp", "file": "OsiCut.hpp", "brief": "Abstract base class for cutting planes in branch-and-cut\n\nCutting planes are linear inequalities that can be added to an LP\nrelaxation to tighten the formulation without cutting off any\ninteger-feasible solutions.", "algorithm": "Global vs Local Cuts:\nScope of cut validity in B&B tree.\n\n  GLOBAL (globallyValid_ = true):\n    Valid throughout entire B&B tree\n    Can be added at any node\n    Example: Gomory cuts from root LP\n\n  LOCAL (globallyValid_ = false):\n    Valid only in subtree where generated\n    Must track node of origin\n    Example: Cuts using branching bounds", "math": "A valid inequality (cut) a'x ≥ β satisfies:\n  - a'x* ≥ β for all x* ∈ conv(feasible integer solutions)\n  - May be violated by LP optimum x_LP: a'x_LP < β\n\nCUTTING PLANE METHOD:\n  1. Solve LP relaxation → x_LP\n  2. If x_LP integer: DONE (optimal)\n  3. Find violated cut: a'x ≥ β with a'x_LP < β\n  4. Add cut to LP, goto 1", "see": ["OsiRowCut for linear inequality cuts", "OsiColCut for variable bound cuts", "OsiCuts for managing collections of cuts", "Cgl (Cut Generation Library) for cut generators"], "has_pass2": true}, "src/OsiMsk/OsiMskSolverInterface.hpp": {"path": "layer-1/Osi/src/OsiMsk/OsiMskSolverInterface.hpp", "filename": "OsiMskSolverInterface.hpp", "file": "OsiMskSolverInterface.hpp", "brief": "Osi interface for MOSEK optimizer (version 5.0+)\nAuthor: Bo Jensen (MOSEK ApS). EPL license.\n\nOsiMskSolverInterface wraps MOSEK via the Osi abstraction. Uses MSKtask_t\nand MSKenv_t handles (void* for forward declaration without mosek.h).\nImplements initialSolve/resolve/branchAndBound, parameter handling, and\nsupports LP/MIP/QP/conic solving through MOSEK's unified interface.", "see": ["OsiSolverInterface for base class"], "param": ["<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose </em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "<code>[indexfirst,indexLast]</code> contains the indices of\n    \t         the constraints whose type changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}}}, "qpOASES": {"name": "qpOASES", "file_count": 17, "pass2_count": 16, "files": {"include/qpOASES.hpp": {"path": "layer-1/qpOASES/include/qpOASES.hpp", "filename": "qpOASES.hpp", "file": "qpOASES.hpp", "brief": "Main include file for qpOASES quadratic programming solver\n\nqpOASES solves convex quadratic programs (QPs) of the form:", "math": "lbx <= x <= ubx    (bounds)\n\nKey features:\n- Online active set strategy (efficient for parametric QPs)\n- Hot-starting from previous solutions\n- Handles dense and sparse problems\n- Real-time capable (bounded iteration counts)\n\nMain classes:\n- QProblemB: QP with only box constraints (no A matrix)\n- QProblem: Full QP with linear constraints\n- SQProblem: Sequential QP with varying Hessian", "algorithm": "Online Active Set Strategy - works well for model\n           predictive control (MPC) where QPs are solved in sequence", "ref": ["Ferreau et al. (2014). \"qpOASES: A parametric active-set algorithm\n     for quadratic programming\". Math. Prog. Comp. 6(4):327-363."], "see": ["QProblem for general QP solving", "QProblemB for box-constrained QPs (faster)"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}, "include/qpOASES/ConstraintProduct.hpp": {"path": "layer-1/qpOASES/include/qpOASES/ConstraintProduct.hpp", "filename": "ConstraintProduct.hpp", "file": "ConstraintProduct.hpp", "brief": "User-defined constraint evaluation interface for structured matrices", "algorithm": "User-Defined Constraint Product for Structured QPs\nAllows exploiting constraint matrix structure (banded, sparse patterns,\nnetwork flows) by providing custom Ax evaluation instead of dense GEMV.\nCritical for MPC applications with predictable sparsity.", "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2009-2017", "has_pass2": true}, "include/qpOASES/SparseSolver.hpp": {"path": "layer-1/qpOASES/include/qpOASES/SparseSolver.hpp", "filename": "SparseSolver.hpp", "file": "include/qpOASES/SparseSolver.hpp", "brief": "Sparse linear solver interfaces for Schur-complement QP method", "algorithm": "Sparse Symmetric Indefinite Factorization Interface\n\nSparseSolver provides an abstract interface to sparse symmetric\nindefinite factorization codes used in qpOASES's Schur-complement\napproach for large-scale QP.\n\n**Supported backends:**\n- MA27: Harwell multifrontal solver (classic, robust)\n- MA57: Harwell multifrontal with pivoting improvements\n- MUMPS: Parallel multifrontal solver\n\n**Schur complement context:**\nFor QPs with many simple bounds but few general constraints,\nthe KKT system can be solved efficiently via:", "math": "[A  0 ]\n\nThe sparse solver handles the reduced system on active constraints\nafter eliminating variables at simple bounds.\n\n**Key operations:**\n- setMatrixData(): Set lower-triangular Harwell-Boeing format\n- factorize(): Compute LDL' factorization with pivoting\n- solve(): Forward/backward substitution\n- getNegativeEigenvalues(): Inertia for optimality verification", "complexity": "O(n³) worst case, O(n·nnz) typical for sparse systems", "author": "Andreas Waechter, Dennis Janka\n@version 3.2", "date": "2012-2017", "see": ["SQProblemSchur.hpp for Schur-complement QP implementation"], "has_pass2": true}, "include/qpOASES/SQProblemSchur.hpp": {"path": "layer-1/qpOASES/include/qpOASES/SQProblemSchur.hpp", "filename": "SQProblemSchur.hpp", "file": "SQProblemSchur.hpp", "brief": "Sparse QP solver using Schur complement for active-set updates", "algorithm": "Schur Complement Active-Set Method:\nExploits sparsity in QPs with changing active sets using Schur complement\nupdates instead of full refactorization.\n\nKKT SYSTEM STRUCTURE:\nAt each iteration, solve:\n  [H   Aₐ'] [x]   [-g ]\n  [Aₐ  0  ] [λ] = [bₐ ]\nwhere Aₐ is the active constraint matrix.\n\nSCHUR COMPLEMENT APPROACH:\nFactor H once (sparse Cholesky/LDL): H = LDL'\nWhen active set changes by adding/removing constraint i:\n\nAdding constraint (rank-1 update):\n  New KKT = Old KKT + [0; aᵢ][0; aᵢ]'\n  Schur complement: S_new = S + aᵢ'H⁻¹aᵢ\n  Update S via low-rank formula (no refactor of H)\n\nRemoving constraint:\n  Downdate Schur complement analogously\n\nSOLVE WITH SCHUR:\n  1. Solve H·y = g (using fixed factorization)\n  2. Solve S·λ = Aₐy - bₐ (small dense system)\n  3. Solve H·x = g - Aₐ'λ (reuse factorization)", "math": "Schur complement:\n  S = Aₐ·H⁻¹·Aₐ'\n  dim(S) = |active_set| << n (typically small)\n\nWhen active set changes by ±1:\n  S_new = S ± (H⁻¹aᵢ)(H⁻¹aᵢ)' / (aᵢ'H⁻¹aᵢ)", "complexity": "- Initial H factorization: O(nnz(H)^{3/2}) sparse Cholesky\n- Schur update per active-set change: O(|A|²) dense operations\n- Total per QP: O(nnz + |A|²·k) for k iterations\n- Advantage: avoids O(n³) refactorization each iteration", "ref": ["Waechter & Biegler (2006). \"On the Implementation of an Interior-Point\n  Filter Line-Search Algorithm for Large-Scale Nonlinear Programming\".\n  Mathematical Programming 106(1):25-57."], "author": "Andreas Waechter, Dennis Janka\n@version 3.2", "date": "2012-2017", "see": ["SQProblem for dense version", "SparseSolver for underlying sparse factorization"], "has_pass2": true}, "include/qpOASES/Bounds.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Bounds.hpp", "filename": "Bounds.hpp", "algorithm": "Working Set Management for Bounds:\nTrack which box constraints are active in active-set QP.", "math": "QP with bounds: min ½x'Hx + g'x  s.t. l ≤ x ≤ u\n\n  WORKING SET PARTITION:\n    For each bound i ∈ {1,...,n}:\n      - INACTIVE (ST_INACTIVE): l_i < x_i < u_i\n      - LOWER (ST_LOWER): x_i = l_i (active at lower)\n      - UPPER (ST_UPPER): x_i = u_i (active at upper)\n      - FIXED (ST_EQUALITY): l_i = u_i (equality)\n\n  INDEX SETS:\n    - free: indices where bound is inactive\n    - fixed: indices where bound is active\n    - freeFreezer/fixedFreezer: frozen during QP iteration\n\n  ACTIVE SET CHANGES:\n    moveActiveToInactive(i): x_i leaves boundary\n    moveInactiveToActive(i, bound): x_i hits boundary", "complexity": "O(1) per bound status query/change", "has_pass2": true}, "include/qpOASES/Constraints.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Constraints.hpp", "filename": "Constraints.hpp", "file": "include/qpOASES/Constraints.hpp", "brief": "Working set management for general constraints in active-set QP", "algorithm": "Active Set Working Set Management\n\nConstraints manages the working set of general linear constraints\n(Ax ≤ b) in the active-set QP algorithm. It tracks which constraints\nare currently active (binding) vs inactive.\n\n**Working set partitioning:**\n- Active: Constraints treated as equalities at current iterate\n- Inactive: Constraints with slack (not binding)\n\n**Status tracking:**\n- ST_LOWER: Active at lower bound (Aᵢx = lᵢ)\n- ST_UPPER: Active at upper bound (Aᵢx = uᵢ)\n- ST_INACTIVE: Strictly satisfied (lᵢ < Aᵢx < uᵢ)\n- ST_DISABLED: Temporarily removed from consideration\n\n**Active-set transitions:**\n- moveActiveToInactive(): Constraint leaves working set (ratio test)\n- moveInactiveToActive(): Constraint enters working set (blocking)\n- flipFixed(): Switch between lower/upper bound activity\n\n**MPC support:**\n- shift(): Hot-start for receding horizon (drop old, replicate new)\n- rotate(): Circular buffer for periodic problems", "complexity": "O(1) for status queries, O(n) for index list operations", "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "see": ["SubjectTo.hpp for base class", "Bounds.hpp for simple bound management"], "has_pass2": true}, "include/qpOASES/QProblemB.hpp": {"path": "layer-1/qpOASES/include/qpOASES/QProblemB.hpp", "filename": "QProblemB.hpp", "file": "QProblemB.hpp", "brief": "Box-constrained QP solver (bounds only, no linear constraints)\n\nSolves QPs with only variable bounds (no constraint matrix A):", "math": "s.t. lb <= x <= ub\n\nThis is more efficient than QProblem when there are no general\nlinear constraints. The active set consists only of bounds.\n\nKey methods:\n- init(): First QP solve (cold start)\n- hotstart(): Subsequent solves with modified g, lb, ub", "algorithm": "Solves KKT conditions directly when active set is known.\n           Uses Cholesky factorization of reduced Hessian.", "see": ["QProblem for QPs with general linear constraints"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}, "include/qpOASES/SQProblem.hpp": {"path": "layer-1/qpOASES/include/qpOASES/SQProblem.hpp", "filename": "SQProblem.hpp", "file": "SQProblem.hpp", "brief": "Sequential QP solver with varying Hessian and constraint matrices\n\nSQProblem extends QProblem to handle QPs where the Hessian H and\nconstraint matrix A change between solves. This is common in:\n- Nonlinear MPC (linearization changes each step)\n- Sequential Quadratic Programming (SQP) for NLP\n- Moving horizon estimation", "algorithm": "Sequential QP with Matrix Updates:\nEfficiently solves sequences of related QPs where both H and A change.\n\nHOTSTART WITH MATRIX CHANGES:\nGiven previous optimal active set W*, when H, A change:\n1. Update factorization of reduced Hessian Z'HZ\n2. Recompute KKT system factors with new matrices\n3. Use W* as initial working set (likely still optimal or close)\n4. Apply standard active-set iterations if needed\n\nMATRIX UPDATE STRATEGY:\n- Small changes: Rank-k updates to existing factorization\n- Large changes: Full refactorization (still faster than cold start)\n- Sparse matrices: Use SQProblemSchur for Schur complement approach\n\nSQP APPLICATION:\nFor min f(x) s.t. c(x)=0, g(x)≤0:\n  At iterate xₖ, solve QP subproblem:\n  min ∇f'p + ½p'∇²Lp  s.t. ∇c'p + c = 0, ∇g'p + g ≤ 0\n  where L is Lagrangian, H = ∇²L changes each iteration", "math": "QP subproblem at SQP iteration k:\n  Hₖ = ∇²ₓₓL(xₖ, λₖ)  (Hessian of Lagrangian)\n  Aₖ = [∇c(xₖ)'; ∇g(xₖ)']  (Jacobian of constraints)\n  Both vary with xₖ, requiring matrix updates each iteration", "complexity": "- Hotstart with matrix update: O(n³) for factorization update\n- Typically 1-5 active-set iterations after matrix change\n- Much faster than cold-start from scratch", "ref": ["Ferreau, Bock & Diehl (2008). \"An online active set strategy to\n  overcome the limitations of explicit MPC\". Int. J. Robust Nonlinear\n  Control 18(8):816-830."], "see": ["QProblem for fixed H and A", "Ipopt for full NLP solving that uses QP subproblems"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}, "include/qpOASES/Options.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Options.hpp", "filename": "Options.hpp", "file": "Options.hpp", "brief": "Configuration options for qpOASES QP solver\n\nThe Options class controls solver behavior including:\n- Termination criteria (tolerances, iteration limits)\n- Numerical parameters (regularization, pivoting)\n- Output/printing verbosity\n- Initial homotopy and scaling options\n\nPreset configurations:\n- setToDefault(): Balanced defaults\n- setToReliable(): Maximum robustness\n- setToMPC(): Fast for model predictive control\n- setToFast(): Maximum speed, less robust", "see": ["QProblem::setOptions for applying options"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": false}, "include/qpOASES/Indexlist.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Indexlist.hpp", "filename": "Indexlist.hpp", "file": "include/qpOASES/Indexlist.hpp", "brief": "Sorted index lists for efficient working set operations", "algorithm": "Sorted Index List with Binary Search\n\nIndexlist maintains sorted lists of variable/constraint indices for\nthe active-set QP algorithm, enabling O(log n) membership tests and\nefficient submatrix extraction.\n\n**Data structure:**\n- number[]: Stores indices in insertion order\n- iSort[]: Maintains sorted order for binary search\n- Doubly-linked via first/last for O(1) append/prepend\n\n**Key operations:**\n- addNumber(): Insert index, update sorted array\n- removeNumber(): Delete index, compact arrays\n- getIndex(): O(log n) lookup via binary search (findInsert)\n- isMember(): O(log n) membership test\n- getNumberArray(): O(1) access to raw index array\n\n**Usage in QP:**\n- Active bounds/constraints index sets\n- Free variable indices for null-space computation\n- Submatrix row/column selection for working set", "complexity": "O(log n) lookup, O(n) insertion/deletion", "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "see": ["SubjectTo.hpp for working set management using index lists"], "has_pass2": true}, "include/qpOASES/QProblem.hpp": {"path": "layer-1/qpOASES/include/qpOASES/QProblem.hpp", "filename": "QProblem.hpp", "file": "QProblem.hpp", "brief": "QP solver with general linear constraints\n\nSolves convex QPs with bounds and linear constraints:", "math": "lb  <= x <= ub       (bounds)\n\nKey methods:\n- init(): First QP solve (cold start)\n- hotstart(): Subsequent solves with modified data", "algorithm": "Online Active Set Strategy:\n  1. Start from a primal-dual feasible point\n  2. Track active set (constraints at bounds)\n  3. Update active set incrementally when parameters change\n  Efficient for parametric QPs where data varies smoothly.", "complexity": "O(n^3) worst case per iteration, but typically much\n            faster due to hot-starting and sparse updates.", "see": ["QProblemB for box-constrained QPs (no A matrix)", "SQProblem for varying Hessian"], "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "has_pass2": true}, "include/qpOASES/Flipper.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Flipper.hpp", "filename": "Flipper.hpp", "algorithm": "Flipping Bounds Recovery:\nStore factorization state for fast recovery from bound changes.\n\n  PROBLEM:\n    When bound moves to opposite side (l → u or u → l),\n    factorization update can be numerically unstable.\n    \"Flipping\" refers to this sudden direction change.\n\n  SOLUTION:\n    Before potentially problematic step:\n      1. Store R, Q, T factorizations in Flipper\n      2. Store working set (Bounds, Constraints)\n      3. Attempt active set change\n      4. If numerical trouble: restore from Flipper\n\n  STORAGE:\n    - R_: Cholesky factor of reduced Hessian\n    - Q_: Orthogonal factor\n    - T_: Schur complement factor (for constraints)\n    - bounds_/constraints_: working set snapshot\n\n  USE CASE:\n    Online QP in MPC: parameters change between solves\n    Hot-starting may flip bounds, needing recovery option", "complexity": "O(n² + nC·n) memory for snapshot", "has_pass2": true}, "include/qpOASES/Matrices.hpp": {"path": "layer-1/qpOASES/include/qpOASES/Matrices.hpp", "filename": "Matrices.hpp", "file": "include/qpOASES/Matrices.hpp", "brief": "Matrix classes for QP data with working-set-aware operations", "algorithm": "Matrix-Vector Operations for Active-Set QP\n\nProvides matrix abstractions optimized for active-set QP solving,\nsupporting efficient submatrix operations based on working sets.\n\n**Class hierarchy:**\n- Matrix: Abstract base with times(), transTimes(), getRow(), getCol()\n- SymmetricMatrix: Adds bilinear form x'Hx computation\n- DenseMatrix / SymDenseMat: Row-major dense storage\n- SparseMatrix / SymSparseMat: Column-compressed sparse (CSC)\n- SparseMatrixRow: Row-compressed sparse (CSR)\n\n**Key QP operations:**\n- times(irows, icols, ...): A[irows,icols] * x for working set\n- transTimes(irows, icols, ...): A[irows,icols]' * x\n- bilinear(icols, x): x'·H[icols,icols]·x for reduced Hessian\n- getSparseSubmatrix(): Extract in Harwell-Boeing format\n\n**Storage formats:**\n- Dense: val[i*leaDim + j] row-major\n- CSC: jc[col] to jc[col+1] gives row indices in ir[], values in val[]\n- CSR: jr[row] to jr[row+1] gives col indices in ic[], values in val[]", "complexity": "Dense O(mn), Sparse O(nnz) for matrix-vector products", "author": "Andreas Potschka, Christian Kirches, Hans Joachim Ferreau\n@version 3.2", "date": "2009-2017", "see": ["QProblem.hpp for usage in active-set algorithm"], "has_pass2": true}, "include/qpOASES/SubjectTo.hpp": {"path": "layer-1/qpOASES/include/qpOASES/SubjectTo.hpp", "filename": "SubjectTo.hpp", "file": "include/qpOASES/SubjectTo.hpp", "brief": "Base class for working set management in active-set QP", "algorithm": "Working Set Index Management\n\nSubjectTo is the base class for managing bounds (Bounds) and constraints\n(Constraints) in qpOASES's parametric active-set algorithm.\n\n**Type classification:**\n- ST_UNBOUNDED: No bounds (-∞, +∞)\n- ST_BOUNDED: Both bounds finite [l, u]\n- ST_EQUALITY: Fixed value (l = u)\n- ST_DISABLED: Temporarily ignored\n\n**Status tracking:**\n- ST_LOWER: Active at lower bound\n- ST_UPPER: Active at upper bound\n- ST_INACTIVE: Currently not binding\n- ST_INFEASIBLE_LOWER/UPPER: Violated (restoration needed)\n\n**Index list operations:**\n- addIndex(): Add to active/inactive list on setup\n- removeIndex(): Remove during working set change\n- swapIndex(): Reorder for numerical stability\n\n**Online MPC support:**\n- shift(): For receding horizon control (discard old data)\n- rotate(): For periodic problems (circular reordering)", "complexity": "O(1) type/status access, O(n) for list modifications", "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "see": ["Bounds.hpp, Constraints.hpp for derived classes", "Indexlist.hpp for index set implementation"], "has_pass2": true}, "include/qpOASES/LapackBlasReplacement.hpp": {"path": "layer-1/qpOASES/include/qpOASES/LapackBlasReplacement.hpp", "filename": "LapackBlasReplacement.hpp", "file": "LapackBlasReplacement.hpp", "brief": "LAPACK/BLAS interface declarations for qpOASES linear algebra", "algorithm": "BLAS-3/LAPACK Interface (GEMM, POTRF, TRTRS, TRCON)\nExternal declarations for core linear algebra: matrix multiplication (GEMM),\nCholesky factorization (POTRF), triangular solves (TRTRS), and condition\nnumber estimation (TRCON). Supports single/double precision and optional\nnamespace prefixing to avoid symbol conflicts.", "author": "Andreas Potschka, Hans Joachim Ferreau, Christian Kirches\n@version 3.2", "date": "2009-2017", "has_pass2": true}, "include/qpOASES/extras/SolutionAnalysis.hpp": {"path": "layer-1/qpOASES/include/qpOASES/extras/SolutionAnalysis.hpp", "filename": "SolutionAnalysis.hpp", "file": "include/qpOASES/extras/SolutionAnalysis.hpp", "brief": "Post-optimality analysis: KKT verification and sensitivity", "algorithm": "QP Solution Analysis and Sensitivity\n\nSolutionAnalysis provides tools for verifying QP solutions and\ncomputing sensitivity information for uncertain problem data.\n\n**KKT violation analysis:**\nComputes maximum violation of optimality conditions:", "math": "Var(x*, λ*, z*) = J · Var(g, b) · J'\nwhere J is the sensitivity Jacobian at the solution.\n\n**Curvature analysis:**\ncheckCurvatureOnStronglyActiveConstraints() verifies positive\ndefiniteness of reduced Hessian on strongly active constraints,\ndetecting potential negative curvature directions.", "complexity": "O(n²) for KKT check, O(n³) for variance-covariance", "ref": ["Ferreau et al., \"qpOASES: A parametric active-set algorithm\n     for quadratic programming\", Math. Prog. Comp., 2014"], "author": "Hans Joachim Ferreau (thanks to Boris Houska)\n@version 3.2", "date": "2008-2017", "see": ["QProblem.hpp, SQProblem.hpp for QP solvers"], "has_pass2": true}, "include/qpOASES/extras/OQPinterface.hpp": {"path": "layer-1/qpOASES/include/qpOASES/extras/OQPinterface.hpp", "filename": "OQPinterface.hpp", "file": "include/qpOASES/extras/OQPinterface.hpp", "brief": "Online QP Benchmark Collection interface", "algorithm": "Online QP Benchmark Execution and Validation\n\nOQPinterface provides utilities for running parametric QP sequences\nfrom the Online QP Benchmark Collection, testing hot-starting\nperformance on MPC-style problem sequences.\n\n**Benchmark structure:**\nEach benchmark contains a sequence of nQP related QPs with:\n- Fixed H (Hessian) and A (constraint matrix)\n- Varying g (gradient), lb/ub (bounds), lbA/ubA (constraint bounds)\n- Optional reference solutions (xOpt, yOpt, objOpt)\n\n**Key functions:**\n- readOqpDimensions(): Parse problem size from files\n- readOqpData(): Load full benchmark data\n- solveOqpBenchmark(): Solve sequence, measure performance\n- runOqpBenchmark(): End-to-end benchmark execution\n\n**Performance metrics:**\n- maxNWSR/avgNWSR: Working set recalculations (iterations)\n- maxCPUtime/avgCPUtime: Solve time per QP\n- maxStationarity/Feasibility/Complementarity: KKT residuals\n\n**Hot-start evaluation:**\nWith useHotstarts=true, each QP initializes from previous solution,\ntesting the parametric active-set method's online performance.", "complexity": "O(nQP × solve_time) for benchmark sequence", "author": "Hans Joachim Ferreau, Andreas Potschka, Christian Kirches\n@version 3.2", "date": "2007-2017", "see": ["http://www.qpOASES.org/onlineQP (archived)"], "has_pass2": true}}}}}, "layer-2": {"name": "layer-2", "library_count": 4, "libraries": {"ADOL-C": {"name": "ADOL-C", "file_count": 25, "pass2_count": 12, "files": {"ADOL-C/swig/pydirectors.hpp": {"path": "layer-2/ADOL-C/ADOL-C/swig/pydirectors.hpp", "filename": "pydirectors.hpp", "file": "pydirectors.hpp", "brief": "Python director classes for external differentiated functions", "has_pass2": false}, "ADOL-C/swig/adolc_all_in.hpp": {"path": "layer-2/ADOL-C/ADOL-C/swig/adolc_all_in.hpp", "filename": "adolc_all_in.hpp", "file": "adolc_all_in.hpp", "brief": "Unified ADOL-C header for SWIG bindings", "has_pass2": false}, "ADOL-C/swig/pyedfclasses.hpp": {"path": "layer-2/ADOL-C/ADOL-C/swig/pyedfclasses.hpp", "filename": "pyedfclasses.hpp", "file": "pyedfclasses.hpp", "brief": "Python wrapper classes for external differentiated functions", "has_pass2": false}, "ADOL-C/swig/adubswigfuncs.hpp": {"path": "layer-2/ADOL-C/ADOL-C/swig/adubswigfuncs.hpp", "filename": "adubswigfuncs.hpp", "file": "adubswigfuncs.hpp", "brief": "SWIG operator overloads for badouble/adub types", "has_pass2": false}, "ADOL-C/swig/matrixmemory.hpp": {"path": "layer-2/ADOL-C/ADOL-C/swig/matrixmemory.hpp", "filename": "matrixmemory.hpp", "file": "matrixmemory.hpp", "brief": "C-style multi-dimensional array allocation for SWIG", "has_pass2": false}, "ADOL-C/include/adolc/checkpointing.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/checkpointing.h", "filename": "checkpointing.h", "file": "checkpointing.h", "brief": "Checkpointing support for memory-efficient reverse mode AD\n\nImplements checkpointing (also known as \"time-stepping\" or \"revolve\")\nfor computing adjoints of long time-stepping computations with bounded\nmemory. Instead of storing all intermediate states, only selected\n\"checkpoints\" are stored, and segments are recomputed as needed.", "algorithm": "Binomial Checkpointing for Reverse Mode AD:\nEnables reverse-mode differentiation of N-step computations with only\ns checkpoints (where s << N) by intelligently scheduling recomputation.\n\nPROBLEM:\nReverse mode AD of y = F_N(F_{N-1}(...F_1(x)...)) requires storing all\nintermediate states x_1, x_2, ..., x_N to compute adjoints. For long\ntime-stepping simulations with N = 10^6+ steps, this is infeasible.\n\nSOLUTION (Optimal Binomial Schedule):\n1. Divide N steps into segments using s checkpoint slots\n2. Store state at strategically chosen checkpoints\n3. During reverse sweep:\n   a. Restore from nearest checkpoint before current position\n   b. Recompute forward to current position\n   c. Perform single reverse step\n   d. Repeat with optimal checkpoint repositioning\n\nThe binomial schedule is provably optimal: with s checkpoints and\nallowing r repetitions of each step, exactly C(s+r, r) steps can\nbe differentiated (where C is the binomial coefficient).", "math": "Memory-computation trade-off:\n- Without checkpointing: Memory O(N), Recomputation O(1)\n- With s checkpoints: Memory O(s), Recomputation O(log(N)·N/s)\n- Optimal s ≈ √(N/c) where c = ratio of checkpoint to compute cost\n\nFor N steps with s checkpoints allowing r=1 repetition:\nMaximum steps: C(s+1,1) = s+1 (linear)\nMaximum steps with r repetitions: C(s+r,r) (grows super-linearly)", "complexity": "O(N·log(N)) total forward evaluations for N steps with\nO(√N) checkpoints. Per-step reverse cost remains O(1).", "ref": ["Griewank & Walther (2000). \"Algorithm 799: Revolve: An\n  Implementation of Checkpointing for the Reverse or Adjoint Mode\n  of Computational Differentiation\". ACM TOMS 26(1):19-45.", "Griewank & Walther (2008). \"Evaluating Derivatives\", Ch. 12.\n\nUse cases:\n- Time-stepping ODEs/PDEs where storing all steps would exhaust memory\n- Long computational chains with limited storage budget\n- Trade-off between memory usage and recomputation cost"], "see": ["revolve.h for the underlying scheduling algorithm", "externfcts.h for external function differentiation"], "has_pass2": true}, "ADOL-C/include/adolc/adolc.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adolc.h", "filename": "adolc.h", "file": "adolc.h", "brief": "Master include file for ADOL-C automatic differentiation library\n\nADOL-C (Automatic Differentiation by Overloading in C++) computes derivatives\nof mathematical functions via operator overloading and tape-based recording.\nRecords computation as a \"tape\" then replays in forward/reverse mode.", "algorithm": "Tape-Based Automatic Differentiation:\nComputes exact derivatives using operator overloading and tape replay:\n\nRECORDING PHASE:\n1. trace_on(tag) - Begin recording to tape with identifier 'tag'\n2. Use adouble type - Operations recorded as instruction sequence\n3. trace_off() - Finalize tape, optionally write to disk\n\nFORWARD MODE (compute directional derivatives):\n- Propagate derivatives from inputs to outputs: ẏ = J·ẋ\n- Cost: O(p·ops) for p directions, 'ops' = tape operations\n- Modes: zos (0th order), fos (1st order scalar), hos (higher order)\n\nREVERSE MODE (compute gradients/adjoints):\n- Propagate adjoints from outputs to inputs: x̄ = Jᵀ·ȳ\n- Cost: O(q·ops) for q adjoints, independent of input dimension!\n- Modes: fos_reverse, hos_reverse, hov_reverse", "math": "Forward vs Reverse complexity:\nFor f: ℝⁿ → ℝᵐ, computing full Jacobian J ∈ ℝᵐˣⁿ:\n- Forward mode: O(n) tape evaluations (one per input direction)\n- Reverse mode: O(m) tape evaluations (one per output adjoint)\nReverse is O(n/m) faster for m << n (e.g., scalar objective functions).\n\nHigher-order Taylor coefficients computed via univariate Taylor propagation.", "complexity": "O(ops) per tape evaluation, where ops = number of recorded operations.\nMemory: O(ops) for tape storage + O(ops) for reverse mode intermediates.", "ref": ["Griewank & Walther (2008). \"Evaluating Derivatives: Principles and\n  Techniques of Algorithmic Differentiation\", 2nd ed. SIAM.", "Walther & Griewank (2012). \"Getting Started with ADOL-C\".\n  Combinatorial Scientific Computing, Chapman & Hall/CRC."], "see": ["adtb_types.h for adouble/pdouble class definitions", "interfaces.h for low-level forward/reverse mode calls", "drivers/drivers.h for high-level derivative computation", "tape_interface.h for tape management functions", "Ipopt for NLP optimization using ADOL-C derivatives"], "has_pass2": true}, "ADOL-C/include/adolc/adolc_sparse.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adolc_sparse.h", "filename": "adolc_sparse.h", "file": "adolc_sparse.h", "brief": "Convenience header for sparse derivative computation\n\nInclude this file to access all sparse derivative functionality:\n- sparse_jac(): Sparse Jacobian computation\n- sparse_hess(): Sparse Hessian computation\n- jac_pat() / hess_pat(): Sparsity pattern detection\n- Bit-pattern forward/reverse modes", "see": ["adolc.h for main ADOL-C include (does not include sparse by default)", "sparse/sparsedrivers.h for detailed sparse driver documentation"], "has_pass2": false}, "ADOL-C/include/adolc/revolve.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/revolve.h", "filename": "revolve.h", "file": "revolve.h", "brief": "Optimal binomial checkpointing for memory-efficient reverse mode\n\nImplements the revolve algorithm (Griewank & Walther) for optimal\ncheckpoint placement in reverse mode automatic differentiation.", "algorithm": "Revolve - Optimal Checkpointing Schedule:\nComputes the provably optimal sequence of checkpoint operations to\nminimize total recomputation while staying within memory budget.\n\nSTATE MACHINE:\nThe revolve() function returns action codes that drive the outer loop:\n\n  revolve_advance   → Run forward computation from 'capo' to new position\n  revolve_takeshot  → Save current state to checkpoint slot 'check'\n  revolve_restore   → Restore state from checkpoint slot 'check'\n  revolve_firsturn  → Begin reverse sweep (first reverse step)\n  revolve_youturn   → Continue reverse sweep (subsequent steps)\n  revolve_terminate → Adjoint computation complete\n\nTYPICAL USAGE PATTERN:\n```\nwhile ((action = revolve(&check, &capo, &fine, snaps, &info)) != terminate) {\n  switch(action) {\n    case advance:   forward_sweep(capo, fine); break;\n    case takeshot:  save_checkpoint(check); break;\n    case restore:   load_checkpoint(check); break;\n    case firsturn:\n    case youturn:   reverse_step(); fine--; break;\n  }\n}\n```", "math": "Binomial Bound:\nWith s checkpoint slots and r repetitions allowed, revolve handles exactly:\n  N_max = C(s+r, r) = (s+r)! / (s! · r!)\nsteps optimally.\n\nFor r=1: N_max = s+1 (minimal repetition)\nFor large r: N_max grows super-exponentially in s\n\nThe schedule minimizes total forward evaluations subject to checkpoint\nbudget constraint. This is achieved by placing checkpoints at binomial\ncoefficients: if you have s checkpoints for N steps, place first checkpoint\nat position C(s-1+r,r)/C(s+r,r) · N.", "complexity": "Total forward evaluations: O(r·N) for N steps with r repetitions.\nWith s = O(log N) checkpoints and r = O(log N), achieves O(N log N) work.\nSpace: O(s · state_size) for checkpoint storage.", "ref": ["Griewank & Walther (2000). \"Algorithm 799: Revolve\". ACM TOMS.", "Stumm & Walther (2010). \"New Algorithms for Optimal Online\n  Checkpointing\". SIAM J. Sci. Comput."], "see": ["checkpointing.h for ADOL-C checkpointing interface"], "has_pass2": true}, "ADOL-C/include/adolc/externfcts.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/externfcts.h", "filename": "externfcts.h", "file": "externfcts.h", "brief": "Support for externally differentiated functions in ADOL-C tapes\n\nProvides mechanisms to incorporate user-supplied derivative code for\nfunctions that cannot or should not be traced (e.g., library calls,\nspecialized solvers, discontinuous functions). The user registers\ncallback functions for forward and reverse mode evaluation.\n\nUse cases:\n- Integrating external solvers (linear algebra, sparse systems)\n- Hand-coded derivatives for performance-critical sections\n- Functions from non-ADOL-C libraries\n- Discontinuous or non-differentiable operations with custom handling\n\nCallback modes:\n- zos_forward: zero-order scalar (function evaluation only)\n- fos_forward/fos_reverse: first-order scalar\n- fov_forward/fov_reverse: first-order vector\n- hos_forward/hos_reverse: higher-order scalar\n- hov_forward/hov_reverse: higher-order vector", "see": ["interfaces.h for standard AD interfaces", "checkpointing.h for memory-efficient reverse mode"], "has_pass2": false}, "ADOL-C/include/adolc/adolcerror.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adolcerror.h", "filename": "adolcerror.h", "brief": "Exception class for ADOL-C errors with source location tracking.\n\nThis exception captures the error message and the source location (file,\nline, column) where it was thrown. Integrates with the C++ standard exception\nhierarchy through std::runtime_error.\n\n@example\nThrow example:\n@code\nthrow ADOLCError(\"wrong number of independents\");  // Auto-captures source\nlocation\n@endcode\n\nHandle example:\n@code\ntry {\n  // ADOL-C operations...\n}\ncatch (ADOLCError& e) {\n  std::cerr << e.what() << std::endl;         // Formatted message\n  std::cerr << \"Error occurred at: \"\n            << e.where().file() << \":\"\n            << e.where().func() << \":\"\n            << e.where().line() << std::endl; // Direct source location\naccess\n}\n@endcode", "param": ["message Error description (will be stored as std::string)", "info Source location"], "return": "Const reference to the source_location object", "has_pass2": false}, "ADOL-C/include/adolc/adtl.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adtl.h", "filename": "adtl.h", "file": "adtl.h", "brief": "Tape-less (traceless) forward-mode automatic differentiation\n\nProvides the adtl::adouble class for direct forward-mode AD without\ntape recording. Each adouble carries both its value and directional\nderivatives, which are propagated immediately through operations.", "algorithm": "Memory Trade-off:\nTapeless vs. tape-based comparison.\n\n  TAPELESS:\n    Memory: O(n_active · p) where n_active = live variables, p = #directions\n    No tape overhead, no storage of operation sequence\n    Best for: few directions, simple control flow\n\n  TAPE-BASED:\n    Memory: O(operations) for tape + O(n) per evaluation\n    Supports reverse mode: O(1) directions for gradient\n    Best for: many directions, need reverse mode", "math": "For function y = f(x) with seed vectors s_i:\n  adval[i] = ∇f(x)ᵀ s_i (directional derivative in direction s_i)\n  With s_i = e_i (standard basis), get ∇f component-wise.", "complexity": "Per operation: O(p) for p directions\nTotal function evaluation: O(operations · p)\nGradient via p=n directions: O(n · operations)\n\nAdvantages over tape-based mode:\n- No memory overhead from tape storage\n- Lower per-operation cost for simple functions\n- Simpler for computing few directional derivatives\n\nLimitations:\n- Only forward mode (no reverse/adjoint mode)\n- Memory scales as O(n·p) for n variables and p directions\n- Must set numDir before creating adoubles via setNumDir(p)", "see": ["adtb_types.h for tape-based adouble (supports reverse mode)", "interfaces.h for tape evaluation routines"], "has_pass2": true}, "ADOL-C/include/adolc/tape_interface.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/tape_interface.h", "filename": "tape_interface.h", "file": "tape_interface.h", "brief": "Tape management interface for ADOL-C automatic differentiation\n\nProvides functions for managing the \"tape\" - a recorded sequence of\noperations that can be replayed in forward or reverse mode to compute\nderivatives.", "algorithm": "Tape-Based Operation Recording:\nADOL-C implements automatic differentiation by recording a \"tape\" of\nelementary operations during function evaluation:\n\nRECORDING PHASE (trace_on to trace_off):\n1. Each adouble gets a unique location on the tape\n2. Operations (+, -, *, /, sin, exp, etc.) record:\n   - Operation code (opcode)\n   - Input locations\n   - Output location\n   - Primal value (for reverse mode)\n3. Result: Directed acyclic graph (DAG) of operations\n\nTAPE STRUCTURE:\n- Operations buffer: sequence of opcodes\n- Locations buffer: tape indices for operands/results\n- Values buffer: constant values embedded in computation\n- Taylor buffer: intermediate values for forward/reverse\n\nTHREAD SAFETY:\nEach thread has its own tape buffer (thread_local storage).\nTapes are identified by short integer IDs (0-32767).", "math": "Tape interpretation:\nA tape represents a function F: ℝⁿ → ℝᵐ as composition of elementary ops:\n  v_{-n+1}, ..., v_0           = x (inputs)\n  v_i = φ_i(v_{j(i)}, v_{k(i)})  for i = 1, ..., p (intermediates)\n  y = (v_{p-m+1}, ..., v_p)    = F(x) (outputs)\n\nForward mode propagates tangents: v̇_i = (∂φ_i/∂v_j)·v̇_j + (∂φ_i/∂v_k)·v̇_k\nReverse mode propagates adjoints: v̄_j += (∂φ_i/∂v_j)·v̄_i (in reverse order)", "complexity": "Recording: O(ops) time and space\nForward evaluation: O(ops) per direction\nReverse evaluation: O(ops) per adjoint", "see": ["adolc.h for main include", "interfaces.h for forward/reverse mode API", "adtb_types.h for adouble/pdouble types"], "return": "Reference to the thread-local vector of unique pointers to ValueTape.", "param": ["tapeId The ID of the tape to search for.", "tapeId The ID of the tape to locate.", "tapeId The ID of the tape to retrieve.", "tapeId The ID of the tape to set as current.\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId The ID of the new tape to create.\n\n@throws ADOLCError::ErrorType::TAPE_ALREADY_EXIST if a tape with the same ID\nalready exists.", "tapeId ID of the tape to activate for tracing.", "keepTaylors Flag indicating whether to keep Taylor coefficients\n(non-zero to keep).", "tapeId ID of the tape to trace.", "keepTaylors Whether to store Taylor coefficients.", "obs Operation buffer size.", "lbs Location buffer size.", "vbs Value buffer size.", "tbs Taylor buffer size.", "skipFileCleanup Whether to skip cleaning up tape files after tracing.", "flag If non-zero, forces the tape to write op/loc/val files.\n\n@throws ADOLCError::ErrorType::TAPING_NOT_ACTUALLY_TAPING if no active\ntracing is detected.\n\n@note assert in debug mode if currentTape is nullptr", "result vector to store the IDs\n\n@note the vector is resized in this function to the size of the tapeBuffer", "tapeId the ID of the tape to set nested", "nested char to set\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId the ID of the tape to set nested", "tapeId the ID of the tape to read the tapestats", "tapeId the ID of the tape to print the stats\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId ID of the tape to read the number of parameters", "tapeId ID of the tape to store the jacobian information", "sJinfos the information to store\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist.", "tapeId ID of the tape to store the hessian information", "sHInfos the information to store\n\n@throws ADOLCError::ErrorType::NO_TAPE_ID if the specified tape does not\nexist."], "has_pass2": true}, "ADOL-C/include/adolc/adtb_types.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/adtb_types.h", "filename": "adtb_types.h", "file": "adtb_types.h", "brief": "Core AD types: adouble, pdouble, and tape_location\n\nDefines the fundamental types for tape-based automatic differentiation:\n\n- **adouble**: Active double that records operations on the tape.\n  Use for variables whose derivatives you want to compute.\n\n- **pdouble**: Parameter double for non-differentiable constants that\n  can be changed without re-taping. Use for parameters you want to\n  vary across multiple derivative evaluations.\n\n- **tape_location<T>**: RAII wrapper managing tape location allocation\n  and deallocation for adouble/pdouble.", "algorithm": "Operator Overloading for AD:\nADOL-C uses C++ operator overloading to transparently record operations:\n\nADOUBLE SEMANTICS:\nEach adouble a has:\n- Value: a.value() = current numerical value (for function evaluation)\n- Location: a.loc() = index on tape where derivatives are tracked\n\nWhen operations are performed on adoubles:\n1. Compute primal result normally (same as double arithmetic)\n2. Record operation to tape: {opcode, input_locs, output_loc, value}\n3. Return new adouble with result value and new tape location\n\nExample: c = a * b records:\n- opcode: mult_a_a (adouble * adouble)\n- inputs: a.loc(), b.loc()\n- output: c.loc() (newly allocated)\n- value: a.value() * b.value()\n\nPDOUBLE FOR PARAMETERS:\npdouble values are stored separately and can be changed between derivative\nevaluations without re-recording the tape. This enables efficient sensitivity\nanalysis with respect to parameters.", "math": "Derivative propagation through operators:\nFor each operation c = φ(a,b), the chain rule gives:\n- Forward: ċ = (∂φ/∂a)·ȧ + (∂φ/∂b)·ḃ\n- Reverse: ā += (∂φ/∂a)·c̄, b̄ += (∂φ/∂b)·c̄\n\nPartial derivatives for common operations:\n- Addition c = a + b: ∂c/∂a = 1, ∂c/∂b = 1\n- Multiplication c = a * b: ∂c/∂a = b, ∂c/∂b = a\n- sin(a): ∂/∂a = cos(a)\n- exp(a): ∂/∂a = exp(a)", "complexity": "Each operation: O(1) time to record, O(1) space on tape.\nDerivative propagation: O(1) per operation in forward/reverse sweep.", "see": ["adolc.h for main include", "interfaces.h for derivative computation functions", "adtl.h for tape-less forward mode alternative"], "tparam": ["T The type to check.", "T Either `adouble` or `pdouble`."], "param": ["loc_ Location on the tape", "valid_ Specifies whether `tape_location` was moved. Used to decide if\n`loc_` can be free'd or not.", "tape The tape to retrieve the next location from.", "other The `tape_location` to move from.", "other The `tape_location` to move from.", "coval Value that is stored on the tape at the new location.", "a The `adouble` to copy.", "other The `adouble` to transfer.", "coval The value to assign.", "a The `adouble` to assign.", "other The `adouble` to transfer.", "p The `pdouble` to assign.", "coval the new value to assign.", "in Value that is assigned to the `adouble`", "out Value that will get the value of `adouble`", "other The `pdouble` to transfer.", "other The `pdouble` to transfer.", "pval The initial value for the `pdouble` on the parameter tape.", "pval The new value to assign.", "a adouble to check whether its value on the tape is normal", "a adouble to check whether its value on the tape is nan", "a adouble to check whether its value on the tape is inf", "a adouble to check whether its value on the tape is finite", "p pdouble to check whether its value on the tape is normal", "p pdouble to check whether its value on the tape is nan", "p pdouble to check whether its value on the tape is inf", "p pdouble to check whether its value on the tape is finite"], "return": "The next location index.", "has_pass2": true}, "ADOL-C/include/adolc/fixpoint.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/fixpoint.h", "filename": "fixpoint.h", "file": "fixpoint.h", "brief": "Differentiation through fixed-point iterations\n\nEnables automatic differentiation of implicit functions defined as\nfixed points x* = G(x*, u), where u are parameters.\n\n**Problem:** Many numerical methods involve iterative solvers:\n- Newton's method: x_{k+1} = x_k - f(x_k)/f'(x_k)\n- Nonlinear system solvers\n- Implicit time integrators\n\n**Solution:** fp_iteration() differentiates through the converged result\nwithout differentiating through all iteration steps, using the implicit\nfunction theorem: dx", "has_pass2": false}, "ADOL-C/include/adolc/advector.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/advector.h", "filename": "advector.h", "file": "advector.h", "brief": "Traceable vector with differentiable subscript operations\n\nProvides advector, a vector of adoubles that can trace subscripting\nwith adouble indices. This enables automatic differentiation through\narray lookups where the index itself depends on independent variables.\n\n**Key classes:**\n- advector: Vector container with contiguous tape locations\n- adubref: Reference type returned when indexing with adouble (lvalue case)\n\n**Use case example:**\n@code\nadvector table(n);        // Create table of adoubles\nadouble index = ...;      // Index depends on independent variables\nadouble result = table[index];  // Differentiation tracks index dependency\n@endcode\n\n@note This is only for taped (not tapeless) ADOL-C mode", "see": ["adtb_types.h for adouble and pdouble definitions"], "has_pass2": false}, "ADOL-C/include/adolc/edfclasses.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/edfclasses.h", "filename": "edfclasses.h", "file": "edfclasses.h", "brief": "Object-oriented interface for external differentiated functions\n\nProvides C++ class-based wrappers for external functions with user-supplied\nderivatives. Inherit from these base classes to integrate external code\n(e.g., legacy Fortran, optimized BLAS, specialized solvers) into AD.\n\n**Classes:**\n- EDFobject: Basic external function with fixed array sizes\n- EDFobject_iArr: External function with integer array parameters\n- EDFobject_v2: Version 2 interface with variable-size arrays and context\n\n**Required overrides:**\n- function(): Evaluate the external function\n- zos_forward(): Zero-order scalar (function evaluation during taping)\n- fos_forward(): First-order scalar forward mode\n- fov_forward(): First-order vector forward mode\n- fos_reverse(): First-order scalar reverse mode\n- fov_reverse(): First-order vector reverse mode", "see": ["externfcts.h for C-style external function interface", "externfcts2.h for v2 C-style interface"], "has_pass2": false}, "ADOL-C/include/adolc/interfaces.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/interfaces.h", "filename": "interfaces.h", "file": "interfaces.h", "brief": "Low-level forward and reverse mode interfaces for tape evaluation\n\nProvides the core differentiation routines that evaluate recorded tapes\nin forward mode (computing directional derivatives) and reverse mode\n(computing adjoints/gradients). These are the building blocks used by\nhigher-level drivers like gradient() and hessian().", "algorithm": "Forward and Reverse Mode Tape Evaluation:\n\nFORWARD MODE (propagate tangents ẋ → ẏ):\n- zos_forward(tag, m, n, x, y): Function value only, y = f(x)\n- fos_forward(tag, m, n, x, xdot, y, ydot): First derivative, ẏ = f'(x)·ẋ\n- hos_forward(tag, m, n, d, x, X, y, Y): Taylor to order d\n- fov_forward(tag, m, n, p, x, X, y, Y): p directions simultaneously\n- hov_forward(tag, m, n, d, p, x, X, y, Y): p Taylor series to order d\n\nREVERSE MODE (propagate adjoints ȳ → x̄):\n- fos_reverse(tag, m, n, ybar, xbar): Gradient, x̄ = f'(x)ᵀ·ȳ\n- hos_reverse(tag, m, n, d, ybar, Z): Higher-order adjoints\n- hov_reverse(tag, m, n, d, q, U, Z, nz): q adjoints for d-order Taylor", "math": "Key complexity insight:\nFor f: ℝⁿ → ℝ (scalar output), one reverse sweep computes ∇f ∈ ℝⁿ.\nCost: ~4× forward evaluation, independent of n.\nThis is why reverse mode dominates in optimization (gradients are cheap).\n\nMode naming conventions:\n- zos: zero-order-scalar (function value only)\n- fos: first-order-scalar (single direction/adjoint)\n- hos: higher-order-scalar (Taylor series to order d)\n- fov: first-order-vector (p directions simultaneously)\n- hov: higher-order-vector (p Taylor series to order d)\n- wk: with keep (retain intermediates for subsequent reverse)\n- ti: Taylor input (provide input Taylor coefficients)", "complexity": "Forward: O(d·p·ops) for d-order, p directions.\nReverse: O(d·q·ops) for d-order, q adjoints.", "see": ["drivers/drivers.h for easy-to-use wrapper functions", "tape_interface.h for tape management (trace_on, trace_off)"], "has_pass2": true}, "ADOL-C/include/adolc/sparse/sparsedrivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/sparse/sparsedrivers.h", "filename": "sparsedrivers.h", "file": "sparse/sparsedrivers.h", "brief": "High-level drivers for sparse Jacobian and Hessian computation\n\nProvides efficient computation of sparse derivatives by exploiting\nsparsity structure using graph coloring and compressed computation.", "algorithm": "Sparse Derivative Computation via Graph Coloring:\nReduces the number of directional derivative evaluations by exploiting\nsparsity structure:\n\nPHASE 1 - SPARSITY DETECTION (jac_pat, hess_pat):\nPropagate bit-patterns through the tape to determine structural non-zeros:\n- Initialize: each input gets unique bit (e.g., x_1 = 0001, x_2 = 0010)\n- Propagate: output bits = OR of dependent input bits\n- Result: J[i,j] ≠ 0 iff bit j is set in output i's pattern\n\nPHASE 2 - SEED MATRIX via GRAPH COLORING (generate_seed_jac/hess):\nConstruct seed matrix S such that J·S has recoverable columns:\n- Build column intersection graph: edge (i,j) if columns i,j share a row\n- Color graph: adjacent columns get different colors\n- Seed matrix: S[:,c] = sum of unit vectors for columns with color c\n\nPHASE 3 - COMPRESSED EVALUATION (sparse_jac, sparse_hess):\nCompute B = J·S using p directional derivative evaluations (p = #colors)\nRecover J[i,j] = B[i, color(j)] since non-zero positions are known", "math": "Complexity analysis:\nLet nnz = number of non-zeros, p = chromatic number of intersection graph\n\nFor Jacobian (m×n):\n- Dense: O(min(m,n)) tape evaluations\n- Sparse: O(p) tape evaluations where p ≤ max_row_degree + 1\n\nFor Hessian (n×n, symmetric):\n- Dense: O(n) tape evaluations\n- Sparse: O(p) where p = chromatic number of adjacency graph\n- Symmetry exploitation: use symmetric coloring (fewer colors needed)\n\nFor banded/sparse structures (p = O(1)), cost is O(1) tape evaluations\nregardless of matrix dimension! This is the key advantage.", "complexity": "- Sparsity detection: O(ops · n/wordsize) for n inputs, ops tape operations\n- Graph coloring: O(nnz) greedy, optimal coloring is NP-hard\n- Sparse evaluation: O(p · ops) where p = number of colors", "ref": ["Curtis, Powell & Reid (1974). \"On the Estimation of Sparse Jacobian\n  Matrices\". J. Inst. Math. Appl. 13:117-119.", "Gebremedhin, Manne & Pothen (2005). \"What Color Is Your Jacobian?\n  Graph Coloring for Computing Derivatives\". SIAM Review 47(4):629-705."], "see": ["drivers/drivers.h for dense derivative computation", "sparse/sparse_fo_rev.h for bit-pattern forward/reverse modes", "generate_seed_jac for computing seed matrix from pattern", "sparse_jac for computing values", "jac_pat for sparsity detection only", "jacobian for dense Jacobian computation", "generate_seed_hess for computing seed matrix from pattern", "sparse_hess for computing values", "hess_pat for sparsity detection only", "hessian for dense Hessian computation"], "param": ["tag Tape identifier", "m Number of dependent variables (rows)", "n Number of independent variables (columns)", "x Evaluation point x[n]", "crs Output sparsity pattern crs[m][*], where crs[i][0] = count\n           and crs[i][1..count] = column indices of non-zeros in row i", "options Control options[3] for propagation mode", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "repeat If non-zero, reuse sparsity pattern from previous call", "x Evaluation point x[n]", "nnz Output: number of non-zeros", "row_ind Output: row indices (COO format)", "col_ind Output: column indices (COO format)", "values Output: non-zero values", "options Control options[3]", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Evaluation point x[n]", "crs Output sparsity pattern crs[n][*], where crs[i][0] = count\n           and crs[i][1..count] = column indices of non-zeros in row i", "option Control option for propagation mode", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "repeat If non-zero, reuse sparsity pattern from previous call", "x Evaluation point x[n]", "nnz Output: number of non-zeros (lower triangle only)", "row_ind Output: row indices (COO format)", "col_ind Output: column indices (COO format)", "values Output: non-zero values", "options Control options[2]"], "return": "0 on success", "has_pass2": true}, "ADOL-C/include/adolc/sparse/sparse_fo_rev.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/sparse/sparse_fo_rev.h", "filename": "sparse_fo_rev.h", "file": "sparse/sparse_fo_rev.h", "brief": "Bit-pattern propagation for sparsity detection\n\nProvides forward and reverse mode interfaces that propagate bit patterns\ninstead of numerical values. Used to efficiently determine the sparsity\nstructure of Jacobian and Hessian matrices.\n\nBit patterns are packed into size_t words for efficiency. For n independent\nvariables, the seed matrix X[n][p] uses p = ceil(n / bits_per_long) words\nper row, where bits_per_long = 8 * sizeof(size_t).\n\nTwo modes:\n- **Tight**: Uses actual values x[] during propagation (more accurate)\n- **Safe**: Uses only bit patterns (faster, may overestimate sparsity)", "see": ["sparse/sparsedrivers.h for high-level sparse derivative computation"], "has_pass2": false}, "ADOL-C/include/adolc/drivers/taylor.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/taylor.h", "filename": "taylor.h", "file": "drivers/taylor.h", "brief": "Higher-order derivative tensors and implicit function differentiation\n\nProvides drivers for computing higher-order derivative tensors and\ndifferentiating through implicit/inverse functions.", "algorithm": "Higher-Order Taylor Propagation:\nComputes arbitrary-order derivative tensors via univariate Taylor arithmetic.\n\nTENSOR COMPUTATION:\nGiven f: ℝⁿ → ℝᵐ and p seed directions S ∈ ℝⁿˣᵖ, compute the d-th order\nderivative tensor containing all mixed partials up to order d:\n\n1. Evaluate f along each direction: f(x + t·S[:,j]) as Taylor series\n2. Extract coefficients: [f]_k = (1/k!) · (d^k f/dt^k)|_{t=0}\n3. Combine to form symmetric tensor with C(p+d,d) entries per output\n\nINVERSE FUNCTION DIFFERENTIATION:\nGiven tape computing y = F(x), compute derivatives of x = F⁻¹(y):\n\nFor implicit function G(x,y) = F(x) - y = 0:\n- Jacobian: ∂x/∂y = -(∂G/∂x)⁻¹ · (∂G/∂y) = (∂F/∂x)⁻¹\n- Higher orders: Propagate Taylor series through linear solve\n\ninverse_Taylor_prop(tag, n, d, Y, X):\nGiven Y[i][k] = (1/k!) · y^(k), computes X[i][k] = (1/k!) · x^(k)\nwhere x(t) = F⁻¹(y(t)) as Taylor series in parameter t.", "math": "Tensor dimensions:\nFor d-th order tensor with p directions:\n- Number of entries: C(p+d, d) = (p+d)! / (p! · d!)\n- Storage: tensor[m][C(p+d,d)] where m = number of outputs\n\nMulti-index notation: tensor[i][α] = (1/α!) · ∂^|α| f_i / ∂s^α\nwhere α = (α₁,...,α_p) is multi-index with |α| = Σα_j ≤ d", "complexity": "- tensor_eval: O(C(p+d,d) · ops) where ops = tape operations\n- inverse_tensor_eval: O(C(p+d,d) · n³) includes linear solves\n- jac_solv: O(n³) for dense LU factorization", "ref": ["Griewank, Utke & Walther (2000). \"Evaluating Higher Derivative\n  Tensors by Forward Propagation of Univariate Taylor Series\".\n  Mathematics of Computation 69(231):1117-1130.\n\n@note These are advanced drivers for specialized applications"], "see": ["drivers/drivers.h for standard gradient/Jacobian/Hessian", "drivers/odedrivers.h for ODE-specific Taylor propagation"], "has_pass2": true}, "ADOL-C/include/adolc/drivers/odedrivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/odedrivers.h", "filename": "odedrivers.h", "file": "drivers/odedrivers.h", "brief": "Taylor-based ODE integration drivers\n\nProvides drivers for solving and differentiating ODEs of the form\nx' = f(x) using Taylor series expansion. The tape records f(x),\nthen these drivers compute higher-order Taylor coefficients.", "algorithm": "Taylor Series ODE Integration:\nComputes Taylor coefficients of ODE solution via automatic differentiation\nof the right-hand side.\n\nPROBLEM: Given autonomous ODE x'(t) = f(x(t)) with initial condition x(0) = x₀,\ncompute Taylor expansion: x(t) = Σₖ x[k] · tᵏ\n\nMETHOD (forode):\n1. Record tape of f(x) = x' mapping state to derivative\n2. Given x[0] = x₀, iteratively compute higher coefficients:\n\n   x[k+1] = (1/(k+1)) · f[k]\n\nwhere f[k] is the k-th Taylor coefficient of f(x(t)), computed by\npropagating x[0..k] through the tape.\n\nRECURRENCE RELATION:\nFor x' = f(x): x[k+1] = (τ/(k+1)) · [f(x)]_k\nwhere τ is a time scaling factor and [·]_k denotes k-th Taylor coefficient.\n\nThe coefficients are computed via the composition rule:\n[f(g)]_k = Σᵢ (∂f/∂xᵢ)[k] · [gᵢ]_k + lower order terms", "math": "Taylor coefficient extraction:\nFor function y = f(x), the Taylor coefficient relationship:\ny(t) = f(x(t)) where x(t) = Σₖ x[k] · tᵏ\nyields y[k] = (1/k!) · (dᵏy/dtᵏ)|_{t=0}\n\nSENSITIVITY COMPUTATION (accode):\nGiven partial Jacobians A[i][j][k] = ∂x[k]_i/∂x[0]_j at each order,\naccumulate total Jacobians B via the chain rule:\n\nB[i][j][k] = A[i][j][k] + Σₗ (∂x[k]_i/∂x[ℓ]) · B[ℓ][j][k-1]", "complexity": "- forode: O(d · ops) for d Taylor coefficients, ops = tape operations\n- accode: O(d · n²) for n-dimensional state with d coefficients", "ref": ["Jorba & Zou (2005). \"A Software Package for the Numerical Integration\n  of ODEs by Means of High-Order Taylor Methods\".", "Griewank & Walther (2008). \"Evaluating Derivatives\", Section 13.2."], "param": ["tau Time scaling factor (default 1.0)", "dold Previous Taylor degree computed (default 0)", "dnew New target Taylor degree"], "see": ["interfaces.h for forward/reverse mode primitives", "drivers/taylor.h for general tensor computation"], "has_pass2": true}, "ADOL-C/include/adolc/drivers/psdrivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/psdrivers.h", "filename": "psdrivers.h", "file": "drivers/psdrivers.h", "brief": "Drivers for piecewise smooth (PS) functions with abs-normal form\n\nProvides differentiation tools for functions containing absolute values\nand other piecewise linear operations. These functions are not classically\ndifferentiable at kink points, but have well-defined generalized derivatives.", "algorithm": "Abs-Normal Form for Piecewise Smooth Functions:\nRepresents piecewise linear functions in a canonical form enabling\nalgorithmic computation of generalized derivatives.\n\nREPRESENTATION:\nA function f: ℝⁿ → ℝᵐ containing s absolute value operations can be\nwritten in abs-normal form:\n\n  z = cz + Z·x + L·|z|    (switching equation)\n  y = cy + Y·x + J·z      (output equation)\n\nwhere:\n- z ∈ ℝˢ are \"switching variables\" (arguments to |·|)\n- cz ∈ ℝˢ, cy ∈ ℝᵐ are constant offsets\n- Z ∈ ℝˢˣⁿ, Y ∈ ℝᵐˣⁿ are input Jacobians\n- L ∈ ℝˢˣˢ is strictly lower triangular (coupling between switches)\n- J ∈ ℝᵐˣˢ maps switching variables to outputs\n\nKEY PROPERTY: L being strictly lower triangular ensures the implicit\nequation for z has a unique solution (can be solved by forward substitution).\n\nGENERALIZED DERIVATIVES:\nAt a kink (where some zᵢ = 0), classical derivative doesn't exist.\nInstead, compute directional derivative along direction d:\n\n  σ = sign(Z·d + L·σ·|Z·d + L·σ|)  (signature equation)\n  ∇_d f = Y + J·diag(σ)·(I - L·diag(σ))⁻¹·Z\n\nwhere σ ∈ {-1, 0, +1}ˢ encodes which branch of each |·| is active.", "math": "Bouligand subdifferential:\nThe set of all limiting Jacobians as we approach a kink from different\ndirections forms the Bouligand subdifferential ∂_B f. For piecewise\nlinear f, this is a finite set with at most 2ˢ elements (one per sign\ncombination of the s switching variables).\n\nClarke generalized gradient: conv(∂_B f) - convex hull", "complexity": "- abs_normal: O(ops) to extract form, where ops = tape operations\n- directional_active_gradient: O(s²) for forward substitution\n- Space: O(s²) for L matrix, O(s·n) for Z matrix", "ref": ["Griewank (2013). \"On Stable Piecewise Linearization and Generalized\n  Algorithmic Differentiation\". Optimization Methods and Software.", "Griewank & Walther (2016). \"First- and Second-Order Optimality\n  Conditions for Piecewise Smooth Objective Functions\"."], "see": ["drivers/drivers.h for smooth function derivatives", "interfaces.h for underlying forward/reverse modes"], "param": ["tag Tape identifier", "n Number of independent variables", "x Evaluation point x[n]", "d Direction vector d[n]", "g Output directional active gradient g[n]", "sigma_g Output signature indicating which piece is active", "tag Tape identifier", "m Number of dependent variables (outputs)", "n Number of independent variables (inputs)", "swchk Number of switching variables s (abs operations in tape)", "x Base point x[n] for linearization", "y Output function value y[m] at base point", "z Output switching variable values z[s]", "cz Output constant vector cz[s] for z equation", "cy Output constant vector cy[m] for y equation", "Y Output Jacobian Y[m][n] of y w.r.t. x", "J Output Jacobian J[m][s] of y w.r.t. z", "Z Output Jacobian Z[s][n] of z (before abs) w.r.t. x", "L Output lower triangular L[s][s] coupling between switching vars"], "return": "0 on success\n\n@note The signature sigma_g encodes which branch of each abs() is taken", "has_pass2": true}, "ADOL-C/include/adolc/drivers/drivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/drivers/drivers.h", "filename": "drivers.h", "file": "drivers/drivers.h", "brief": "High-level driver functions for derivative computation\n\nProvides convenient functions for computing common derivative quantities:\n- gradient(): First derivative of scalar function (∇f)\n- jacobian(): First derivative of vector function (∂F/∂x)\n- hessian(): Second derivative of scalar function (∇²f)\n- hess_vec(): Hessian-vector product (∇²f · v)\n- jac_vec(): Jacobian-vector product (J · v)\n- vec_jac(): Vector-Jacobian product (u^T · J)\n\nThese drivers wrap the lower-level forward/reverse interfaces and handle\nmemory allocation and mode selection automatically. All functions require\na pre-recorded tape (via trace_on/trace_off).", "see": ["interfaces.h for low-level forward/reverse mode calls", "tape_interface.h for tape recording functions", "drivers/taylor.h for higher-order Taylor coefficient drivers", "gradient for computing first derivatives", "jacobian for vector-valued functions", "hessian for second derivatives", "gradient for scalar functions (m=1)", "vec_jac for vector-Jacobian products", "jac_vec for Jacobian-vector products", "jac_vec for Jacobian-vector product (forward mode)", "vec_jac for vector-Jacobian product (reverse mode)", "hessian2 for alternative using Hessian-matrix product", "hess_vec for Hessian-vector product only", "hessian for standard approach", "hess_mat for multiple Hessian-vector products", "hess_mat for multiple directions", "lagra_hess_vec for Lagrangian Hessian-vector product", "hess_vec for single direction", "hess_vec for single scalar function"], "param": ["tag Tape identifier", "m Number of dependent variables (outputs)", "n Number of independent variables (inputs)", "x Input point x[n]", "y Output values y[m] = F(x)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "g Output gradient g[n] = ∇f(x)", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "x Input point x[n]", "J Output Jacobian J[m][n]", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "k Strip size (number of columns computed per forward sweep)", "x Input point x[n]", "y Output function values y[m] = F(x)", "J Output Jacobian J[m][n]", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "repeat If non-zero, reuse Taylor coefficients from previous call", "x Input point x[n]", "u Adjoint seed vector u[m]", "v Output vector v[n] = u^T · J = Σ_i u_i · ∇F_i", "tag Tape identifier", "m Number of dependent variables", "n Number of independent variables", "x Input point x[n]", "v Tangent direction vector v[n]", "u Output vector u[m] = J · v (directional derivative)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "H Output Hessian H[n][n] (lower triangle filled)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "H Output Hessian H[n][n] (lower triangle filled)", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "x Input point x[n]", "v Direction vector v[n]", "w Output vector w[n] = H · v = ∇²f(x) · v", "tag Tape identifier (must have m=1 dependent variable)", "n Number of independent variables", "q Number of direction vectors", "x Input point x[n]", "V Direction matrix V[n][q] (q column vectors)", "W Output matrix W[n][q] where W[:,j] = H · V[:,j]", "tag Tape identifier", "m Number of dependent variables (constraints)", "n Number of independent variables", "x Input point x[n]", "v Direction vector v[n]", "u Lagrange multipliers u[m]", "w Output vector w[n] = ∇²(Σ u_i F_i) · v"], "return": "0 on success, non-zero on error", "algorithm": "Reverse mode automatic differentiation", "complexity": "O(c·n) where c = cost of function evaluation (typically c ≈ 4-5)", "has_pass2": true}, "ADOL-C/include/adolc/lie/drivers.h": {"path": "layer-2/ADOL-C/ADOL-C/include/adolc/lie/drivers.h", "filename": "drivers.h", "file": "lie/drivers.h", "brief": "Lie derivative computation for nonlinear control systems\n\nComputes Lie derivatives used in nonlinear control theory for:\n- Observability analysis (lie_scalar, lie_gradient)\n- Controllability analysis (lie_covector, lie_bracket)\n\n**Mathematical background:**\nGiven vector field f(x) and scalar function h(x), the Lie derivative is:\n  L_f h = ∇h · f = Σ (∂h/∂x_i) f_i(x)\n\nHigher-order Lie derivatives (L_f^k h) reveal observability structure.\nLie brackets [f,g] = ∂g/∂x·f - ∂f/∂x·g reveal controllability structure.\n\n**Functions:**\n- lie_scalar(): Compute L_f^k h (scalar Lie derivatives)\n- lie_gradient(): Compute ∇(L_f^k h) (gradients of Lie derivatives)\n- lie_covector(): Compute covector fields\n- lie_bracket(): Compute Lie bracket [f,g]", "see": ["drivers/drivers.h for standard derivative computation"], "has_pass2": false}}}, "Cbc": {"name": "Cbc", "file_count": 85, "pass2_count": 79, "files": {"src/CbcConsequence.hpp": {"path": "layer-2/Cbc/src/CbcConsequence.hpp", "filename": "CbcConsequence.hpp", "file": "CbcConsequence.hpp", "brief": "Abstract base for bound implications from branching", "algorithm": "Branch Consequence Interface for Implied Bounds\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcConsequence: Abstract interface for side effects when branching.\nWhen a variable is fixed, other variables may have implied bounds\nthat go beyond what LP constraint propagation would derive.\n\nExample: Binary y controls whether continuous x can be nonzero.\nWhen y=0, we know x=0 even if the constraint linking them is weak.\n\nThe applyToSolver() method is called after branching to impose\nthese derived bounds on the solver, based on the branch state.", "see": ["CbcFixVariable for concrete implementation", "CbcObject for objects that may have consequences\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": true}, "src/CbcSOS.hpp": {"path": "layer-2/Cbc/src/CbcSOS.hpp", "filename": "CbcSOS.hpp", "file": "CbcSOS.hpp", "brief": "Special Ordered Sets (SOS) Type 1 and Type 2 branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSOS: Branching for Special Ordered Sets (Beale & Tomlin, 1970):\n\nSOS Type 1: At most ONE variable can be nonzero\n- Common for selection between alternatives (choose one option)\n- SUM x_i <= 1 (or = 1 for exactly one)\n- Binary SOS1 is a special case of clique\n\nSOS Type 2: At most TWO CONSECUTIVE variables can be nonzero\n- Used for piecewise linear approximation (interpolation)\n- Variables ordered by weights; sum to 1\n- Represents point between two breakpoints", "algorithm": "SOS Branching (Beale & Tomlin 1970):\n  createCbcBranch() partitions ordered set {x_1,...,x_n}:\n  1. Find weighted center: w̄ = Σ(w_j·x_j) / Σx_j.\n  2. Choose separator s closest to w̄.\n  3. Down branch: x_j = 0 for all w_j > s.\n  4. Up branch: x_j = 0 for all w_j ≤ s.\n  Binary tree partitions the ordered set.", "math": "Infeasibility measure:\n  SOS1: inf = min(Σx_j, 1-max(x_j)) - prefer fixing near-integer.\n  SOS2: inf based on violation of adjacency (non-consecutive nonzeros).\n  separator_ stored in branching object for partition point.\n\nBranching partitions the ordered set at a separator weight,\nforcing variables on one side to zero.", "see": ["CbcSOSBranchingObject for branching action", "CbcClique for related binary set branching", "OsiSOS for OSI-level representation\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcBranchDecision.hpp": {"path": "layer-2/Cbc/src/CbcBranchDecision.hpp", "filename": "CbcBranchDecision.hpp", "file": "CbcBranchDecision.hpp", "brief": "Abstract base for branching variable selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "Branching Variable Selection Strategies:\nAfter identifying fractional variables, must choose which to branch on.\nDifferent strategies trade off computational cost vs tree size.\n\nCommon Scoring Functions:\n- Most infeasible: max |x_j - round(x_j)| - cheap but often poor\n- Product score: score_j = max(D⁺_j·D⁻_j, ε) where D are degradations\n- Hybrid: (1-μ)·min(D⁺,D⁻) + μ·max(D⁺,D⁻), μ typically 1/6\n- Pseudo-costs: ψ⁺_j·(⌈x_j⌉-x_j) × ψ⁻_j·(x_j-⌊x_j⌋)\n\nStrong branching workflow:\n1. Select k candidates (most fractional or pseudocost-based)\n2. For each candidate, solve LP for both branches (with iteration limit)\n3. Record D⁺_j = z⁺_j - z, D⁻_j = z⁻_j - z (or infeasible/cutoff)\n4. Score candidates, pick best\n5. Update pseudo-costs from observed degradations", "math": "Reliability branching: Use pseudo-costs ψ_j unless branched on\nvariable < η_rel times, then strong branch to improve estimate.\nBalances accuracy vs cost of strong branching.", "complexity": "Strong branching: O(k × LP_iterations) per node.\nPseudo-costs: O(n) after warmup. Reliability: adaptive O(k') where k' ≤ k.", "ref": ["Achterberg, Koch & Martin, \"Branching rules revisited\",\n     Operations Research Letters 33 (2005) 42-54\n\nCbcBranchDecision defines HOW TO CHOOSE between branching candidates:\n- betterBranch(): Compare two CbcBranchingObjects after strong branching\n- bestBranch(): Compare N candidates and return best index\n\nUsed after strong branching evaluates candidates. Decision considers:\n- changeUp/changeDown: Objective change estimates\n- numInfeasibilitiesUp/Down: Remaining infeasibilities"], "see": ["CbcBranchDefaultDecision for simple selection", "CbcBranchDynamicDecision for pseudocost-based selection", "CbcBranchingObject for what gets compared", "OsiChooseVariable for variable selection integration\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": true}, "src/CbcHeuristicVND.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicVND.hpp", "filename": "CbcHeuristicVND.hpp", "file": "CbcHeuristicVND.hpp", "brief": "VND - Variable Neighborhood Descent\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicVND: Variable Neighborhood Descent metaheuristic.\nSystematically explores neighborhoods of increasing size.", "algorithm": "VND - Variable Neighborhood Descent (Hansen & Mladenovic):\n  solution() systematic neighborhood exploration:\n  1. k ← 1; x ← baseSolution_.\n  2. Search N_k(x) using solutionFix() (restrict k·stepSize_ vars).\n  3. If improved: x ← x', k ← 1 (restart from smallest).\n  4. Else: k ← k + 1 (expand neighborhood).\n  5. Repeat until k > kmax_.\n  Systematic intensification with dynamic neighborhood sizing.", "math": "Neighborhood structure:\n  N_k(x) = {x' : |{j : x'_j ≠ x_j}| ≤ k·stepSize_}.\n  Size increases as k increases: |N_1| ⊆ |N_2| ⊆ ... ⊆ |N_kmax|.\n  On improvement: restart with small neighborhood (intensify).\n  On failure: expand neighborhood (diversify).\n\nParameters:\n- stepSize_: Base neighborhood size\n- k_, kmax_: Current and maximum neighborhood index\n- nDifferent_: Tracks solution diversity\n- baseSolution_: Starting point for descent\n\nUses solutionFix() to solve restricted MIPs.", "see": ["CbcHeuristicRINS for related neighborhood search", "CbcHeuristicLocal for simpler local search\n\nEdwin 12/5/09 carved out of CbcHeuristicRINS"], "has_pass2": true}, "src/CbcFathom.hpp": {"path": "layer-2/Cbc/src/CbcFathom.hpp", "filename": "CbcFathom.hpp", "file": "CbcFathom.hpp", "brief": "Fathoming methods to complete subproblems", "algorithm": "Specialized Fathoming for Small Subproblems\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFathom: Abstract base for specialized fathoming techniques.\nAfter branching, the subproblem may be small enough for specialized\nexact methods faster than general B&C.\n\nfathom() return codes:\n- 0: No fathoming attempted\n- 1: Fully fathomed (optimal for subtree)\n- 2: Incomplete search\n- 3: Incomplete but treat as complete\n\nExample implementations:\n- Presolve to smaller problem, then B&C\n- Dynamic programming for special structure\n\nAlso contains CbcOsiSolver: OsiClpSolverInterface that knows about\nCbcModel (base class for CbcLinked).", "see": ["CbcFathomDynamicProgramming for DP fathoming", "CbcLinked for linked solver interface"], "has_pass2": true}, "src/CbcDebug.hpp": {"path": "layer-2/Cbc/src/CbcDebug.hpp", "filename": "CbcDebug.hpp", "file": "CbcDebug.hpp", "brief": "Debugging utilities and test message handlers\n\nCbcDebug: Contains debug memory allocation routines and test message handlers.\n\nFeatures when CLP_DEBUG_MALLOC defined:\n- clp_memory(): Memory debugging\n- clp_malloc()/clp_free(): Custom allocation with tracking\n\nFeatures when TEST_MESSAGE_HANDLER defined:\n- MyMessageHandler2: Example custom message handler\n- Demonstrates trapping and customizing Cbc messages\n- Prefixes messages with \"==\" for identification\n\nThis file is primarily for testing and debugging purposes.\nProduction builds typically have neither macro defined.", "see": ["CoinMessageHandler for message handling base", "CbcMessage for Cbc message definitions"], "has_pass2": false}, "src/CbcGenMessages.hpp": {"path": "layer-2/Cbc/src/CbcGenMessages.hpp", "filename": "CbcGenMessages.hpp", "file": "CbcGenMessages.hpp", "brief": "Message IDs for cbc-generic standalone solver\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcGenMsgCode: Enum of message IDs for the cbc-generic application.\nThese are distinct from CbcMessage (library messages) and are used\nonly by the standalone command-line solver.\n\nMessage codes:\n- CBCGEN_TEST_MSG: Test/debug message\n- CBCGEN_NEW_SOLVER: Solver change notification\n- CBCGEN_CONFUSION: Error/confusion state\n\nThe actual message text is defined in CbcGenMessages.cpp.", "see": ["CbcMessage for library-level messages", "CoinMessageHandler for message infrastructure"], "has_pass2": false}, "src/CbcHeuristic.hpp": {"path": "layer-2/Cbc/src/CbcHeuristic.hpp", "filename": "CbcHeuristic.hpp", "file": "CbcHeuristic.hpp", "brief": "Base class for MIP primal heuristics\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "MIP Primal Heuristics Framework:\nPrimal heuristics find feasible integer solutions during B&C search.\nGood solutions improve the upper bound, enabling more pruning.\n\nHeuristic Categories:\n1. Construction: Build solution from scratch (rounding, greedy)\n2. Improvement: Refine existing solution (local search, RINS)\n3. Diving: Fix variables sequentially toward integrality\n4. Large Neighborhood Search: Fix subset, re-optimize remainder\n\nIncluded heuristics:\n- CbcRounding: Round LP solution using constraint \"locks\"\n  Lock = coefficient sign × bound direction. Round toward locked side.\n- CbcHeuristicPartial: Fix high-priority variables, solve sub-MIP\n- CbcSerendipity: Capture solutions found incidentally by solver\n\nScheduling via when_/whereFrom_/howOften_ controls invocation frequency\nand timing (root vs tree, before/after cuts).", "math": "Rounding with locks: For x_j fractional, count constraints where\nrounding down vs up would help feasibility. Round toward majority vote.\nQuality vs speed tradeoff: quick heuristics often, expensive ones rarely.", "complexity": "Rounding: O(nnz). Sub-MIP heuristics: depends on time limit.\nHeuristics typically limited to fraction of total solve time.", "ref": ["Berthold, \"Primal heuristics for mixed integer programs\", PhD thesis (2006)", "Achterberg, \"Constraint Integer Programming\", TU Berlin (2007), Ch. 7\n\nCbcHeuristic: Abstract base for primal heuristics that find feasible\nsolutions during branch-and-cut search. Key method is solution() which\nreturns 1 if a valid solution is found, 0 otherwise.\n\nExecution control:\n- when_: 0=off, 1=root only, 2=non-root, 3=always\n- whereFrom_: Bit field for timing (before/during/after cuts)\n- howOften_: Frequency of invocation\n\nIncludes several concrete heuristics:\n- CbcRounding: Simple rounding with lock-based feasibility\n- CbcHeuristicPartial: Fix priority-based partial solution\n- CbcSerendipity: Capture solver-found solutions\n- CbcHeuristicJustOne: Randomly select one from multiple heuristics\n\nAlso includes CbcHeuristicNode/NodeList for tracking where heuristics\nhave been invoked (distance-based diversification)."], "see": ["CbcHeuristicFPump for Feasibility Pump", "CbcHeuristicDive for diving heuristics", "CbcHeuristicRINS, CbcHeuristicRENS for neighborhood search", "CbcHeuristicLocal for local search"], "has_pass2": true}, "src/CbcGeneralDepth.hpp": {"path": "layer-2/Cbc/src/CbcGeneralDepth.hpp", "filename": "CbcGeneralDepth.hpp", "file": "CbcGeneralDepth.hpp", "brief": "Depth-limited partial evaluation branching", "algorithm": "Lookahead Branching with Partial Evaluation\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcGeneralDepth: Advanced branching using partial evaluation (lookahead).\nInstead of simple two-way branch, evaluates the search tree to a\nspecified depth and creates subproblems from the resulting nodes.\n\nUses ClpNode infrastructure (from Clp) for efficient node representation\nduring the lookahead phase. Can generate multiple subproblems from a\nsingle branching decision.\n\nAlso includes:\n- CbcGeneralBranchingObject: Multi-subproblem branching action\n- CbcOneGeneralBranchingObject: Single subproblem selection\n\nSupports orbital branching for symmetry handling.", "see": ["CbcGeneral for base class", "CbcSubProblem for subproblem representation", "ClpNode, ClpNodeStuff for Clp integration\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcHeuristicLocal.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicLocal.hpp", "filename": "CbcHeuristicLocal.hpp", "file": "CbcHeuristicLocal.hpp", "brief": "Local search and related improvement heuristics\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nContains several improvement heuristics for MIP:", "algorithm": "Crossover/Path Relinking - CbcHeuristicCrossover:\n  solution() combines multiple solutions:\n  1. Fix variables where all useNumber_ solutions agree.\n  2. Use random_ for tie-breaking diversification.\n  3. Solve restricted MIP on free variables.\n  Path relinking through solution pool agreement.\n\nCbcHeuristicNaive: Simple construction heuristic.\n(a) Fix integers close to zero\n(b) Fix integers with small costs to zero\n(c) Tighten continuous bounds and optimize", "see": ["CbcHeuristicFPump for Feasibility Pump", "CbcHeuristicRINS for neighborhood search", "CbcHeuristic for base class"], "has_pass2": true}, "src/CbcObject.hpp": {"path": "layer-2/Cbc/src/CbcObject.hpp", "filename": "CbcObject.hpp", "file": "CbcObject.hpp", "brief": "Abstract base for branching entities (variables, SOS, etc.)", "algorithm": "Branch-and-Bound: Branching Entity Interface\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcObject defines WHAT can be branched on:\n- infeasibility(): Measure of violation (0 = feasible)\n- feasibleRegion(): Bound tightening without branching\n- createCbcBranch(): Create CbcBranchingObject for this entity\n\nDerived classes: CbcSimpleInteger, CbcSOS, CbcClique, CbcNWay, etc.\n\nCbcStrongInfo struct holds strong branching results:\nup/down movement, infeasibilities, solver status.\n\nNote: Current implementation assumes binary branching (2 arms).", "see": ["CbcSimpleInteger for integer variable objects", "CbcSOS for SOS Type 1/2 constraints", "CbcBranchingObject for branching actions", "OsiObject for OSI-level base class\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": true}, "src/CbcCompare.hpp": {"path": "layer-2/Cbc/src/CbcCompare.hpp", "filename": "CbcCompare.hpp", "file": "CbcCompare.hpp", "brief": "Functor wrapper for node comparison", "algorithm": "Node Priority Functor Wrapper\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompare: Thin wrapper making CbcCompareBase usable as functor.\nUsed by standard heap algorithms for node priority queue.\n\nHolds pointer to CbcCompareBase strategy object (test_).\noperator() delegates to test_->test(x,y).", "see": ["CbcCompareBase for strategy interface", "CbcTree for usage in node management"], "has_pass2": true}, "src/CbcCompareDefault.hpp": {"path": "layer-2/Cbc/src/CbcCompareDefault.hpp", "filename": "CbcCompareDefault.hpp", "file": "CbcCompareDefault.hpp", "brief": "Default adaptive node comparison strategy\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "Adaptive Hybrid Node Selection:\nDynamically switches between search strategies based on solve progress.\n\nPhase 1 (Before first solution):\n- weight_ = -1: Fewest infeasibilities (greedy toward integrality)\n- weight_ = -2: Breadth-first for 1000 nodes (explore diversity)\n- weight_ = -3: Depth-first (find solution fast, low memory)\n\nPhase 2 (After solution found):\n- weight_ = 0: Auto-compute from gap to balance exploration/exploitation\n- weight_ > 0: Hybrid score = objective + weight × numInfeasibilities\n  Higher weight emphasizes depth, lower weight emphasizes best-bound\n\nAdaptive adjustments:\n- newSolution(): Recalibrate weight based on improvement\n- every1000Nodes(): Check tree size, possibly switch strategy or dive\n- Diving: Temporary depth-first from promising node to find solution", "math": "Node score: S(n) = z(n) + w × k(n)\nwhere z(n) = LP bound, k(n) = number of infeasibilities, w = weight.\nOptimal w depends on gap: w ≈ (z* - z_LP) / k_avg", "complexity": "Comparison: O(1). Strategy switches: O(n log n) for re-heapify.\nAdaptive strategies typically 20-50% faster than fixed strategies.", "ref": ["Linderoth & Savelsbergh, \"A computational study of search strategies\",\n     INFORMS Journal on Computing 11 (1999) 173-187\n\nCbcCompareDefault: Sophisticated hybrid search strategy.\nAdapts behavior based on search phase and solution history.\n\nStrategy phases:\n- Before solution: Use depth-first (weight_ < 0) or breadth-first\n- After solution: Weight-based hybrid of objective and infeasibilities\n\nweight_ special values:\n- 0.0: Auto-compute from first solution\n- -1.0: Fewest infeasibilities (before solution)\n- -2.0: Breadth-first for first 1000 nodes\n- -3.0: Depth-first before solution\n\nAdaptive callbacks:\n- newSolution(): Recomputes weight from solution gap\n- every1000Nodes(): Adjusts for tree size, triggers diving\n\nDiving support:\n- startDive()/cleanDive(): Focused depth search from promising node"], "see": ["CbcCompareBase for interface", "CbcCompareDepth for simple depth-first\n\nEdwin 11/25/09 carved out of CbcCompareActual"], "has_pass2": true}, "src/CbcHeuristicDINS.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDINS.hpp", "filename": "CbcHeuristicDINS.hpp", "file": "CbcHeuristicDINS.hpp", "brief": "DINS - Distance-Induced Neighborhood Search\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDINS: Uses multiple solutions to define neighborhoods.\nMaintains a pool of solutions and fixes variables based on\nagreement across the solution pool (Ghosh 2007).", "algorithm": "DINS - Distance-Induced Neighborhood Search (Ghosh 2007):\n  solution() with solution pool {x^1,...,x^k}:\n  1. Fix variable j if x^1_j = x^2_j = ... = x^k_j (unanimous).\n  2. Add distance constraint: Σ|x_j - x^best_j| ≤ localSpace_.\n  3. Solve restricted MIP with distance bound.\n  4. If improved, add to pool; maintain maximumKeepSolutions_.\n  Uses multiple solutions for more informed fixing.", "math": "Distance neighborhood:\n  N_DINS = {x : x_j = v_j ∀j unanimous, dist(x, x^best) ≤ r}.\n  dist(x, y) = Σ_j |x_j - y_j| (Hamming for binary, L1 for general).\n  Pool-based: More robust than two-solution RINS.\n\nKey parameters:\n- maximumKeepSolutions_: Size of solution pool\n- localSpace_: Tightness of distance constraint\n- howOften_: Frequency of application", "see": ["CbcHeuristicRINS for two-solution comparison", "CbcHeuristic for base class\n\nEdwin 12/5/09 carved out of CbcHeuristicRINS"], "has_pass2": true}, "src/CbcNodeInfo.hpp": {"path": "layer-2/Cbc/src/CbcNodeInfo.hpp", "filename": "CbcNodeInfo.hpp", "file": "CbcNodeInfo.hpp", "brief": "Persistent information for recreating search tree nodes", "algorithm": "Search Tree Node Reconstruction with Reference Counting\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcNodeInfo stores information to reconstruct a subproblem:\n- Warm start basis\n- Variable bound changes\n- Cutting planes added at this node\n- Parent linkage and reference counting\n\nTwo flavors:\n- CbcFullNodeInfo: Complete state (used at root)\n- CbcPartialNodeInfo: Differences from parent (saves memory)\n\nReference counting: Sum of potential + actual children.\nNode deleted when count reaches zero (subtree pruned).", "see": ["CbcNode for live node information", "CbcFullNodeInfo for complete state storage", "CbcPartialNodeInfo for incremental storage\n\nEdwin 11/24/09 carved from CbcNode"], "has_pass2": true}, "src/CbcHeuristicDiveFractional.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveFractional.hpp", "filename": "CbcHeuristicDiveFractional.hpp", "file": "CbcHeuristicDiveFractional.hpp", "brief": "Dive heuristic selecting most fractional variable\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveFractional: Simplest diving strategy.\nSelects the variable with value closest to 0.5 (most fractional).", "algorithm": "Most Fractional Variable Selection:\n  selectVariableToBranch() for fractional integer x_j:\n  1. Compute fractionality: f_j = min(x̄_j - ⌊x̄_j⌋, ⌈x̄_j⌉ - x̄_j).\n  2. Select j* = argmax{f_j : j ∈ fractional integers}.\n  3. Round toward nearest integer (bestRound from f_j comparison).\n  Rationale: Most fractional = hardest to round = fix early.", "math": "Fractionality measure:\n  f(x) = min(x - floor(x), ceil(x) - x).\n  f(x) ∈ [0, 0.5], with f(x) = 0.5 being maximally fractional.\n  Breaking hardest decisions first often finds feasibility faster.\n\nSimple but effective for breaking ties and finding feasibility.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicDivePseudoCost for pseudocost-guided variant"], "has_pass2": true}, "src/CbcCutModifier.hpp": {"path": "layer-2/Cbc/src/CbcCutModifier.hpp", "filename": "CbcCutModifier.hpp", "file": "CbcCutModifier.hpp", "brief": "Abstract base class for cut modification", "algorithm": "Cut Post-Processing Interface\nCopyright (C) 2003, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCutModifier: Allows post-processing of generated cuts.\nCalled after cut generation to strengthen, weaken, or remove cuts.\n\nmodify() return codes:\n- 0: Unchanged\n- 1: Strengthened (tightened)\n- 2: Weakened (loosened)\n- 3: Deleted (should be discarded)\n\nUse cases:\n- Strengthen cuts using problem-specific knowledge\n- Remove cuts that are too dense or weak\n- Apply numerical cleanup", "see": ["CbcCutGenerator for cut generation framework", "CbcCutSubsetModifier for subset-specific modification\n\nEdwin 11/25/09 carved out of CbcCutGenerator"], "has_pass2": true}, "src/CbcCompareActual.hpp": {"path": "layer-2/Cbc/src/CbcCompareActual.hpp", "filename": "CbcCompareActual.hpp", "file": "CbcCompareActual.hpp", "brief": "Aggregator header for concrete comparison classes", "algorithm": "Node Comparison Strategy Collection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nConvenience header including all concrete node comparison strategies.\nSimilar pattern to CbcBranchActual.hpp.", "see": ["CbcCompareBase for abstract interface", "CbcCompareDepth for depth-first", "CbcCompareDefault for adaptive hybrid"], "has_pass2": true}, "src/CbcSolverHeuristics.hpp": {"path": "layer-2/Cbc/src/CbcSolverHeuristics.hpp", "filename": "CbcSolverHeuristics.hpp", "file": "CbcSolverHeuristics.hpp", "brief": "Heuristic setup and execution routines for cbc-generic", "algorithm": "VUB Fixing and Heuristic Orchestration\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nHelper functions used by the standalone solver for heuristic management.\n\ncrunchIt(): Compress/simplify a ClpSimplex model.\n\nfixVubs(): Fix variable upper bound constraints.\n- doAction values:\n  - 0: Just fix in original, return NULL\n  - 1: Return fixed non-presolved solver\n  - 2: Use presolve internally\n  - 3: Presolve and fix high-cost variables\n  - 10+: Use lastSolution and relax a few\n  - -2: Cleanup after mode 2\n- Returns number of variables fixed\n\ndoHeuristics(): Configure and run heuristics.\n- type 1: Add heuristics to model\n- type 2: Run heuristics (set cutoff and solution)\n- type 3: For miplib testing (skip some)", "see": ["CbcHeuristic for heuristic base class", "CbcParameters for parameter management"], "has_pass2": true}, "src/CbcHeuristicGreedy.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicGreedy.hpp", "filename": "CbcHeuristicGreedy.hpp", "file": "CbcHeuristicGreedy.hpp", "brief": "Greedy construction heuristics for set covering/partitioning\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nContains greedy heuristics suited for set covering/partitioning models:", "algorithm": "Greedy SOS/GUB - CbcHeuristicGreedySOS:\n  solution() for models with GUB structure:\n  Handles SOS constraints (originalRhs_=-1) and <= constraints.\n  algorithm_ bits: 1=current model, 2=solution-guided, 4=merit, 8/16=duals.", "math": "Greedy ratio with perturbation:\n  ratio_j = c_j / coverage_j × (1 + ε·random).\n  ε larger with algorithm_ += 10.\n  Classic greedy O(n·m) where n=variables, m=constraints.\n\nAll three use algorithm_ flags:\n- 0: Use current upper bounds\n- 1: Use original upper bounds\n- +10: More perturbation in ratios\n- +100: Round up all >= 0.5", "see": ["CbcHeuristic for base class", "CbcSOS for SOS constraint branching"], "has_pass2": true}, "src/CbcSymmetry.hpp": {"path": "layer-2/Cbc/src/CbcSymmetry.hpp", "filename": "CbcSymmetry.hpp", "file": "CbcSymmetry.hpp", "brief": "Symmetry detection and orbital branching using nauty\nAuthors: Pietro Belotti (Lehigh), Andreas Waechter (IBM)\nAdapted from Couenne (Carnegie-Mellon University, 2006-11)\nThis file is licensed under the Eclipse Public License (EPL)\n\nCbcSymmetry: Detects problem symmetry and exploits it for faster solving.\nUses the nauty library for automorphism group computation.", "algorithm": "Orbital Branching - CbcOrbitalBranchingObject:\n  branch() exploits symmetry to avoid symmetric subtrees:\n  1. Down branch: Fix x_column = 0, all orbit members = 0.\n  2. Up branch: Fix x_column = 1.\n  Breaks symmetry: symmetric nodes pruned implicitly.", "math": "Automorphism group:\n  Aut(G) = {π : V → V | π preserves edges and colors}.\n  |Aut(G)| can be exponential; nauty computes generators.\n  Orbit O(v) = {π(v) : π ∈ Aut(G)} - equivalence class.\n\nKey concepts:\n- Orbits: Sets of symmetric variables\n- Orbital fixing: If one variable in orbit is fixed, fix others by symmetry\n- Orbital branching: Branch to break symmetry\n\nBuild requirements (when CBC_HAS_NAUTY defined):\n- nauty library (configure with --enable-tls --enable-wordsize=32)\n- Add -DCBC_HAS_NAUTY and -DNTY_TRACES (optional) to CXXDEFS\n- Link with -lnauty\n\nRuntime: Use -orbit on to enable\n\nCbcNauty: Wrapper for nauty graph automorphism library.\nHandles graph construction, partition refinement, and orbit computation.\n\nCbcOrbitalBranchingObject: Branching object that exploits symmetry.\nOn one branch, fix variable to 1 and symmetric variables to 0.", "see": ["CbcBranchingObject for branching base class", "nauty documentation for algorithm details"], "has_pass2": true}, "src/CbcBranchActual.hpp": {"path": "layer-2/Cbc/src/CbcBranchActual.hpp", "filename": "CbcBranchActual.hpp", "file": "CbcBranchActual.hpp", "brief": "Aggregator for concrete branching classes", "algorithm": "Branching Object Implementation Collection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nConvenience header that includes all concrete branching implementations:\n- CbcSimpleInteger, CbcSimpleIntegerPseudoCost: Integer variables\n- CbcClique, CbcSOS, CbcNWay: Constraint branching\n- CbcFollowOn, CbcFixVariable: Derived bound manipulation\n- CbcGeneral, CbcGeneralDepth: Advanced branching strategies\n\nMost code should include CbcBranchBase.hpp instead for just the\nabstract interfaces; use this header when concrete classes are needed.", "see": ["CbcBranchBase.hpp for abstract interface aggregation"], "has_pass2": true}, "src/CbcHeuristicDiveCoefficient.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveCoefficient.hpp", "filename": "CbcHeuristicDiveCoefficient.hpp", "file": "CbcHeuristicDiveCoefficient.hpp", "brief": "Dive heuristic based on objective coefficients\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveCoefficient: Selects variables based on objective impact.\nPrioritizes fractional variables with large objective coefficients.", "algorithm": "Objective Coefficient Variable Selection:\n  selectVariableToBranch() for fractional integer x_j:\n  1. Score each fractional variable: score_j = |c_j| × lock_factor_j.\n  2. Select j* = argmax{score_j : j ∈ fractional integers}.\n  3. Round to minimize objective (down if c_j > 0 for min).\n  Rationale: Large |c_j| = high objective impact = resolve early.", "math": "Objective impact heuristic:\n  For min c^T x: rounding x_j affects objective by c_j × (round - x̄_j).\n  Variables with large |c_j| have most impact on objective.\n  Fixing high-impact variables first guides toward good solutions.\n\nUseful when objective-driven rounding is desired.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicDivePseudoCost for estimate-based variant"], "has_pass2": true}, "src/CbcBranchBase.hpp": {"path": "layer-2/Cbc/src/CbcBranchBase.hpp", "filename": "CbcBranchBase.hpp", "file": "CbcBranchBase.hpp", "brief": "Base includes for CBC branching model", "algorithm": "Three-Class Branching Model (Object/BranchingObject/Decision)\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nAggregates the three-class branching model:\n- CbcObject: What to branch on (integer vars, SOS, cliques)\n- CbcBranchingObject: How to branch (bound changes per arm)\n- CbcBranchDecision: How to choose (compare candidates)\n\nAlso defines CbcRangeCompare enum for bound comparisons\nand CbcCompareRanges() utility function.", "see": ["CbcObject for branching candidates", "CbcBranchingObject for branching actions", "CbcBranchDecision for selection criteria", "OsiBranchingObject for OSI-level interface"], "has_pass2": true}, "src/CbcGeneral.hpp": {"path": "layer-2/Cbc/src/CbcGeneral.hpp", "filename": "CbcGeneral.hpp", "file": "CbcGeneral.hpp", "brief": "Abstract base for general multi-way branching", "algorithm": "Abstract Base for Multi-Way/Lookahead Branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcGeneral: Abstract base class for advanced branching strategies\nthat create lists of subproblems beyond simple two-way branching.\n\nDesigned for:\n- Partial evaluation (lookahead) branching\n- Multi-way branching strategies\n- Complex branching objects that need customized subproblem generation\n\nPure virtual class - see CbcGeneralDepth for main implementation.", "see": ["CbcGeneralDepth for depth-limited partial evaluation", "CbcNWay for simpler N-way branching\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcHeuristicDiveGuided.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveGuided.hpp", "filename": "CbcHeuristicDiveGuided.hpp", "file": "CbcHeuristicDiveGuided.hpp", "brief": "Dive heuristic guided by incumbent solution\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveGuided: Uses existing incumbent to guide diving.\nRequires a feasible solution (canHeuristicRun checks this).", "algorithm": "Guided Dive Variable Selection (Danna, Rothberg, Le Pape):\n  selectVariableToBranch() given incumbent x*:\n  1. For each fractional integer x_j, compute distance to x*_j.\n  2. Select j* with largest discrepancy: |x̄_j - x*_j|.\n  3. Round toward incumbent: bestRound = round(x̄_j toward x*_j).\n  Requires incumbent: canHeuristicRun() returns false if none.", "math": "Incumbent-guided rounding:\n  Score_j = |x̄_j - x*_j| for fractional j.\n  Intuition: Variables far from incumbent are \"wrong\"; fix them first.\n  Rounds toward x*_j to stay in proven-good neighborhood.\n  Similar to RINS but integrated into diving framework.\n\nPart of the \"Guided Dive\" approach from Danna et al.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicRINS for related incumbent-guided search"], "has_pass2": true}, "src/CbcLinked.hpp": {"path": "layer-2/Cbc/src/CbcLinked.hpp", "filename": "CbcLinked.hpp", "file": "CbcLinked.hpp", "brief": "Extended solver for nonlinear and bilinear problems", "algorithm": "Bilinear Term Linearization (4-Lambda SOS2)\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nThis file contains classes for handling nonlinear/bilinear optimization:\n\nOsiSolverLink: Extended OsiSolver that handles coefficient updates.\n- Nonlinear SLP (Successive Linear Programming)\n- Quadratic objective linearization\n- Bilinear term handling via lambda formulations\n\nOsiLinkedBound: Tracks bounds that depend on other bounds.\n- Coefficient updates when bounds change\n- Used for McCormick-style relaxations\n\nOsiBiLinear: Represents x*y bilinear terms.\n- Constructs 4-lambda SOS2 formulation\n- Handles mesh refinement for tighter bounds\n- Supports branching on either x or y\n\nOsiLink/OsiOldLink: Linked ordered sets for structured problems.\n- Models y and x*f(y) relationships\n- SOS-style branching with linked variables\n\nCbcHeuristicDynamic3: Heuristic for picking up feasible solutions.\nCglTemporary: Temporary cut storage (destroyed after first use).\nOsiSolverLinearizedQuadratic: Solver with linearized quadratic objective.", "see": ["CbcOsiSolver for base solver-model link", "OsiBranchingObject for branching interface"], "has_pass2": true}, "src/CbcCutGenerator.hpp": {"path": "layer-2/Cbc/src/CbcCutGenerator.hpp", "filename": "CbcCutGenerator.hpp", "file": "CbcCutGenerator.hpp", "brief": "Interface between Cbc and Cut Generation Library (CGL)\nCopyright (C) 2003, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCutGenerator: Wraps a CglCutGenerator with Cbc-specific controls.\nManages when and how cut generation is called during B&C search.\n\nKey control parameters:\n- whenCutGenerator_: Frequency (every N nodes, -100=disabled, -99=root only)\n- depthCutGenerator_: Depth-based triggering\n- switches_: Bit flags for various options (normal, atSolution, timing, etc.)\n\nStatistics tracked:\n- numberTimes_: Times generator was called\n- numberCuts_: Total cuts generated\n- numberCutsActive_: Cuts still binding at end\n- timeInCutGenerator_: CPU time spent\n\nSupports adaptive behavior:\n- Can be switched off if ineffective\n- Tracks cuts at root vs tree\n- Global cuts and Lagrangean relaxation support", "algorithm": "Root Node vs. Tree Strategy:\n  - Root: Aggressive cuts (high whenCutGenerator_, all generators active)\n  - Tree: Selective cuts (numberActiveCutsAtRoot_ guides which to keep)\n  - Deep tree: Often disable most generators (computational cost)\n  Trade-off: Stronger root LP → fewer nodes vs. more time per node.", "ref": ["Cornuejols, G. (2008). \"Valid Inequalities for Mixed Integer\n       Linear Programs\". Math. Programming 112(1):3-44."], "complexity": "generateCuts: O(generator_cost × frequency)\n  Generator costs vary: Clique O(n), Gomory O(n²), Probing O(n × LP_solve)\n  Overall B&C impact: cuts reduce nodes but add per-node overhead", "see": ["CglCutGenerator for cut generation algorithms", "CbcCutModifier for post-generation cut modification", "CbcModel::addCutGenerator()"], "has_pass2": true}, "src/CbcEventHandler.hpp": {"path": "layer-2/Cbc/src/CbcEventHandler.hpp", "filename": "CbcEventHandler.hpp", "file": "CbcEventHandler.hpp", "brief": "Event handling and callbacks for Cbc", "algorithm": "Callback Mechanism for B&C Control\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcEventHandler: Callback mechanism for monitoring and controlling B&C.\nProvides hooks at key points in the search process.\n\nEvents (CbcEvent enum):\n- node: After processing a node\n- solution: Integer solution found\n- heuristicSolution: Solution from heuristic\n- treeStatus: Periodic tree status report\n- endSearch: Search termination\n- afterRootCuts: Root node complete\n\nActions (CbcAction enum):\n- noAction: Continue normally\n- stop: Abort search\n- restart: Restart from current point\n- restartRoot: Restart from scratch\n- killSolution: Reject found solution\n\nUsage: Derive subclass, override event() method.\nDefault implementation uses event/action map (eaMap_).\n\nDesign matches ClpEventHandler for Clp compatibility.", "see": ["CbcModel::setEventHandler()", "ClpEventHandler for Clp equivalent"], "has_pass2": true}, "src/CbcSolverAnalyze.hpp": {"path": "layer-2/Cbc/src/CbcSolverAnalyze.hpp", "filename": "CbcSolverAnalyze.hpp", "file": "CbcSolverAnalyze.hpp", "brief": "Problem analysis for integer constraint detection", "algorithm": "Integer Structure Detection and Strengthening\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nanalyze(): Examines model to detect integer structure.\n\nLooks for constraints where:\n- All variables are integer\n- All coefficients are integer\n- RHS is integer\n\nSuch constraints can be strengthened or exploited specially.\nIf changeInt is true, may change continuous variables to integer\nwhen they only appear in integer-coefficient constraints.\n\nReturns array of changed variable indices.\nSets numberChanged to count of modifications.\nSets increment to objective coefficient GCD for integer solutions.", "see": ["CbcModel for problem analysis during solve", "CoinPackedMatrix for constraint access"], "has_pass2": true}, "src/CbcPartialNodeInfo.hpp": {"path": "layer-2/Cbc/src/CbcPartialNodeInfo.hpp", "filename": "CbcPartialNodeInfo.hpp", "file": "CbcPartialNodeInfo.hpp", "brief": "Incremental subproblem storage as differences from parent", "algorithm": "Delta-Encoded Node State for Memory Efficiency\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcPartialNodeInfo stores subproblem state as changes from parent:\n- CoinWarmStartDiff for basis changes\n- Only modified variable bounds\n- New cuts added at this node\n\nMemory-efficient: Stores only differences, reconstructs via parent chain.\napplyToModel() walks ancestor chain to rebuild complete state.", "see": ["CbcNodeInfo for base class", "CbcFullNodeInfo for complete state storage", "CoinWarmStartDiff for basis difference representation\n\nEdwin 11/24/09 carved from CbcNode"], "has_pass2": true}, "src/CbcCutSubsetModifier.hpp": {"path": "layer-2/Cbc/src/CbcCutSubsetModifier.hpp", "filename": "CbcCutSubsetModifier.hpp", "file": "CbcCutSubsetModifier.hpp", "brief": "Cut modifier that filters cuts based on variable indices", "algorithm": "Variable-Index Cut Filtering for Decomposition\nCopyright (C) 2003, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCutSubsetModifier: Filters cuts to exclude those involving certain variables.\nUsed in decomposition-style approaches where only a subset of variables\nshould appear in cuts.\n\nPrimary use case:\n- Remove cuts that reference variables >= firstOdd_\n- Useful when auxiliary variables shouldn't appear in cuts\n- Can weaken cuts instead of deleting (future enhancement)\n\nmodify() return values:\n- 0: Cut unchanged\n- 1: Cut strengthened\n- 2: Cut weakened\n- 3: Cut deleted (set to NULL)", "see": ["CbcCutModifier for base class", "CbcCutGenerator for cut generation"], "has_pass2": true}, "src/ClpConstraintAmpl.hpp": {"path": "layer-2/Cbc/src/ClpConstraintAmpl.hpp", "filename": "ClpConstraintAmpl.hpp", "file": "ClpConstraintAmpl.hpp", "brief": "AMPL-interface constraint for nonlinear Clp extensions", "algorithm": "AMPL ASL Nonlinear Constraint Gradient\nCopyright (C) 2007, International Business Machines Corporation.\nThis code is licensed under the Eclipse Public License (EPL).\n\nClpConstraintAmpl: Nonlinear constraint class using AMPL's ASL\n(Solver Library) for gradient evaluation. Enables nonlinear\nconstraints in Clp through AMPL model interface.", "see": ["ClpConstraint for base constraint interface", "ClpAmplObjective for corresponding objective class"], "has_pass2": true}, "src/CbcSimpleIntegerDynamicPseudoCost.hpp": {"path": "layer-2/Cbc/src/CbcSimpleIntegerDynamicPseudoCost.hpp", "filename": "CbcSimpleIntegerDynamicPseudoCost.hpp", "file": "CbcSimpleIntegerDynamicPseudoCost.hpp", "brief": "Integer variable with dynamic (learning) pseudocosts\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSimpleIntegerDynamicPseudoCost: Implements reliability branching\nbased on Achterberg, Koch & Martin's work. Pseudocosts are learned\nfrom actual branching history rather than being static estimates.\n\nKey statistics tracked per variable:\n- sumUpCost_/sumDownCost_: Cumulative objective changes\n- numberTimesUp_/Down_: Branch count for averaging\n- numberBeforeTrust_: Initialization threshold before trusting estimates", "algorithm": "Reliability Branching (Achterberg, Koch, Martin 2005):\n  For each integer variable x_j:\n  1. If numberTimesDown_ < numberBeforeTrust_: Use strong branching.\n  2. Else: Use pseudocost estimate ψ_j^- × (x̄_j - floor(x̄_j)).\n  3. Update pseudocost after each branch: ψ_j ← Σ(ΔLP)/Σ(Δx).\n  Hybrid of strong branching (accurate) and pseudocosts (fast).", "math": "Pseudocost estimation:\n  Down estimate: D_j = ψ_j^- × (x̄_j - ⌊x̄_j⌋).\n  Up estimate: U_j = ψ_j^+ × (⌈x̄_j⌉ - x̄_j).\n  Score: μ·min(D_j,U_j) + (1-μ)·max(D_j,U_j), μ ∈ [0,1].\n  WEIGHT_PRODUCT variant: score = D_j^{1-μ} × U_j^{μ}.\n\nReliability branching concept:\n- Initially uses strong branching until numberBeforeTrust_ reached\n- Then relies on accumulated pseudocost estimates\n- Balances exploration (strong branching) vs exploitation (estimates)\n\nAlso includes CbcIntegerPseudoCostBranchingObject and optionally\nCbcSwitchingBinary for coupled binary/continuous variables.", "see": ["CbcSimpleIntegerPseudoCost for static pseudocost version", "CbcBranchDynamicDecision for decision making with dynamic costs\n\nEdwin 11/17/2009 carved out of CbcBranchDynamic"], "has_pass2": true}, "src/CbcSimpleIntegerPseudoCost.hpp": {"path": "layer-2/Cbc/src/CbcSimpleIntegerPseudoCost.hpp", "filename": "CbcSimpleIntegerPseudoCost.hpp", "file": "CbcSimpleIntegerPseudoCost.hpp", "brief": "Integer variable with static pseudocosts\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSimpleIntegerPseudoCost: Extends CbcSimpleInteger with static\npseudocost estimates for branch direction preference:\n- upPseudoCost_: Estimated objective increase per unit ceiling\n- downPseudoCost_: Estimated objective increase per unit floor", "algorithm": "Static Pseudocost Branching:\n  infeasibility() and createCbcBranch() for variable x_j:\n  1. Compute fractionality: f = x̄_j - floor(x̄_j).\n  2. Down estimate: D = downPseudoCost_ × f.\n  3. Up estimate: U = upPseudoCost_ × (1-f).\n  4. Return infeasibility based on method_:\n     0: min(D,U), 1-3: max(D,U) under various conditions.\n  5. preferredWay from upDownSeparator_ threshold.", "math": "Static vs dynamic pseudocosts:\n  Static: ψ_j^+, ψ_j^- fixed from problem structure or user input.\n  Dynamic: ψ_j learned from branching history.\n  Static faster (no updates) but less accurate.\n  Often initialized from constraint matrix analysis.\n\nUnlike CbcSimpleIntegerDynamicPseudoCost, these values are fixed\n(typically from problem structure or user-provided estimates).\n\nMethod modes for infeasibility calculation:\n- 0: Return min(up, down) - conservative\n- 1: Return max before any solution\n- 2: Return max before branched solution\n- 3: Always return max - aggressive", "see": ["CbcSimpleInteger for base class", "CbcSimpleIntegerDynamicPseudoCost for learning-based version\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcSubProblem.hpp": {"path": "layer-2/Cbc/src/CbcSubProblem.hpp", "filename": "CbcSubProblem.hpp", "file": "CbcSubProblem.hpp", "brief": "Compact subproblem state for diving heuristics", "algorithm": "Lightweight Subproblem State for Diving\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSubProblem stores a lightweight subproblem state:\n- objectiveValue_, sumInfeasibilities_\n- Variable bound changes\n- Basis status changes\n\nUsed by diving heuristics to backtrack efficiently.\napply() restores state to solver (bounds, basis, or both).", "see": ["CbcHeuristicDive for diving heuristics using this", "CbcNodeInfo for full search tree node storage", "ClpNode for Clp-specific node representation\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcMipStartIO.hpp": {"path": "layer-2/Cbc/src/CbcMipStartIO.hpp", "filename": "CbcMipStartIO.hpp", "file": "CbcMipStartIO.hpp", "brief": "Read MIP starting solutions from files", "algorithm": "MIP Warm-Start Solution I/O\nCbcMipStartIO: Utilities for reading partial or complete MIP solutions.\nWarm-starting B&C with a known solution can dramatically speed up solving.\n\nread(): Parses solution file in format: varname value\n- Returns 0 on success, 1 on failure\n- Fills colValues with (name, value) pairs\n- Reports objective value if available\n\ncomputeCompleteSolution(): Extends partial solution to all variables.\n- extraActions controls how unmentioned integers are set:\n  - 0: Default handling\n  - 1: Set to lower bound\n  - 2: Set to upper bound\n  - 3,5: Without costs as 1,2; with costs to cheapest\n  - 4,6: Without costs as 1,2; with costs to expensive\n\nUse case: Providing initial solutions from heuristics, previous runs,\nor domain knowledge to accelerate MIP solving.", "see": ["CbcModel::setBestSolution for using solutions", "CbcHeuristic for algorithmic solution generation"], "has_pass2": true}, "src/CbcDummyBranchingObject.hpp": {"path": "layer-2/Cbc/src/CbcDummyBranchingObject.hpp", "filename": "CbcDummyBranchingObject.hpp", "file": "CbcDummyBranchingObject.hpp", "brief": "No-op branching object for special cases", "algorithm": "Placeholder One-Way Branch (No-Op)\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcDummyBranchingObject: A placeholder branch that does nothing.\nUsed when the branching framework requires a branching object\nbut no actual branching action is needed.\n\nUse cases:\n- Continue tree exploration when LP appears feasible but isn't\n- Placeholder for deferred branching decisions\n- Testing and debugging the branching infrastructure\n\nThe branch() method returns immediately without modifying bounds.", "see": ["CbcBranchingObject for base class\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcHeuristicDiveVectorLength.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveVectorLength.hpp", "filename": "CbcHeuristicDiveVectorLength.hpp", "file": "CbcHeuristicDiveVectorLength.hpp", "brief": "Dive heuristic based on constraint participation\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveVectorLength: Selects variables by column density.\nVariables appearing in many constraints are fixed first.", "algorithm": "Vector Length Variable Selection:\n  selectVariableToBranch() for fractional integer x_j:\n  1. Compute column length: len_j = |{i : a_ij ≠ 0}| (nonzeros).\n  2. Select j* = argmax{len_j : j ∈ fractional integers}.\n  3. Round based on lock counts (from base CbcHeuristicDive).\n  Rationale: High-density columns affect many constraints.", "math": "Column density propagation:\n  Fixing x_j propagates to len_j constraints via bound tightening.\n  High len_j → more constraint propagation → faster feasibility detection.\n  Similar logic to \"most constrained variable\" in constraint programming.\n  Uses matrix_ from base class for efficient column length lookup.\n\nUses matrix_ from CbcHeuristicDive base class for column lengths.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicDiveFractional for simpler selection"], "has_pass2": true}, "src/CbcCompareDepth.hpp": {"path": "layer-2/Cbc/src/CbcCompareDepth.hpp", "filename": "CbcCompareDepth.hpp", "file": "CbcCompareDepth.hpp", "brief": "Depth-first node selection strategy\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareDepth: Implements depth-first search (DFS).\nDefault strategy before first solution is found.\n\ntest(x,y) returns true if y is deeper than x in the tree.\nDeepest nodes explored first -> LIFO stack behavior.", "algorithm": "Depth-First Search (DFS) Node Selection:\n  test(x, y) comparison for heap ordering:\n  Returns true if depth(y) > depth(x).\n  Effect: Deepest nodes processed first (LIFO behavior).\n  Memory: O(depth) nodes stored vs O(breadth) for best-first.\n  Finds feasible solution quickly; weak bounds until backtrack.", "math": "Complexity trade-off:\n  DFS memory: O(d) where d = tree depth.\n  Best-first memory: O(2^d) worst case.\n  DFS bound gap: May be large until substantial backtracking.\n  Best-first bound: Optimal LP bound tracked throughout.\n\nAdvantages:\n- Low memory (linear in depth)\n- Fast to first feasible solution\n- Good for proving feasibility\n\nDisadvantages:\n- May miss better solutions at other branches\n- Weak objective bounds early in search", "see": ["CbcCompareObjective for best-bound search", "CbcCompareDefault for hybrid approach\n\nEdwin 11/24/09 carved out of CbcCompareActual"], "has_pass2": true}, "src/CbcBranchingObject.hpp": {"path": "layer-2/Cbc/src/CbcBranchingObject.hpp", "filename": "CbcBranchingObject.hpp", "file": "CbcBranchingObject.hpp", "brief": "Abstract base for branching actions", "algorithm": "Branch-and-Bound: Branching Action Interface\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchingObject describes HOW to branch (the action):\n- branch(): Execute next branch arm, modify solver bounds\n- numberBranches(): Total arms (typically 2)\n- way(): Current branch direction (+1 up, -1 down)\n\nCbcBranchObjType enum identifies concrete types:\nSimpleIntegerBranchObj, CliqueBranchObj, SoSBranchObj,\nNWayBranchObj, CutBranchingObj, etc.\n\nExtends OsiBranchingObject with CBC-specific features.", "see": ["CbcObject for what creates branching objects", "CbcBranchDecision for choosing between objects", "CbcIntegerBranchingObject for integer variable branching\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": true}, "src/CbcNWay.hpp": {"path": "layer-2/Cbc/src/CbcNWay.hpp", "filename": "CbcNWay.hpp", "file": "CbcNWay.hpp", "brief": "N-way branching (exactly one variable at upper bound)\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcNWay: Multi-way (N-ary) branching for selection constraints.\nExactly one variable at upper bound, all others at lower bound.\nCreates N children, each fixing one variable to its UB.", "algorithm": "N-Way Branching for Selection Constraints:\n  For constraint Σx_j = 1 where x_j ∈ {0,1} (exactly one selected):\n  createCbcBranch() creates N child nodes:\n  Child k: x_k = 1 (UB), all other x_j = 0 (LB), j ≠ k.\n  applyConsequence() triggers bound propagation when x_k fixed.\n  Branch order determined by order_ array in CbcNWayBranchingObject.", "math": "N-way vs binary branching tree size:\n  Binary: O(2^n) nodes for n variables in selection constraint.\n  N-way: O(n) nodes per selection constraint (linear in choices).\n  Trade-off: More children per node but shallower tree.\n  Optimal for GUB/SOS1 structures where exactly one variable selected.\n\nDiffers from binary branching:\n- Traditional: 2 children (x <= floor vs x >= ceil)\n- N-way: N children, each selecting a different option\n\nSupports CbcConsequence for each member - when a variable\nis selected (fixed to UB), additional bound changes can be applied.\n\nExample: Facility location with 5 candidate sites.\nN-way branch creates 5 children, each opening a different site.", "see": ["CbcNWayBranchingObject for branching action", "CbcConsequence for side effects", "CbcClique for clique-based (2-way) branching\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcHeuristicPivotAndFix.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicPivotAndFix.hpp", "filename": "CbcHeuristicPivotAndFix.hpp", "file": "CbcHeuristicPivotAndFix.hpp", "brief": "Pivot and Fix heuristic using simplex pivots\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicPivotAndFix: Exploits LP basis structure.\nPerforms simplex pivots to explore nearby basic solutions,\nthen fixes integer variables at their current values.", "algorithm": "Pivot and Fix Heuristic:\n  solution() from LP optimal basis B:\n  1. Identify basic integer variables with fractional values.\n  2. For each: attempt simplex pivot to move to adjacent BFS.\n  3. After pivots, fix integers at current (hopefully integral) values.\n  4. Re-optimize continuous variables and check feasibility.\n  Exploits LP degeneracy for alternate basic solutions.", "math": "Simplex neighborhood exploration:\n  Adjacent basic feasible solutions differ by one exchange.\n  Pivot: x_B[i] leaves basis, x_N[j] enters (maintaining feasibility).\n  May reach integer-feasible BFS via sequence of degenerate pivots.\n  Effective when LP has many alternate optima.\n\nUses the LP optimal basis and pivots to find integer-feasible\nsolutions nearby in the simplex sense.", "see": ["CbcHeuristic for base class", "CbcHeuristicRandRound for randomized rounding"], "has_pass2": true}, "src/CbcMessage.hpp": {"path": "layer-2/Cbc/src/CbcMessage.hpp", "filename": "CbcMessage.hpp", "file": "CbcMessage.hpp", "brief": "Message IDs and handler for Cbc output\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcMessage: Defines message IDs (CBC_Message enum) for Cbc logging.\nUses COIN message handling framework (CoinMessageHandler).\n\nKey message categories:\n- CBC_SOLUTION: Integer solution found\n- CBC_STATUS/CBC_STATUS2/CBC_STATUS3: Progress reports\n- CBC_GAP: Optimality gap information\n- CBC_ROOT: Root node summary\n- CBC_GENERATOR: Cut generator statistics\n- CBC_BRANCH: Branching decisions\n- CBC_THREAD_STATS: Parallel statistics\n\nText in CbcMessage.cpp supports multiple languages (default: us_en).", "see": ["CoinMessageHandler for message handling infrastructure", "CbcGenMessages.hpp for command-line solver messages"], "has_pass2": false}, "src/CbcHeuristicRandRound.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicRandRound.hpp", "filename": "CbcHeuristicRandRound.hpp", "file": "CbcHeuristicRandRound.hpp", "brief": "Randomized rounding heuristic\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicRandRound: Probabilistic rounding of LP solution.\nRounds fractional variables randomly with probabilities based on\ntheir fractional values (e.g., x=0.7 rounds up with prob 0.7).", "algorithm": "Randomized Rounding Heuristic:\n  solution() from LP solution x̄:\n  1. For each fractional integer x_j with f = x̄_j - ⌊x̄_j⌋:\n     - Round up with probability f (x̂_j = ⌈x̄_j⌉).\n     - Round down with probability 1-f (x̂_j = ⌊x̄_j⌋).\n  2. Check feasibility of rounded solution x̂.\n  3. If feasible and improving, accept; else retry with new seed.", "math": "Expected value preservation:\n  E[x̂_j] = f·⌈x̄_j⌉ + (1-f)·⌊x̄_j⌋ = x̄_j.\n  Rounded solution is unbiased estimator of LP solution.\n  For packing/covering: Pr[constraint satisfied] analyzable.\n  Often O(log m) factor from optimal in expectation.\n\nMultiple trials with different random seeds can find diverse solutions.\nSimple but can be surprisingly effective for certain problem structures.", "see": ["CbcHeuristic for base class", "CbcHeuristicFPump for deterministic rounding approach"], "has_pass2": true}, "src/ClpAmplObjective.hpp": {"path": "layer-2/Cbc/src/ClpAmplObjective.hpp", "filename": "ClpAmplObjective.hpp", "file": "ClpAmplObjective.hpp", "brief": "AMPL-interface nonlinear objective for Clp", "algorithm": "AMPL ASL Nonlinear Objective Gradient\nCopyright (C) 2007, International Business Machines Corporation.\nThis code is licensed under the Eclipse Public License (EPL).\n\nClpAmplObjective: Nonlinear objective function using AMPL's ASL\nfor gradient evaluation. Provides gradient, reduced gradient, and\nstep length computation for line search in nonlinear Clp.", "see": ["ClpObjective for base objective interface", "ClpConstraintAmpl for corresponding constraint class"], "has_pass2": true}, "src/CbcModel.hpp": {"path": "layer-2/Cbc/src/CbcModel.hpp", "filename": "CbcModel.hpp", "file": "CbcModel.hpp", "brief": "Main branch-and-cut MIP solver class\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcModel is the central class for COIN-OR branch-and-cut MIP solving.\nKey methods:\n- initialSolve(): Solve LP relaxation\n- branchAndBound(): Run B&C algorithm to optimality\n\nArchitecture:\n- CbcNode/CbcNodeInfo: Subproblem representation in search tree\n- CbcTree: Priority queue of live nodes (heap)\n- CbcCutGenerator: Wrapper for CGL cut generators\n- CbcHeuristic: Primal heuristics for finding solutions\n- CbcBranchingObject: Branching decisions", "algorithm": "Branch-and-Cut (B&C) for Mixed-Integer Programming:\n  1. Solve LP relaxation at root node\n  2. While open nodes remain:\n     a. Select node from priority queue (best-first or depth-first)\n     b. Solve LP relaxation, apply cuts (Gomory, MIR, clique, etc.)\n     c. If fractional: branch on integer variable, create child nodes\n     d. If integer-feasible: update incumbent if improved\n     e. Prune by bound if LP ≥ incumbent\n  3. Return optimal solution when tree exhausted", "complexity": "Worst-case O(2^n) where n = number of integer variables.\n  Practical performance depends heavily on:\n  - Strength of LP relaxation and cutting planes\n  - Quality of branching variable selection\n  - Effectiveness of primal heuristics\n  - Problem structure (e.g., total unimodularity)", "ref": ["Land, A.H. and Doig, A.G. (1960). \"An automatic method of solving\n  discrete programming problems\". Econometrica 28(3):497-520.\n  [Original branch-and-bound algorithm]", "Padberg, M. and Rinaldi, G. (1991). \"A branch-and-cut algorithm\n  for the resolution of large-scale symmetric traveling salesman problems\".\n  SIAM Review 33(1):60-100. [Branch-and-cut methodology]", "Savelsbergh, M.W.P. (1994). \"Preprocessing and probing techniques\n        for mixed integer programming problems\". ORSA J. Computing 6(4):445-454.\n\n      \\todo It remains to work out the cleanest way of getting a solution to\n            the original problem at the end. So this is very preliminary."], "see": ["CbcNode for search tree node representation", "CbcTree for node selection/storage", "CbcCutGenerator for cutting plane management", "CbcHeuristic for primal heuristics", "OsiSolverInterface for underlying LP solver"], "has_pass2": true}, "src/CbcFullNodeInfo.hpp": {"path": "layer-2/Cbc/src/CbcFullNodeInfo.hpp", "filename": "CbcFullNodeInfo.hpp", "file": "CbcFullNodeInfo.hpp", "brief": "Complete subproblem state storage (typically for root node)", "algorithm": "Full State Storage for Root Node\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFullNodeInfo stores complete information to recreate a subproblem:\n- Full warm start basis\n- Complete variable bounds\n- All active cutting planes\n\nUsed at root node or when incremental storage isn't beneficial.\nChild nodes typically use CbcPartialNodeInfo to save memory.", "see": ["CbcNodeInfo for base class", "CbcPartialNodeInfo for incremental storage", "CbcNode for live node information\n\nEdwin 11/24/09 carved from CbcNode"], "has_pass2": true}, "src/CbcParamUtils.hpp": {"path": "layer-2/Cbc/src/CbcParamUtils.hpp", "filename": "CbcParamUtils.hpp", "file": "CbcParamUtils.hpp", "brief": "Utility functions for parameter handling in cbc-generic\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcParamUtils namespace: Callback functions for parameter processing.\nThese are invoked when parameters are set via the command line.\n\nSolution I/O:\n- saveSolution(): Write solution to file\n- readSolution(): Load solution from file\n\nAction handlers (doXxxParam):\n- doBaCParam(): Execute branch-and-cut solve\n- doImportParam(): Load problem from file\n- doSolutionParam(): Write/print solution\n- doHelpParam(): Display help information\n- doExitParam(): Exit the program\n\nPush functions (pushCbcSolverXxxParam):\n- Transfer parameter values to solver/model\n- Separate handlers for Dbl, Int, Kwd, Str, Bool types\n- Special handlers for Cut and Heur parameters\n\nModel parameter handlers:\n- pushCbcModelDblParam/IntParam(): Set CbcModel parameters\n- setCbcModelDefaults(): Initialize model with default values", "see": ["CbcParam for parameter definitions", "CbcParameters for parameter storage"], "has_pass2": false}, "src/CbcTreeLocal.hpp": {"path": "layer-2/Cbc/src/CbcTreeLocal.hpp", "filename": "CbcTreeLocal.hpp", "file": "CbcTreeLocal.hpp", "brief": "Local branching search tree (Fischetti-Lodi 2002)\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "Local Branching (Fischetti-Lodi):\n  Neighborhood search around incumbent using Hamming distance cuts:\n  1. **Initialize:** Obtain feasible solution x*\n  2. **Local search:** Add cut Σ |x_i - x*_i| ≤ k (k-neighborhood)\n     - For 0-1: Σ_{i:x*=1}(1-x_i) + Σ_{i:x*=0}x_i ≤ k\n  3. **Solve subproblem:** B&C in restricted region (time/node limits)\n  4. **Improvement found:** Update x* = new solution, goto 2\n  5. **No improvement (optimal in neighborhood):**\n     - Reverse cut: Σ |x_i - x*_i| ≥ k+1 (exclude region)\n     - Widen search: increase k (diversification_++)\n  6. **Termination:** maxDiversification reached or global optimum proved", "math": "Local branching cut for binary variables:\n  Let S = {i : x*_i = 1}. The k-neighborhood constraint is:\n  Σ_{i∈S} (1-x_i) + Σ_{i∉S} x_i ≤ k\n  ⟺ |S| - Σ_{i∈S} x_i + Σ_{i∉S} x_i ≤ k\n  This limits Hamming distance from x* to at most k.", "complexity": "Each subproblem bounded by timeLimit_/nodeLimit_.\n  Total: O(maxDiversification_ × subproblem_limit).\n  Effective when incumbent is near-optimal.", "ref": ["Fischetti & Lodi (2003). \"Local Branching\". Mathematical\n     Programming 98:23-47.\n\n**Parameters:**\n- range_: Initial neighborhood radius k\n- typeCuts_: 0 = 0-1 only, 1 = general integer (weaker cuts)\n- maxDiversification_: Max radius increases before giving up\n- timeLimit_/nodeLimit_: Sub-tree resource limits\n- refine_: Whether to prove optimality fixing 0-1 vars"], "see": ["CbcTree for base tree class", "CbcHeuristicLocal for related local search heuristic"], "has_pass2": true}, "src/CbcBranchLotsize.hpp": {"path": "layer-2/Cbc/src/CbcBranchLotsize.hpp", "filename": "CbcBranchLotsize.hpp", "file": "CbcBranchLotsize.hpp", "brief": "Lot-sizing variable with discrete valid values\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcLotsize: Variable restricted to specific discrete values (lot sizes).\nUnlike integers (any value in range), lot-sizing variables can only\ntake values from a predefined set: {v1, v2, ..., vn}.", "algorithm": "Lot-Size Branching:\n  For variable x restricted to values V = {v_1, ..., v_n}:\n  infeasibility(): findRange() locates current value in V.\n  If x̄ ∉ V, compute distance to nearest valid values.\n  createCbcBranch() partitions V:\n  - Down: x ∈ {v_1, ..., v_k} (values ≤ floor)\n  - Up: x ∈ {v_{k+1}, ..., v_n} (values ≥ ceiling)\n  floorCeiling() returns nearest valid values below/above x̄.", "math": "Semi-continuous and lot-size variables:\n  Standard integer: x ∈ {0, 1, 2, ..., u}.\n  Lot-size: x ∈ V where V is arbitrary finite set.\n  MIP formulation: x = Σv_i·y_i, Σy_i = 1, y binary (SOS1).\n  Direct handling avoids |V| binary variables.\n  Range mode: union of intervals, useful for batch sizes.\n\nTwo modes:\n- Points mode (rangeType_=1): Discrete set of valid values\n- Range mode (rangeType_=2): Valid intervals [lo_i, hi_i]\n\nExample: Order quantity must be 0, 100, 250, or 500 units.\n\nBranching creates children that partition the valid value set,\nsimilar to SOS branching but for a single variable.", "see": ["CbcLotsizeBranchingObject for branching action", "CbcSOS for set-based branching on multiple variables"], "has_pass2": true}, "src/CbcHeuristicDivePseudoCost.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDivePseudoCost.hpp", "filename": "CbcHeuristicDivePseudoCost.hpp", "file": "CbcHeuristicDivePseudoCost.hpp", "brief": "Dive heuristic using pseudocost estimates\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDivePseudoCost: Most informed diving strategy.\nUses pseudocosts to estimate objective change from fixing.", "algorithm": "Pseudocost-Guided Variable Selection:\n  selectVariableToBranch() for fractional integer x_j:\n  1. Compute f_j = x̄_j - ⌊x̄_j⌋ (fractional part).\n  2. Down estimate: D_j = f_j × ψ_j^- (from downArray_).\n  3. Up estimate: U_j = (1-f_j) × ψ_j^+ (from upArray_).\n  4. Score_j = product or weighted combination of D_j, U_j.\n  5. Select j* = argmax{Score_j}; round toward smaller estimate.", "math": "Pseudocost scoring for diving:\n  ψ_j^- ≈ (Δobj/Δx̄_j) for down branches on x_j.\n  ψ_j^+ ≈ (Δobj/Δx̄_j) for up branches on x_j.\n  Score typically: (1-f)·ψ^- + f·ψ^+ or product-based.\n  fixOtherVariables() exploits reduced costs: fix if |r̄_j| > gap.\n\ninitializeData() prepares pseudocost arrays (downArray_, upArray_).\nfixOtherVariables() uses reduced costs for additional fixing.\n\nGenerally most effective but requires pseudocost information.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcSimpleIntegerDynamicPseudoCost for pseudocost learning"], "has_pass2": true}, "src/CbcFollowOn.hpp": {"path": "layer-2/Cbc/src/CbcFollowOn.hpp", "filename": "CbcFollowOn.hpp", "file": "CbcFollowOn.hpp", "brief": "Follow-on branching for crew scheduling problems\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFollowOn: Specialized branching for air-crew scheduling and\nsimilar set-partitioning problems. When crew can fly in on flight A\nand out on flight B (or other flights), branch on the connection.", "algorithm": "Follow-On Branching for Set Partitioning:\n  For crew scheduling: constraint rows = flights, columns = pairings.\n  gutsOfFollowOn() finds connection (row_A, row_B) where:\n  - Some pairings cover both flights A and B.\n  - Other pairings cover A but not B (or vice versa).\n  createCbcBranch() creates disjunction:\n  - Down: Fix all pairings with A->B connection to 0.\n  - Up: Fix all pairings WITHOUT A->B connection to 0.\n  Effect: Forces decision on whether crew connects A to B.", "math": "Set partitioning connection branching:\n  Model: min c^T x s.t. Ax = 1, x ∈ {0,1}^n (each row covered exactly once).\n  Connection (i,j): columns k where a_ik = a_jk = 1.\n  Branch separates columns by connection presence.\n  Reduces symmetry in set partitioning structure.\n  High priority: apply before standard variable branching.\n\nBranch disjunction:\n- Down: Fix all pairings using connection A->B to 0\n- Up: Fix all pairings NOT using A->B to 0\n\nAlso includes:\n- CbcFixingBranchingObject: Fix lists of variables to bounds\n- CbcIdiotBranch: Experimental random-sum branching\n\nShould be used as a supplementary branching rule with high priority,\nbefore standard variable branching.", "see": ["CbcFixingBranchingObject for bulk variable fixing", "CbcBranchCut for related cut-based branching\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcObjectUpdateData.hpp": {"path": "layer-2/Cbc/src/CbcObjectUpdateData.hpp", "filename": "CbcObjectUpdateData.hpp", "file": "CbcObjectUpdateData.hpp", "brief": "Data carrier for updating branching objects after branching", "algorithm": "Feedback Carrier for Dynamic Pseudocost Learning\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcObjectUpdateData: Encapsulates information about a completed branch\nto update the originating CbcObject. Used primarily for learning\ndynamic pseudocosts from actual branching outcomes.\n\nCaptured data:\n- change_: Actual objective change from this branch\n- status_: LP outcome (0=optimal, 1=infeasible, 2=unknown)\n- intDecrease_: Reduction in integer infeasibilities\n- way_: Branch direction taken (-1 down, +1 up)\n\nFlow: After solving a child node, CbcNode creates this object\nand calls object_->updateInformation() to feed back the results.", "see": ["CbcSimpleIntegerDynamicPseudoCost for main consumer", "CbcObject::createUpdateInformation() for creation", "CbcObject::updateInformation() for consumption\n\nEdwin 11/12/2009 carved from CbcBranchBase"], "has_pass2": true}, "src/CbcBranchDynamic.hpp": {"path": "layer-2/Cbc/src/CbcBranchDynamic.hpp", "filename": "CbcBranchDynamic.hpp", "file": "CbcBranchDynamic.hpp", "brief": "Dynamic pseudocost-based branching decision\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchDynamicDecision: Selects branches using dynamic pseudocosts.\n- Before first solution: Uses infeasibility counts\n- After first solution: Uses objective change estimates\n\nPseudocosts are updated during search based on observed\nobjective changes from actual branching decisions.", "algorithm": "Dynamic Branching Selection (betterBranch):\n  Phase 1 (before solution): Score = numInfUp × numInfDn (product score).\n  Phase 2 (after solution): Score = (1-λ)·min(Δup,Δdn) + λ·max(Δup,Δdn).\n  Where Δup, Δdn = predicted objective changes from pseudocosts.\n  Select variable maximizing score.", "math": "Pseudocost update:\n  After branching on x_j with Δx = |x_j - x̄_j|:\n  ψ_j^+ ← (n·ψ_j^+ + Δz^+/Δx) / (n+1) for up branch,\n  ψ_j^- ← (n·ψ_j^- + Δz^-/Δx) / (n+1) for down branch.\n  n = number of previous observations.", "see": ["CbcBranchDecision for base class", "CbcSimpleIntegerDynamicPseudoCost for dynamic pseudocost objects", "CbcBranchDefaultDecision for simpler selection"], "has_pass2": true}, "src/CbcCountRowCut.hpp": {"path": "layer-2/Cbc/src/CbcCountRowCut.hpp", "filename": "CbcCountRowCut.hpp", "file": "CbcCountRowCut.hpp", "brief": "Reference-counted row cuts with ownership tracking", "algorithm": "Reference-Counted Cuts with Hash Deduplication\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCountRowCut: OsiRowCut with reference counting for memory management.\nTracks which nodes use the cut and auto-deletes when count reaches zero.\n\nBookkeeping:\n- numberPointingToThis_: Reference count\n- owner_/ownerCut_: Back-pointer to creating node\n- whichCutGenerator_: ID of generator (+10000 if global)\n\nUsage pattern:\n- Increment when child nodes inherit the cut\n- Decrement when cut becomes loose or node is fathomed\n- Delete when count reaches zero\n\nCbcRowCuts: Collection class for cuts with hash-based deduplication.\nPrevents duplicate cuts from being added multiple times.", "see": ["CbcNodeInfo for cut ownership", "CbcCutGenerator for cut generation"], "has_pass2": true}, "src/CbcNode.hpp": {"path": "layer-2/Cbc/src/CbcNode.hpp", "filename": "CbcNode.hpp", "file": "CbcNode.hpp", "brief": "Search tree node for branch-and-cut\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "Branch-and-Bound Node Management:\nEach CbcNode represents a subproblem (LP relaxation with additional bounds)\nin the B&B tree. Key operations:\n\n1. Node Creation: When branching on variable x_j with fractional value f:\n   - Down child: add bound x_j ≤ floor(f)\n   - Up child: add bound x_j ≥ ceil(f)\n\n2. Variable Selection (chooseBranch):\n   - Strong branching: solve LP for each candidate, pick best degradation\n   - Pseudo-costs: estimate from historical branching improvements\n   - Reliability branching: strong branch until pseudo-costs reliable\n\n3. Node Processing:\n   - Reconstruct LP from parent using CbcNodeInfo diffs\n   - Solve LP relaxation\n   - Apply cuts, check integrality, branch or fathom", "math": "Strong branching score: score_j = (1-μ)·min(D⁻_j, D⁺_j) + μ·max(D⁻_j, D⁺_j)\nwhere D⁻_j, D⁺_j are objective degradations for down/up branches, μ ∈ [0,1].\nPseudo-cost: ψ⁻_j = ΔZ⁻/(f_j - floor(f_j)), initialized via strong branching.", "complexity": "Strong branching: O(k·LP) where k candidates evaluated.\nPseudo-costs: O(n) for scoring after initialization. Node reconstruction:\nO(depth) to replay bound changes from root.", "ref": ["Land & Doig, \"An automatic method for solving discrete programming problems\",\n     Econometrica 28 (1960) 497-520 (original B&B)", "Achterberg, Koch & Martin, \"Branching rules revisited\",\n     Operations Research Letters 33 (2005) 42-54 (reliability branching)\n\nCbcNode represents a live subproblem in the B&C search tree.\nContains information needed while the node is active:\n- Depth in tree, objective value, branching state\n- Links to CbcNodeInfo for reconstruction\n\nLifecycle: Created when branching, deleted when all branches evaluated.\nExtends CoinTreeNode for heap-based storage in CbcTree.\n\nKey methods:\n- createInfo(): Create CbcNodeInfo for subproblem storage\n- branch(): Apply branching and create child subproblem\n- chooseBranch(): Select branching variable/object"], "see": ["CbcNodeInfo for persistent subproblem information", "CbcTree for node storage and selection", "CbcBranchingObject for branching decisions"], "has_pass2": true}, "src/CbcHeuristicFPump.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicFPump.hpp", "filename": "CbcHeuristicFPump.hpp", "file": "CbcHeuristicFPump.hpp", "brief": "Feasibility Pump heuristic (Fischetti, Glover & Lodi)\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "Feasibility Pump (FP):\nAlternates between LP relaxation and rounding to find feasible MIP solution.\n\nBasic Algorithm:\n1. x* ← solve LP relaxation\n2. x̄ ← round(x*) (integer point)\n3. While x* ≠ x̄ and iterations < max:\n   a. x* ← argmin ||x - x̄|| s.t. Ax = b, l ≤ x ≤ u (LP toward x̄)\n   b. x̄ ← round(x*)\n4. If x* = x̄, return feasible solution\n\nEnhancements:\n- Objective blending: min α·c^Tx + (1-α)·||x-x̄|| trades feasibility vs quality\n- Cycle detection: Track visited roundings, perturb if cycling\n- Randomization: Random flips to escape local minima\n- General integers: Auxiliary binary formulation or specialized rounding", "math": "Distance LP: min Σ_j |x_j - x̄_j| for integer j\nLinearized as: min Σ_j d_j s.t. d_j ≥ x_j - x̄_j, d_j ≥ x̄_j - x_j\nOr squared Euclidean: uses modified objective coefficients.", "complexity": "Each iteration: one LP solve, O(n_int) rounding.\nTypically 10-100 iterations. Very effective for finding first solution.\nSuccess rate ~80% on MIPLIB instances.", "ref": ["Fischetti, Glover & Lodi, \"The feasibility pump\",\n     Mathematical Programming 104 (2005) 91-104", "Achterberg & Berthold, \"Improving the feasibility pump\",\n     Discrete Optimization 4 (2007) 77-86\n\nCbcHeuristicFPump: Implements the Feasibility Pump algorithm\n(Fischetti, Glover & Lodi, 2005) for finding initial MIP solutions.\n\nAlgorithm overview:\n1. Solve LP relaxation, get fractional x*\n2. Round x* to nearest integers -> x_bar\n3. Solve LP with objective: minimize distance to x_bar\n4. If new x* = x_bar, done (integer feasible)\n5. Otherwise round and repeat\n\nKey parameters:\n- maximumPasses_: Max iterations before giving up\n- initialWeight_: Blend original objective with distance\n- defaultRounding_: Threshold for rounding (default 0.5)\n\nHandles cycling through randomization and perturbation.\n\nAlso includes CbcDisasterHandler for Clp solver recovery."], "see": ["CbcHeuristic for base class", "CbcHeuristicDive for related diving approaches"], "has_pass2": true}, "src/CbcBranchToFixLots.hpp": {"path": "layer-2/Cbc/src/CbcBranchToFixLots.hpp", "filename": "CbcBranchToFixLots.hpp", "file": "CbcBranchToFixLots.hpp", "brief": "Branch to fix many variables simultaneously\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchToFixLots: Heuristic branching that fixes multiple variables\nin one branch, cutting off the current solution in the other. Useful\nfor reducing problem size when reduced costs indicate fixable variables.", "algorithm": "Reduced Cost Fixing Branch:\n  shallWe() decides whether to activate:\n  1. Count variables at bound with |dj| ≥ djTolerance_.\n  2. Check if count ≥ fractionFixed_ × total_integers.\n  3. Also check numberClean_ equality constraints satisfied.\n  createCbcBranch() creates asymmetric disjunction:\n  - Fix branch: Set all qualifying variables to bounds permanently.\n  - Cut branch: Add cut excluding current fractional solution.\n  Effect: One arm reduces problem size; other explores alternatives.", "math": "Reduced cost fixing in branch-and-bound:\n  If x_j at lower bound and r̄_j ≥ (incumbent - LP_bound):\n    x_j = lb_j in any improving solution.\n  Mass fixing: Apply to all such variables simultaneously.\n  Trade-off: Aggressive fixing may cut off optimal, but reduces problem.\n  depth_ controls frequency (every depth_ levels).\n\nTriggering conditions:\n- Variables at bounds with large reduced costs (dj >= djTolerance_)\n- Sufficient fraction of variables fixable (fractionFixed_)\n- Clean satisfaction of equality/packing constraints (numberClean_)\n\nOne branch arm fixes variables based on reduced cost directions;\nthe other adds a cut excluding the current fractional solution.\n\nControlled by depth_ to avoid excessive use at every node.", "see": ["CbcBranchCut for base class", "CbcBranchAllDifferent for another cut-based branching\n\nEdwin 11/13/2009 carved out of CbcBranchCut"], "has_pass2": true}, "src/CbcBranchAllDifferent.hpp": {"path": "layer-2/Cbc/src/CbcBranchAllDifferent.hpp", "filename": "CbcBranchAllDifferent.hpp", "file": "CbcBranchAllDifferent.hpp", "brief": "All-different constraint for integer variables\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchAllDifferent: Enforces that a set of integer variables\nmust all have different values. When two variables i,j have the\nsame value, creates branching disjunction:\n  x_i <= x_j - 1  OR  x_i >= x_j + 1", "algorithm": "All-Different Branching:\n  infeasibility() detects violations:\n  1. For each pair (i,j) in set: check if x̄_i ≈ x̄_j (same value).\n  2. If collision found, return infeasibility > 0.\n  createCbcBranch() creates disjunction:\n  - Branch 1: x_i ≤ x_j - 1 (i strictly less than j).\n  - Branch 2: x_i ≥ x_j + 1 (i strictly greater than j).\n  Iterates until all pairs have distinct values.", "math": "All-different constraint in MIP:\n  CP: alldifferent(x_1, ..., x_n) enforced by arc consistency.\n  MIP: No direct encoding; branch on violations.\n  O(n²) pair comparisons per infeasibility check.\n  Less efficient than CP propagation but integrates with MIP solver.\n\nCommon in constraint programming problems:\n- Scheduling: No two tasks at same time\n- Assignment: No two agents assigned same job\n- Sudoku: No repeated digits in row/column/box\n\nNot as efficient as specialized CP propagation but integrates\nwith MIP framework for hybrid approaches.", "see": ["CbcBranchCut for base class", "CbcClique for related set constraints\n\nEdwin 11/13/2009 carved out of CbcBranchCut"], "has_pass2": true}, "src/CbcHeuristicRINS.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicRINS.hpp", "filename": "CbcHeuristicRINS.hpp", "file": "CbcHeuristicRINS.hpp", "brief": "RINS - Relaxation Induced Neighborhood Search (Danna et al.)\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicRINS: Implements RINS (Danna, Rothberg & Le Pape, 2005).\nUses LP relaxation to define a neighborhood around the incumbent solution.", "algorithm": "RINS - Relaxation Induced Neighborhood Search:\n  solution() at node with incumbent x* and LP solution x̄:\n  1. For each integer variable j: If x*_j = x̄_j, fix x_j = x*_j.\n  2. Solve restricted MIP on free variables (sub-MIP).\n  3. If sub-MIP solution z' < z*: Accept as new incumbent.\n  4. Otherwise: Continue B&C with original incumbent.\n  Triggered periodically (howOften_) or at solution events.", "math": "Neighborhood definition:\n  N_RINS(x*) = {x : x_j = x*_j ∀j where x*_j = round(x̄_j)}.\n  Fixes variables where LP agrees with incumbent.\n  Neighborhood size depends on LP-incumbent agreement.\n\nTracks which variables have appeared in solutions (used_ array)\nto focus search on promising variables.\n\nAlso includes (via headers) related neighborhood searches:\n- RENS: Relaxation Enforced Neighborhood Search\n- DINS: Distance-Induced Neighborhood Search\n- VND: Variable Neighborhood Descent", "see": ["CbcHeuristic for base class", "CbcHeuristicRENS, CbcHeuristicDINS, CbcHeuristicVND", "CbcHeuristicLocal for pure local search"], "has_pass2": true}, "src/CbcCompareEstimate.hpp": {"path": "layer-2/Cbc/src/CbcCompareEstimate.hpp", "filename": "CbcCompareEstimate.hpp", "file": "CbcCompareEstimate.hpp", "brief": "Estimate-based node selection strategy\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareEstimate: Node selection using solution estimates.\nUsed during rounding phases where estimated objective matters.", "algorithm": "Estimate-Based Node Selection:\n  test(x, y) comparison for heap ordering:\n  Returns true if estimate(y) < estimate(x).\n  estimate = guessedObjectiveValue() from pseudocost extrapolation.\n  Effect: Explores nodes most likely to yield good solutions first.\n  Used during diving/rounding phases when LP bound is less informative.", "math": "Pseudocost-based estimation:\n  estimate(node) = LP_bound + Σ ψ_j × distance_to_integer(x_j).\n  Approximates objective of integer-feasible descendant.\n  More predictive than LP bound when many variables fractional.\n  Particularly useful early in search before strong bounds available.\n\ntest(x,y) compares node estimates (guessedObjectiveValue).\nEstimates typically come from pseudocost extrapolation or\nother predictive methods.\n\nUseful when LP relaxation bound is weak but estimates\nfrom branching information are more informative.", "see": ["CbcCompareObjective for LP bound comparison", "CbcCompareDefault for adaptive strategy", "CbcNode::guessedObjectiveValue() for estimate source\n\nEdwin 11/25/09 carved out of CbcCompareActual"], "has_pass2": true}, "src/CbcFixVariable.hpp": {"path": "layer-2/Cbc/src/CbcFixVariable.hpp", "filename": "CbcFixVariable.hpp", "file": "CbcFixVariable.hpp", "brief": "Fix variable bounds as branching consequence", "algorithm": "State-Based Variable Fixing from Branch Decisions\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFixVariable: Concrete CbcConsequence that fixes variable bounds\nbased on branching decisions. Implements state-dependent bound changes\nthat go beyond LP propagation.\n\nState encoding:\n- -9999: Variable at lower bound\n- +9999: Variable at upper bound\n- Other: Variable fixed to that specific value\n\nFor each state, stores arrays of:\n- Variables to modify (variable_)\n- New bounds to apply (newBound_)\n- Ranges for lower vs upper bound changes (startLower_, startUpper_)\n\nExample: Binary y controls continuous x. When y=0 (state -9999),\nfix x to 0; when y=1 (state +9999), x can be in [0, 100].", "see": ["CbcConsequence for base class", "CbcObject for objects with consequences\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcHeuristicDW.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDW.hpp", "filename": "CbcHeuristicDW.hpp", "file": "CbcHeuristicDW.hpp", "brief": "Dantzig-Wolfe decomposition based heuristic\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDW: Advanced heuristic exploiting block structure.\nVery compute-intensive - detects and exploits Dantzig-Wolfe\ndecomposable structure in the constraint matrix.", "algorithm": "Dantzig-Wolfe Decomposition Heuristic:\n  solution() exploits block-angular structure:\n  1. findStructure(): Detect blocks via matrix analysis.\n  2. setupDWStructures(): Create master + subproblem structure.\n  3. For each block k: Solve MIP subproblem for proposals.\n  4. addDW(): Add block solutions as columns to master.\n  5. Solve master (convexity + linking constraints).\n  6. If fractional, apply heuristics; iterate via callback.\n  fingerPrint_ tracks integer patterns for diversity.", "math": "Dantzig-Wolfe reformulation:\n  Original: min c^T x, Ax ≤ b, D_k x_k ≤ d_k (block constraints).\n  Master: min Σλ_j (c^T x^j), Σλ_j (Ax^j) ≤ b, Σλ_j = 1 per block.\n  x^j are extreme points of block polyhedra.\n  Subproblems: min (c - π^T A) x_k s.t. D_k x_k ≤ d_k.\n  Column generation adds proposals dynamically.\n  affinity_ guides block solution combination.\n\nAlgorithm:\n1. findStructure() detects block-angular structure\n2. setupDWStructures() prepares decomposition data\n3. Solves subproblems by block (numberBlocks_)\n4. Combines block solutions via master problem\n5. Callback mechanism for customization (functionPointer_)\n\nKey data structures:\n- whichRowBlock_/whichColumnBlock_: Block membership\n- fingerPrint_: Bitmask for integer patterns per block\n- affinity_: Block similarity for combination\n- dwSolver_: DW master problem solver\n\nParameters controllable via setters:\n- nNeeded_/nNodes_: Search intensity\n- numberPasses_/numberBadPasses_: Iteration limits", "see": ["CbcHeuristic for base class", "CbcHeuristicRINS for simpler neighborhood search"], "has_pass2": true}, "src/CbcHeuristicDive.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDive.hpp", "filename": "CbcHeuristicDive.hpp", "file": "CbcHeuristicDive.hpp", "brief": "Abstract base for diving heuristics\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDive: Abstract base for diving heuristics that explore\nthe tree greedily by fixing variables and re-solving LPs.", "algorithm": "Diving Heuristic Framework:\n  solution() greedy feasibility search:\n  1. Solve LP relaxation at current node.\n  2. While fractional variables exist:\n     a. selectVariableToBranch() → bestColumn, bestRound.\n     b. Fix x[bestColumn] in direction bestRound.\n     c. Re-solve LP; if infeasible, backtrack or fail.\n  3. If integer feasible: Return solution.\n  4. Apply reduced cost fixing during dive for speedup.", "math": "Lock counting for variable selection:\n  downLocks_[j] = #{constraints where x_j has positive coefficient}.\n  upLocks_[j] = #{constraints where x_j has negative coefficient}.\n  Variables with fewer locks are easier to round feasibly.\n\nSubclasses implement selectVariableToBranch() with different strategies:\n- Fractional: Most fractional variable\n- PseudoCost: Best pseudocost estimate\n- VectorLength: Based on constraint participation\n- Coefficient: Based on objective coefficients\n- Guided: Guided by incumbent solution\n- LineSearch: Along line to LP optimum\n\nUses lock counting for feasibility analysis.", "see": ["CbcHeuristicDiveFractional, CbcHeuristicDivePseudoCost, etc.", "CbcHeuristic for base class", "CbcHeuristicFPump for related approach"], "has_pass2": true}, "src/CbcClique.hpp": {"path": "layer-2/Cbc/src/CbcClique.hpp", "filename": "CbcClique.hpp", "file": "CbcClique.hpp", "brief": "Clique branching for binary variable sets\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcClique: Branching for cliques (sets of binary variables with\nat most one at its \"strong\" value). Generalizes binary SOS1.\n\nStandard form: x1 + x2 + ... + xn <= 1 (all strong at 1)\nGeneral form allows negated variables (y_j = 1 - x_j).\n\nMember types (type_[i]):\n- 1: SOS-style, coefficient +1, strong value is 1\n- 0: Non-SOS, coefficient -1, strong value is 0", "algorithm": "Clique Branching:\n  createCbcBranch() for clique {x_1,...,x_n} with Σx_j ≤ 1:\n  1. Find most fractional variable x_k.\n  2. Down branch: Fix x_k to weak value (0 if type=1, 1 if type=0).\n  3. Up branch: Fix x_k to strong value, all others to weak.\n  Uses bitmask encoding: downMask_, upMask_ indicate fixings.", "math": "Clique constraint forms:\n  Standard: x_1 + x_2 + ... + x_n ≤ 1 (at most one = 1).\n  Negated: -y_1 - y_2 + x_3 ≤ -1 where y_j = 1-x_j.\n  RHS implicit = 1 - numberNonSOSMembers_.\n\nBranching: Pick a fractional variable and partition the set.\nUses bitmasks for efficient subset representation (up to 64 members\nwith CbcCliqueBranchingObject, unlimited with CbcLongCliqueBranchingObject).", "see": ["CbcCliqueBranchingObject for sets <= 64 members", "CbcLongCliqueBranchingObject for larger sets", "CbcSOS for ordered set branching\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcHeuristicRENS.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicRENS.hpp", "filename": "CbcHeuristicRENS.hpp", "file": "CbcHeuristicRENS.hpp", "brief": "RENS - Relaxation Enforced Neighborhood Search\nCopyright (C) 2006, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicRENS: Fixes variables based on LP relaxation solution.\nUnlike RINS (which needs an incumbent), RENS works from LP alone.", "algorithm": "RENS - Relaxation Enforced Neighborhood Search:\n  solution() from LP solution x̄ (no incumbent required):\n  1. For integer variable j: If x̄_j at bound, fix x_j = round(x̄_j).\n  2. Fixing strategy controlled by rensType_ (bounds, dj, priorities).\n  3. Solve restricted MIP on free variables.\n  4. Return solution if found, else continue B&C.\n  No incumbent needed - works early in search tree.", "math": "Reduced cost fixing in RENS:\n  rensType_=1: Fix x_j if d̄_j > threshold (reduced cost high).\n  rensType_=3: Fix if d̄_j > 0.01 × avg(d̄).\n  Variables with high reduced cost unlikely to change in optimal.\n\nrensType_ controls fixing strategy:\n- 0: Fix at lower bound only\n- 1: Fix based on reduced costs\n- 2: Fix at upper bound as well\n- 3: Fix based on 0.01*average reduced cost\n- +16: Allow two tries\n- +32: Use existing solution to keep more variables\n- +64: Honor high priority variables\n- +128: Honor low priority variables", "see": ["CbcHeuristicRINS for incumbent-based neighborhood", "CbcHeuristic for base class\n\nEdwin 12/5/09 carved out of CbcHeuristicRINS"], "has_pass2": true}, "src/CbcTree.hpp": {"path": "layer-2/Cbc/src/CbcTree.hpp", "filename": "CbcTree.hpp", "file": "CbcTree.hpp", "brief": "Heap-based storage for live search tree nodes\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "Search Tree Management with Priority Heap:\nMaintains live (unexplored) nodes in a binary heap for efficient selection.\n\nNode Selection Strategies (via CbcCompare):\n- Best-first: Select node with best (lowest) LP bound → optimal first solution\n- Depth-first: Select deepest node → memory efficient, finds solutions fast\n- Best-estimate: Use pseudo-cost estimate of integer solution value\n- Hybrid: Switch strategies based on incumbent, gap, or tree size\n\nTree Operations:\n- push(node): Add to heap, O(log n) sift-up\n- pop(): Remove best, O(log n) sift-down\n- cleanTree(cutoff): Prune nodes with bound ≥ cutoff (fathoming)\n- setComparison(): Change strategy, O(n) re-heapify", "math": "Node bound vs incumbent: If LB(node) ≥ UB (best incumbent), fathom node.\nBest-first guarantees: first integer solution found is optimal.\nDepth-first memory: O(depth × branching_factor) vs O(2^depth) for best-first.", "complexity": "push/pop: O(log n). cleanTree: O(n). Space: O(live_nodes).\nTypical MIP: live nodes peaks at thousands to millions depending on strategy.", "ref": ["Linderoth & Savelsbergh, \"A computational study of search strategies\",\n     INFORMS Journal on Computing 11 (1999) 173-187", "Achterberg, \"Constraint Integer Programming\", PhD thesis, TU Berlin (2007)\n\nCbcTree manages the set of live (unexplored) nodes as a priority heap.\nThe comparison function (CbcCompare) determines node selection strategy:\n- Best-first: Select node with best bound\n- Depth-first: Select deepest node\n- Best-estimate: Use pseudocost estimates\n\nKey methods:\n- push(): Add node to heap\n- top()/pop(): Get and remove best node\n- setComparison(): Change selection strategy (re-heapifies)\n- cleanTree(): Remove nodes worse than cutoff"], "see": ["CbcCompareBase for comparison function interface", "CbcNode for node representation", "CbcTreeLocal for local search tree variant"], "has_pass2": true}, "src/CbcParameters.hpp": {"path": "layer-2/Cbc/src/CbcParameters.hpp", "filename": "CbcParameters.hpp", "file": "CbcParameters.hpp", "brief": "Central parameter collection for Cbc algorithm control\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcParameters: Master class holding all Cbc solver parameters.\nUsed by cbc-generic to configure and control the MIP solver.\n\nMajor subsystems configured:\n- Cut generators: Gomory, MIR, Probing, Clique, FlowCover, etc.\n- Heuristics: FPump, RINS, RENS, DINS, Diving variants, VND, etc.\n- Search strategy: Node selection, preprocessing, orbital branching\n- Limits: Time, nodes, gap tolerance, solution count\n\nIncludes instances of CGL cut generators and Cbc heuristics.\nLinks to ClpParameters for underlying LP solver control.\n\nKey methods:\n- init(): Initialize with strategy preset\n- setModel(): Associate with CbcModel\n- Various getters/setters for each parameter category\n\nStrategy presets provide common configurations:\n- Default aggressive cutting at root\n- Balanced cut/heuristic emphasis\n- Heuristic-focused for quick solutions", "see": ["CbcParam for individual parameter definitions", "CbcParamUtils for parameter push/pull callbacks", "CbcModel for solver integration"], "algorithm": "Preprocessing Pipeline (IPPMode):\n    Controls integer preprocessing before B&C:\n    - Probing: fix variables by logical implication\n    - Clique detection: identify mutual exclusion constraints\n    - Coefficient strengthening: tighten constraint coefficients\n    - SOS extraction: identify special ordered sets from structure", "ref": ["Margot, F. (2002). \"Pruning by isomorphism in branch-and-cut\".\n    Math. Programming 94:71-90. (Orbital branching foundations)"], "has_pass2": true}, "src/CbcThread.hpp": {"path": "layer-2/Cbc/src/CbcThread.hpp", "filename": "CbcThread.hpp", "file": "CbcThread.hpp", "brief": "Multi-threaded parallel B&C support", "algorithm": "Parallel B&C with Deterministic and Opportunistic Modes\nCopyright (C) 2009, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nParallel execution modes:\n- Opportunistic: Threads process nodes as available (nondeterministic)\n- Deterministic: Controlled node batching for reproducibility\n- Parallel cuts: Multiple cut generators run concurrently\n\nCbcSpecificThread: Low-level thread API wrapper (currently pthreads).\n- Mutex locking for thread synchronization\n- Condition variables for thread signaling\n- Thread lifecycle management\n\nCbcThread: Per-thread state and statistics.\n- Node being processed (node_) and created child (createdNode_)\n- Timing: timeInThread_, timeWaitingToStart_, timeLocked_\n- Return codes: -1 available, 0 busy, 1 finished\n\nCbcBaseModel: Coordinates multiple worker threads.\n- Manages thread pool for tree exploration\n- Handles deterministic vs opportunistic parallelism\n- Collects statistics across threads\n\nIf CBC_THREAD not defined, provides dummy stub classes.", "see": ["CbcModel for single-threaded B&C", "CbcTree for node management"], "has_pass2": true}, "src/CbcSolver.hpp": {"path": "layer-2/Cbc/src/CbcSolver.hpp", "filename": "CbcSolver.hpp", "file": "CbcSolver.hpp", "brief": "Top-level driver class for standalone CBC solver", "algorithm": "Command-Line Driver with Scripting Support\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcSolver wraps CbcModel with command-line parsing and scripting:\n- solve(): Execute command sequence (argc/argv interface)\n- fillParameters(): Centralized parameter initialization\n- Supports CbcUser callbacks for extensibility\n- Supports CbcStopNow for termination control\n\nNote: This class is designed to wrap CbcMain0/CbcMain1 functionality.\nFor direct API usage, prefer CbcModel directly.", "see": ["CbcModel for the core B&C solver", "CbcParameters for parameter definitions", "Cbc_C_Interface.h for C language bindings"], "has_pass2": true}, "src/CbcSimpleInteger.hpp": {"path": "layer-2/Cbc/src/CbcSimpleInteger.hpp", "filename": "CbcSimpleInteger.hpp", "file": "CbcSimpleInteger.hpp", "brief": "Integer variable branching object", "algorithm": "Floor/Ceiling Integer Branching\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcIntegerBranchingObject: Standard floor/ceiling branch on integer x:\n- Down arm: lb <= x <= floor(x*)\n- Up arm: ceil(x*) <= x <= ub\n\nCbcSimpleInteger: CbcObject for integer variables.\nCreates CbcIntegerBranchingObject instances.\n\nAlso supports one-way branching for fixing variables.", "see": ["CbcObject for base class", "CbcSimpleIntegerPseudoCost for pseudocost variant", "CbcSimpleIntegerDynamicPseudoCost for dynamic pseudocosts", "CbcBranchingObject for branching action base\n\nEdwin 11/9/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcStrategy.hpp": {"path": "layer-2/Cbc/src/CbcStrategy.hpp", "filename": "CbcStrategy.hpp", "file": "CbcStrategy.hpp", "brief": "Strategy pattern for configuring CbcModel components", "algorithm": "Strategy Pattern for B&C Configuration\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcStrategy provides a strategy pattern for bundling solver configuration.\nSubclasses define complete solving strategies by implementing:\n- setupCutGenerators(): Configure cutting plane generators\n- setupHeuristics(): Configure primal heuristics\n- setupPrinting(): Configure output/logging\n- setupOther(): Strong branching, preprocessing, etc.\n\nSupports nested models (depth_) and preprocessing state tracking.", "see": ["CbcStrategyDefault for default strategy", "CbcModel::setStrategy() to apply a strategy", "CglPreProcess for preprocessing integration"], "has_pass2": true}, "src/CbcBranchCut.hpp": {"path": "layer-2/Cbc/src/CbcBranchCut.hpp", "filename": "CbcBranchCut.hpp", "file": "CbcBranchCut.hpp", "brief": "Branching by adding cuts (split disjunctions)\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchCut and CbcCutBranchingObject: Branch by adding cuts\nrather than tightening variable bounds. Implements split disjunctions\nwhere each branch arm adds a different cut to the LP.", "algorithm": "Cut-Based Branching (Disjunctive Programming):\n  Instead of x ≤ k vs x ≥ k+1 (bound changes):\n  createCbcBranch() creates cut disjunction:\n  - Down branch: Add cut down_ to LP.\n  - Up branch: Add cut up_ to LP.\n  CbcCutBranchingObject::branch() adds appropriate OsiRowCut.\n  Cuts remain in LP for all descendants of that branch.\n  canFix_ indicates if one branch can fix variables.", "math": "Split disjunctions and Gomory cuts:\n  Split cut: π^T x ≤ ⌊π_0⌋ OR π^T x ≥ ⌈π_0⌉ for integer π.\n  Derived from tableau row with fractional RHS.\n  Branching on cuts generalizes variable branching.\n  Each Gomory cut implicitly branches on slack integrality.\n  Enables disjunctive programming within B&C framework.\n\nUse cases:\n- Gomory cuts can be viewed as branching on row slack integrality\n- Split cuts: ax <= b OR ax >= b+1 for integer a, fractional b\n- General disjunctive programming branches\n\nCbcCutBranchingObject stores two OsiRowCut objects:\n- down_: Cut added when way_ = -1\n- up_: Cut added when way_ = 1\n\nBase class for CbcBranchToFixLots and CbcBranchAllDifferent.", "see": ["CbcBranchToFixLots for reduced-cost-based fixing", "CbcBranchAllDifferent for all-different constraint", "OsiRowCut for cut representation"], "has_pass2": true}, "src/CbcParam.hpp": {"path": "layer-2/Cbc/src/CbcParam.hpp", "filename": "CbcParam.hpp", "file": "CbcParam.hpp", "brief": "Individual parameter definitions for cbc-generic\nCopyright (C) 2007, Lou Hafer, IBM Corporation and others.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcParam: Extends CoinParam with Cbc-specific parameter codes.\nUsed by the standalone cbc-generic solver for command-line processing.\n\nParameter categories (CbcParamCode enum):\n- Help: GENERALQUERY, HELP\n- Action: BAB, IMPORT, EXPORT, PRINTSOL, READMIPSTART\n- File: IMPORTFILE, EXPORTFILE, SOLUTIONFILE, MIPSTARTFILE\n- Cut: GOMORYCUTS, MIRCUTS, PROBINGCUTS, KNAPSACKCUTS, etc.\n- Heuristic: FPUMP, RINS, RENS, DIVINGC, GREEDY, etc.\n- Boolean: CPX, DOHEURISTIC, SOS, USESOLUTION\n- Keyword: STRATEGY, PREPROCESS, NODESTRATEGY, ORBITAL\n- Integer: LOGLEVEL, THREADS, CUTDEPTH, MAXNODES, etc.\n- Double: CUTOFF, ALLOWABLEGAP, TIMELIMIT, INCREMENT, etc.\n\nEach parameter links to:\n- parameters_: Parent CbcParameters collection\n- model_: Associated CbcModel (for model parameters)", "see": ["CoinParam for base parameter class", "CbcParameters for parameter collection", "CbcParamUtils for push/pull functions"], "has_pass2": false}, "src/CbcCompareBase.hpp": {"path": "layer-2/Cbc/src/CbcCompareBase.hpp", "filename": "CbcCompareBase.hpp", "file": "CbcCompareBase.hpp", "brief": "Abstract base class for node comparison/selection", "algorithm": "Search Tree Node Priority Strategy Interface\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareBase: Defines interface for search tree node prioritization.\nThe node list is stored as a heap where test(x,y) returns true if\nnode y is \"better\" (higher priority) than node x.\n\nKey methods:\n- test(x,y): Core comparison - returns true if y preferred over x\n- newSolution(): Called when incumbent found, can change strategy\n- every1000Nodes(): Periodic callback for strategy adjustment\n- equalityTest(): Tiebreaker using node numbers\n\nNode selection strategies critically affect B&C performance:\n- Depth-first: Low memory, finds feasible solutions quickly\n- Best-first: Optimal objective bound, may use much memory\n- Hybrid: Combine based on search phase", "see": ["CbcCompareDepth for depth-first search", "CbcCompareObjective for best-bound search", "CbcCompareDefault for adaptive hybrid strategy", "CbcTree for tree management"], "has_pass2": true}, "src/CbcSolverExpandKnapsack.hpp": {"path": "layer-2/Cbc/src/CbcSolverExpandKnapsack.hpp", "filename": "CbcSolverExpandKnapsack.hpp", "file": "CbcSolverExpandKnapsack.hpp", "brief": "Knapsack constraint expansion for tighter formulations", "algorithm": "Knapsack Enumeration Reformulation\nCopyright (C) 2007, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nexpandKnapsack(): Transforms x*y products (both integer) into knapsack form.\n\nCreates tighter formulation by:\n- Enumerating possible x*y values\n- Creating auxiliary binary variables for each value\n- Adding SOS constraints for mutual exclusion\n\nOutputs:\n- whichColumn: Mapping from new to original columns\n- knapsackStart/knapsackRow: Structure of expanded knapsacks\n- stored: Generated cuts from expansion\n- tightenedModel: Reformulated CoinModel\n\nafterKnapsack(): Recovers original solution from expanded form.\nMaps knapsackSolution back to original variable values.", "see": ["CoinModel for model representation", "CglStored for storing generated cuts"], "has_pass2": true}, "src/CbcCompareObjective.hpp": {"path": "layer-2/Cbc/src/CbcCompareObjective.hpp", "filename": "CbcCompareObjective.hpp", "file": "CbcCompareObjective.hpp", "brief": "Best-bound (objective-based) node selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcCompareObjective: Implements best-first search.\nAlways explores node with best (lowest for min) LP bound.\n\ntest(x,y) returns true if y has smaller objective than x.\nPrioritizes most promising nodes for optimality proof.", "algorithm": "Best-First Search (BFS) Node Selection:\n  test(x, y) comparison for heap ordering:\n  Returns true if objective(y) < objective(x).\n  Effect: Node with smallest LP bound processed first.\n  Global bound: Always equals objective of next node to process.\n  Optimal for proving optimality (fewest nodes theoretically).", "math": "Optimality proof efficiency:\n  Best-first guarantees: global_bound = min{LP(node) : node open}.\n  Gap = (incumbent - global_bound) / |incumbent|.\n  Minimizes gap at each step → optimal for gap closure.\n  Memory: May store O(2^d) nodes; gap improves slowly until near proof.\n\nAdvantages:\n- Optimal search order for proving optimality\n- Best global bound progression\n- Minimizes total nodes for proof (in theory)\n\nDisadvantages:\n- Can use exponential memory\n- May take long to find first feasible solution\n- Poor anytime behavior", "see": ["CbcCompareDepth for depth-first alternative", "CbcCompareDefault for practical hybrid\n\nEdwin 11/25/09 carved out of CbcCompareActual"], "has_pass2": true}, "src/CbcFathomDynamicProgramming.hpp": {"path": "layer-2/Cbc/src/CbcFathomDynamicProgramming.hpp", "filename": "CbcFathomDynamicProgramming.hpp", "file": "CbcFathomDynamicProgramming.hpp", "brief": "Dynamic programming fathoming for special problem structures\nCopyright (C) 2004, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFathomDynamicProgramming: Exact enumeration via DP for small subproblems.\nAfter branching reduces the problem sufficiently, DP may be faster than B&C.\n\nRequirements for DP applicability:\n- Small integral RHS values\n- All-integer problem with positive integral coefficients\n- Currently: set partitioning (not general covering)", "algorithm": "Dynamic Programming Fathoming:\n  fathom() for set partitioning with small integral data:\n  1. Encode constraint satisfaction as bit pattern (state).\n  2. For each column j: tryColumn() updates reachable states.\n  3. cost_[state] = minimum cost to reach state.\n  4. back_[state] = column that achieved minimum.\n  5. Return solution achieving cost_[target_].\n  Exact enumeration when state space tractable.", "math": "State space size:\n  Each constraint with RHS=1 contributes 1 bit → 2× states.\n  RHS=b contributes ⌈log₂(b+1)⌉ bits → (b+1)× states.\n  Total: Π(RHS_i + 1) states, bounded by maximumSizeAllowed_.\n  Type 0 (all RHS=1): 2^m states for m constraints.\n\nState space considerations:\n- Each RHS=1 doubles state space\n- RHS=2,3 quadruples; RHS=4-7 multiplies by 8\n- maximumSizeAllowed_ controls maximum tractable size\n\nAlgorithm types:\n- Type 0: All coefficients and RHS are 1\n- Type 1: General case with coefficients > 1 or RHS > 1\n\nUses bit patterns to encode constraint satisfaction states.\nFinds optimal solution by enumeration over all feasible states.", "see": ["CbcFathom for base class", "CbcModel::fathom for integration point"], "has_pass2": true}, "src/CbcHeuristicDiveLineSearch.hpp": {"path": "layer-2/Cbc/src/CbcHeuristicDiveLineSearch.hpp", "filename": "CbcHeuristicDiveLineSearch.hpp", "file": "CbcHeuristicDiveLineSearch.hpp", "brief": "Dive heuristic along line to LP optimum\nCopyright (C) 2008, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcHeuristicDiveLineSearch: Geometric diving approach.\nSelects variables along the line from current point to LP optimum.", "algorithm": "Line Search Diving:\n  Geometric diving that follows ray toward LP optimum:\n  1. Compute direction: d = x_LP - x_current (vector to LP optimum)\n  2. For each fractional variable x_j:\n     - Calculate how rounding affects position along d\n     - Score: progress toward LP optimum vs integer violation\n  3. Select x_j* that maximizes progress when rounded\n  4. Fix x_j* to nearest integer, resolve LP\n  5. Repeat until integer feasible or stuck", "math": "Selection criterion:\n  Let f_j = fraction of x_j, d_j = direction component\n  Round down: progress = f_j · |d_j| toward LP optimum\n  Round up: progress = (1-f_j) · |d_j| toward LP optimum\n  Select: argmax_j {progress_j / violation_j}", "complexity": "O(n) per variable selection\n  Total: O(k · LP) where k = diving depth\n  Typically faster than branching for finding feasible solutions\n\nselectVariableToBranch() considers how rounding affects\nmovement toward the LP optimal solution. Picks variables\nwhere rounding makes most progress along this direction.\n\nCombines geometric intuition with integer rounding.", "see": ["CbcHeuristicDive for base class and diving algorithm", "CbcHeuristicFPump for related LP-based approach"], "has_pass2": true}, "src/CbcBranchDefaultDecision.hpp": {"path": "layer-2/Cbc/src/CbcBranchDefaultDecision.hpp", "filename": "CbcBranchDefaultDecision.hpp", "file": "CbcBranchDefaultDecision.hpp", "brief": "Default branching variable selection\nCopyright (C) 2002, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcBranchDefaultDecision: Simple selection algorithm without pseudocosts.", "algorithm": "Default Branching Selection (betterBranch):\n  Two-phase decision rule:\n  Phase 1 (before solution): Select variable minimizing infeasibility count.\n    Score = numInfUp + numInfDn (prefer variables that eliminate more)\n  Phase 2 (after solution): Select variable maximizing objective degradation.\n    Score = changeUp + changeDn (prefer variables that bound objective)", "math": "Direction selection within variable:\n  Prefer up branch (+1) if numInfUp < numInfDn or changeUp > changeDn.\n  Prefer down branch (-1) otherwise.\n  Ties broken by direction that constrains solution space more.", "complexity": "O(1) per comparison - just evaluates score function.\n  No pseudocost tracking or update cost.\n\nReturns +1 for up branch preferred, -1 for down branch.\nRemembers best candidate for comparison chain.", "see": ["CbcBranchDecision for base class", "CbcBranchDynamicDecision for pseudocost-based selection", "CbcBranchingObject for what gets compared\n\nEdwin 11/10/2009 carved out of CbcBranchActual"], "has_pass2": true}, "src/CbcFeasibilityBase.hpp": {"path": "layer-2/Cbc/src/CbcFeasibilityBase.hpp", "filename": "CbcFeasibilityBase.hpp", "file": "CbcFeasibilityBase.hpp", "brief": "User-defined feasibility checking", "algorithm": "User-Defined Feasibility Override Interface\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcFeasibilityBase: Allows user to override default feasibility tests.\nCalled after LP solve to let user examine solution and declare status.\n\nfeasible() return values:\n- 0: Use default determination\n- 1: Pretend solution is integer feasible\n- -1: Pretend problem is infeasible\n\nmode parameter:\n- 0: Called after solve, before cuts\n- -1: Called after strong branching\n\nUse cases:\n- Accept near-integer solutions as integer\n- Reject solutions violating problem-specific constraints\n- Implement custom tolerances", "see": ["CbcModel::setFeasibilityTest()"], "has_pass2": true}, "src/CbcStatistics.hpp": {"path": "layer-2/Cbc/src/CbcStatistics.hpp", "filename": "CbcStatistics.hpp", "file": "CbcStatistics.hpp", "brief": "Statistics gathering for node processing", "algorithm": "B&C Performance Statistics Collection\nCopyright (C) 2005, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).\n\nCbcStatistics: Captures detailed data about each branching operation.\nUsed for analysis and debugging of B&C search behavior.\n\nPer-node data:\n- id_/parentId_: Node identification\n- depth_: Tree depth\n- way_: Branch direction (-1/+1 first, -10/+10 second)\n- sequence_: Variable branched on\n\nObjective tracking:\n- startingObjective_/endingObjective_: LP values\n- startingInfeasibility_/endingInfeasibility_: Integer violations\n- numberIterations_: LP iterations used\n\nUseful for performance tuning and understanding B&C behavior.", "see": ["CbcModel for statistics collection", "CbcNode for node data"], "has_pass2": true}, "src/OsiCbc/OsiCbcSolverInterface.hpp": {"path": "layer-2/Cbc/src/OsiCbc/OsiCbcSolverInterface.hpp", "filename": "OsiCbcSolverInterface.hpp", "file": "OsiCbcSolverInterface.hpp", "brief": "OSI-compliant solver interface wrapping Cbc\nCopyright (C) 2000, International Business Machines Corporation.\nThis code is licensed under the Eclipse Public License (EPL).\n\nOsiCbcSolverInterface: Implements OsiSolverInterface by wrapping a\nCbcModel. Provides standard OSI methods (initialSolve, resolve,\nbranchAndBound) that delegate to Cbc's MIP solver. Enables Cbc\nto be used as a plug-in solver in OSI-based applications.", "see": ["OsiSolverInterface for base OSI interface", "CbcModel for underlying MIP solver"], "param": ["indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the variables whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the variables", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>either</em> bound changes", "boundList the new lower/upper bound pairs for the constraints", "indexFirst,indexLast pointers to the beginning and after the\n      end of the array of the indices of the constraints whose\n      <em>any</em> characteristics changes", "senseList the new senses", "rhsList   the new right hand sides", "rangeList the new ranges"], "has_pass2": false}, "src/Cbc_C_Interface.h": {"path": "layer-2/Cbc/src/Cbc_C_Interface.h", "filename": "Cbc_C_Interface.h", "file": "Cbc_C_Interface.h", "author": "COIN-OR CBC Development team", "date": "15 Aug 2019\n\nThe C API for the COIN-OR Branch-and-Cut solver", "algorithm": "LP Method Selection (LPMethod enum):\n  - LPM_Dual: Dual simplex (default, best for warm starts)\n  - LPM_Primal: Primal simplex\n  - LPM_Barrier: Interior-point with crossover to basis\n  - LPM_BarrierNoCross: IPM without crossover (for large LPs)", "complexity": "MIP: Exponential worst-case, highly structure-dependent\n  Each LP solve: O(m·n·iterations) for simplex", "ref": ["Land, A.H. and Doig, A.G. (1960). \"An automatic method of solving\n  discrete programming problems\". Econometrica 28(3):497-520."], "brief": "Creates an empty problem", "param": ["model problem object", "array string with problem name", "model problem object", "store: 1 maintain indexes of column and constraints names for searching indexes, 0 not", "model problem object", "name variable name", "lb column lower bound", "ub column upper bound", "obj objective function coefficient", "isInteger 1 if variable is integral, 0 otherwise", "nz number of rows (constraints) where this column appears, can be 0 if constraints will be added later", "rows index of rows where this column appears, NULL if rows will be added later", "coefs coefficients that this column appears in its rows, NULL if rows will be added later\n*", "model problem object", "numCols number of columns that will be deleted", "cols Vector with indexes of columns that will be deleted", "model problem object", "name constraint name", "nz number of variables with non-zero coefficients in this row", "cols index of variables that appear in this row", "coefs coefficients that that variables appear", "sense constraint sense: L if <=, G if >=, E if =, R if ranged and N if free", "rhs right hand size", "model problem object", "numRows number of rows", "rows rows to be deleted", "model problem object", "iColumn column index", "column name", "model problem object", "iRow row index", "name row name", "model problem object", "sense: direction of optimization (1 - minimize, -1 - maximize, 0 - ignore)", "model problem object", "index row index", "value new row lower bound", "model problem object", "index row index", "value new row upper bound", "model problem object", "row row index", "rhs value of the new RHS", "model problem object", "index variable index", "value new objective function coefficient for this variable", "model problem object", "index variable index", "value variable lower bound", "model problem object", "index variable index", "value new variable upper bound", "model problem object", "iColumn column index", "model problem object", "iColumn column index", "model problem object", "row row index", "column column index", "newValue new value of the coefficient", "model problem object", "model problem object", "count number of variables", "colNames names of variables", "colValues variable values", "model problem object", "count number of variables", "colIdxs indexes of variables", "colValues variable values", "model problem object", "fileName problem object", "model problem object", "model problem object", "model problem object", "maxNumberCharacters space in string array", "array string where problem name will be saved", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "row index", "name string where row name will be stored", "string where row name will be stored", "model problem object", "iColumn column index", "name where name will be stored", "maxLength maximum length of name string", "model problem object", "name column (variable) name", "model problem object", "name row (constraint) name", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "rowIdx row index", "model problem object", "col column index", "model problem object", "col column index", "model problem object", "col column index", "model problem object", "idx column index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "colIdx column index", "model problem object", "model problem object", "colIdx column index", "model problem object", "model problem object", "colIdx column index", "model problem object", "i variable index", "model problem object", "features vector of size Cbc_nFeatures() that will be filled with problem features", "model problem object", "model problem object", "model problem object", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "model problem object", "fileName file name", "writeValues If writeValues = 1 writes values of structurals (and adds VALUES to end of NAME card), 0 otherwise", "formatType 0 - normal,   1 - extra accuracy,   2 - IEEE hex(later)", "model problem object", "name parameter name, e.g. cuts", "name parameter value, e.g. off", "model problem object", "which which integer parameter", "model problem object", "which which integer parameter", "val  value", "model problem object", "which which integer parameter", "val  value", "model problem object", "which which double parameter", "model model object", "model model object", "allowedGap the maximum allowable gap between the lower bound and the upper bound, when\n        the gap decrease to a smaller value the search is concluded", "model mip model", "cutcb cut callback function", "name cut generator name", "appData optional pointer to some additional information that the cut generator may need", "howOften 1 if the cut generator should be called at every node, > 1 at every howOften nodes negative\n       values have the same meaning but in this case the cut generator may be disable if not bound improvement\n       was obtained with these cuts. -99 for cut generators that will be called only at the root node", "atSolution if the cut generator must to be called also when an integer solution if found (=1) or zero otherwise", "model problem object", "model problem object", "model problem object", "n number of rows", "rows rows indices", "model problem object", "model problem object", "x solution vector", "maxViolRow pointer to double where max violation in rows will be stored", "rowIdx pointer to integer where index of most violated row will be stored", "maxViolCol pointer to double where max violation in columns will be stored", "colIdx pointer to integer where index of most violated column will be stored", "model problem object", "model problem object", "model problem object", "model problem object", "whichSol index of the solution to be retrieved", "model problem object", "whichSol solution index", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "model problem object", "row row index", "model problem object", "row row index", "model problem object", "row row index", "osi OsiSolverInterface object", "sense: direction of optimization (1 - minimize, -1 - maximize, 0 - ignore)", "model problem object", "col column index", "model problem object", "col column index", "model problem object", "col column index", "osi OsiSolverInterface object", "name variable name", "lb column lower bound", "ub column upper bound", "obj objective function coefficient", "isInteger 1 if variable is integral, 0 otherwise", "nz number of rows (constraints) where this column appears, can be 0 if constraints will be added later", "rows index of rows where this column appears, NULL if rows will be added later", "coefs coefficients that this column appears in its rows, NULL if rows will be added later\n*", "osi OsiSolverInterface object", "name constraint name", "nz number of variables with non-zero coefficients in this row", "cols index of variables that appear in this row", "coefs cofficients that that variables appear", "sense constraint sense: L if <=, G if >=, E if =, R if ranged and N if free", "rhs right hand size", "cbcModel problem object", "ct cut type", "oc an OsiCuts object where cuts will be stored", "depth current three depth, cuts may use this info to decide which strategy to use", "pass cut pass number"], "return": "model copy", "has_pass2": true}}}, "Cgl": {"name": "Cgl", "file_count": 37, "pass2_count": 23, "files": {"src/CglResidualCapacity/CglResidualCapacity.hpp": {"path": "layer-2/Cgl/src/CglResidualCapacity/CglResidualCapacity.hpp", "filename": "CglResidualCapacity.hpp", "file": "CglResidualCapacity.hpp", "brief": "Residual capacity cuts for network design", "author": "Francisco Barahona (IBM)\n\nImplements residual capacity inequalities from:\n- Magnanti, Mirchandani, Vachani \"The convex hull of two core\n  capacitated network design problems\" (Math Programming 60, 1993)\n- Atamturk, Rajan \"On splittable and unsplittable flow capacitated\n  network design arc-set polyhedra\" (Math Programming 92, 2002)", "algorithm": "Residual Capacity Cut Separation:\n  For rows with structure: Σ a_i·c_i - d·Σ z_j ≤ b (c continuous, z integer):\n  1. Preprocess: Classify rows as ROW_L/ROW_G/ROW_BOTH/ROW_OTHER\n  2. For each suitable row, identify: continuous flows c, integer design z\n  3. Compute residual capacity: r = b + d·⌊Σz_j*⌋ - Σ a_i·c_i*\n  4. If r < 0, derive cut: Σ a_i·c_i ≤ b + d·⌊Σz_j*⌋\n  5. Strengthen using integrality of design variables", "math": "Residual capacity inequality derivation:\n  Original: Σ_i a_i·c_i ≤ b + d·Σ_j z_j where z_j ∈ {0,1}\n  At solution (c*, z*): capacity slack = b + d·Σz_j* - Σa_i·c_i*\n  Since Σz_j ∈ ℤ: valid cut uses ⌊Σz_j*⌋ or ⌈Σz_j*⌉\n  Residual capacity cut: Σa_i·c_i ≤ b + d·⌊Σz_j*⌋", "complexity": "O(m·n) for preprocessing, O(m) per cut separation\n  where m = rows, n = columns. Total O(m²) per generateCuts call.", "ref": ["Magnanti, Mirchandani, Vachani (1993). \"The convex hull of two core\n     capacitated network design problems\". Math Programming 60:233-250.", "Atamtürk & Rajan (2002). \"On splittable and unsplittable flow\n     capacitated network design arc-set polyhedra\". Math Programming 92:315-333.\n\nRow types detected:\n- ROW_L: a_1*c_1 + ... + a_k*c_k - d*z_1 - ... - d*z_p <= b\n  (continuous c_i, integer z_j with common coefficient d)\n- ROW_G: Same structure with >= sense\n- ROW_BOTH: Equations treatable as both\n- ROW_OTHER: Doesn't fit the pattern\n\nTarget problems: Capacitated network design where flows are\ncontinuous but arc installation decisions are integer."], "see": ["CglFlowCover for related flow-based cuts", "CglMixedIntegerRounding for general MIR approach"], "has_pass2": true}, "src/CglAllDifferent/CglAllDifferent.hpp": {"path": "layer-2/Cgl/src/CglAllDifferent/CglAllDifferent.hpp", "filename": "CglAllDifferent.hpp", "file": "CglAllDifferent.hpp", "brief": "All-different constraint propagation for CSP-style constraints\n\nPropagates all-different constraints: variables in a set must all\ntake different integer values. Common in constraint satisfaction\nproblems (CSP) mapped to MIP.", "algorithm": "All-Different Constraint Propagation:\nDomain filtering based on value exclusion.\n\nCONSTRAINT SEMANTICS:\n  AllDifferent(x_1, x_2, ..., x_k) means:\n    x_i != x_j for all pairs i != j\n\nVALUE EXCLUSION:\n  When variable x_i is fixed to value v:\n    For all j != i in same set:\n      Remove v from domain(x_j)\n\n  If domain(x_j) becomes {v}: fix x_j = v\n  If domain(x_j) becomes empty: infeasible\n\nPROPAGATION LOOP:\n  queue = all_fixed_variables\n  while (queue not empty):\n    x_i = dequeue()\n    v = value(x_i)\n    for each x_j in same AllDiff set:\n      if v in domain(x_j):\n        remove v from domain(x_j)\n        if |domain(x_j)| == 1: enqueue(x_j)\n        if |domain(x_j)| == 0: return INFEASIBLE", "math": "Hall's theorem gives stronger propagation (not implemented here):\n  If |{x_i : domain(x_i) subset S}| > |S|, infeasible.\n\nThis is a column cut generator (fixes/tightens variable bounds),\nnot a row cut generator. mayGenerateRowCutsInTree() returns false.\n\nEach set defines: x_i != x_j for all i,j in the set.\n\nConstructor takes:\n- numberSets: How many all-different sets\n- starts: Start indices into which array (size numberSets+1)\n- which: Variable indices for all sets (referenced via starts)", "complexity": "O(k^2 * d) per propagation where k = set size, d = domain size", "ref": ["van Hoeve (2001). \"The alldifferent Constraint: A Survey\"."], "see": ["CglProbing for more general variable fixing via implications"], "has_pass2": true}, "src/CglFlowCover/CglFlowCover.hpp": {"path": "layer-2/Cgl/src/CglFlowCover/CglFlowCover.hpp", "filename": "CglFlowCover.hpp", "file": "CglFlowCover.hpp", "brief": "Lifted Simple Generalized Flow Cover cuts", "author": "Yan Xu (SAS), Jeff Linderoth (Lehigh), Martin Savelsbergh (Georgia Tech)\n\nGenerates flow cover cuts for constraints with variable upper bounds.\nSpecialization of MIR for single-node flow polytopes:\n  sum_j x_j - sum_k y_k <= b, where y_k <= u_k * d_k (d_k binary)\n\nRow classification (CglFlowRowType):\n- CGLFLOW_ROW_VARUB/VARLB: Variable bound defining rows\n- CGLFLOW_ROW_MIXUB/MIXEQ: Mixed integer rows suitable for cuts\n- CGLFLOW_ROW_SUMVARUB: Binary with 2+ other variables\n\nVariable bounds (CglFlowVUB/CglFlowVLB):\n- VUB: x_j <= a * y_k where y_k is binary indicator\n- VLB: x_j >= a * y_k\n- Enable stronger flow cover cuts than simple bounds\n\nAlgorithm:\n1. flowPreprocess(): Classify rows, identify VUBs/VLBs\n2. For each MIXUB row, try to generate flow cover\n3. generateOneFlowCut(): Find cover, apply lifting\n4. liftMinus()/liftPlus(): Strengthen coefficients\n\nColumn status (CglFlowColCut):\n- INCUT: Variable in cover set\n- INLMIN/INLMINMIN: Variable in L-/L-- for lifting\n- PRIME/SECONDARY: Candidate priority", "see": ["CglMixedIntegerRounding for general MIR approach", "CglKnapsackCover for simpler cover cuts"], "algorithm": "Row Classification (determineOneRowType):\n  Categories for cut generation:\n  - VARUB/VARLB: Two-variable rows defining variable bounds\n  - MIXUB/MIXEQ: Main candidates for flow cover generation\n  - NOBINUB: No binary → cannot generate flow cuts", "math": "VUB enables stronger cuts than simple bounds:\n        x_j ≤ u_j vs x_j ≤ u_j d_j gives tighter when d_j = 0\n  Stored in vubs_[] array indexed by continuous variable.", "ref": ["Padberg, Van Roy, Wolsey (1985) - Valid inequalities for 0-1 programs", "Gu, Nemhauser, Savelsbergh (1999) - Lifted flow cover inequalities"], "complexity": "O(nnz) per row for classification", "has_pass2": true}, "src/CglGomory/CglGomory.hpp": {"path": "layer-2/Cgl/src/CglGomory/CglGomory.hpp", "filename": "CglGomory.hpp", "file": "CglGomory.hpp", "brief": "Gomory mixed-integer cuts from optimal LP basis\n\nGenerates Gomory fractional cuts, the classic MIP cutting planes\nderived by Ralph Gomory in the 1950s-60s. For each fractional basic\nvariable, derives a valid inequality from the simplex tableau row.\n\nAlgorithm:\n1. Requires optimal LP basis (needsOptimalBasis() = true)\n2. For each basic integer variable with fractional value:\n   - Get tableau row: x_i + sum_j a_ij * x_j = b_i\n   - Apply fractional parts: sum_j f(a_ij)*x_j >= f(b_i)\n   - For mixed-integer: separate continuous/integer coefficients\n3. Strengthen using integrality of non-basic integers\n\nKey parameters:\n- limit_/limitAtRoot_: Max nonzeros in cut (dense cuts are expensive)\n- away_/awayAtRoot_: Min distance from integrality to generate cut\n- conditionNumberMultiplier_: Relax cut if basis is ill-conditioned\n- gomoryType_: 0=normal, 1=add original, 2=replace with original matrix\n\nNumerical considerations:\n- Basis condition number affects cut reliability\n- largestFactorMultiplier_ controls cut relaxation for stability\n- alternateFactorization_ uses different factorization for accuracy\n\nGomory cuts are finite and can solve pure integer programs in theory,\nbut numerically fragile. Modern solvers combine with other cuts.", "see": ["CglCutGenerator for base interface", "ClpFactorization for tableau row computation"], "algorithm": "Integrality Distance Threshold (away_):\n  Skip variables near-integer to avoid numerical issues:", "math": "Generate cut only if away_ ≤ f_0 ≤ 1 - away_\n  Default 0.05; tighter at root for more cuts.", "complexity": "O(m) per cut for m nonzeros in tableau row", "ref": ["Gomory (1958) - Outline of an algorithm for integer solutions", "Cornuéjols (2007) - Revival of GMI cuts in MIP", "Cook et al. (2009) - Numerically safe GMI cuts"], "has_pass2": true}, "src/CglTwomir/CglTwomir.hpp": {"path": "layer-2/Cgl/src/CglTwomir/CglTwomir.hpp", "filename": "CglTwomir.hpp", "file": "CglTwomir.hpp", "brief": "Two-step MIR (TMIR) cut generator\n\nGenerates tMIR and 2-step MIR cuts by applying the MIR inequality\nrecursively with different scaling factors. More general than\nsimple Gomory or MIR cuts.", "algorithm": "Two-Step Mixed Integer Rounding (2-MIR):\n  Derives cuts by repeated MIR application with scaling:\n  1. Start with base inequality: Σ a_j x_j ≤ b (from tableau or formulation)\n  2. First MIR: Scale by t, apply MIR function, get inequality with floor(tb)\n  3. Second MIR: Scale result by q, apply MIR again\n  4. For each (t,q) in [t_min,t_max]×[q_min,q_max], generate candidate cut\n  5. Select cuts meeting quality thresholds (steepness, violation)", "math": "MIR function derivation:\n  For Σ a_j x_j ≤ b with x_j ∈ [0,u_j], x_j integer for j ∈ I:\n  Let f = b - floor(b), f_j = a_j - floor(a_j)\n  MIR: Σ_{j:f_j≤f} floor(a_j)x_j + Σ_{j:f_j>f} (f_j-f)/(1-f)·x_j ≤ floor(b)\n  Scaling by t before MIR: multiplies a_j, b by t → different f values.\n  Two-step: Apply MIR, then scale and apply again for stronger cuts.", "complexity": "O(m·n·|T|·|Q|) where m=rows, n=cols, |T|, |Q| = scaling ranges.\n  Typical: t ∈ [1,4], q ∈ [1,2] with step 1 → 8 cuts per source row.", "ref": ["Dash & Günlük (2006). \"Valid inequalities based on simple mixed-integer\n     sets\". Math. Programming 105:29-53.\n\nCut types generated (controlled by do_mir_, do_2mir_, do_tab_, do_form_):\n- tMIR: Apply MIR with scaling t (t_min_ to t_max_)\n- 2-step MIR: Apply MIR twice with scaling q (q_min_ to q_max_)\n- Tableau cuts: From simplex tableau rows\n- Formulation cuts: From original constraint rows\n\nKey data structures:\n- DGG_data_t: Problem data including bounds, solution, basis info\n- DGG_constraint_t: Sparse constraint representation\n- DGG_list_t: Collection of generated cuts\n\nAlgorithm flow:\n1. DGG_getData(): Extract LP data from solver\n2. DGG_generateTabRowCuts()/DGG_generateFormulationCuts(): Build base rows\n3. DGG_transformConstraint(): Shift/complement variables\n4. DGG_buildMir()/DGG_build2step(): Apply MIR function\n5. DGG_unTransformConstraint(): Restore original variables\n\nNumerical precision:\n- DGG_GOMORY_THRESH: Min fractionality for cut generation (0.005)\n- DGG_MIN_TABLEAU_COEFFICIENT: Zero threshold (1e-12)\n- DGG_MIN_STEEPNESS: Cut quality threshold"], "see": ["CglMixedIntegerRounding for simpler single-step MIR", "CglGomory for basic Gomory cuts"], "has_pass2": true}, "src/CglRedSplit/CglRedSplit.hpp": {"path": "layer-2/Cgl/src/CglRedSplit/CglRedSplit.hpp", "filename": "CglRedSplit.hpp", "file": "CglRedSplit.hpp", "brief": "Reduce-and-Split cuts for enhanced Gomory generation", "author": "Francois Margot (Carnegie Mellon)\n\nBased on Anderson, Cornuejols, Li \"Reduce-and-Split Cuts: Improving\nthe Performance of Mixed Integer Gomory Cuts\" (Management Science, 2005).", "algorithm": "Reduce-and-Split Cut Generation:\n  Strengthens Gomory cuts by reducing continuous variable coefficients:\n  1. Extract simplex tableau rows for fractional integer basic vars\n  2. Build contNonBasicTab: coefficients for continuous nonbasics\n  3. Reduce: For each row pair (r1, r2), find integer k minimizing\n     ||row1 + k·row2||, update pi_mat with multipliers\n  4. Generate GMI cut from reduced row using Chvatal-Gomory formula\n  5. Eliminate slacks by substituting original constraint rows", "math": "Reduction operation:\n  Let a^i = row i of continuous nonbasic tableau\n  Goal: minimize ||a^i + k·a^j||² over k ∈ ℤ\n  Optimal k = round(-(a^i·a^j)/(a^j·a^j))\n  Accept if ||a^i + k·a^j|| < (1 - minReduc)·||a^i||\n\n  GMI cut from reduced row: Σ_N f̄_j·x_j ≥ f₀/(1-f₀)\n  where f̄_j = f_j if j integer, else max(f_j, (f₀/(1-f₀))(1-f_j))", "complexity": "O(mTab² · max(mTab, nTab)) where mTab = fractional integer\n  basic vars, nTab = continuous nonbasics. Controlled by maxTab parameter.\n  Typically 10-100x slower than basic Gomory but produces stronger cuts.", "ref": ["Anderson, Cornuéjols, Li (2005). \"Reduce-and-Split Cuts: Improving\n     the Performance of Mixed Integer Gomory Cuts\". Management Science 51:1720-1732.\n\nKey idea: Combine simplex tableau rows to reduce coefficients on\ncontinuous non-basic variables before applying Gomory formula.\nThis produces stronger cuts with better numerical properties.\n\nReduction process:\n- pi_mat: Integer multipliers for row combinations\n- test_pair(r1,r2): Check if combining rows reduces norm\n- find_step(): Optimal integer step for reduction\n- update_pi_mat()/update_redTab(): Apply reduction\n\nComplexity control:\n- maxTab_: Limits mTab * mTab * max(mTab,nTab) computation\n- Reducing maxTab makes generator faster but weaker\n\nRequires optimal basis (Clp or CPLEX)."], "see": ["CglRedSplitParam for parameters (away, LUB, tolerances)", "CglRedSplit2 for enhanced version", "CglGomory for basic Gomory cuts"], "has_pass2": true}, "src/CglRedSplit/CglRedSplitParam.hpp": {"path": "layer-2/Cgl/src/CglRedSplit/CglRedSplitParam.hpp", "filename": "CglRedSplitParam.hpp", "file": "CglRedSplitParam.hpp", "brief": "Parameters for the Reduce-and-Split cut generator", "author": "Francois Margot (Tepper School of Business, Carnegie Mellon)\n\nKey parameters control numerical tolerances and algorithm behavior:\n- LUB: Threshold for \"large\" bounds (default 1000)\n- MAXDYN: Max coefficient ratio in cuts (default 1e8)\n- away: Min fractionality for row selection (default 0.05)\n- maxTab: Controls complexity (mTab^2 * max(mTab,nTab))\n- normIsZero, minReduc: Reduction termination criteria", "see": ["CglRedSplit for the generator using these parameters", "CglRedSplit2Param for enhanced version parameters", "CglParam for base parameter class\n\nDate:     11/24/06\n\n-----------------------------------------------------------------------------\nCopyright (C) 2006, Francois Margot and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglPreProcess/CglPreProcess.hpp": {"path": "layer-2/Cgl/src/CglPreProcess/CglPreProcess.hpp", "filename": "CglPreProcess.hpp", "file": "CglPreProcess.hpp", "brief": "MIP preprocessing with cut generator integration\n\nComprehensive MIP preprocessing that combines OsiPresolve with\ncut generators to strengthen the formulation before branch-and-cut.\nUnlike simple presolve, can add cuts that replace/strengthen constraints.", "algorithm": "Coefficient Strengthening:\nTighten constraint coefficients.\n\n  Given: sum_j a_j x_j <= b with x_j in {0,1}\n  For variable x_k with a_k > b - sum_{j != k} max(a_j, 0):\n    Can reduce a_k without changing feasible region\n\nMain operations:\n- preProcess(): Default strategy preprocessing\n- preProcessNonDefault(): User-configured cut generators\n- postProcess(): Reconstruct solution in original space\n- tightenPrimalBounds(): Bound tightening for faster dual\n\nPreprocessing features:\n- Variable fixing from reduced costs\n- Bound tightening propagation\n- Clique detection and strengthening (x + y <= 1 -> x + y == 1)\n- SOS (Special Ordered Set) detection\n- Constraint row replacement with stronger versions\n- Duplicate row elimination\n\nHelper classes:\n- CglBK: Bron-Kerbosch algorithm for maximal clique finding\n- CglUniqueRowCuts: Hash-based duplicate cut detection\n- CglHashLink: Hash table support structure\n\nOption flags (options_):\n- 1: Original had integer bounds\n- 2: Don't do probing\n- 4: Don't do duplicate rows\n- 8: Don't do cliques\n- 16/64: Heavy probing options", "complexity": "- Presolve pass: O(nnz) per pass\n- Clique finding: O(3^(n/3)) worst case, fast in practice\n- Total: O(passes * (nnz + probing_work))", "ref": ["Savelsbergh (1994). \"Preprocessing and Probing Techniques for MIP\".\n  ORSA J. Computing 6(4):445-454."], "see": ["OsiPresolve for basic LP presolve", "CglProbing for variable fixing cuts", "CglClique for clique cut generation"], "has_pass2": true}, "src/CglLandP/CglLandP.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandP.hpp", "filename": "CglLandP.hpp", "file": "CglLandP.hpp", "brief": "Lift-and-Project cuts via simplex pivoting", "author": "Pierre Bonami (Carnegie Mellon/IBM)\n\nAdvanced lift-and-project implementation that generates cuts by\npivoting in the extended (lifted) space. More sophisticated than\nCglLiftAndProject.", "algorithm": "Lift-and-Project Cut Generation via Pivoting:\n  Generates disjunctive cuts by solving auxiliary optimization in lifted space:\n  1. For fractional x_j with disjunction (x_j ≤ k ∨ x_j ≥ k+1):\n  2. Lift to extended space: introduce y¹, y² copies for each disjunct\n     x = λy¹ + (1-λ)y², λ ∈ [0,1], y¹ feasible for x_j ≤ k, y² for x_j ≥ k+1\n  3. Pivot in lifted space to optimize cut depth (separation)\n  4. Project: Extract cut α'x ≤ β valid for original space\n  5. Apply strengthening: modularization, coefficient tightening", "math": "Lift-and-Project formulation (Balas, 1979):\n  Original: P = {x : Ax ≤ b, 0 ≤ x ≤ u}\n  Disjunction: D = (π'x ≤ π₀) ∨ (π'x ≥ π₀ + 1)\n  Lifted LP: max α'x̄ - β s.t. valid for P ∩ D\n  Optimal (α*, β*) gives deepest cut separating x̄ from conv(P ∩ D).", "complexity": "O(pivotLimit · n) per cut where n = variables.\n  Total: O(|fractional| · pivotLimit · n) per call to generateCuts.\n  Pivoting typically converges in 10-50 iterations.", "ref": ["Balas & Perregaard (2003). \"A precise correspondence between lift-and-\n     project cuts, simple disjunctive cuts, and mixed integer Gomory cuts\n     for 0-1 programming\". Math. Programming 94:221-245.\n\nAlgorithm: For each fractional integer variable:\n1. Lift problem to higher dimension with disjunction\n2. Pivot in lifted space using CglLandPSimplex\n3. Project resulting constraint back to original space\n\nPivot strategies (SelectionRules):\n- mostNegativeRc: Most negative reduced cost (Dantzig-like)\n- bestPivot: Best possible pivot element\n- initialReducedCosts: Only rows with initially zero reduced cost\n\nCut generation modes (ExtraCutsMode):\n- none: Just primary L&P cuts\n- AtOptimalBasis: Also generate from optimal lifted basis\n- WhenEnteringBasis: Generate as structurals enter basis\n- AllViolatedMigs: Generate all violated GMI cuts during optimization\n\nNormalization options:\n- Unweighted, WeightRHS, WeightLHS, WeightBoth\n- LHS norms: L1, L2, Infinity, Average, Uniform, SupportSize\n\nKey parameters (Parameters class):\n- pivotLimit/pivotLimitInTree: Max pivots before generating cut\n- degeneratePivotLimit: Handle degenerate pivots\n- modularize/strengthen: Cut strengthening options\n\nRequires optimal basis."], "see": ["LAP::CglLandPSimplex for pivoting implementation", "LAP::Validator for cut validation", "CglLiftAndProject for simpler implementation"], "has_pass2": true}, "src/CglLandP/CglLandPMessages.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPMessages.hpp", "filename": "CglLandPMessages.hpp", "file": "CglLandPMessages.hpp", "brief": "Message handler for Lift-and-Project simplex", "author": "Pierre Bonami (CNRS, Aix-Marseille Universites)\n\nDefines LAP_messages enum for L&P algorithm status reporting:\n- Separating, FoundImprovingRow, FoundBestImprovingCol\n- PivotLog, FinishedOptimal, HitLimit\n- Warning messages for numerical issues\n\nLandPMessages class extends CoinMessages for log formatting.", "see": ["CglLandPSimplex for the algorithm using these messages", "CglLandP for the main cut generator", "CoinMessageHandler for message infrastructure\n\nDate:     02/23/08\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglLandP/CglLandPSimplex.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPSimplex.hpp", "filename": "CglLandPSimplex.hpp", "file": "CglLandPSimplex.hpp", "brief": "Simplex algorithm for Lift-and-Project cut generation", "author": "Pierre Bonami (Tepper School of Business, Carnegie Mellon)\n\nCore CGLP (Cut Generation LP) solver for finding optimal L&P cuts.\nImplements the pivoting procedure to optimize the intersection cut\nover adjacent bases using a specialized simplex method.\n\nKey operations:\n- optimize(): Find best cut by pivoting\n- generateMig(): Generate Gomory cuts without pivoting\n- createIntersectionCut(): Build cut from tableau row\n\nUses TabRow for tableau manipulation, Validator for cut quality.\nSupports OsiClp for efficient basis operations when available.", "algorithm": "Lift-and-Project Cutting Planes:\n  Given MIP with integrality on x_j, the L&P disjunction is:\n    (x_j ≤ ⌊x*_j⌋) OR (x_j ≥ ⌈x*_j⌉)\n\n  Cut generation via CGLP (Cut Generation LP):\n  1. Formulate normalization: ||cut|| = 1 (various norms)\n  2. For each disjunction term, lift the LP relaxation\n  3. Find cut valid for intersection of lifted polyhedra\n  4. Project cut back to original variable space\n\n  This class optimizes the CGLP by pivoting through adjacent bases,\n  seeking the most violated intersection cut.", "math": "For source row i with f_i fractional:\n  Intersection cut: Σ (a_j/f_i if a_j ≥ 0, else a_j/(f_i-1)) x_j ≥ 1\n  L&P strengthens by optimizing choice of disjunction via pivoting.", "complexity": "O(pivots × m × n) where pivots typically O(m).\n  Each pivot requires tableau row computation O(m) and ratio test O(n).\n  Heuristics limit pivot count to avoid excessive computation.", "ref": ["Balas, E. (1979). \"Disjunctive Programming\". Annals of Discrete\n  Mathematics 5:3-51. [Theoretical foundation]", "Balas, E. and Perregaard, M. (2003). \"A precise correspondence between\n  lift-and-project cuts, simple disjunctive cuts, and mixed integer\n  Gomory cuts for 0-1 programming\". Math. Programming 94:221-245."], "see": ["CglLandP for the main cut generator interface", "CglLandPTabRow for tableau row structure", "CglLandPValidator for cut acceptance criteria\n\nDate:     21/07/05\n\nCopyright (C) 2005-2009 Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": true}, "src/CglLandP/CglLandPValidator.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPValidator.hpp", "filename": "CglLandPValidator.hpp", "file": "CglLandPValidator.hpp", "brief": "Cut validation and cleaning for Lift-and-Project", "author": "Pierre Bonami (Tepper School of Business, Carnegie Mellon)\n\nValidator class rejects numerically unstable cuts based on:\n- RejectionsReasons enum: SmallViolation, SmallCoefficient,\n  BigDynamic, DenseCut, EmptyCut\n\nParameters:\n- maxFillIn_: Maximum density (fraction of columns)\n- maxRatio_: Maximum coefficient dynamic range\n- minViolation_: Minimum cut violation\n\ncleanCut()/cleanCut2() methods scale and filter cuts.\nTracks rejection statistics by reason.", "see": ["CglLandPSimplex which uses this validator", "CglLandP for the main cut generator", "CglGMIParam for similar validation parameters\n\nDate:     11/22/05\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglLandP/CglLandPTabRow.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPTabRow.hpp", "filename": "CglLandPTabRow.hpp", "file": "CglLandPTabRow.hpp", "brief": "Tableau row structure for Lift-and-Project simplex", "author": "Pierre Bonami (CNRS, Aix-Marseille Universites)\n\nTabRow struct extends CoinIndexedVector for sparse tableau rows:\n- num: Row number in the basis\n- rhs: Right-hand side value\n- modularized_: Flag for GMI strengthening applied\n\nProvides modularize() method for computing fractional parts\nused in intersection cut coefficient derivation.", "see": ["CglLandPSimplex for the simplex using these rows", "CglLandPUtils for row manipulation utilities", "CoinIndexedVector for sparse vector base\n\nDate:     02/23/08\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglLandP/CglLandPUtils.hpp": {"path": "layer-2/Cgl/src/CglLandP/CglLandPUtils.hpp", "filename": "CglLandPUtils.hpp", "file": "CglLandPUtils.hpp", "brief": "Utility functions for Lift-and-Project cut generation", "author": "Pierre Bonami (CNRS, Aix-Marseille Universites)\n\nKey functions for intersection cut computation:\n- intersectionCutCoef(): Computes cut coefficient from tableau entry\n- modularizedCoef(): Fractional part for integer variables\n- normCoef(): Row normalization factor\n- scale(): Cut scaling utilities\n\nCuts struct: Storage for extra cuts generated during pivoting,\nindexed by generating disjunction variable.", "see": ["CglLandPSimplex for the algorithm using these utilities", "CglLandPTabRow for tableau row structure", "CglLandP for the main cut generator\n\nDate:     02/23/08\n\nCopyright (C) 2005-2009, Pierre Bonami and others.  All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL)."], "has_pass2": false}, "src/CglKnapsackCover/CglKnapsackCover.hpp": {"path": "layer-2/Cgl/src/CglKnapsackCover/CglKnapsackCover.hpp", "filename": "CglKnapsackCover.hpp", "file": "CglKnapsackCover.hpp", "brief": "Knapsack cover cuts for binary packing constraints\n\nGenerates cover inequalities for knapsack constraints of the form\nsum_j a_j * x_j <= b where x_j are binary. A cover C is a subset\nwhere sum_{j in C} a_j > b, making x_j = 1 for all j in C infeasible.\n\nCover inequality: sum_{j in C} x_j <= |C| - 1\n\nAlgorithm phases:\n1. deriveAKnapsack(): Convert constraint to canonical binary knapsack\n2. Find minimal violated cover using:\n   - findExactMostViolatedMinCover(): Exact via Horowitz-Sahni\n   - findLPMostViolatedMinCover(): LP relaxation heuristic\n   - findGreedyCover(): Simple greedy heuristic\n3. Lifting: Strengthen cover to full-dimensional inequality\n   - liftCoverCut(): Sequence-independent lifting\n   - seqLiftAndUncomplementAndAdd(): Sequence-dependent lifting\n   - liftUpDownAndUncomplementAndAdd(): Up/down lifting for binaries\n\nAdditional features:\n- createCliques(): Build clique structure for probing\n- Horowitz-Sahni algorithm for exact knapsack solving\n- John-Ellis cover finding from OSL (IBM's solver)\n\nKey parameters:\n- maxInKnapsack_: Maximum variables in processed knapsack\n- expensiveCuts_: Enable/disable costly exact methods\n- rowsToCheck_: Restrict which rows to examine", "see": ["CglProbing for variable implications derived from covers", "CglClique for clique cuts (special case of covers)"], "algorithm": "Greedy Cover Heuristic (findGreedyCover):\n  Order variables by a_j/x̄_j ratio (LP solution weighted):", "math": "Add variables greedily until ∑_{j∈C} a_j > b\n  Fast but may miss most violated cover.", "complexity": "O(2^n) worst case, often O(n * c) practical", "ref": ["Balas & Zemel (1978) - Facets of the knapsack polytope", "Crowder, Johnson, Padberg (1983) - Solving large-scale 0-1 LPs", "Zemel (1989) - Easily computable facets of the knapsack polytope", "Horowitz & Sahni (1974) - Computing partitions", "Martello & Toth (1990) - Knapsack Problems (p. 30)"], "has_pass2": true}, "src/CglOddHole/CglOddHole.hpp": {"path": "layer-2/Cgl/src/CglOddHole/CglOddHole.hpp", "filename": "CglOddHole.hpp", "file": "CglOddHole.hpp", "brief": "Odd hole cuts from conflict graphs\n\nGenerates odd hole inequalities based on the method from\nGrotschel, Lovasz, and Schrijver (1988). An odd hole is a\nchordless cycle of odd length in the conflict graph.", "algorithm": "Odd Hole Cut Generation:\n  Finds violated odd hole inequalities in conflict graph:\n  1. Build conflict graph G from set packing rows Σx_i ≤ 1\n     (edge (i,j) exists iff x_i + x_j ≤ 1 for some row)\n  2. For each fractional variable x_s with x_s* > 0:\n  3. Use shortest path to find odd cycle C through s in G\n     (weighted by 1 - x_i* to separate current solution)\n  4. If cycle C has length 2k+1 with weight < k: violated cut found\n  5. Generate cut: Σ_{i∈C} x_i ≤ k = ⌊|C|/2⌋", "math": "Odd hole validity and lifting:\n  For odd cycle C = (v₁,...,v_{2k+1},v₁) in conflict graph:\n  Basic cut: Σ_{i∈C} x_i ≤ k (at most k of 2k+1 mutually conflicting)\n  Chvatal-Gomory lift: Sum rows in cycle (RHS = 2k+1, odd)\n  Weaken coefficients: ⌊a_j/2⌋·2 for odd a_j\n  Divide by 2: Σ ⌊a_j/2⌋·x_j ≤ k (stronger cut)", "complexity": "O(n·m) per odd hole search using Dijkstra/Bellman-Ford.\n  Total O(f·n·m) where f = fractional variables. Can be slow for\n  large f; use createRowList() to filter candidate rows.", "ref": ["Grötschel, Lovász, Schrijver (1988). \"Geometric Algorithms and\n     Combinatorial Optimization\". Springer, Ch. 9.\n\nFor set packing rows sum x_i <= 1 (or == 1):\n- Build conflict graph where edge (i,j) means x_i + x_j <= 1\n- Find odd cycles C of length 2k+1\n- Odd hole inequality: sum_{i in C} x_i <= k\n\nPerformance considerations:\n- Only examines rows with unsatisfied 0-1 variables\n- Can be slow with many fractional variables\n- Uses shortest path algorithm (could be improved)\n- createRowList() to prefilter candidate rows\n- createCliqueList() for external clique information"], "see": ["CglClique for clique cuts (simpler structure)", "CglOddWheel for related odd wheel cuts"], "has_pass2": true}, "src/CglRedSplit2/CglRedSplit2Param.hpp": {"path": "layer-2/Cgl/src/CglRedSplit2/CglRedSplit2Param.hpp", "filename": "CglRedSplit2Param.hpp", "file": "CglRedSplit2Param.hpp", "brief": "Parameters for the enhanced Reduce-and-Split cut generator", "author": "Giacomo Nannicini (Singapore University of Technology and Design)\n\nExtends CglRedSplitParam with combinable strategy selection:\n- RowSelectionStrategy: RS1-RS8 for choosing reduction rows\n- ColumnSelectionStrategy: CS1-CS21 for continuous variable selection\n- ColumnScalingStrategy: SC_* for Lift & Project integration\n\nMultiple strategies can be combined (3 row × 2 col = 6 combinations).\nSee Cornuejols & Nannicini, \"Practical strategies for generating\nrank-1 split cuts in MIP\", Math. Prog. Computation.", "see": ["CglRedSplit2 for the generator using these parameters", "CglRedSplitParam for simpler parameter version", "CglLandP for Lift & Project integration\n\nDate:     03/09/09\n-----------------------------------------------------------------------------\nCopyright (C) 2010, Giacomo Nannicini and others.  All Rights Reserved."], "has_pass2": false}, "src/CglRedSplit2/CglRedSplit2.hpp": {"path": "layer-2/Cgl/src/CglRedSplit2/CglRedSplit2.hpp", "filename": "CglRedSplit2.hpp", "file": "CglRedSplit2.hpp", "brief": "Enhanced Reduce-and-Split cuts with multiple strategies", "author": "Giacomo Nannicini (SUTD), based on CglRedSplit by F. Margot\n\nBased on papers:\n- \"Practical strategies for generating rank-1 split cuts\" (MPC)\n- \"Combining Lift-and-Project and Reduce-and-Split\" (INFORMS JOC)", "algorithm": "Enhanced Reduce-and-Split with Row Selection Strategies:\n  Generates strong GMI cuts via systematic row combination:\n  1. Select columns for reduction (fill_workNonBasicTab):\n     - Strategy 1: All continuous nonbasics\n     - Strategy 2: Only those with positive reduced cost\n  2. Select rows to combine (get_list_rows_reduction):\n     - BRS1: Fewest nonzeros overlapping with target row\n     - BRS2: Greedy nonzero coverage minimization\n     - BRS3: Maximum |cos(θ)| angle alignment (most effective)\n  3. Solve small linear systems to find optimal integer multipliers\n  4. Apply reduction, generate GMI cut from combined row", "math": "Row selection by cosine (BRS3):\n  cos(θ_{ij}) = (a^i · a^j) / (||a^i|| · ||a^j||)\n  Rows with |cos(θ)| ≈ 1 are most effective for reduction.\n  L&P tilting: λ' = λ + Π·μ where Π = reduction multipliers", "complexity": "O(mTab² · max(mTab, nTab)) per row selection strategy.\n  Multiple strategies run sequentially: total O(k · mTab² · nTab)\n  where k = number of strategies enabled. Time limit available.", "ref": ["Cornuéjols & Nannicini (2011). \"Practical strategies for generating\n     rank-1 split cuts\". Math. Programming Computation 3:281-318.", "Balas, Cornuéjols, Kis, Nannicini (2013). \"Combining Lift-and-Project\n     and Reduce-and-Split\". INFORMS J. Computing 25:475-487.\n\nEnhanced version of CglRedSplit with:\n- Multiple column selection strategies (fill_workNonBasicTab)\n- Multiple row selection strategies (get_list_rows_reduction)\n- Integration with Lift-and-Project (tiltLandPcut)\n- Time limits and more parameters\n\nRow selection strategies:\n- sort_rows_by_nonzeroes: BRS1 - fewest nonzeros first\n- sort_rows_by_nonzeroes_greedy: BRS2 - greedy nonzero selection\n- sort_rows_by_cosine: BRS3 - angle-based (most effective)\n\nL&P integration:\n- generateMultipliers(): Get row multipliers for L&P\n- tiltLandPcut(): Improve L&P cut via reduction\n\nDoes NOT generate same cuts as CglRedSplit - use both for best results."], "see": ["CglRedSplit2Param for extensive parameter control", "CglRedSplit for original implementation", "CglLandP for combined L&P + R&S cuts"], "has_pass2": true}, "src/CglBKClique/CglBKClique.hpp": {"path": "layer-2/Cgl/src/CglBKClique/CglBKClique.hpp", "filename": "CglBKClique.hpp", "file": "CglBKClique.hpp", "brief": "Clique cut separator using Bron-Kerbosch algorithm", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020", "algorithm": "Bron-Kerbosch Clique Separation:\n  Enumerates violated maximal cliques in the conflict graph:\n  1. Build induced subgraph on fractional variables (x* > minFrac)\n  2. BronKerbosch(R, P, X): recursive backtracking\n     - R = current clique, P = candidates, X = excluded\n     - If P∪X = ∅: R is maximal, check violation Σ_{i∈R} x_i* > 1\n     - Choose pivot u ∈ P∪X maximizing |N(u)∩P|\n     - For each v ∈ P \\ N(u): recurse with R∪{v}, P∩N(v), X∩N(v)\n  3. Extend found cliques using selected strategy\n  4. Generate cut: Σ_{i∈C} x_i ≤ 1 for violated clique C", "math": "Clique inequality validity:\n  Conflict graph G: edge (i,j) iff x_i + x_j ≤ 1 implied\n  Clique C ⊆ V(G): pairwise conflicts → at most one can be 1\n  Cut: Σ_{i∈C} x_i ≤ 1, violated when Σx_i* > 1 + minViol", "complexity": "O(3^{n/3}) worst case for all maximal cliques.\n  With pivoting: O(d·n·3^{d/3}) where d = degeneracy.\n  Controlled by maxCallsBK_ to limit recursive calls.", "ref": ["Bron & Kerbosch (1973). \"Algorithm 457: Finding All Cliques\n     of an Undirected Graph\". CACM 16:575-577.", "Tomita et al. (2006). \"The worst-case time complexity for\n     generating all maximal cliques\". Theoretical Computer Science 363:28-42.\n\nUses CoinBronKerbosch for maximal clique enumeration on the conflict\ngraph. More efficient than CglClique for large graphs due to the\nwell-known BK algorithm with pivoting.\n\nExtension methods (extMethod_):\n- 0: No extension\n- 1: Random extension\n- 2: Max degree extension\n- 3: Max modified degree\n- 4: Reduced cost (inversely proportional)\n- 5: Reduced cost + modified degree\n\nPivoting strategies delegate to CoinBronKerbosch::PivotingStrategy."], "see": ["CoinBronKerbosch for the BK algorithm implementation", "CoinConflictGraph for the graph data structure", "CglClique for alternative clique separator", "CglCliqueStrengthening for preprocessing with cliques\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "has_pass2": true}, "src/CglCommon/CglMessage.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglMessage.hpp", "filename": "CglMessage.hpp", "file": "CglMessage.hpp", "brief": "Message definitions for CGL cut generators\n\nDefines CGL_Message enum codes and CglMessage class for\ninternationalized message output through CoinMessageHandler.\n\nMessage categories:\n- Infeasibility detection: CGL_INFEASIBLE, CGL_UNBOUNDED\n- Preprocessing results: CGL_FIXED, CGL_MADE_INTEGER, CGL_ADDED_INTEGERS\n- Statistics: CGL_PROCESS_STATS, CGL_PROCESS_STATS2\n- Clique processing: CGL_CLIQUES, CGL_PROCESS_CLQSTR, CGL_WARNING_CLQSTR\n- SOS handling: CGL_PROCESS_SOS1, CGL_PROCESS_SOS2\n- Postsolve: CGL_POST_INFEASIBLE, CGL_POST_CHANGED\n- Matrix changes: CGL_ELEMENTS_CHANGED1, CGL_ELEMENTS_CHANGED2", "see": ["CoinMessageHandler for the message output system", "CoinMessages for base message collection class"], "has_pass2": false}, "src/CglCommon/CglParam.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglParam.hpp", "filename": "CglParam.hpp", "file": "CglParam.hpp", "brief": "Base parameter class for cut generator configuration", "author": "Francois Margot (Carnegie Mellon)\n\nProvides common numerical parameters used across cut generators.\nIndividual generators extend this with algorithm-specific parameters.\n\nParameters:\n- INFINIT: Value representing infinity (default: COIN_DBL_MAX)\n- EPS: Tolerance for double comparisons (default: 1e-6)\n- EPS_COEFF: Minimum absolute coefficient in generated cuts (default: 1e-5)\n  Coefficients smaller than this are zeroed to avoid numerical issues\n- MAX_SUPPORT: Maximum nonzeros allowed in a cut (default: unlimited)\n  Dense cuts are expensive to store and may cause fill-in\n\nDesign pattern: Base class with virtual setters so derived classes\ncan validate parameter combinations or apply constraints.", "see": ["CglGMIParam, CglRedSplitParam for extended parameter classes"], "has_pass2": false}, "src/CglCommon/CglCutGenerator.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglCutGenerator.hpp", "filename": "CglCutGenerator.hpp", "file": "CglCutGenerator.hpp", "brief": "Abstract base class for all CGL cutting plane generators\n\nDefines the interface that all cut generators must implement. In MIP\nbranch-and-cut, cutting planes tighten the LP relaxation to cut off\nfractional solutions while keeping all integer-feasible points.", "algorithm": "Aggressiveness Control:\nBalance cut quality vs. generation cost.\n\n  aggressive_ = 0:   Minimal cuts (fastest)\n  aggressive_ = 50:  Normal effort\n  aggressive_ = 100: Root node intensive search\n\nPure virtual interface:\n- generateCuts(): Main entry point - examines LP solution and adds violated cuts\n- clone(): Deep copy for use in parallel B&C\n\nConfiguration:\n- aggressive_: Hint for cut generation intensity (0=minimal, 100=root node default)\n- canDoGlobalCuts_: Whether cuts are valid globally (requires no general integers)\n- originalSolver_: Some generators need unpreprocessed model\n\nIntegration with branch-and-cut:\n- mayGenerateRowCutsInTree(): Whether to call at tree nodes (affects matrix structure)\n- needsOptimalBasis(): Whether LP must be solved to optimality first\n- maximumLengthOfCutInTree(): Limits cut density for efficiency\n\nDerived classes implement specific separation algorithms:\n- Gomory, MIR, knapsack covers (general MIP)\n- Clique, odd hole, zero-half (0-1 structure)\n- Flow cover, lift-and-project (specialized)", "complexity": "Separation is typically NP-hard in general, but polynomial\n  heuristics work well in practice.", "ref": ["Cornuejols (2008). \"Valid Inequalities for Mixed Integer Linear Programs\".\n  Math. Programming 112(1):3-44."], "see": ["OsiCuts for the cut collection interface", "CglTreeInfo for branch-and-cut context"], "has_pass2": true}, "src/CglCommon/CglTreeInfo.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglTreeInfo.hpp", "filename": "CglTreeInfo.hpp", "file": "CglTreeInfo.hpp", "brief": "Context information for cut generation in branch-and-cut\n\nPasses branch-and-cut tree state to cut generators so they can adapt\ntheir behavior based on tree depth, pass number, and other factors.\n\nCglTreeInfo fields:\n- level: Search tree depth (0 = root)\n- pass: Which cut generation round at this node\n- formulation_rows: Original row count (before generated cuts)\n- inTree: True after first branch\n- options: Bit flags for various modes (see below)\n\nOption flags (can be combined):\n- 1: Treat costed integers as important\n- 4/8: Set global cut flag at root\n- 16: Make cuts globally valid\n- 32: Last round ineffective - be more aggressive\n- 64: In preprocessing stage\n- 128: Looks like we found a solution\n- 8192: Problem is infeasible\n\nCglTreeProbingInfo (derived class):\nExtends CglTreeInfo with variable fixing implications from probing.\n- fixEntries_: What gets fixed when a variable is set\n- toZero_/toOne_: Starts of implication lists\n- Supports analyze() to derive implied bounds", "see": ["CglCutGenerator for the generator interface", "CglProbing which populates probing info"], "has_pass2": false}, "src/CglCommon/CglStored.hpp": {"path": "layer-2/Cgl/src/CglCommon/CglStored.hpp", "filename": "CglStored.hpp", "file": "CglStored.hpp", "brief": "Cut generator that stores and replays previously found cuts\n\nInstead of computing cuts, CglStored maintains a collection of cuts\nfrom prior solves or other sources, and adds violated ones to the LP.\n\nUse cases:\n- Warm-starting branch-and-cut with cuts from a similar problem\n- Cut pool management across nodes in the search tree\n- Importing cuts computed externally or in a preprocessing phase\n- Debugging by replaying a known cut sequence\n\nCut selection:\n- Only adds cuts violated by more than requiredViolation_ (default 1e-5)\n- Violation = constraint LHS evaluated at current LP solution\n- Avoids adding nearly-satisfied cuts that waste basis updates\n\nCan also store:\n- Best known solution (for primal bounds)\n- Tight variable bounds (from probing/preprocessing)\n- Probing implication information (CglTreeProbingInfo)", "see": ["CglCutGenerator for the base interface", "CglTreeProbingInfo for implication data from probing", "OsiCuts for the cut collection interface"], "has_pass2": false}, "src/CglMixedIntegerRounding/CglMixedIntegerRounding.hpp": {"path": "layer-2/Cgl/src/CglMixedIntegerRounding/CglMixedIntegerRounding.hpp", "filename": "CglMixedIntegerRounding.hpp", "file": "CglMixedIntegerRounding.hpp", "brief": "Mixed Integer Rounding (MIR) cuts via Marchand-Wolsey procedure", "author": "Joao Goncalves (Lehigh), Laszlo Ladanyi (IBM)\n\nImplements c-MIR (complemented MIR) cuts from the seminal paper by\nMarchand and Wolsey (Operations Research, 2001). These generalize\nGomory cuts by:\n- Aggregating multiple constraint rows\n- Applying bound substitution heuristics\n- Using the MIR function to round coefficients\n\nAlgorithm phases:\n1. mixIntRoundPreprocess(): Classify rows (ROW_MIX, ROW_VARUB, etc.)\n2. Row aggregation: Combine rows to create mixed-integer base inequality\n3. boundSubstitution(): Replace continuous vars using VUB/VLB\n4. cMirSeparation(): Apply MIR function to generate cut\n\nKey structures:\n- CglMixIntRoundVUB/VLB: Variable upper/lower bounds x_j <= a*y_k\n- RowType enum: Categorizes constraints for aggregation\n\nParameters:\n- MAXAGGR_: Maximum rows to aggregate (default: 3)\n- MULTIPLY_: Whether to try row * (-1) during aggregation\n- CRITERION_: Bound substitution rule (1, 2, or 3)\n- doPreproc_: When to run preprocessing (-1, 0, or 1)\n\nMIR function: For f(b)*x integer, MIR creates floor/ceil combination\nthat cuts off fractional solutions while preserving integer feasibility.", "algorithm": "c-MIR (Complemented Mixed Integer Rounding):\nStrengthened Gomory cuts via row aggregation and bound substitution.\nGiven mixed-integer set S = {(x,y) : Σa_j x_j + Σb_j y_j ≤ β, x ≥ 0, y ∈ Z}:\n1. Aggregate rows to obtain base inequality with good fractionality\n2. Substitute bounds: replace continuous x_j by VUB x_j ≤ d_j z_j\n3. Apply MIR function to derive valid cut", "math": "The MIR inequality for a_j x_j + b_j y_j ≤ β where y integer:\nLet f = β - ⌊β⌋ (fractional part of RHS), f_j = b_j - ⌊b_j⌋\nMIR cut: Σ_{f_j≤f} ⌊b_j⌋y_j + Σ_{f_j>f} (⌊b_j⌋ + (f_j-f)/(1-f))y_j + (1/(1-f))Σa_j x_j ≤ ⌊β⌋\n\nThe G-function used for coefficient strengthening:\nG(d,f) = ⌊d⌋ if frac(d) ≤ f, else ⌊d⌋ + (frac(d)-f)/(1-f)\n\nBound substitution: For continuous x_j with VUB x_j ≤ d_j z_j,\nsubstitute x_j = d_j z_j - s_j (s_j ≥ 0 slack) to convert to integer.", "complexity": "Preprocessing: O(m·n) to classify rows and find VUB/VLBs.\nCut generation: O(m·MAXAGGR + n) per source row where MAXAGGR is small constant.\nTotal: O(m²·MAXAGGR + m·n) per generateCuts() call.", "ref": ["Marchand & Wolsey (2001). \"Aggregation and Mixed Integer Rounding to\n  Solve MIPs\". Operations Research 49(3):363-371. [Original c-MIR paper]", "Nemhauser & Wolsey (1988). \"Integer and Combinatorial Optimization\".\n  Wiley. Section II.4: MIR inequalities. [Foundation]", "Wolsey (1998). \"Integer Programming\". Wiley. Chapter 8. [Textbook treatment]"], "see": ["CglMixedIntegerRounding2 for alternative implementation", "CglGomory for the base Gomory cuts MIR generalizes"], "has_pass2": true}, "src/CglGMI/CglGMI.hpp": {"path": "layer-2/Cgl/src/CglGMI/CglGMI.hpp", "filename": "CglGMI.hpp", "file": "CglGMI.hpp", "brief": "Gomory Mixed-Integer cuts with numerical safety testing", "author": "Giacomo Nannicini (SUTD)\n\nEnhanced Gomory cut generator with multiple cleaning procedures\nto test numerical safety of generated cuts. More robust than\nbasic CglGomory for difficult problems.\n\nRejection tracking (via RejectionType enum):\n- failureFractionality: Variable not sufficiently fractional\n- failureDynamism: Coefficient ratio too large\n- failureViolation: Cut doesn't violate LP solution enough\n- failureSupport: Too many nonzeros in cut\n- failureScale: Scaling failed to produce good cut\n\nCleaning procedures (param.CLEANING_PROCEDURE):\n- Check violation, dynamism, support\n- Remove small coefficients\n- Scale cuts (integral, norm-based, or largest=1)\n- nearestRational() for converting to exact fractions\n\nNumerical enhancements:\n- areEqual()/isZero()/isIntegerValue() with tolerances\n- factorize() to recompute basis for accuracy\n- flip()/unflipOrig()/unflipSlack() for bound handling\n\nRequires optimal basis (Clp or CPLEX 9.0+).", "see": ["CglGMIParam for parameter settings", "CglGomory for simpler Gomory implementation"], "algorithm": "Slack Elimination (eliminateSlack):\n  GMI from tableau uses slack variables; convert to original:", "math": "Substitute s_i = b_i - a_i'x using original constraint\n  Produces cut in terms of structural variables only.", "complexity": "O(nnz * log(maxdenom)) for rational approximation", "ref": ["Gomory (1960) - An algorithm for mixed-integer problems", "Cornuéjols (2008) - Valid inequalities for MIP", "Cook, Kannan, Schrijver (1990) - Chvátal closures", "Continued fraction algorithm for rational approximation"], "has_pass2": true}, "src/CglGMI/CglGMIParam.hpp": {"path": "layer-2/Cgl/src/CglGMI/CglGMIParam.hpp", "filename": "CglGMIParam.hpp", "file": "CglGMIParam.hpp", "brief": "Parameters for the GMI (Gomory Mixed-Integer) cut generator", "author": "Giacomo Nannicini (based on CglRedSplitParam by Francois Margot)\n\nKey parameters for cut quality and numerical safety:\n- AWAY: Min fractionality for row selection (default 0.005)\n- MAXDYN: Max coefficient ratio (default 1e6)\n- MINVIOL: Min cut violation (default 1e-4)\n- CleaningProcedure: CP_CGLLANDP1/2, CP_CGLREDSPLIT, CP_INTEGRAL_CUTS\n\nScaling options: INTEGRAL_SCALE_CONT, ENFORCE_SCALING\nDuplicate checking: CHECK_DUPLICATES (can be slow)", "see": ["CglGMI for the generator using these parameters", "CglGomory for the classic Gomory generator", "CglParam for base parameter class\n\nDate:     11/17/09\n-----------------------------------------------------------------------------\nCopyright (C) 2009, Giacomo Nannicini and others.  All Rights Reserved."], "has_pass2": false}, "src/CglClique/CglClique.hpp": {"path": "layer-2/Cgl/src/CglClique/CglClique.hpp", "filename": "CglClique.hpp", "file": "CglClique.hpp", "brief": "Clique cuts from set packing structure\n\nGenerates clique inequalities by finding maximal cliques in the\nconflict graph of binary variables. Two binaries conflict if they\ncannot both be 1 in any feasible solution.\n\nClique inequality: sum_{j in C} x_j <= 1 (or = 1 if equality)\n\nGraph construction (frac_graph):\n- Nodes: Binary variables at fractional LP values\n- Edges: Variables that conflict (appear in same set packing row)\n- Edge costs: 1 - x_i - x_j (for odd hole detection)\n\nTwo clique-finding methods:\n1. Star clique (do_star_clique):\n   - Pick a center node, find all neighbors (star)\n   - Enumerate maximal cliques in the star\n   - SCL_MIN_DEGREE/MAX_DEGREE/MAX_XJ_MAX_DEG selection rules\n2. Row clique (do_row_clique):\n   - Start from existing set packing row\n   - Extend to larger clique using adjacent variables\n\nThresholds:\n- scl_candidate_length_threshold: Max size for exact enumeration\n- rcl_candidate_length_threshold: Same for row clique method\n- Beyond threshold: use greedy heuristic\n\nCglFakeClique (derived class):\n- Works on \"fake\" solver with invented rows from probing\n- Integrates with CglProbing for stronger implications", "see": ["CglOddHole for odd-hole cuts (related conflict graph structure)", "CglProbing for variable fixing from cliques"], "algorithm": "Greedy Maximal Clique (greedy_maximal_clique):\n  When candidate list exceeds threshold:", "math": "Greedily add highest-degree nodes until no more fit\n  Fast but may miss maximum violated clique.", "complexity": "O(n² * d) for greedy approach", "ref": ["Padberg (1973) - On the facial structure of set packing", "Bron-Kerbosch (1973) - Algorithm for finding all cliques"], "has_pass2": true}, "src/CglProbing/CglProbing.hpp": {"path": "layer-2/Cgl/src/CglProbing/CglProbing.hpp", "filename": "CglProbing.hpp", "file": "CglProbing.hpp", "brief": "Probing cut generator - variable fixing and implications\n\nProbing systematically tests setting integer variables to bounds,\npropagating implications to derive:\n- Variable fixings (column cuts)\n- Bound tightenings\n- Disaggregation cuts (row cuts)\n- Coefficient strengthening\n- Clique structures\n\nProbing outcomes when testing x_j = 0 vs x_j = 1:\n1. One direction infeasible → fix variable (column cut)\n2. Both infeasible → problem infeasible\n3. Both feasible, y fixed same way → y can be fixed\n4. Both feasible, y fixed opposite → can substitute y = 1-x_j\n5. Constraint went slack by c → strengthen coefficient\n\nKey structures:\n- disaggregationAction: What gets fixed when probing a variable\n- cutVector_: Stores implications for each integer\n- Clique arrays: oneFixStart_, zeroFixStart_ for clique membership\n\nModes:\n- mode_ 0: Only unsatisfied integers, use snapshot (fast, global)\n- mode_ 1: Unsatisfied integers with current bounds\n- mode_ 2: All integers with current bounds\n\nCglImplication (separate class):\n- Generates cuts directly from stored implication info\n- Uses CglTreeProbingInfo built during probing", "algorithm": "Probing (Variable Fixing and Implication Analysis):\nFor each binary variable x_j, temporarily fix to 0 and 1, propagate bounds:\n1. If x_j = 0 infeasible → fix x_j = 1 (column cut)\n2. If x_j = 1 infeasible → fix x_j = 0 (column cut)\n3. If x_j = 0 → y_k fixed AND x_j = 1 → y_k fixed same → fix y_k\n4. If x_j = 0 → y_k = 1 AND x_j = 1 → y_k = 0 → record implication\n5. Coefficient strengthening: if constraint slack increases by c when\n   x_j = 1, tighten coefficient from a_j to (a_j + c)", "math": "Bound propagation logic for constraint Σ a_i x_i ≤ b:\nFor each variable x_j: compute min/max contribution from others\n- min_j = Σ_{i≠j} min(a_i L_i, a_i U_i)\n- max_j = Σ_{i≠j} max(a_i L_i, a_i U_i)\nNew bounds: x_j ≤ (b - min_j)/a_j if a_j > 0, else x_j ≥ (b - max_j)/a_j\n\nDisaggregation cuts from implications x_j = 1 → x_k = 0:\nValid cut: x_j + x_k ≤ 1 (clique inequality)", "complexity": "Per variable probed: O(nnz) for bound propagation where\nnnz = non-zeros in constraints containing the variable.\nTotal: O(n · nnz · maxPass) where n = integers, maxPass typically 1-3.\nRoot node: more aggressive (maxProbeRoot_, maxPassRoot_).", "ref": ["Savelsbergh (1994). \"Preprocessing and Probing Techniques for Mixed\n  Integer Programming Problems\". ORSA J. Computing 6(4):445-454.", "Achterberg (2007). \"Constraint Integer Programming\". PhD thesis,\n  TU Berlin. Chapter 3.6: Probing. [Modern treatment]"], "see": ["CglTreeProbingInfo for storing probing results", "CglClique for cliques discovered during probing", "CglKnapsackCover which also builds clique info"], "has_pass2": true}, "src/CglZeroHalf/CglZeroHalf.hpp": {"path": "layer-2/Cgl/src/CglZeroHalf/CglZeroHalf.hpp", "filename": "CglZeroHalf.hpp", "file": "CglZeroHalf.hpp", "brief": "Zero-half ({0,1/2}) cutting planes\n\nGenerates {0,1/2}-cuts by taking mod-2 combinations of constraint rows.\nBased on Andreello, Caprara, Fischetti (INFORMS J. Computing, 2007).\n\nTheory: If we combine constraints with {0, 1/2} multipliers such that\nall LHS coefficients become even, we get a valid cut by dividing by 2\nand rounding down the RHS.\n\nAlgorithm outline:\n1. Convert constraint matrix to integers (scaling)\n2. Reduce coefficients mod 2 (0-1 matrix)\n3. Find combinations where LHS sums to 0 mod 2 per column\n4. These yield valid {0,1/2}-cuts when RHS is odd\n\nInternal representation:\n- mr_, mc_, mnz_: Matrix dimensions and nonzeros\n- mtbeg_, mtcnt_, mtind_, mtval_: Sparse row storage\n- vlb_, vub_: Variable bounds (integer scaled)\n- Cgl012Cut cutInfo_: Separation algorithm state\n\nUses Dijkstra shortest path (cglShortestPath) for separation.", "algorithm": "{0,1/2}-Chvátal-Gomory Cuts via Mod-2 Separation:\nWorking over GF(2), find row combinations that zero out LHS coefficients:\n1. Scale and round matrix to integers\n2. Reduce all coefficients mod 2 → binary (0-1) matrix A'\n3. Find subset S of rows where Σ_{i∈S} A'_{ij} ≡ 0 (mod 2) for all j\n4. If Σ_{i∈S} b_i is odd → valid cut: (1/2)Σ_{i∈S} (a_i x ≤ b_i)\n   becomes Σ_{i∈S} ⌊a_ij/2⌋ x_j ≤ ⌊(Σb_i)/2⌋", "math": "The validity comes from Chvátal-Gomory closure theory:\nGiven Ax ≤ b, x ≥ 0 integer, any combination uᵀA with u ∈ {0, 1/2}ᵐ\nwhere uᵀA is integral gives valid inequality: ⌊uᵀA⌋x ≤ ⌊uᵀb⌋\n\nThe separation problem reduces to finding odd cycles in an auxiliary\ngraph, solved via shortest-path algorithms. The graph encodes:\n- Nodes: variables and their complements\n- Edges: constraints linking variable parities", "complexity": "Separation via shortest path: O(V + E log V) per source.\nGraph construction: O(m·n) where m = rows, n = columns.\nHeuristic enumeration may be polynomial in practice.", "ref": ["Andreello, Caprara & Fischetti (2007). \"Embedding Cuts in a Branch\n  and Cut Framework: a Computational Study with {0,1/2}-Cuts\".\n  INFORMS J. Computing 19(2):229-238. [Implementation reference]", "Caprara & Fischetti (1996). \"{0,1/2}-Chvátal-Gomory Cuts\".\n  Mathematical Programming 74:221-235. [Theory foundation]"], "see": ["Cgl012cut.hpp for internal separation machinery", "CglGomory for related MIP cutting plane approach"], "has_pass2": true}, "src/CglZeroHalf/Cgl012cut.hpp": {"path": "layer-2/Cgl/src/CglZeroHalf/Cgl012cut.hpp", "filename": "Cgl012cut.hpp", "file": "Cgl012cut.hpp", "brief": "Low-level C-style implementation for {0,1/2}-cut separation\nCopyright (C) 2010, IBM Corporation and others. All Rights Reserved.\nThis code is licensed under the terms of the Eclipse Public License (EPL).", "algorithm": "{0,1/2}-Cuts (Zero-Half Cuts):\nDerive cuts from mod-2 combinations of constraint rows.\n\nDERIVATION:\n  Given inequalities a_i'x <= b_i with a_i integer:\n  Take subset S with |S| odd\n  Sum: (sum_{i in S} a_i)'x <= sum_{i in S} b_i\n  Divide by 2: (1/2)(sum a_i)'x <= (1/2)(sum b_i)\n  Round down RHS: (1/2)(sum a_i)'x <= floor((1/2)(sum b_i))\n\n  The rounding is valid because LHS is half-integral.\n\nSEPARATION PROBLEM:\n  Find S such that the derived cut is violated by current LP solution x*.\n  Violation = (1/2)(sum_{i in S} a_i)'x* - floor((1/2)(sum_{i in S} b_i))\n\nMOD-2 FORMULATION:\n  Work in GF(2) (binary field):\n  - Coefficients: a_ij mod 2\n  - RHS: b_i mod 2\n  - Find odd cycle in auxiliary graph\n\nAUXILIARY GRAPH:\n  Node for each row, edge weight = slack contribution\n  Find shortest odd cycle = most violated cut\n\nCAPRARA-FISCHETTI ALGORITHM:\n  1. Build parity matrix (mod-2)\n  2. Construct separation graph\n  3. Find shortest paths between node pairs\n  4. Combine paths to form odd cycles\n  5. Extract violated cuts from cycles", "math": "Cut validity: sum_{i in S} (a_i'x <= b_i) implies\n  floor((sum a_i)/2)'x <= floor((sum b_i)/2) when |S| odd and a_i integral\n\nCore data structures for mod-2 arithmetic based separation:\n- ilp: Original ILP matrix in row format\n- parity_ilp: Parity (mod-2) representation with weakening info\n- separation_graph: Graph for finding odd cycles\n- auxiliary_graph, cgl_graph: Shortest path computation\n\nCgl012Cut class provides:\n- sep_012_cut(): Main separation routine (Caprara-Fischetti method)\n- basic_separation(): Original MP paper algorithm\n- tabu_012(): Tabu search for violated cuts\n\nUses local search with scoring to find good {0,1/2}-cuts.", "complexity": "O(m^2 * n) for full separation where m = rows, n = columns", "ref": ["Caprara & Fischetti (1996). \"{0,1/2}-Chvatal-Gomory Cuts\".\n  Math. Programming 74(3):221-235."], "see": ["CglZeroHalf for the OO wrapper around this implementation", "CglOddHole for related odd-cycle based cuts"], "has_pass2": true}, "src/CglDuplicateRow/CglDuplicateRow.hpp": {"path": "layer-2/Cgl/src/CglDuplicateRow/CglDuplicateRow.hpp", "filename": "CglDuplicateRow.hpp", "file": "CglDuplicateRow.hpp", "brief": "Detect and eliminate duplicate/dominated rows\n\nPreprocessing generator that identifies:\n- Duplicate rows (identical constraints)\n- Dominated rows (one implies another)\n- Variables fixable due to coefficient > effective RHS\n\nDesigned for problems with few rows, many integer variables,\n<= or == constraints, and small integer coefficients/RHS.\n\nduplicate_ array interpretation:\n- -1: Row still active\n- -2: Row removed (all variables fixed)\n- k >= 0: Row is same as row k\n\nModes (mode_):\n- 1: Process rows only\n- 2: Process columns only (dominated columns)\n- 3: Process both rows and columns\n- 4/8: Alternative algorithms\n\nAlgorithm for duplicate detection:\n- If row A is subset of row B with same effective RHS\n- Can fix variables in B not in A\n- Then rows become identical\n\nShould be called before first solve (preprocessing phase).", "see": ["CglPreProcess which uses this for preprocessing", "CglStored for storing cuts from dominated columns"], "has_pass2": false}, "src/CglMixedIntegerRounding2/CglMixedIntegerRounding2.hpp": {"path": "layer-2/Cgl/src/CglMixedIntegerRounding2/CglMixedIntegerRounding2.hpp", "filename": "CglMixedIntegerRounding2.hpp", "file": "CglMixedIntegerRounding2.hpp", "brief": "Enhanced MIR (Mixed Integer Rounding) cut generator", "author": "Joao Goncalves (Lehigh), Laszlo Ladanyi (IBM)\n\nAlternative implementation of MIR cuts with different heuristics\nthan CglMixedIntegerRounding. Uses CoinIndexedVector for efficiency.", "algorithm": "c-MIR (Complemented Mixed Integer Rounding) Separation:\n  Generates MIR cuts with variable bound substitution:\n  1. Preprocess: Classify rows, identify VUBs (x ≤ a·y) and VLBs (x ≥ a·y)\n  2. For each ROW_MIX row with fractional RHS:\n  3. Aggregate: Combine rows to improve cut coefficients\n  4. Substitute: Replace continuous vars using VUB/VLB constraints\n     c_j ≤ u_j·y_k → substitute c_j = u_j·y_k - s_j, s_j ≥ 0\n  5. Apply MIR: round coefficients, derive valid inequality\n  6. Complementation: Choose C ⊆ integers to complement\n     x_j → u_j - x_j for j ∈ C to maximize violation", "math": "MIR function and c-MIR derivation:\n  Base inequality: Σ a_j·x_j ≤ b (mixed integer)\n  Let f = b - ⌊b⌋ (fractional part of RHS)\n  MIR cut: Σ_{j: f_j ≤ f} f_j·x_j + Σ_{j: f_j > f} (f/(1-f))(1-f_j)·x_j\n          + continuous terms ≥ f\n  c-MIR adds complementation: for each j, choose x_j or (u_j - x_j)", "complexity": "O(m²) for aggregation, O(n·2^k) for complementation search\n  where k = integer vars (usually limited by MAXAGGR_).\n  Aggregation limited by MAXAGGR_ parameter.", "ref": ["Marchand & Wolsey (2001). \"Aggregation and Mixed Integer Rounding to\n     Solve MIPs\". Operations Research 49:363-371.\n\nRow classification (RowType):\n- ROW_VARUB/VARLB: Variable bound rows (x <= ay or x >= ay)\n- ROW_VAREQ: Equality variable bound\n- ROW_MIX: Mixed continuous/integer\n- ROW_CONT/ROW_INT: All continuous or all integer\n\nKey classes:\n- CglMixIntRoundVUB2/VLB2: Variable bound structures\n\nParameters:\n- MAXAGGR_: Max rows to aggregate (-1/-2 for unlimited)\n- MULTIPLY_: Also try row * (-1)\n- CRITERION_: Bound substitution strategy (1, 2, or 3)"], "see": ["CglMixedIntegerRounding for original implementation", "CglTwomir for two-step MIR"], "has_pass2": true}, "src/CglSimpleRounding/CglSimpleRounding.hpp": {"path": "layer-2/Cgl/src/CglSimpleRounding/CglSimpleRounding.hpp", "filename": "CglSimpleRounding.hpp", "file": "CglSimpleRounding.hpp", "brief": "Simple rounding cuts via GCD (greatest common divisor)\n\nGenerates simple rounding cuts using the GCD method from\nNemhauser and Wolsey (1988, p.211).", "algorithm": "Simple Rounding via GCD:\n  Strengthens constraints using integer coefficient structure:\n  1. For each constraint a'x ≤ b, derive all-integer version:\n     - Net out continuous variables if possible\n     - Scale to obtain integer coefficients\n  2. Compute g = GCD(a_1, a_2, ..., a_n) using Euclid's algorithm\n  3. Divide: (a/g)'x ≤ b/g\n  4. Round RHS: (a/g)'x ≤ ⌊b/g⌋ (valid since LHS is integer)", "math": "Example derivation:\n  Original: 6x + 9y ≤ 20 with x,y ∈ ℤ\n  GCD(6,9) = 3\n  Divide: 2x + 3y ≤ 6.67\n  Round: 2x + 3y ≤ 6 (strengthened RHS)\n  Cut tightens by 0.67, may cut off LP optimum", "complexity": "O(n·log(max_coef)) per constraint\n  GCD via Euclid's algorithm: O(log(min(a,b))) per pair\n  Total: O(m·n·log(max_coef)) for full matrix", "ref": ["Nemhauser & Wolsey (1988). \"Integer and Combinatorial Optimization\".\n     Wiley, p.211.\n\nHelper methods:\n- gcd(a,b): Euclid's algorithm for two integers\n- gcdv(n,vi): GCD of integer vector\n- power10ToMakeDoubleAnInt(): Scale coefficients to integers\n- deriveAnIntegerRow(): Create integer-only version of constraint\n\nWarning: Sensitive to numerical precision. The power10 scaling\nmust not overflow 32-bit integers."], "see": ["CglGomory for more sophisticated cut generation"], "has_pass2": true}, "src/CglCliqueStrengthening/CglCliqueStrengthening.hpp": {"path": "layer-2/Cgl/src/CglCliqueStrengthening/CglCliqueStrengthening.hpp", "filename": "CglCliqueStrengthening.hpp", "file": "CglCliqueStrengthening.hpp", "brief": "Preprocessing to extend and merge clique constraints", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\nNot a CglCutGenerator - this is preprocessing that modifies the model.\nStrengthens set packing constraints sum x_i <= 1 by:\n1. Extending: Add variables that conflict with all in the clique\n2. Merging: Remove dominated cliques (subsets of stronger ones)\n\nExtension methods:\n- 0: No extension\n- 1: Random\n- 2: Max degree\n- 3: Max modified degree\n- 4: Reduced cost (inversely proportional) - default\n- 5: Reduced cost + modified degree\n\nHelper classes:\n- CliqueRows: Storage for clique constraints\n- CliqueRowStatus: NotDominated or Dominated\n\nUses CoinConflictGraph to find extension candidates.", "see": ["CglBKClique for cut generation (vs preprocessing)", "CoinCliqueSet for clique storage", "CglPreProcess which may call this\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["extMethod Extension method: 0 = no extension;1 = random;\n2 = max degree; 3 = max modified degree;\n4 = reduced cost (inversely proportional);\n5 = reduced cost (inversely proportional) + modified degree.", "n number of rows to be strengthened", "rows rows to be strengthened", "extMethod Extension method: 0 = no extension;\n1 = random; 2 = max degree; 3 = max modified degree;\n4 = reduced cost (inversely proportional);\n5 = reduced cost (inversely proportional) + modified degree."], "has_pass2": false}, "src/CglLiftAndProject/CglLiftAndProject.hpp": {"path": "layer-2/Cgl/src/CglLiftAndProject/CglLiftAndProject.hpp", "filename": "CglLiftAndProject.hpp", "file": "CglLiftAndProject.hpp", "brief": "Lift-and-project cuts for 0-1 programming\n\nImplements lift-and-project cuts based on Balas, Ceria, and Cornuejols.\nCreates cuts by \"lifting\" the problem into higher dimensions where\nthe LP relaxation is tighter, then \"projecting\" back.", "algorithm": "Lift-and-Project Cut Generation (Balas):\n  Generates cuts from simple disjunctions x_j ∈ {0,1}:\n  1. For fractional x_j*, consider disjunction (x_j=0) ∨ (x_j=1)\n  2. Lift: Create extended formulation in higher dimension\n     - Variables: (x⁰,x¹,λ) where x = λx¹ + (1-λ)x⁰\n     - Constraints: Ax⁰ ≤ b, Ax¹ ≤ b, x⁰_j = 0, x¹_j = 1, λ ∈ [0,1]\n  3. Optimize: Find deepest cut separating x* from lifted polytope\n     max (α'x* - β) s.t. α'x ≤ β valid for lifted set\n  4. Project: Cut α'x ≤ β is valid in original space", "math": "Lift-and-project formulation (Balas, Ceria, Cornuéjols 1993):\n  P = {x : Ax ≤ b, 0 ≤ x ≤ 1}\n  Disjunction D_j: (x_j = 0) ∨ (x_j = 1)\n  Lifted set: conv(P ∩ D_j) via higher-dimensional representation\n  Normalization: β = ±1 (parameter) determines cut family", "complexity": "O(n³) per cut for solving the LP in lifted space.\n  Each disjunction variable requires a separate LP solve.\n  Computationally expensive; typically limited to root node.", "ref": ["Balas, Ceria, Cornuéjols (1993). \"A lift-and-project cutting plane\n     algorithm for mixed 0-1 programs\". Math. Programming 58:295-324.\n\nFor a disjunction x_j = 0 OR x_j = 1:\n- Create two copies of constraints: one with x_j=0, one with x_j=1\n- Convexify the union of these two polyhedra\n- Project back to original space\n- The projection gives a valid cut\n\nNormalization:\n- beta_ = +1 or -1 controls the normalization constraint\n- Choice affects which cuts are generated\n\nThis is a foundational method - computationally expensive but\ntheoretically powerful. Modern implementations often use\nspecialized variants like split cuts."], "see": ["CglLandP for related Lift-and-Project implementation (Bonami)", "CglGomory which can be viewed as a special case"], "has_pass2": true}, "src/CglOddWheel/CglOddWheel.hpp": {"path": "layer-2/Cgl/src/CglOddWheel/CglOddWheel.hpp", "filename": "CglOddWheel.hpp", "file": "CglOddWheel.hpp", "brief": "Odd-wheel cut separator (odd cycles with lifting)", "author": "Samuel Souza Brito and Haroldo Gambini Santos\nContact: samuelbrito@ufop.edu.br and haroldo@ufop.edu.br", "date": "03/27/2020\n\nAn odd wheel is an odd hole (chordless odd cycle) with a \"center\"\nvertex connected to all cycle vertices. The wheel inequality:\n  sum_{i in cycle} x_i + (k)*x_center <= k\nwhere k = (cycle_length - 1) / 2.\n\nLifting strategies (extMethod_):\n- 0: No lifting (just odd holes)\n- 1: Single variable as wheel center\n- 2: Clique as wheel center (strongest)\n\nUses CoinConflictGraph to find odd cycles via shortest paths.", "see": ["CglOddHole for simpler odd-hole cuts", "CglBKClique for clique cuts from same conflict graph", "CoinConflictGraph for conflict graph structure\n\n\\copyright{Copyright 2020 Brito, S.S. and Santos, H.G.}\n\\license{This This code is licensed under the terms of the Eclipse Public License (EPL).}"], "param": ["extMethod strategy that will be used to lift odd cycles,\ntransforming them into odd wheels: 0 = no lifting, 1 = only one\nvariable as wheel center, 2 = a clique as wheel center."], "has_pass2": false}}}, "Ipopt": {"name": "Ipopt", "file_count": 180, "pass2_count": 130, "files": {"tutorial/CodingExercise/Cpp/2-mistake/TutorialCpp_nlp.hpp": {"path": "layer-2/Ipopt/tutorial/CodingExercise/Cpp/2-mistake/TutorialCpp_nlp.hpp", "filename": "TutorialCpp_nlp.hpp", "file": "TutorialCpp_nlp.hpp", "brief": "Ipopt C++ tutorial exercise NLP\n\nTutorialCpp_nlp: tutorial exercise for learning TNLP interface.\nStep-by-step guide to implementing optimization problems.\nThree versions: skeleton (to fill in), mistake (to debug), solution.", "has_pass2": false}, "tutorial/CodingExercise/Cpp/1-skeleton/TutorialCpp_nlp.hpp": {"path": "layer-2/Ipopt/tutorial/CodingExercise/Cpp/1-skeleton/TutorialCpp_nlp.hpp", "filename": "TutorialCpp_nlp.hpp", "file": "TutorialCpp_nlp.hpp", "brief": "Ipopt C++ tutorial exercise NLP\n\nTutorialCpp_nlp: tutorial exercise for learning TNLP interface.\nStep-by-step guide to implementing optimization problems.\nThree versions: skeleton (to fill in), mistake (to debug), solution.", "has_pass2": false}, "tutorial/CodingExercise/Cpp/3-solution/TutorialCpp_nlp.hpp": {"path": "layer-2/Ipopt/tutorial/CodingExercise/Cpp/3-solution/TutorialCpp_nlp.hpp", "filename": "TutorialCpp_nlp.hpp", "file": "TutorialCpp_nlp.hpp", "brief": "Ipopt C++ tutorial exercise NLP\n\nTutorialCpp_nlp: tutorial exercise for learning TNLP interface.\nStep-by-step guide to implementing optimization problems.\nThree versions: skeleton (to fill in), mistake (to debug), solution.", "has_pass2": false}, "src/Interfaces/IpTNLP.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpTNLP.hpp", "filename": "IpTNLP.hpp", "file": "IpTNLP.hpp", "brief": "User interface for defining NLP problems in standard form\n\nTNLP (Templated NLP) is the primary user-facing class for defining\noptimization problems. Users inherit from TNLP and implement:", "algorithm": "User NLP Callback Interface:\n  Ipopt calls user methods in this order during optimization:\n  1. get_nlp_info() - once: dimensions n, m, nnz_jac_g, nnz_h_lag.\n  2. get_bounds_info() - once: fill x_L, x_U, g_L, g_U arrays.\n  3. get_starting_point() - once: initial x (and optionally z, λ).\n  4. Per iteration (possibly multiple times):\n     - eval_f(x) → f(x): scalar objective value.\n     - eval_grad_f(x) → ∇f(x): n-vector.\n     - eval_g(x) → g(x): m-vector of constraint values.\n     - eval_jac_g(x) → J_g(x): sparse Jacobian (triplet format).\n     - eval_h(x, σ, λ) → H: sparse Hessian of Lagrangian.\n  5. finalize_solution() - once: receive final x*, λ*, z* and status.", "math": "Standard NLP formulation:\n  min  f(x)  subject to  g_L ≤ g(x) ≤ g_U,  x_L ≤ x ≤ x_U.\n  Equality: set g_L[i] = g_U[i].  No bound: ±nlp_lower/upper_bound_inf.\n  Hessian: H = σ·∇²f + Σᵢ λᵢ·∇²gᵢ (lower triangle in triplet format).", "complexity": "User-defined. Ipopt benefits from:\n  - Exploiting sparsity: provide true nnz counts, not dense.\n  - new_x flag: cache intermediate results when x unchanged.\n  - Exact Hessian: superlinear convergence vs quasi-Newton.\n\nRequired methods:\n- get_nlp_info(): Problem dimensions and sparsity\n- get_bounds_info(): Variable and constraint bounds\n- get_starting_point(): Initial primal/dual values\n- eval_f(): Objective function value\n- eval_grad_f(): Objective gradient\n- eval_g(): Constraint function values\n- eval_jac_g(): Constraint Jacobian (sparse triplet format)\n- finalize_solution(): Receive final solution\n\nOptional methods:\n- eval_h(): Hessian of Lagrangian (or use quasi-Newton)\n- intermediate_callback(): Monitor progress per iteration", "see": ["IpIpoptApplication.hpp for solving TNLP problems", "IpTNLPAdapter.hpp for internal conversion to NLP interface"], "param": ["n           (out) Storage for the number of variables \\f$x\\f$", "m           (out) Storage for the number of constraints \\f$g(x)\\f$", "nnz_jac_g   (out) Storage for the number of nonzero entries in the Jacobian", "nnz_h_lag   (out) Storage for the number of nonzero entries in the Hessian", "index_style (out) Storage for the index style,\n                    the numbering style used for row/col entries in the sparse matrix format\n                    (TNLP::C_STYLE: 0-based, TNLP::FORTRAN_STYLE: 1-based; see also \\ref TRIPLET)", "n   (in) the number of variables \\f$x\\f$ in the problem", "x_l (out) the lower bounds \\f$x^L\\f$ for the variables \\f$x\\f$", "x_u (out) the upper bounds \\f$x^U\\f$ for the variables \\f$x\\f$", "m   (in) the number of constraints \\f$g(x)\\f$ in the problem", "g_l (out) the lower bounds \\f$g^L\\f$ for the constraints \\f$g(x)\\f$", "g_u (out) the upper bounds \\f$g^U\\f$ for the constraints \\f$g(x)\\f$", "n      (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "init_x (in) if true, this method must provide an initial value for \\f$x\\f$", "x      (out) the initial values for the primal variables \\f$x\\f$", "init_z (in) if true, this method must provide an initial value for the bound multipliers \\f$z^L\\f$ and \\f$z^U\\f$", "z_L    (out) the initial values for the bound multipliers \\f$z^L\\f$", "z_U    (out) the initial values for the bound multipliers \\f$z^U\\f$", "m      (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "init_lambda (in) if true, this method must provide an initial value for the constraint multipliers \\f$\\lambda\\f$", "lambda (out) the initial values for the constraint multipliers, \\f$\\lambda\\f$", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the values for the primal variables \\f$x\\f$ at which the objective function \\f$f(x)\\f$ is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise.\n                   This can be helpful when users have efficient implementations that calculate multiple outputs at once.\n                   %Ipopt internally caches results from the TNLP and generally, this flag can be ignored.", "obj_value (out) storage for the value of the objective function \\f$f(x)\\f$", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the values for the primal variables \\f$x\\f$ at which the gradient \\f$\\nabla f(x)\\f$ is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "grad_f (out) array to store values of the gradient of the objective function \\f$\\nabla f(x)\\f$.\n                     The gradient array is in the same order as the \\f$x\\f$ variables\n                     (i.e., the gradient of the objective with respect to `x[2]` should be put in `grad_f[2]`).", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the values for the primal variables \\f$x\\f$ at which the constraint functions \\f$g(x)\\f$ are to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "g     (out) array to store constraint function values \\f$g(x)\\f$, do not add or subtract the bound values \\f$g^L\\f$ or \\f$g^U\\f$.", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) first call: NULL; later calls: the values for the primal variables \\f$x\\f$ at which the constraint Jacobian \\f$\\nabla g(x)^T\\f$ is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "nele_jac (in) the number of nonzero elements in the Jacobian; it will have the same value that was specified in TNLP::get_nlp_info", "iRow  (out) first call: array of length nele_jac to store the row indices of entries in the Jacobian of the constraints; later calls: NULL", "jCol  (out) first call: array of length nele_jac to store the column indices of entries in the Jacobian of the constraints; later calls: NULL", "values (out) first call: NULL; later calls: array of length nele_jac to store the values of the entries in the Jacobian of the constraints", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) first call: NULL; later calls: the values for the primal variables \\f$x\\f$ at which the Hessian is to be evaluated", "new_x (in) false if any evaluation method (`eval_*`) was previously called with the same values in x, true otherwise; see also TNLP::eval_f", "obj_factor (in) factor \\f$\\sigma_f\\f$ in front of the objective term in the Hessian", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "lambda (in) the values for the constraint multipliers \\f$\\lambda\\f$ at which the Hessian is to be evaluated", "new_lambda (in) false if any evaluation method was previously called with the same values in lambda, true otherwise", "nele_hess (in) the number of nonzero elements in the Hessian; it will have the same value that was specified in TNLP::get_nlp_info", "iRow  (out) first call: array of length nele_hess to store the row indices of entries in the Hessian; later calls: NULL", "jCol  (out) first call: array of length nele_hess to store the column indices of entries in the Hessian; later calls: NULL", "values (out) first call: NULL; later calls: array of length nele_hess to store the values of the entries in the Hessian", "status @parblock (in) gives the status of the algorithm\n  - SUCCESS: Algorithm terminated successfully at a locally optimal\n    point, satisfying the convergence tolerances (can be specified\n    by options).\n  - MAXITER_EXCEEDED: Maximum number of iterations exceeded (can be specified by an option).\n  - CPUTIME_EXCEEDED: Maximum number of CPU seconds exceeded (can be specified by an option).\n  - STOP_AT_TINY_STEP: Algorithm proceeds with very little progress.\n  - STOP_AT_ACCEPTABLE_POINT: Algorithm stopped at a point that was converged, not to \"desired\" tolerances, but to \"acceptable\" tolerances (see the acceptable-... options).\n  - LOCAL_INFEASIBILITY: Algorithm converged to a point of local infeasibility. Problem may be infeasible.\n  - USER_REQUESTED_STOP: The user call-back function TNLP::intermediate_callback returned false, i.e., the user code requested a premature termination of the optimization.\n  - DIVERGING_ITERATES: It seems that the iterates diverge.\n  - RESTORATION_FAILURE: Restoration phase failed, algorithm doesn't know how to proceed.\n  - ERROR_IN_STEP_COMPUTATION: An unrecoverable error occurred while %Ipopt tried to compute the search direction.\n  - INVALID_NUMBER_DETECTED: Algorithm received an invalid number (such as NaN or Inf) from the NLP; see also option check_derivatives_for_nan_inf).\n  - INTERNAL_ERROR: An unknown internal error occurred.\n  @endparblock", "n     (in) the number of variables \\f$x\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "x     (in) the final values for the primal variables", "z_L   (in) the final values for the lower bound multipliers", "z_U   (in) the final values for the upper bound multipliers", "m     (in) the number of constraints \\f$g(x)\\f$ in the problem; it will have the same value that was specified in TNLP::get_nlp_info", "g     (in) the final values of the constraint functions", "lambda (in) the final values of the constraint multipliers", "obj_value (in) the final value of the objective function", "ip_data (in) provided for expert users", "ip_cq (in) provided for expert users", "ip_data (in)  Ipopt Data", "ip_cq   (in)  Ipopt Calculated Quantities", "scaled  (in)  whether to retrieve scaled or unscaled iterate", "n       (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping x, z_L, and z_U", "x       (out) buffer to store value of primal variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving x", "z_L     (out) buffer to store the lower bound multipliers \\f$z_L\\f$, must have length at least n; pass NULL to skip retrieving z_L and z_U", "z_U     (out) buffer to store the upper bound multipliers \\f$z_U\\f$, must have length at least n; pass NULL to skip retrieving z_U and z_U", "m       (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping g and lambda", "g       (out) buffer to store the constraint values \\f$g(x)\\f$, must have length at least m; pass NULL to skip retrieving g", "lambda  (out) buffer to store the constraint multipliers \\f$\\lambda\\f$, must have length at least m; pass NULL to skip retrieving lambda", "ip_data    (in)  Ipopt Data", "ip_cq      (in)  Ipopt Calculated Quantities", "scaled     (in)  whether to retrieve scaled or unscaled violations", "n          (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping compl_x_L, compl_x_U, and grad_lag_x", "x_L_violation (out) buffer to store violation of original lower bounds on variables (max(orig_x_L-x,0)), must have length at least n; pass NULL to skip retrieving orig_x_L", "x_U_violation (out) buffer to store violation of original upper bounds on variables (max(x-orig_x_U,0)), must have length at least n; pass NULL to skip retrieving orig_x_U", "compl_x_L  (out) buffer to store violation of complementarity for lower bounds on variables (\\f$(x-x_L)z_L\\f$), must have length at least n; pass NULL to skip retrieving compl_x_L", "compl_x_U  (out) buffer to store violation of complementarity for upper bounds on variables (\\f$(x_U-x)z_U\\f$), must have length at least n; pass NULL to skip retrieving compl_x_U", "grad_lag_x (out) buffer to store gradient of Lagrangian w.r.t. variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving grad_lag_x", "m          (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping lambda", "nlp_constraint_violation (out) buffer to store violation of constraints \\f$max(g_l-g(x),g(x)-g_u,0)\\f$, must have length at least m; pass NULL to skip retrieving constraint_violation", "compl_g    (out) buffer to store violation of complementarity of constraint (\\f$(g(x)-g_l)*\\lambda^+ + (g_l-g(x))*\\lambda^-\\f$, where \\f$\\lambda^+=max(0,\\lambda)\\f$ and \\f$\\lambda^-=max(0,-\\lambda)\\f$ (componentwise)), must have length at least m; pass NULL to skip retrieving compl_g"], "return": "true if success, false otherwise.\n\nThe values of `n` and `m` that were specified in TNLP::get_nlp_info are passed\nhere for debug checking. Setting a lower bound to a value less than or\nequal to the value of the option \\ref OPT_nlp_lower_bound_inf \"nlp_lower_bound_inf\"\nwill cause %Ipopt to assume no lower bound. Likewise, specifying the upper bound above or\nequal to the value of the option \\ref OPT_nlp_upper_bound_inf \"nlp_upper_bound_inf\"\nwill cause %Ipopt to assume no upper bound. These options are set to -10<sup>19</sup> and\n10<sup>19</sup>, respectively, by default, but may be modified by changing these\noptions.", "has_pass2": true}, "src/Interfaces/IpTNLPAdapter.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpTNLPAdapter.hpp", "filename": "IpTNLPAdapter.hpp", "file": "IpTNLPAdapter.hpp", "brief": "Adapter converting user TNLP interface to internal NLP\n\nTNLPAdapter is a Design Pattern Adapter that converts from the\nuser-facing TNLP format to Ipopt's internal NLP representation:\n\nTNLP (user):           NLP (internal):\n  g_L <= g(x) <= g_U  ->  c(x) = 0           (equalities)\n                          d_L <= d(x) <= d_U (inequalities)\n\nKey responsibilities:\n- Separate mixed constraints into equalities and inequalities\n- Handle fixed variables (as parameters or relaxed bounds)\n- Convert sparse triplet (i,j,val) to internal matrix format\n- Detect linear dependencies in constraints\n- Map between TNLP multipliers and internal dual variables", "algorithm": "TNLP to NLP Adaptation:\n  GetSpaces() performs constraint classification:\n  1. Scan g_L, g_U: E = {i: g_L[i] = g_U[i]}, I = complement.\n  2. Create c(x) = g_E(x) - g_L[E] (equalities, shifted to zero RHS).\n  3. Create d(x) = g_I(x) with d_L = g_L[I], d_U = g_U[I].\n  4. Build permutation matrices P_c_g, P_d_g for index mapping.\n  5. Handle fixed vars: MAKE_PARAMETER (remove), MAKE_CONSTRAINT (add c).", "math": "Index mapping:\n  P_c_g: c indices → g indices (equalities in g).\n  P_d_g: d indices → g indices (inequalities in g).\n  P_x_full_x: internal x → full x (when fixed vars removed).\n  Jacobians split: J_c from rows of J_g in E, J_d from rows in I.", "see": ["IpTNLP.hpp for user problem definition", "IpNLP.hpp for internal representation"], "return": "True, if bound multipliers could be assigned. False if there was an evaluation error when calculating bound multipliers for fixed variables.\n@since 3.14.0", "has_pass2": true}, "src/Interfaces/IpSolveStatistics.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpSolveStatistics.hpp", "filename": "IpSolveStatistics.hpp", "file": "IpSolveStatistics.hpp", "brief": "Statistics collected during optimization run\n\nSolveStatistics provides post-solve information accessible via\nIpoptApplication::Statistics(). Captures optimization outcomes\nat the time of construction from internal Ipopt objects.\n\nAvailable statistics:\n- Iteration count\n- Final objective value\n- Final constraint violation (primal infeasibility)\n- Final optimality error (dual infeasibility)\n- Final complementarity\n- Timing information", "see": ["IpIpoptApplication.hpp for accessing statistics after solve", "TNLP::finalize_solution() for receiving solution data"], "has_pass2": false}, "src/Interfaces/IpInterfacesRegOp.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpInterfacesRegOp.hpp", "filename": "IpInterfacesRegOp.hpp", "file": "IpInterfacesRegOp.hpp", "brief": "Option registration for Interfaces subsystem\n\nDeclares RegisterOptions_Interfaces() which registers all options\nspecific to the Interfaces module with the RegisteredOptions object.\n\nCalled during IpoptApplication initialization to register options\nlike fixed_variable_treatment, dependency_detection_with_rhs, etc.", "see": ["IpRegOptions.hpp for the registration framework"], "has_pass2": false}, "src/Interfaces/IpAlgTypes.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpAlgTypes.hpp", "filename": "IpAlgTypes.hpp", "file": "IpAlgTypes.hpp", "brief": "Algorithm-level return codes and exception types\n\nDefines SolverReturn enum for internal algorithm status:\n- SUCCESS: Converged to optimality tolerances\n- MAXITER_EXCEEDED, CPUTIME_EXCEEDED, WALLTIME_EXCEEDED\n- STOP_AT_ACCEPTABLE_POINT: Met acceptable (looser) tolerances\n- LOCAL_INFEASIBILITY: Problem appears locally infeasible\n- USER_REQUESTED_STOP: Callback requested termination\n- RESTORATION_FAILURE: Feasibility restoration failed\n- DIVERGING_ITERATES, INVALID_NUMBER_DETECTED\n\nAlso declares standard exceptions thrown during solve:\nLOCALLY_INFEASIBLE, TOO_FEW_DOF, TINY_STEP_DETECTED, etc.", "see": ["IpReturnCodes.hpp for application-level return codes"], "has_pass2": false}, "src/Interfaces/IpReturnCodes.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpReturnCodes.hpp", "filename": "IpReturnCodes.hpp", "file": "IpReturnCodes.hpp", "brief": "C++ namespace wrapper for Ipopt return codes\n\nWraps IpReturnCodes_inc.h in Ipopt namespace for C++ usage.\nDefines ApplicationReturnStatus enum used by IpoptApplication:\n\n- Solve_Succeeded: Optimal solution found\n- Solved_To_Acceptable_Level: Acceptable tolerance met\n- Infeasible_Problem_Detected: Locally infeasible\n- Maximum_Iterations_Exceeded, Maximum_CpuTime_Exceeded\n- Restoration_Failed, Error_In_Step_Computation\n- Invalid_Option, Invalid_Number_Detected\n- Not_Enough_Degrees_Of_Freedom, Internal_Error", "see": ["IpReturnCodes_inc.h for actual enum definitions", "IpIpoptApplication.hpp for usage context"], "has_pass2": false}, "src/Interfaces/IpNLP.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpNLP.hpp", "filename": "IpNLP.hpp", "file": "IpNLP.hpp", "brief": "Internal NLP representation using Vector/Matrix abstractions\n\nNLP is Ipopt's internal problem representation with equality constraints\nseparated from inequalities:\n  min  f(x)\n  s.t. c(x) = 0           (equality constraints)\n       d_L <= d(x) <= d_U (inequality constraints)\n       x_L <= x <= x_U    (variable bounds)\n\nUnlike TNLP (user interface with arrays), NLP uses Vector and Matrix\nobjects for all operations. TNLPAdapter converts TNLP to this form.", "algorithm": "NLP Evaluation Interface:\n  Ipopt calls these methods during optimization:\n  - GetSpaces(): Create VectorSpace/MatrixSpace for problem dimensions.\n  - GetBoundsInformation(): Fill bound vectors x_L, x_U, d_L, d_U.\n  - GetStartingPoint(): Provide initial x (optionally y, z).\n  - Eval_f(x): Return objective value f(x).\n  - Eval_grad_f(x): Return gradient ∇f(x).\n  - Eval_c(x), Eval_d(x): Return constraint values.\n  - Eval_jac_c(x), Eval_jac_d(x): Return constraint Jacobians.\n  - Eval_h(x, obj_factor, yc, yd): Return Hessian of Lagrangian.", "math": "Problem transformation from TNLP:\n  TNLP: g_L ≤ g(x) ≤ g_U mixed constraints.\n  NLP: c(x) = 0 (equalities where g_L = g_U),\n       d(x) (inequalities where g_L ≠ g_U).\n\nKey methods provide:\n- Problem dimensions and bounds\n- Function/gradient/Jacobian/Hessian evaluation\n- Space factories for creating appropriately-sized vectors", "see": ["IpTNLP.hpp for user-facing problem definition", "IpTNLPAdapter.hpp for TNLP-to-NLP conversion"], "has_pass2": true}, "src/Interfaces/IpTNLPReducer.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpTNLPReducer.hpp", "filename": "IpTNLPReducer.hpp", "file": "IpTNLPReducer.hpp", "brief": "Wrapper to solve subproblems with fewer constraints\n\nTNLPReducer wraps an existing TNLP and presents a reduced problem\nwith specified constraints, lower bounds, and upper bounds removed.\nUseful for experimentation with constraint subsets.\n\nCan skip:\n- Constraints (by index)\n- Lower bounds on variables\n- Upper bounds on variables\n- Fix certain variables\n\n@note Not efficient - still evaluates full problem internally.\nPrimarily for debugging and experimentation.", "see": ["IpTNLP.hpp for the underlying interface"], "has_pass2": false}, "src/Interfaces/IpStdInterfaceTNLP.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpStdInterfaceTNLP.hpp", "filename": "IpStdInterfaceTNLP.hpp", "file": "IpStdInterfaceTNLP.hpp", "brief": "TNLP implementation wrapping C function pointer interface\n\nStdInterfaceTNLP implements TNLP by delegating to C function pointers\nprovided through IpStdCInterface.h. This enables C programs to use\nIpopt without C++ inheritance.\n\nThe C interface passes:\n- Problem dimensions (n_var, n_con, nele_jac, nele_hess)\n- Bounds arrays (x_L, x_U, g_L, g_U)\n- Starting points (start_x, start_lam, start_z_L, start_z_U)\n- Function pointers for eval_f, eval_g, eval_grad_f, eval_jac_g, eval_h\n\nStdInterfaceTNLP converts C callbacks to TNLP virtual method calls.", "see": ["IpStdCInterface.h for C API documentation", "IpTNLP.hpp for the underlying interface"], "has_pass2": false}, "src/Interfaces/IpIpoptApplication.hpp": {"path": "layer-2/Ipopt/src/Interfaces/IpIpoptApplication.hpp", "filename": "IpIpoptApplication.hpp", "file": "IpIpoptApplication.hpp", "brief": "Main application class for calling Ipopt from C++\n\nIpoptApplication is the entry point for C++ applications using Ipopt.\nProvides methods for initialization, option handling, and solving.", "algorithm": "Application Lifecycle and Solve Flow:\n  1. Create: IpoptApplicationFactory() → configured app instance.\n  2. Initialize(params_file): Parse ipopt.opt, set up Journalist.\n  3. Configure: app->Options()->SetNumericValue(\"tol\", 1e-8), etc.\n  4. OptimizeTNLP(tnlp):\n     a. TNLPAdapter wraps user TNLP → internal NLP interface.\n     b. AlgorithmBuilder creates strategy objects per options.\n     c. IpoptAlgorithm::Optimize() runs interior-point iterations.\n     d. Return status + populate Statistics().\n  5. ReOptimizeTNLP(tnlp): Warm-start from previous solution.\n     Reuses factorizations, starting point close to optimum.", "math": "Warm-start considerations:\n  Cold start: x₀ user-provided, multipliers from least-squares.\n  Warm start: x₀, λ₀, z₀ from previous solve, μ₀ from complementarity.\n  ReOptimize skips rebuilding algorithm objects for efficiency.", "complexity": "OptimizeTNLP: O(iterations × per_iteration_cost).\n  Per iteration: O(nnz²/n) for sparse factorization.\n  ReOptimize: Fewer iterations when problem is similar.\n\nTypical usage:\n  SmartPtr<IpoptApplication> app = IpoptApplicationFactory();\n  app->Initialize();\n  app->Options()->SetStringValue(\"linear_solver\", \"ma57\");\n  ApplicationReturnStatus status = app->OptimizeTNLP(mynlp);\n\nKey methods:\n- Initialize(): Read options from ipopt.opt file\n- OptimizeTNLP(): Solve a TNLP problem\n- ReOptimizeTNLP(): Warm-start solve (reuses algorithm state)\n- Options(): Access OptionsList for setting parameters\n- Statistics(): Get solve statistics after optimization", "see": ["IpTNLP.hpp for defining optimization problems", "IpReturnCodes.hpp for ApplicationReturnStatus values"], "return": "Solve_Succeeded or something else if there was a\n problem in the initialization (such as an invalid option).\n\n You should call one of the initialization methods at some\n point before the first optimize call.\n Set @par allow_clobber to true if you want to allow\n overwriting options that are set by the input stream.", "has_pass2": true}, "src/Common/IpSmartPtr.hpp": {"path": "layer-2/Ipopt/src/Common/IpSmartPtr.hpp", "filename": "IpSmartPtr.hpp", "file": "IpSmartPtr.hpp", "brief": "Intrusive reference-counting smart pointer for Ipopt\n\nProvides SmartPtr<T> template class implementing automatic memory\nmanagement via reference counting. All managed objects must inherit\nfrom ReferencedObject. Used throughout Ipopt for safe object lifetime\nmanagement without manual delete calls.\n\nKey design: intrusive (count stored in object) vs non-intrusive\n(count stored externally). Intrusive chosen to avoid double-counting\nwhen raw pointers are passed between SmartPtr instances.\n\n@note Does NOT handle circular references - use weak pointers or\nexplicit release methods for bidirectional object graphs.", "see": ["IpReferenced.hpp for ReferencedObject base class", "IpObserver.hpp for observer pattern using SmartPtr"], "has_pass2": false}, "src/Common/IpTaggedObject.hpp": {"path": "layer-2/Ipopt/src/Common/IpTaggedObject.hpp", "filename": "IpTaggedObject.hpp", "file": "IpTaggedObject.hpp", "brief": "Base class for objects with change-tracking tags\n\nTaggedObject provides efficient change detection for expensive\ncomputations. Each object maintains a unique tag that changes whenever\nthe object's internal state is modified. Consumers compare stored tags\nto detect changes and avoid redundant recalculation.", "algorithm": "Change-Tracking via Versioning Tags:\n  Each TaggedObject maintains a monotonically increasing tag (version):\n  1. On creation: tag ← next_unique_id++ (global counter).\n  2. On modification: ObjectChanged() → tag ← next_unique_id++.\n  3. Consumer stores last_seen_tag after using object.\n  4. Before recomputation: if (obj.GetTag() != last_seen_tag) recompute.\n  Avoids redundant NLP evaluations when iterate hasn't changed.", "math": "Correctness invariant:\n  ∀ objects A, B: A.tag == B.tag ⟹ A and B are the same object instance\n  with identical state (since tags are globally unique and monotonic).\n  Tag comparison is equality-based: O(1) check, no deep comparison.", "complexity": "O(1) for GetTag(), HasChanged(), ObjectChanged().\n  Memory: single integer per object.\n  Combined with Observer pattern for automatic cache invalidation.\n\nUsage pattern:\n1. Store last-used tag locally\n2. Before computation, call HasChanged(stored_tag)\n3. If changed: recompute and update stored tag via GetTag()\n4. If unchanged: reuse cached result\n\nInherits from both ReferencedObject (for SmartPtr) and Subject\n(to notify Observers of changes).", "see": ["IpCachedResults.hpp for cached computation with auto-invalidation", "IpObserver.hpp for the Observer pattern"], "has_pass2": true}, "src/Common/IpReferenced.hpp": {"path": "layer-2/Ipopt/src/Common/IpReferenced.hpp", "filename": "IpReferenced.hpp", "file": "IpReferenced.hpp", "brief": "Reference counting base class for SmartPtr management\n\nDefines ReferencedObject - the base class that any object must inherit\nfrom to be managed by SmartPtr<T>. Implements intrusive reference counting\nwhere the count is stored in the object itself rather than externally.\n\nAlso defines Referencer pseudo-class used to track which SmartPtr\ninstances hold references (useful for debugging circular references).", "see": ["IpSmartPtr.hpp for the SmartPtr template class"], "has_pass2": false}, "src/Common/IpException.hpp": {"path": "layer-2/Ipopt/src/Common/IpException.hpp", "filename": "IpException.hpp", "file": "IpException.hpp", "brief": "Exception base class and macros for Ipopt error handling\n\nProvides IpoptException base class and convenience macros:\n- DECLARE_STD_EXCEPTION(Type): Define new exception class\n- THROW_EXCEPTION(Type, msg): Throw with file/line info\n- ASSERT_EXCEPTION(cond, Type, msg): Conditional throw\n\nExceptions carry source location (file, line) and message for\ndebugging. Can be reported via Journalist::ReportException().\n\nUses special visibility attributes on macOS to ensure exceptions\ncan be caught across shared library boundaries.", "see": ["IpJournalist.hpp for exception reporting"], "has_pass2": false}, "src/Common/IpUtils.hpp": {"path": "layer-2/Ipopt/src/Common/IpUtils.hpp", "filename": "IpUtils.hpp", "file": "IpUtils.hpp", "brief": "General utility functions for Ipopt\n\nProvides platform-independent utility functions:\n- Min/Max template functions for up to 4 arguments\n- IsFiniteNumber() for NaN/Inf detection\n- CpuTime(), SysTime(), WallclockTime() for profiling\n- IpRandom01() for random number generation\n- Snprintf() for safe formatted output\n- Compare_le() for numerical comparison with tolerance\n- Signal handling for graceful interrupt", "see": ["IpTypes.hpp for Number and Index type definitions", "IpTimedTask.hpp for high-level timing wrapper"], "return": "whether registering the handler was successful\n@since 3.14.17", "has_pass2": false}, "src/Common/IpOptionsList.hpp": {"path": "layer-2/Ipopt/src/Common/IpOptionsList.hpp", "filename": "IpOptionsList.hpp", "file": "IpOptionsList.hpp", "brief": "Container for user-specified algorithmic options\n\nOptionsList stores runtime-configurable parameters as string key-value\npairs. Supports integer, numeric, and string option types with automatic\nconversion. Tracks how many times each option has been accessed (useful\nfor detecting unused options).\n\nOptions are case-insensitive. Values can be set via:\n- SetNumericValue(), SetIntegerValue(), SetStringValue()\n- ReadFromStream() for ipopt.opt format files\n\nIntegrates with RegisteredOptions for validation and default values.", "see": ["IpRegOptions.hpp for option registration and validation", "IpIpoptApplication.hpp for initialization from files"], "return": "false, if an error was encountered", "has_pass2": false}, "src/Common/IpTypes.hpp": {"path": "layer-2/Ipopt/src/Common/IpTypes.hpp", "filename": "IpTypes.hpp", "file": "IpTypes.hpp", "brief": "Fundamental type definitions for Ipopt (C++ namespace wrapper)\n\nWraps the C type definitions from IpTypes.h in the Ipopt namespace:\n- Number (ipnumber): floating-point type for all numerical values\n- Index (ipindex): integer type for array indices and dimensions\n\nActual types depend on compile-time configuration (IPOPT_SINGLE,\nIPOPT_INT64). Default is double for Number and int for Index.", "see": ["IpTypes.h for underlying C definitions", "IpoptConfig.h for configuration macros"], "has_pass2": false}, "src/Common/IpTimedTask.hpp": {"path": "layer-2/Ipopt/src/Common/IpTimedTask.hpp", "filename": "IpTimedTask.hpp", "file": "IpTimedTask.hpp", "brief": "Timer class for profiling algorithm components\n\nTimedTask provides RAII-style timing measurement for code blocks.\nTracks CPU time, system time, and wall clock time. Multiple\nStart()/End() pairs accumulate total time across invocations.\n\nUsage:\n  TimedTask timer;\n  timer.Start();\n  // ... expensive operation ...\n  timer.End();\n  double elapsed = timer.TotalCpuTime();\n\nUsed throughout Ipopt to report timing breakdown by category\n(function evaluation, linear solve, line search, etc.).", "see": ["IpUtils.hpp for CpuTime(), WallclockTime() functions"], "has_pass2": false}, "src/Common/IpJournalist.hpp": {"path": "layer-2/Ipopt/src/Common/IpJournalist.hpp", "filename": "IpJournalist.hpp", "file": "IpJournalist.hpp", "brief": "Logging and output management system for Ipopt\n\nImplements a flexible logging framework with multiple output destinations\n(Journals) and fine-grained control over verbosity by category.\n\nPrint levels (EJournalLevel): J_NONE < J_ERROR < J_WARNING < J_SUMMARY\n  < J_ITERSUMMARY < J_DETAILED < J_VECTOR < J_MATRIX < J_ALL\n\nCategories (EJournalCategory): J_MAIN, J_LINE_SEARCH, J_LINEAR_ALGEBRA,\n  J_BARRIER_UPDATE, J_SOLVE_PD_SYSTEM, etc. plus J_USER1-J_USER17 for apps.\n\nEach Journal (console, file, stream) can have independent level settings\nper category. Printf-style and stream-style output both supported.", "see": ["IpIpoptApplication.hpp for setting up output journals", "IpOptionsList.hpp for print_level option"], "return": "the Journal pointer so you can set specific acceptance criteria, or NULL if there was a problem creating a new Journal.", "has_pass2": false}, "src/Common/IpLibraryLoader.hpp": {"path": "layer-2/Ipopt/src/Common/IpLibraryLoader.hpp", "filename": "IpLibraryLoader.hpp", "file": "IpLibraryLoader.hpp", "brief": "Dynamic library loading for runtime solver selection\n\nLibraryLoader provides platform-independent dynamic library loading:\n- dlopen()/dlsym() on POSIX systems\n- LoadLibrary()/GetProcAddress() on Windows\n\nUsed to load linear solver libraries (HSL, Pardiso, etc.) at runtime\nwithout compile-time linkage. Allows users to provide proprietary\nsolvers without recompiling Ipopt.\n\nIPOPT_SHAREDLIBEXT defined to \"so\", \"dylib\", or \"dll\" per platform.\n\n@since 3.14.0", "see": ["Linear solver interface files for usage examples"], "has_pass2": false}, "src/Common/IpCachedResults.hpp": {"path": "layer-2/Ipopt/src/Common/IpCachedResults.hpp", "filename": "IpCachedResults.hpp", "file": "IpCachedResults.hpp", "brief": "Template class for caching computed results with dependency tracking\n\nCachedResults<T> stores computed values along with their dependencies\n(TaggedObjects and scalar parameters). Results are automatically\ninvalidated when any dependency changes via the Observer pattern.", "algorithm": "LRU Cache with Observer-Based Invalidation:\n  AddCachedResult(result, deps, scalars):\n    1. Create DependentResult with result and dependency tags.\n    2. Subscribe to NT_Changed notifications from each dependency.\n    3. Push to front of cache list.\n    4. If size > max_cache_size: evict oldest (LRU policy).\n\n  GetCachedResult(deps, scalars):\n    1. Clean up stale results (marked by Observer notifications).\n    2. Linear scan: find entry where all tags match current deps.\n    3. If found: return cached result; else: return false (cache miss).\n\n  DependentResult::ReceiveNotification(NT_Changed):\n    Mark result as stale → will be removed on next access.", "math": "Memoization correctness:\n  Result R cached with dependencies D = {d₁, ..., dₖ} and scalars S.\n  R is valid iff ∀i: dᵢ.GetTag() == stored_tag[i] ∧ S unchanged.\n  Tag monotonicity ensures no ABA problem (tag never reused).", "complexity": "O(k) per Add/Get for k cached entries.\n  Tag comparison: O(d) for d dependencies.\n  Observer notification: O(1) invalidation per dependency change.\n  Typical max_cache_size ≈ 3-5 (current, trial, backup iterates).\n\nKey features:\n- LRU-style cache with configurable maximum size\n- Automatic staleness detection via TaggedObject notifications\n- Support for multiple dependency types (objects, scalars)\n- Thread-safe invalidation of dependent results\n\nTypical usage: IpoptCalculatedQuantities stores gradients, Hessians,\nand constraint values, recomputing only when the iterate changes.", "see": ["IpTaggedObject.hpp for change-tracking base class", "IpIpoptCalculatedQuantities.hpp for primary user of this class"], "return": "true, if the result was found", "has_pass2": true}, "src/Common/IpRegOptions.hpp": {"path": "layer-2/Ipopt/src/Common/IpRegOptions.hpp", "filename": "IpRegOptions.hpp", "file": "IpRegOptions.hpp", "brief": "Option registration, validation, and documentation system\n\nProvides infrastructure for declaring algorithm options with:\n- Type information (OT_Number, OT_Integer, OT_String)\n- Valid ranges (bounds for numerics, allowed values for strings)\n- Default values\n- Documentation strings for help generation\n- Categories for organizing options in documentation\n\nRegisteredOptions aggregates all RegisteredOption instances and\nprovides lookup, validation, and documentation generation.\n\nEach Ipopt component registers its options in RegisterAllOptions()\nduring initialization.", "see": ["IpOptionsList.hpp for runtime option storage"], "return": "NULL, if the option does not exist", "has_pass2": false}, "src/Common/IpObserver.hpp": {"path": "layer-2/Ipopt/src/Common/IpObserver.hpp", "filename": "IpObserver.hpp", "file": "IpObserver.hpp", "brief": "Observer design pattern implementation for change notification\n\nImplements Subject/Observer pattern for automatic cache invalidation.\nTaggedObjects (vectors, matrices) act as Subjects; dependent computed\nquantities (CachedResults) act as Observers. When a Subject changes,\nall attached Observers are notified to invalidate their cached values.\n\nNotification types:\n- NT_Changed: Subject data has been modified\n- NT_BeingDestroyed: Subject is being deleted\n- NT_All: Receive all notification types", "see": ["IpTaggedObject.hpp for tagged subjects", "IpCachedResults.hpp for cached observers"], "has_pass2": false}, "src/Common/IpDebug.hpp": {"path": "layer-2/Ipopt/src/Common/IpDebug.hpp", "filename": "IpDebug.hpp", "file": "IpDebug.hpp", "brief": "Debug assertion and verbose tracing macros\n\nProvides compile-time controlled debugging facilities:\n\nIPOPT_CHECKLEVEL > 0 enables:\n- DBG_ASSERT(cond): Runtime assertions\n- DBG_ASSERT_EXCEPTION(cond, type, msg): Conditional exceptions\n- DBG_DO(cmd): Execute only in debug mode\n\nIPOPT_VERBOSITY >= 1 enables tracing:\n- DBG_START_FUN/DBG_START_METH: Trace function entry/exit\n- DBG_PRINT: Printf-style debug output with indentation\n- DBG_EXEC: Conditional execution based on verbosity\n\nAll macros compile to nothing when disabled (zero overhead).", "see": ["IpJournalist.hpp for production-level logging"], "has_pass2": false}, "src/Algorithm/IpLeastSquareMults.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLeastSquareMults.hpp", "filename": "IpLeastSquareMults.hpp", "file": "IpLeastSquareMults.hpp", "brief": "Least-squares estimation of equality constraint multipliers\n\nLeastSquareMultipliers computes initial estimates for the equality\nconstraint multipliers y_c and y_d by solving a least-squares problem.\n\nFormulation: Find y minimizing ||∇_x L(x,y)||^2 where\n  ∇_x L = ∇f(x) + J_c^T y_c + J_d^T y_d - z_L + z_U\n\nThis is equivalent to solving the normal equations:\n  [J_c J_c^T    0    ] [y_c]   [-J_c (∇f - z_L + z_U)]\n  [   0     J_d J_d^T] [y_d] = [-J_d (∇f - z_L + z_U)]\n\nActually solved via augmented system:\n  [0  J_c^T  J_d^T] [r  ]   [∇f - z_L + z_U]\n  [J_c  0     0   ] [y_c] = [     0        ]\n  [J_d  0     0   ] [y_d]   [     0        ]\n\nUses AugSystemSolver to solve the linear system with W=0.\n\nUsage:\n- DefaultIterateInitializer: Initial multiplier estimates\n- MinC_1NrmRestorationPhase: Post-restoration multiplier reset", "algorithm": "Least-Squares Multiplier Estimation:\nFind y minimizing ||∇_x L(x,y)||² via normal equations:\n1. Form gradient residual: g = ∇f(x) - z_L + z_U (bound multiplier contribution)\n2. Solve augmented system with W=0:\n   [0     J_c^T   J_d^T] [r  ]   [g]\n   [J_c    0       0   ] [y_c] = [0]\n   [J_d    0       0   ] [y_d]   [0]\n3. Discard residual r; extract y_c, y_d as multiplier estimates", "math": "Least-squares formulation:\nmin_y ||∇f + Jᵀy - (z_L - z_U)||²\n\nNormal equations: (J·Jᵀ)·y = -J·(∇f - z_L + z_U)\n\nAugmented system form solves this without forming JJᵀ explicitly,\npreserving sparsity: Jᵀy = r, Jx = 0 → y = (JJᵀ)⁻¹J·(-g).\n\nSingular case: If J is rank-deficient, minimum-norm solution is obtained.", "complexity": "Same as one augmented system solve: O(nnz(J)·fill) for sparse,\nO(n³) for dense factorization.", "see": ["IpEqMultCalculator.hpp for base interface", "IpAugSystemSolver.hpp for the linear solver", "IpDefaultIterateInitializer.hpp for initialization usage"], "return": "false, if the least square system could not be solved (the linear system is singular)", "has_pass2": true}, "src/Algorithm/IpGenAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpGenAugSystemSolver.hpp", "filename": "IpGenAugSystemSolver.hpp", "file": "IpGenAugSystemSolver.hpp", "brief": "Augmented system solver using GenKKTSolverInterface\n\nGenAugSystemSolver adapts the AugSystemSolver interface to use\nGenKKTSolverInterface, which provides a more generic linear solver\ninterface supporting iterative methods.\n\nThis class:\n- Extracts raw Number* arrays from Vector objects\n- Passes Matrix objects directly from the NLP\n- Manages caching to avoid redundant matrix updates\n\nMultiSolve() implementation:\n1. Check if augmented system matrices have changed (via tags)\n2. If changed, update solver_interface_ with new matrices\n3. Extract RHS vectors to raw arrays\n4. Call solver_interface_->Solve()\n5. Copy solutions back to Vector objects\n\nTag-based caching tracks:\n- W matrix and W_factor\n- Diagonal matrices D_x, D_s, D_c, D_d\n- Jacobians J_c, J_d\n- Regularization deltas\n\nInertia and quality:\n- NumberOfNegEVals(): Query solver for eigenvalue count\n- ProvidesInertia(): Check if solver supports this\n- IncreaseQuality(): Request better pivoting/tolerance", "algorithm": "Generic Augmented System Solver (Adapter Pattern):\nAdapts AugSystemSolver interface to GenKKTSolverInterface:\n1. Check matrix change via tag comparison (O(1))\n2. If matrices changed:\n   a. Extract diagonal values to Number* arrays\n   b. Pass matrices to solver_interface_->InitializeStructure()\n   c. Update internal tags\n3. For each RHS:\n   a. Extract Vector data to Number* arrays\n   b. Call solver_interface_->Solve()\n   c. Copy solution back to Vector objects", "math": "Tag-based caching:\nEach matrix/vector has monotonic tag that changes on modification.\nComparison: current_tag != cached_tag → matrix changed.\nAvoids redundant matrix assembly/factorization when data unchanged.", "complexity": "Tag check: O(1). Matrix update: O(nnz).\nSolve dominated by underlying solver: O(nnz·fill) sparse, O(n³) dense.", "see": ["IpAugSystemSolver.hpp for base interface", "IpGenKKTSolverInterface.hpp for the solver interface"], "return": "the number of negative eigenvalues of the most recent factorized matrix.", "has_pass2": true}, "src/Algorithm/IpLineSearch.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLineSearch.hpp", "filename": "IpLineSearch.hpp", "file": "IpLineSearch.hpp", "brief": "Strategy interface for globalization via line search\n\nLineSearch is the abstract base for all line search strategies\nin Ipopt's globalization framework. Given a search direction\n(from IpData.delta()), finds an acceptable trial point.", "algorithm": "Backtracking Line Search with Globalization:\n  Given direction Δw from Newton step:\n  1. Initialize α = 1 (full Newton step).\n  2. Compute trial point: w_trial = w + α·Δw.\n  3. Apply fraction-to-boundary: α ← min(α, τ·α_max) to stay positive.\n  4. Check acceptance criterion (filter or merit function).\n  5. If rejected: α ← ρ·α (backtrack, ρ ≈ 0.5) and goto 3.\n  6. If α < α_min: activate fallback (restoration phase).", "math": "Filter-based acceptance (Wächter-Biegler):\n  Accept w_trial if it improves either objective φ(w) OR constraint\n  violation θ(w) = ||c(x)||, and is not dominated by filter entries.\n  Filter F = {(θ_i, φ_i)}: reject if θ(w_trial) ≥ θ_i AND φ(w_trial) ≥ φ_i.\n  Switching condition: use Armijo on φ when θ is small enough.", "complexity": "O(n_backtrack · eval_cost) where n_backtrack typically O(1)-O(10).\n  Each trial requires function/constraint evaluation.\n  Filter operations O(|F|) comparisons, |F| typically small.", "see": ["IpBacktrackingLineSearch.hpp for the main implementation", "IpFilterLSAcceptor.hpp for filter-based acceptance", "IpRestoPhase.hpp for the fallback restoration mechanism"], "return": "false, if no fallback mechanism is available.", "has_pass2": true}, "src/Algorithm/IpLimMemQuasiNewtonUpdater.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLimMemQuasiNewtonUpdater.hpp", "filename": "IpLimMemQuasiNewtonUpdater.hpp", "file": "IpLimMemQuasiNewtonUpdater.hpp", "brief": "Limited-memory quasi-Newton Hessian approximation (L-BFGS/SR1)\n\nLimMemQuasiNewtonUpdater maintains a low-rank approximation of the\nLagrangian Hessian using limited-memory BFGS or SR1 updates.\n\nCompact representation: W = σI + V*M^(-1)*V^T where\n- σ: Scalar initialization factor (B0_ if diagonal)\n- V, U: Low-rank update matrices derived from S, Y history\n- S_: Matrix of step vectors s_k = x_{k+1} - x_k\n- Y_: Matrix of gradient differences y_k = ∇L_{k+1} - ∇L_k\n\nUpdate types (limited_memory_update_type_):\n- BFGS: Positive definite secant update, may skip if s'y <= 0\n- SR1: Symmetric rank-1, can capture negative curvature\n\nInitialization (limited_memory_initialization_):\n- SCALAR1-4: Various heuristics for σ based on y'y/s'y\n- CONSTANT: Fixed σ = limited_memory_init_val_\n\nKey parameters:\n- limited_memory_max_history_: Maximum stored (s,y) pairs\n- limited_memory_max_skipping_: Reset after N consecutive skips\n- sigma_safe_min/max_: Safeguards for initialization factor\n\nRestoration phase (update_for_resto_):\n- Structured update accounting for η*D_r*x quadratic term\n- Ypart_ stores constraint-only gradient differences", "algorithm": "Limited-Memory Quasi-Newton (L-BFGS/SR1):\nApproximates ∇²L using m most recent (s,y) pairs for O(mn) storage:\n1. Store s_k = x_{k+1} - x_k and y_k = ∇L_{k+1} - ∇L_k\n2. Build compact representation: B_k = σI + V·M⁻¹·Vᵀ\n3. Solve B_k·p = -g via two-loop recursion (BFGS) or Sherman-Morrison", "math": "BFGS secant condition: B_{k+1}·s_k = y_k (quasi-Newton equation)\nUpdate formula (compact form for m pairs):\n  B_k = σ_k·I + [Y_k  σS_k]·[D_k + σ·SᵀS_k   L_k ]⁻¹·[Y_k  σS_k]ᵀ\n                             [L_kᵀ           -σ·SᵀS_k]\nwhere D_k = diag(sᵢᵀyᵢ), L_k is strictly lower triangular with (L_k)_{ij} = sᵢᵀy_j.\n\nSR1 update: B_{k+1} = B_k + (y-Bs)(y-Bs)ᵀ / (y-Bs)ᵀs\nCan capture indefiniteness but may become singular.\n\nCurvature condition: sᵀy > 0 required for positive definiteness (BFGS).\nSkip update if violated; reset after limited_memory_max_skipping_ consecutive skips.", "complexity": "Per iteration: O(mn) for update, O(mn) for matrix-vector product.\nTwo-loop recursion: O(mn) for solving B·p = -g.\nStorage: O(mn) for m pairs in n-dimensional space.", "ref": ["Nocedal (1980). \"Updating Quasi-Newton Matrices with Limited Storage\".\n  Mathematics of Computation 35(151):773-782. [Original L-BFGS]", "Byrd et al. (1994). \"Representations of quasi-Newton matrices and\n  their use in limited memory methods\". Mathematical Programming 63:129-156.\n  [Compact representation used here]"], "see": ["IpHessianUpdater.hpp for base interface", "IpExactHessianUpdater.hpp for exact alternative", "IpLowRankUpdateSymMatrix.hpp for matrix representation"], "return": "true, if no update is to be performed this time.", "has_pass2": true}, "src/Algorithm/IpExactHessianUpdater.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpExactHessianUpdater.hpp", "filename": "IpExactHessianUpdater.hpp", "file": "IpExactHessianUpdater.hpp", "brief": "Hessian updater using exact second derivatives from NLP\n\nExactHessianUpdater is a trivial HessianUpdater implementation that\nsimply retrieves the exact Hessian of the Lagrangian from the NLP\nat each iteration.", "algorithm": "Exact Hessian Computation:\n  UpdateHessian() at each iteration:\n  1. Get current x from IpData::curr()->x().\n  2. Get current multipliers y_c, y_d from IpData::curr().\n  3. Call IpoptNLP::h(x, obj_factor, y_c, y_d) → W.\n  4. Store W in IpData for PDSystemSolver.\n  NLP caching handles redundant evaluation avoidance.", "math": "Hessian of Lagrangian:\n  W = σ·∇²f(x) + Σᵢ (y_c)ᵢ·∇²cᵢ(x) + Σⱼ (y_d)ⱼ·∇²dⱼ(x)\n  where σ = obj_factor (typically 1, or barrier-related in restoration).\n  Symmetric matrix, stored in lower triangular sparse format.", "complexity": "O(nnz_hessian) per evaluation.\n  Enables quadratic convergence: ||x_{k+1} - x*|| ≤ C·||x_k - x*||².\n  Quasi-Newton: only superlinear ||x_{k+1} - x*|| ≤ C·||x_k - x*||^{1.5-2}.\n\nThe Hessian is:\n  W = ∇²f(x) + Σ y_c[i] * ∇²c_i(x) + Σ y_d[j] * ∇²d_j(x)\n\nUpdateHessian() method:\n- Calls IpoptNLP::h() with current x and multipliers\n- Stores result in IpData for use by PDSystemSolver\n\nRequirements:\n- User NLP must provide eval_h callback (TNLP) or h() method\n- Option hessian_approximation must be \"exact\"\n\nAdvantages over quasi-Newton:\n- Quadratic local convergence rate\n- Accurate curvature for non-convex problems\n- Better inertia detection\n\nDisadvantages:\n- User must implement second derivatives\n- More function evaluations per iteration\n- Hessian evaluation can be expensive", "see": ["IpHessianUpdater.hpp for the base interface", "IpLimMemQuasiNewtonUpdater.hpp for L-BFGS alternative"], "has_pass2": true}, "src/Algorithm/IpProbingMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpProbingMuOracle.hpp", "filename": "IpProbingMuOracle.hpp", "file": "IpProbingMuOracle.hpp", "brief": "Mehrotra's probing heuristic for barrier parameter selection\n\nProbingMuOracle implements Mehrotra's predictor-corrector approach\nto compute the barrier parameter mu. This is the strategy used in\nmany interior point codes like LOQO and PCx.\n\nAlgorithm:\n1. Compute affine scaling direction (predictor step) with sigma=0\n2. Find maximum step to boundary (alpha_aff)\n3. Compute complementarity after affine step: mu_aff\n4. Compute centering parameter: sigma = (mu_aff/mu)^3\n5. Use sigma in corrector step\n\nThe centering parameter sigma controls the balance between:\n- sigma=0: Pure affine scaling (aggressive, may overshoot)\n- sigma=1: Pure centering (conservative, slow progress)\n\nKey parameter:\n- sigma_max_: Upper bound on sigma to prevent excessive centering", "algorithm": "Mehrotra Predictor-Corrector (Probing):\nAdaptively select centering parameter by probing pure Newton direction:\n1. Solve predictor (affine) system with μ=0 to get Δ_aff\n2. Compute α_aff = max step to boundary for affine direction\n3. Compute μ_aff = complementarity after affine step:\n   μ_aff = (x + α_aff·Δx_aff)ᵀ(z + α_aff·Δz_aff) / n\n4. Set centering: σ = (μ_aff / μ)³\n5. Solve corrector with μ_new = σ · μ_aff", "math": "Centering parameter derivation (Mehrotra):\n  σ = (μ_aff / μ)³\n\nIntuition:\n- Large α_aff → small μ_aff → small σ → aggressive\n- Small α_aff → large μ_aff → large σ → centering\n\nCube power: empirically good balance across problem classes.\nSafeguard: σ ← min(σ, σ_max) prevents excessive centering.", "complexity": "One affine solve O(n³) + O(n) for step length and μ_aff.\nTotal: similar to one Newton iteration.", "ref": ["Mehrotra (1992). \"On the Implementation of a Primal-Dual Interior\n  Point Method\". SIAM J. Optim. 2(4):575-601."], "see": ["IpMuOracle.hpp for base interface", "IpQualityFunctionMuOracle.hpp for alternative mu strategy"], "has_pass2": true}, "src/Algorithm/IpMonotoneMuUpdate.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpMonotoneMuUpdate.hpp", "filename": "IpMonotoneMuUpdate.hpp", "file": "IpMonotoneMuUpdate.hpp", "brief": "Standard monotone barrier parameter update strategy\n\nMonotoneMuUpdate implements the classical IPM approach where the\nbarrier parameter mu is reduced monotonically as the subproblem\nconverges.\n\nUpdate rule:\n1. Solve barrier subproblem to tolerance barrier_tol_factor_ * mu\n2. Reduce mu: new_mu = max(mu_target_, min(kappa_mu * mu, mu^theta_mu))\n   where kappa_mu = mu_linear_decrease_factor_\n   and theta_mu = mu_superlinear_decrease_power_\n3. Update tau (fraction-to-boundary): tau = max(tau_min_, 1 - mu)\n\nKey parameters:\n- mu_init_: Initial barrier parameter\n- mu_linear_decrease_factor_: Linear decrease factor kappa_mu\n- mu_superlinear_decrease_power_: Superlinear power theta_mu\n- tau_min_: Minimum fraction-to-boundary parameter\n- mu_target_: Target mu for termination\n\nFast decrease heuristic:\n- mu_allow_fast_monotone_decrease_: Allow faster decrease when\n  complementarity is already small\n\nInteractions:\n- Calls linesearch_->Reset() when mu changes\n- Filter is cleared on barrier parameter update", "algorithm": "Monotone Barrier Parameter Update (Classical IPM):\nStandard approach where μ decreases monotonically each outer iteration:\n1. Solve barrier subproblem: minimize f(x) - μ·Σlog(x_i) to tolerance ε(μ)\n2. When converged (E(μ) ≤ barrier_tol_factor·μ), reduce μ\n3. Repeat until μ < μ_target and KKT conditions satisfied", "math": "Barrier parameter update formula:\n  μ_{k+1} = max(μ_target, min(κ_μ · μ_k, μ_k^{θ_μ}))\nwhere κ_μ ∈ (0,1) is linear factor, θ_μ > 1 gives superlinear decrease.\n\nFraction-to-boundary parameter:\n  τ_k = max(τ_min, 1 - μ_k)\nensures iterates stay strictly positive: x_{k+1} ≥ (1-τ_k)·x_k > 0.", "complexity": "O(1) per update: simple formula evaluation after convergence\ncheck. The work is in the inner iterations (Newton steps per μ value).", "ref": ["Fiacco & McCormick (1968). \"Nonlinear Programming: Sequential\n  Unconstrained Minimization Techniques\". Wiley. [Original barrier method]", "Wright (1997). \"Primal-Dual Interior-Point Methods\". SIAM.\n  Chapter 6: Practical algorithms. [Modern IPM treatment]"], "see": ["IpMuUpdate.hpp for the base interface", "IpAdaptiveMuUpdate.hpp for non-monotone alternative"], "has_pass2": true}, "src/Algorithm/IpUserScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpUserScaling.hpp", "filename": "IpUserScaling.hpp", "file": "IpUserScaling.hpp", "brief": "NLP scaling using user-provided scaling factors\n\nUserScaling obtains scaling factors directly from the NLP interface\nvia the get_scaling_parameters callback (TNLP) or GetScalingParameters\nmethod (NLP).", "algorithm": "User-Provided Problem Scaling:\n  DetermineScalingParameters():\n  1. Call NLP::GetScalingParameters(obj_scaling, use_x, x_scaling, use_g, g_scaling).\n  2. If use_x_scaling: dx[i] = x_scaling[i] for variable scaling.\n  3. If use_g_scaling: dc[i], dd[j] from g_scaling for constraints.\n  4. df = obj_scaling for objective function.\n  5. If user returns false, use identity scaling (no change).", "math": "Scaling transformation:\n  Scaled problem: min df·f(D_x⁻¹·x̃) s.t. D_c·c(D_x⁻¹·x̃) = 0.\n  User provides: df (scalar), D_x = diag(dx), D_c = diag(dc), D_d = diag(dd).\n  Goal: make scaled gradients and constraints O(1) for numerical stability.\n  Heuristic: scale so ∂f/∂x̃ᵢ ≈ 1 and constraint residuals ≈ 1.", "complexity": "O(n + m) to apply user-provided factors.\n  User determines factors offline based on problem knowledge.\n\nThis allows users to specify problem-specific scaling based on:\n- Prior knowledge of variable magnitudes\n- Physical units of constraints\n- Problem structure\n\nScaling factors:\n- df: Objective function scaling\n- dx: Variable scaling (per-variable)\n- dc: Equality constraint scaling (per-constraint)\n- dd: Inequality constraint scaling (per-constraint)\n\nThe NLP should implement get_scaling_parameters() returning:\n- use_x_scaling, use_g_scaling flags\n- obj_scaling factor\n- x_scaling array (length n)\n- g_scaling array (length m)\n\nNote: If user doesn't provide scaling, falls back to no scaling.", "see": ["IpNLPScaling.hpp for scaling framework", "IpGradientScaling.hpp for automatic scaling"], "has_pass2": true}, "src/Algorithm/IpOrigIterationOutput.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpOrigIterationOutput.hpp", "filename": "IpOrigIterationOutput.hpp", "file": "IpOrigIterationOutput.hpp", "brief": "Standard iteration output for original NLP problem\n\nOrigIterationOutput displays the per-iteration summary line\nfor the original (non-restoration) NLP problem.", "algorithm": "Iteration Summary Output:\n  WriteOutput() at each iteration:\n  1. Check print frequency: skip if not due (by iter or time).\n  2. Format iteration data into fixed-width columns.\n  3. Append info string suffix if enabled (e.g., 'r' for resto).\n  4. Write to Journalist at J_ITERSUMMARY level.\n  First iteration writes header line with column names.", "math": "Iteration metrics displayed:\n  inf_pr = ||c(x)||∞ or ||r||₁/n (primal feasibility measure).\n  inf_du = ||∇L||∞ scaled (dual feasibility / optimality).\n  lg(mu) = log₁₀(μ) (barrier parameter in log scale).\n  lg(rg) = log₁₀(δx) (regularization for inertia control).\n  alpha_pr, alpha_du = primal/dual step lengths ∈ (0,1].", "complexity": "O(1) per iteration (just formatting and printing).\n  Controlled by print_frequency_iter and print_frequency_time options.\n\nOutput format (one line per iteration):\n  iter  objective  inf_pr  inf_du  lg(mu)  ||d||  lg(rg)  alpha_du  alpha_pr  ls\n\nWhere:\n- iter: Iteration number\n- objective: Current objective value (scaled)\n- inf_pr: Primal infeasibility (see inf_pr_output_)\n- inf_du: Dual infeasibility\n- lg(mu): Log10 of barrier parameter\n- ||d||: Norm of search direction\n- lg(rg): Log10 of regularization delta_x\n- alpha_du: Dual step size\n- alpha_pr: Primal step size\n- ls: Line search trials\n\nConfiguration:\n- print_info_string_: Append info character (e.g., 'r' for resto)\n- inf_pr_output_: What to show in inf_pr column\n- print_frequency_iter_: Print every N iterations\n- print_frequency_time_: Print every T seconds", "see": ["IpIterationOutput.hpp for base interface", "IpRestoIterationOutput.hpp for restoration phase output"], "has_pass2": true}, "src/Algorithm/IpRestoRestoPhase.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoRestoPhase.hpp", "filename": "IpRestoRestoPhase.hpp", "file": "IpRestoRestoPhase.hpp", "brief": "Recursive restoration for separable n,p variable initialization\n\nRestoRestorationPhase provides a specialized \"restoration within\nrestoration\" procedure for the MinC_1NrmRestorationPhase. It computes\noptimal values for the slack variables (n_c, p_c, n_d, p_d) by\ntreating them as separable from x and s.", "algorithm": "Separable Slack Optimization:\n  Given current x (fixed), find optimal n, p for restoration problem:\n  1. Compute constraint residuals: r_c = c(x), r_d = d(x) - s.\n  2. For each component i, solve: v² + 2aᵢ·v - bᵢ = 0\n     where aᵢ, bᵢ derived from residual and complementarity target.\n  3. Extract n, p from solution: v = p - n, use sign of residual.\n  4. Ensures n ≥ 0, p ≥ 0 with n·p close to target complementarity.", "math": "Quadratic subproblem:\n  For fixed x: min_{n,p≥0} ρ(||n||₁ + ||p||₁) + barrier(n,p)\n  s.t. c(x) + n - p = 0.\n  Separable → each component independent quadratic in v = p - n.\n  Closed-form: v = -a + √(a² + b) (positive root).", "complexity": "O(m) for m constraints, no linear solve needed.\n  Much cheaper than full restoration iteration for slack initialization.\n\nThe restoration feasibility problem has structure:\n  min ||p + n||_1  s.t.  c(x) + n - p = 0\n\nFor fixed x, optimal n and p can be computed by solving a quadratic\nequation v^2 + 2a*v - b = 0 element-wise, where:\n- a relates to constraint values\n- b relates to complementarity conditions\n\nThis avoids full nested optimization when only n,p need updating.", "see": ["IpRestoMinC_1Nrm.hpp for main restoration phase", "IpRestoPhase.hpp for base restoration interface"], "has_pass2": true}, "src/Algorithm/IpIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIterateInitializer.hpp", "filename": "IpIterateInitializer.hpp", "file": "IpIterateInitializer.hpp", "brief": "Strategy interface for computing initial iterates\n\nIterateInitializer is the abstract base for strategies that\ncompute the starting point (x, s, y_c, y_d, z_L, z_U, v_L, v_U)\nfor the interior point algorithm.", "algorithm": "Starting Point Initialization:\n  1. Primal variables x:\n     - Use user-provided x₀ if available.\n     - Project to bounds: x ← max(x_L + κ, min(x, x_U - κ)).\n     - Push away from bounds by κ = bound_push·max(1, |x_L|).\n  2. Slack variables s: s ← d(x), projected to [d_L + κ, d_U - κ].\n  3. Dual variables y_c, y_d:\n     - Least squares: min ||∇f - J^T y||² to estimate multipliers.\n     - Or use user-provided values if warm starting.\n  4. Bound multipliers z_L, z_U, v_L, v_U:\n     - From complementarity: z_L = μ/(x - x_L), z_U = μ/(x_U - x).\n     - Clamp to [bound_mult_init_val, ∞) for stability.", "math": "Initial complementarity products:\n  Want (x - x_L)·z_L ≈ μ, so z_L = μ / (x - x_L).\n  Initial μ from average: μ₀ = (x - x_L)^T z_L / n_bounds.\n  Slack complementarity: (s - d_L)·v_L ≈ μ similarly.", "complexity": "O(n + m) for cold start (vector operations).\n  O(m²) for least-squares dual initialization if done explicitly.\n  Warm start: O(n + m) using provided values directly.", "see": ["IpDefaultIterateInitializer.hpp for standard initialization", "IpWarmStartIterateInitializer.hpp for warm starts"], "has_pass2": true}, "src/Algorithm/IpMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpMuOracle.hpp", "filename": "IpMuOracle.hpp", "file": "IpMuOracle.hpp", "brief": "Strategy interface for suggesting barrier parameter values\n\nMuOracle is the abstract interface for components that compute\nsuggested values for the barrier parameter mu in adaptive (non-monotone)\nbarrier updates.", "algorithm": "Barrier Parameter Oracle Interface:\n  CalculateMu(mu_min, mu_max) → new_mu ∈ [mu_min, mu_max]:\n  1. Analyze current iterate: complementarity gap, progress, quality.\n  2. Compute target mu balancing centrality vs progress toward optimum.\n  3. Clamp to [mu_min, mu_max] to respect globalization bounds.\n  4. Return false if computation fails (e.g., linear solve needed).", "math": "Adaptive vs monotone barrier strategies:\n  Monotone: μ_{k+1} = σ·μ_k with fixed σ < 1 (predictable decrease).\n  Adaptive: μ chosen per-iteration to balance feasibility and optimality.\n  Quality function oracles: min_σ q(σ) where q measures solution quality.\n  Probing oracles: try μ candidates, pick best by predicted progress.", "complexity": "O(linear_solve) per oracle call for predictor-based methods.\n  Simpler oracles (LOQO-style) are O(1) using only current complementarity.\n\nInterface:\n- CalculateMu(mu_min, mu_max, new_mu): Compute suggested mu in [mu_min, mu_max]\n- Returns false if suggestion cannot be computed (e.g., linear solve fails)\n\nImplementations:\n- QualityFunctionMuOracle: Minimize quality function over sigma\n- ProbingMuOracle: Try candidate mu values and select best\n- LoqoMuOracle: LOQO-style adaptive rule\n\nUsage in AdaptiveMuUpdate:\n- free_mu_oracle_: Computes mu in free (non-monotone) mode\n- fix_mu_oracle_: Optional, computes mu when switching to fixed mode\n\nThe oracle is called each iteration in free mode. If it returns\nfalse or the suggested mu doesn't satisfy globalization, the\nalgorithm may switch to fixed (monotone) mode.", "see": ["IpAdaptiveMuUpdate.hpp for usage", "IpQualityFunctionMuOracle.hpp for quality function approach"], "has_pass2": true}, "src/Algorithm/IpRestoIpoptNLP.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoIpoptNLP.hpp", "filename": "IpRestoIpoptNLP.hpp", "file": "IpRestoIpoptNLP.hpp", "brief": "IpoptNLP adapter for restoration phase feasibility problem\n\nRestoIpoptNLP transforms the original NLP into a feasibility problem\nfor the restoration phase. It introduces slack variables to allow\nconstraint violations and penalizes them in the objective.", "algorithm": "Restoration Phase Problem Formulation:\n  Original: min f(x) s.t. c(x)=0, d_L ≤ d(x) ≤ d_U.\n  Restoration: min ρ·(||p||₁ + ||n||₁) + (η/2)·||D_r(x-x_ref)||₂²\n               s.t. c(x) - p_c + n_c = 0, d(x) - p_d + n_d ∈ [d_L, d_U]\n                    p_c, n_c, p_d, n_d ≥ 0.\n  Where: ρ = resto_penalty_parameter (default 1000).\n         η = η_factor·μ^η_exp (proximity weight, depends on barrier μ).\n         D_r = diag(1/max(1, |x_ref_i|)) for scaling.", "math": "Restoration NLP structure:\n  Variables: (x, p_c, n_c, p_d, n_d) ∈ ℝⁿ × ℝ₊^{m_c} × ℝ₊^{m_c} × ...\n  Gradient: ∇f = (η·D_r²·(x-x_ref), ρ·e, ρ·e, ρ·e, ρ·e).\n  Jacobian: Block structure [J_c, -I, I, 0, 0; J_d, 0, 0, -I, I].\n  Hessian: Block diag(W + η·D_r², 0, 0, 0, 0).\n\nVariable structure (CompoundVector):\n- x: Original primal variables\n- p_c, n_c: Positive/negative slacks for equality constraints\n- p_d, n_d: Positive/negative slacks for inequality constraints\n\nObjective function:\n  f_resto = ρ * (e'p_c + e'n_c + e'p_d + e'n_d)\n          + (η/2) * ||D_r(x - x_ref)||_2^2\nWhere η = η_factor * μ^η_mu_exponent (mu-dependent)\n\nModified constraints:\n- c(x) - p_c + n_c = 0  (equality)\n- d(x) - p_d + n_d (bounded by d_L, d_U)\n\nMatrix structure:\n- Jacobians: Block structure with original Jac and identity blocks\n- Hessian: Original Hessian + diagonal D_r^2 for proximity term\n\nKey methods:\n- f(), grad_f(): Depend on mu (objective_depends_on_mu() = true)\n- Eta(): Computes proximity weight from current mu\n- DR_x(): Scaling factors 1/max(1, |x_ref_i|)", "see": ["IpRestoMinC_1Nrm.hpp for the restoration phase algorithm", "IpIpoptNLP.hpp for the base interface"], "has_pass2": true}, "src/Algorithm/IpFilter.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpFilter.hpp", "filename": "IpFilter.hpp", "file": "IpFilter.hpp", "brief": "Multi-dimensional filter for globalization (Wächter-Biegler)\n\nThe Filter implements the filter globalization mechanism where a\npoint is acceptable if it improves in at least one of multiple\nobjectives (typically constraint violation θ and objective φ).\n\nFilter acceptance rule: A point (θ, φ) is acceptable to the filter\nif for every entry (θ_i, φ_i) in the filter, either θ < θ_i or φ < φ_i.\n\nKey concepts:\n- FilterEntry: Single entry storing coordinates and iteration number\n- Dominated entries are removed when new entries are added\n- Margin parameters (gamma_theta, gamma_phi) provide sufficient decrease\n\nClasses:\n- FilterEntry: Single point in the filter with coordinates\n- Filter: Container managing list of entries with add/query operations\n\nUsed by FilterLSAcceptor for filter line search acceptance tests.", "algorithm": "Filter Globalization (Wächter-Biegler):\nMulti-objective acceptance mechanism replacing merit function:\n1. Maintain set F of pairs (θ, φ) - constraint violation and objective\n2. Acceptable(θ,φ): return true if ∃(θ_i,φ_i)∈F: θ < θ_i OR φ < φ_i\n3. AddEntry(θ,φ): add (θ-γ_θ·θ, φ-γ_φ·φ) and remove dominated entries\n4. Dominated: entry (θ_i,φ_i) dominated if θ ≤ θ_i AND φ ≤ φ_i", "math": "Filter acceptance test (n-dimensional generalization):\nPoint v = (v₁,...,vₙ) acceptable to filter F iff:\n  ∀ e = (e₁,...,eₙ) ∈ F: ∃j: vⱼ ≤ eⱼ\n\nSufficient decrease via margins:\nAdd entry (θ - γ_θ·θ, φ - γ_φ·φ) rather than (θ, φ)\nwhere γ_θ, γ_φ > 0 are margin parameters.\n\nDominance pruning:\nEntry e dominated by v if vⱼ ≤ eⱼ for all j (remove e from filter).", "complexity": "Acceptable: O(|F|·d) for d-dimensional filter.\nAddEntry: O(|F|·d) for dominance check and removal.\nFilter size typically O(iterations) but bounded by dominated pruning.", "ref": ["Fletcher & Leyffer (2002). \"Nonlinear programming without a\n  penalty function\". Mathematical Programming 91(2):239-269.", "Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Section 2.3: Filter]"], "see": ["IpFilterLSAcceptor.hpp for filter line search usage", "IpBacktrackingLineSearch.hpp for the line search framework"], "return": "true, if pair is acceptable", "has_pass2": true}, "src/Algorithm/IpIpoptNLP.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptNLP.hpp", "filename": "IpIpoptNLP.hpp", "file": "IpIpoptNLP.hpp", "brief": "Internal NLP interface with caching and scaling\n\nIpoptNLP abstracts the optimization problem for internal use:\n  min f(x)  s.t.  c(x) = 0, d_L <= d(x) <= d_U, x_L <= x <= x_U", "algorithm": "NLP Evaluation with Caching:\n  Cache f(x), ∇f(x), c(x), J_c(x), d(x), J_d(x), H(x,λ) keyed by x.\n  Invalidate on x change. Recompute only when needed.\n  Scaling applied transparently: f̃ = s_f·f, c̃ = D_c·c, etc.\n  Expansion matrices map variable bounds to full space.", "math": "Internal NLP formulation:\n  Variables: x ∈ ℝⁿ (primal), s ∈ ℝᵐᵈ (slacks for inequalities).\n  Equality constraints: c(x) = 0 (m_c constraints).\n  Inequality constraints: d(x) - s = 0 with d_L ≤ s ≤ d_U.\n  Bounds: x_L ≤ x ≤ x_U (handled via multipliers z_L, z_U).\n  Hessian: ∇²_xx L = ∇²f + Σᵢ yc_i·∇²c_i + Σⱼ yd_j·∇²d_j.", "complexity": "Function evals: O(user-defined).\n  Jacobian sparsity: typically O(nnz) << O(n·m).\n  Caching avoids redundant evaluations within same iteration.\n\nProvides:\n- f(), grad_f(), c(), jac_c(), d(), jac_d(), h() - NLP evaluations\n- Bounds with expansion matrices Px_L, Px_U, Pd_L, Pd_U\n- Scaling via NLPScalingObject", "see": ["IpOrigIpoptNLP.hpp for standard implementation", "IpTNLPAdapter.hpp for user interface adapter", "IpNLPScaling.hpp for scaling strategies"], "has_pass2": true}, "src/Algorithm/IpConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpConvCheck.hpp", "filename": "IpConvCheck.hpp", "file": "IpConvCheck.hpp", "brief": "Strategy interface for checking algorithm termination\n\nConvergenceCheck is the abstract base for convergence testing\nstrategies. Called each iteration to determine if optimization\nshould continue, has converged, or has failed.", "algorithm": "Convergence Testing for Interior Point Methods:\n  Check scaled optimality conditions at each iteration:\n  1. Primal feasibility: ||c(x)||∞ / s_c ≤ tol_feas.\n  2. Dual feasibility: ||∇f - J^T y - z||∞ / s_d ≤ tol_dual.\n  3. Complementarity: ||XZe||∞ / s_c ≤ tol_compl.\n  4. Scaling: s_c = max(1, ||c||), s_d = max(1, ||∇f||, ||y||, ||z||).\n  Acceptable point: same conditions with looser acceptable_tol.\n  After n_acceptable iterations at acceptable level → stop.", "math": "Optimality conditions (KKT) being checked:\n  ∇f(x) - J_c^T y_c - J_d^T y_d - z_L + z_U = 0 (stationarity)\n  c(x) = 0 (equality constraints)\n  d_L ≤ d(x) ≤ d_U (inequality constraints)\n  x_L ≤ x ≤ x_U, z_L ≥ 0, z_U ≤ 0 (bounds + sign)\n  (x - x_L)·z_L = 0, (x_U - x)·z_U = 0 (complementarity)", "complexity": "O(n + m) per convergence check (norms of vectors).\n  Called once per iteration, negligible vs. linear solve cost.", "see": ["IpOptErrorConvCheck.hpp for the main implementation", "IpRestoConvCheck.hpp for restoration phase convergence"], "has_pass2": true}, "src/Algorithm/IpWarmStartIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpWarmStartIterateInitializer.hpp", "filename": "IpWarmStartIterateInitializer.hpp", "file": "IpWarmStartIterateInitializer.hpp", "brief": "Warm start initialization from previous solution\n\nWarmStartIterateInitializer initializes IPM iterates from a\npreviously computed solution, enabling faster convergence for\nrelated problems (e.g., MPC, parametric optimization).\n\nWarm start sources:\n- warm_start_entire_iterate_: Use GetWarmStartIterate() from NLP\n- Otherwise: Use initialization vectors from NLP\n\nProcessing steps:\n1. Push primals away from bounds (warm_start_bound_push/frac_)\n2. Push slacks (warm_start_slack_bound_push/frac_)\n3. Clip multipliers (warm_start_mult_init_max_)\n4. Ensure bound multipliers positive (warm_start_mult_bound_push_)\n\nTarget mu adjustment (warm_start_target_mu_):\n- Adjusts slack/multiplier pairs toward target complementarity\n- process_target_mu(): Scales to achieve s*z ≈ target_mu\n- adapt_to_target_mu(): Fine-tunes pairing\n\nKey parameters:\n- warm_start_bound_push_: Absolute bound push\n- warm_start_bound_frac_: Relative bound push\n- warm_start_mult_init_max_: Maximum multiplier magnitude\n- warm_start_target_mu_: Target barrier parameter", "algorithm": "IPM Warm Start Initialization:\nInitialize from previous solution for faster convergence:\n1. Load iterate from GetWarmStartIterate() or stored vectors\n2. Push primals from bounds: x ← max(x_L + κ, min(x, x_U - κ))\n   where κ = max(warm_start_bound_push, frac·max(1, |x|))\n3. Push slacks similarly\n4. Clip multipliers: y ← sign(y)·min(|y|, mult_init_max)\n5. Ensure bound multipliers positive: z ← max(z, mult_bound_push)\n6. If target_mu set, adjust (s,z) pairs to achieve s·z ≈ μ_target:\n   a. Scale: (s,z) ← √(μ_target/(s·z))·(s,z)\n   b. Fine-tune via adapt_to_target_mu()", "math": "Target μ adjustment (process_target_mu):\nGiven (s_i, z_i) with s_i·z_i ≠ μ_target:\n  ratio = √(μ_target / (s_i·z_i))\n  s_i ← ratio·s_i,  z_i ← ratio·z_i\nPreserves sign while achieving s_i·z_i = μ_target.", "complexity": "O(n + m) for variable processing. No linear solves.\nWarm start typically reduces iterations by 50-90% for related problems.", "see": ["IpIterateInitializer.hpp for the base interface", "IpDefaultIterateInitializer.hpp for cold start"], "has_pass2": true}, "src/Algorithm/IpEqMultCalculator.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpEqMultCalculator.hpp", "filename": "IpEqMultCalculator.hpp", "file": "IpEqMultCalculator.hpp", "brief": "Strategy interface for computing equality constraint multipliers\n\nEqMultiplierCalculator is the abstract base for computing estimates\nof the equality constraint multipliers y_c and y_d. These estimates\nare used for:", "algorithm": "Multiplier Estimation Interface:\n  CalculateMultipliers(y_c, y_d):\n  1. Compute multiplier estimates satisfying dual feasibility.\n  2. Implementations: LeastSquareMultipliers solves:\n     min ||y||² s.t. J^T·y = -∇f (dual feasibility).\n  3. Used for: initialization, recalc_y option, feasibility problems.\n  4. Return false if singular (e.g., rank-deficient Jacobian).", "math": "Least-squares multipliers:\n  Solve: [W, J^T; J, 0] [d; y] = [-∇f; 0] with W=0 (simplified).\n  Gives: y = -(J·J^T)^{-1}·J·∇f (minimum norm dual estimate).\n- Initial multiplier estimation in DefaultIterateInitializer\n- Multiplier recalculation when recalc_y option is set\n- Computing multipliers for feasibility problems\n\nImplementations:\n- LeastSquareMultipliers: Solves min ||y||^2 s.t. KKT conditions\n  (requires solving augmented system)", "see": ["IpLeastSquareMults.hpp for the main implementation", "IpDefaultIterateInitializer.hpp for usage in initialization", "IpIpoptAlg.hpp for recalc_y usage"], "has_pass2": true}, "src/Algorithm/IpLowRankSSAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLowRankSSAugSystemSolver.hpp", "filename": "IpLowRankSSAugSystemSolver.hpp", "file": "IpLowRankSSAugSystemSolver.hpp", "brief": "Low-rank Hessian handling via single backsolve (iterative solver friendly)\n\nLowRankSSAugSystemSolver handles LowRankUpdateSymMatrix Hessians\n(from L-BFGS) by augmenting the system rather than using\nSherman-Morrison. This requires only ONE factorization/backsolve.", "algorithm": "Single-Solve Low-Rank System Augmentation:\n  Solve() handles W = σI + V·M·V^T without multiple backsolves:\n  1. L-BFGS gives W = σI + V·M·V^T (compact representation).\n  2. Instead of Sherman-Morrison (k backsolves for rank-k update):\n  3. Augment KKT: J_c_ext = [J_c; V^T], add k extra \"constraint\" rows.\n  4. Extended system: [σI, J^T, V; J, -D, 0; V^T, 0, M⁻¹] [Δx; y; q] = [r].\n  5. Single factorization of larger (n+m+k) system.\n  6. Advantage: Iterative solvers like GMRES prefer one larger system.", "math": "System equivalence:\n  Sherman-Morrison: (A + VCV^T)⁻¹ = A⁻¹ - A⁻¹V(C⁻¹+V^TA⁻¹V)⁻¹V^TA⁻¹.\n  Augmented form: Same solution, but via single linear solve.\n  Trade-off: Larger system vs multiple backsolves.\n\nL-BFGS Hessian: W = sigma*I + V*M*V^T (compact representation)\n\nStandard approach (LowRankAugSystemSolver):\n- Uses Sherman-Morrison formula\n- Requires multiple backsolves (one per rank)\n- Better for direct solvers\n\nThis approach (LowRankSSAugSystemSolver):\n- Augments the KKT system with V columns\n- Extends J_c with ExpandedMultiVectorMatrix rows\n- Single larger factorization\n- Better for iterative solvers\n\nSystem augmentation:\n- J_c_ext_ = [J_c; V^T] (extended constraint Jacobian)\n- D_c_ext_ includes regularization for V rows\n- y_c_ext_space_ for extended multiplier space", "see": ["IpLowRankAugSystemSolver.hpp for Sherman-Morrison version", "IpLimMemQuasiNewtonUpdater.hpp for L-BFGS Hessian", "ProvidesInertia)."], "return": "the number of negative eigenvalues of the most recent factorized matrix", "has_pass2": true}, "src/Algorithm/IpLoqoMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLoqoMuOracle.hpp", "filename": "IpLoqoMuOracle.hpp", "file": "IpLoqoMuOracle.hpp", "brief": "LOQO formula for barrier parameter selection\n\nLoqoMuOracle computes the barrier parameter using the heuristic\nfrom the LOQO solver (Vanderbei). This provides a simple formula\nbased on current complementarity.\n\nThe LOQO formula typically uses:\n  sigma = min(0.1, 100*mu)\n  mu_new = sigma * (current_complementarity / n)\n\nThis is a simple, stateless oracle that doesn't require solving\nan additional linear system (unlike probing or quality function).\n\nCompared to other strategies:\n- ProbingMuOracle: Requires affine step computation\n- QualityFunctionMuOracle: Requires 1D optimization\n- LoqoMuOracle: Direct formula, no extra computation", "algorithm": "LOQO Barrier Parameter Formula:\nSimple centering strategy based on current complementarity:\n1. Compute average complementarity: μ̄ = (xᵀz + sᵀv) / n\n2. Compute centering parameter: σ = min(0.1, 100·μ)\n3. Return: μ_new = σ · μ̄ (scaled complementarity)", "math": "LOQO centering heuristic:\n  σ = min(σ_max, factor·μ)\n  μ_new = σ · μ̄ = σ · (xᵀz + sᵀv) / n\n\nwhere σ_max limits centering to prevent excessively slow progress,\nand n is the total number of complementarity pairs.\n\nIntuition: Small μ → aggressive (σ small), large μ → centered (σ larger).", "complexity": "O(n) for computing complementarity sum. No linear solves needed.", "ref": ["Vanderbei (1999). \"LOQO: An interior point code for quadratic\n  programming\". Optimization Methods and Software 11(1-4):451-484."], "see": ["IpMuOracle.hpp for base interface", "IpProbingMuOracle.hpp for Mehrotra's predictor-corrector"], "has_pass2": true}, "src/Algorithm/IpPDSearchDirCalc.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDSearchDirCalc.hpp", "filename": "IpPDSearchDirCalc.hpp", "file": "IpPDSearchDirCalc.hpp", "brief": "Newton search direction computation via primal-dual system\n\nPDSearchDirCalculator is the standard SearchDirectionCalculator\nimplementation that computes the Newton step by solving the\nprimal-dual KKT system.", "algorithm": "Primal-Dual Newton Direction:\n  ComputeSearchDirection():\n  1. Assemble RHS from current residuals: r = (r_x, r_s, r_c, r_d, r_zL, ...).\n  2. Call PDSystemSolver::Solve(KKT, rhs) → delta.\n  3. Store result: IpData::delta() = (Δx, Δs, Δy_c, Δy_d, Δz_L, ...).\n  4. If mehrotra_algorithm_: Use predictor-corrector centering.\n  5. If fast_step_computation_: Skip residual verification.", "math": "KKT system solved:\n  [W + Σ,  0, J_c^T, J_d^T] [Δx  ]   [r_x]\n  [  0,    Σ_s,  0,   -I  ] [Δs  ] = [r_s]\n  [ J_c,   0,    0,    0  ] [Δy_c]   [r_c]\n  [ J_d,  -I,    0,    0  ] [Δy_d]   [r_d]\n  Where Σ, Σ_s are barrier/bound terms.\n\nComputeSearchDirection():\n1. Assembles right-hand side from current residuals\n2. Calls PDSystemSolver::Solve() to get direction\n3. Stores result in IpData::delta()\n\nAlgorithm options:\n- fast_step_computation_: Skip residual verification (trust solver)\n- mehrotra_algorithm_: Mehrotra predictor-corrector mode\n  - Forces specific settings for adaptive mu\n  - Disables various globalization features\n\nThe PDSolver() accessor allows other components (e.g., second-order\ncorrection in line search) to compute additional Newton solves.\n\nDirection stored in delta:\n- delta_x, delta_s: Primal steps\n- delta_y_c, delta_y_d: Equality multiplier steps\n- delta_z_L, delta_z_U, delta_v_L, delta_v_U: Bound multiplier steps", "see": ["IpSearchDirCalculator.hpp for base interface", "IpPDSystemSolver.hpp for the system solver interface", "IpPDFullSpaceSolver.hpp for full-space implementation"], "has_pass2": true}, "src/Algorithm/IpGradientScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpGradientScaling.hpp", "filename": "IpGradientScaling.hpp", "file": "IpGradientScaling.hpp", "brief": "NLP scaling based on gradient magnitudes at initial point\n\nGradientScaling computes scaling factors for the NLP based on the\nmaximum gradient norms at the user-provided starting point. This\nimproves problem conditioning by normalizing objective and constraint\nmagnitudes.", "algorithm": "Gradient-Based Automatic Scaling:\n  DetermineScalingParametersImpl() at x₀:\n  1. Evaluate ∇f(x₀), J_c(x₀), J_d(x₀) at starting point.\n  2. For objective: df = min(1, target_grad / ||∇f||∞).\n  3. For each constraint i: dc[i] = min(1, target_grad / ||J_c[i,:]||∞).\n  4. Apply minimum: df, dc, dd ≥ scaling_min_value.\n  5. Skip if ||∇f||∞ < scaling_max_gradient (already well-scaled).", "math": "Scaling rationale:\n  Goal: ||∇f̃||∞ ≈ target_grad, ||J̃[i,:]||∞ ≈ target_grad.\n  Makes Newton step components comparable magnitude.\n  Improves convergence for poorly scaled problems.\n\nScaling computation:\n- Evaluate gradients at x0\n- s_f = 1 / max(1, ||∇f||_∞ / target_grad)\n- s_c[i] = 1 / max(1, ||∇c_i||_∞ / target_grad)\n- s_d[i] = 1 / max(1, ||∇d_i||_∞ / target_grad)\n\nKey parameters:\n- scaling_max_gradient_: Skip scaling if gradients below this\n- scaling_obj_target_gradient_: Target norm for objective gradient\n- scaling_constr_target_gradient_: Target norm for constraint gradients\n- scaling_min_value_: Lower bound on scaling factors\n\nNote: Variable scaling (d_x) is not computed by this class;\nuses identity scaling for x.", "see": ["IpNLPScaling.hpp for the scaling framework", "IpEquilibrationScaling.hpp for MC19-based alternative"], "has_pass2": true}, "src/Algorithm/IpIpoptCalculatedQuantities.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptCalculatedQuantities.hpp", "filename": "IpIpoptCalculatedQuantities.hpp", "file": "IpIpoptCalculatedQuantities.hpp", "brief": "Cached computation of all derived quantities from iterates\n\nIpoptCalculatedQuantities is the central caching layer for computed\nvalues derived from the current/trial iterates in IpoptData:\n\nCached quantities include:\n- Slacks: slack_x_L, slack_x_U, slack_s_L, slack_s_U\n- Objective: f, grad_f, barrier_obj, grad_barrier_obj\n- Constraints: c, d, d-s, jac_c, jac_d, J^T*y products\n- Primal-dual: grad_lag_x/s, complementarity, relaxed_compl\n- Errors: primal/dual infeasibility, complementarity, nlp_error\n- Line search: frac_to_bound, sigma matrices", "algorithm": "Memoization Pattern for NLP Evaluation Caching:\n  Each computed quantity is cached with dependency tracking:\n  1. Check if inputs (x, s, y, z, μ) have changed via TaggedObject tags.\n  2. If unchanged, return cached result immediately.\n  3. If changed, recompute, cache result, update dependency tags.\n  Curr/Trial duality: curr_* uses IpData.curr(), trial_* uses IpData.trial().\n  Dependencies propagate: grad_lag depends on grad_f, jac_c, jac_d.", "math": "Key computed quantities for IPM:\n  Slacks: slack_x_L = x - x_L, slack_x_U = x_U - x (distance to bounds).\n  Barrier objective: φ_μ(x) = f(x) - μ·Σlog(slack_i).\n  Gradient of Lagrangian: ∇_x L = ∇f - J_c^T y_c - J_d^T y_d - z_L + z_U.\n  Complementarity: compl_x_L = slack_x_L ⊙ z_L (should → μe).\n  Primal infeasibility: ||c(x)||, ||d(x) - s||.\n  Dual infeasibility: ||∇_x L||, ||∇_s L||.\n  Fraction-to-boundary: max α s.t. x + α·Δx ≥ (1-τ)·x_L.", "complexity": "O(n+m) per quantity computation (vector operations).\n  Caching reduces redundant NLP evaluations within each iteration.\n  Typical iteration accesses same quantity multiple times (search dir,\n  line search, convergence check) → caching gives 3-10x speedup.", "see": ["IpIpoptData.hpp for the raw iterate storage", "IpIpoptNLP.hpp for NLP function evaluations", "IpCachedResults.hpp for the caching mechanism"], "return": "number of corrected slacks", "has_pass2": true}, "src/Algorithm/IpHessianUpdater.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpHessianUpdater.hpp", "filename": "IpHessianUpdater.hpp", "file": "IpHessianUpdater.hpp", "brief": "Strategy interface for Hessian computation/approximation\n\nHessianUpdater is the abstract base for strategies that provide\nthe Hessian of the Lagrangian (or an approximation) to the algorithm.\nThe result is stored in IpData.W().", "algorithm": "Hessian Approximation Strategies:\n  Exact: W = ∇²_xx L(x, y) evaluated via NLP second derivatives.\n  L-BFGS: W ≈ D + Σ (yᵢyᵢᵀ/yᵢᵀsᵢ - Bsᵢsᵢᵀ/sᵢᵀBsᵢ) (secant updates).\n  L-SR1: W ≈ D + Σ ((y-Bs)(y-Bs)ᵀ/(y-Bs)ᵀs) (symmetric rank-1).\n  Store m recent {sₖ, yₖ} pairs where sₖ = xₖ₊₁ - xₖ, yₖ = ∇Lₖ₊₁ - ∇Lₖ.\n  Limited memory: O(m·n) storage vs O(n²) for full Hessian.", "math": "Hessian of Lagrangian:\n  L(x,y) = f(x) - y_c^T c(x) - y_d^T d(x) (Lagrangian function).\n  W = ∇²f(x) - Σᵢ yc_i·∇²c_i(x) - Σⱼ yd_j·∇²d_j(x).\n  Secant equation: B_{k+1}·s_k = y_k (quasi-Newton condition).\n  Positive definiteness: BFGS maintains if y^T s > 0; damped BFGS if not.", "complexity": "Exact: O(eval_hess) user-provided, often O(nnz_H).\n  L-BFGS/L-SR1: O(m·n) per update, O(m·n) per matvec via two-loop.\n  Trade-off: exact more accurate, L-BFGS cheaper when n >> m pairs.", "see": ["IpExactHessianUpdater.hpp for exact Hessian", "IpLimMemQuasiNewtonUpdater.hpp for quasi-Newton", "IpLowRankUpdateSymMatrix.hpp for L-BFGS matrix representation"], "has_pass2": true}, "src/Algorithm/IpRestoMinC_1Nrm.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoMinC_1Nrm.hpp", "filename": "IpRestoMinC_1Nrm.hpp", "file": "IpRestoMinC_1Nrm.hpp", "brief": "Restoration phase minimizing 1-norm of constraint violation\n\nMinC_1NrmRestorationPhase is the main restoration phase implementation.\nWhen the line search cannot make progress, it minimizes constraint\nviolation to find a feasible point from which optimization can continue.\n\nRestoration NLP formulation:\n  min  ρ * ||[p_c; n_c; p_d; n_d]||_1 + (η/2) * ||D_r(x - x_ref)||_2^2\n  s.t. c(x) - p_c + n_c = 0\n       d_L <= d(x) - p_d + n_d <= d_U\n       x_L <= x <= x_U\n       p_c, n_c, p_d, n_d >= 0\n\nWhere:\n- ρ: Penalty on infeasibility (resto_penalty_parameter)\n- η: Proximity weight (resto_proximity_weight * sqrt(mu))\n- D_r: Diagonal scaling based on reference point\n- x_ref: Starting point for restoration\n\nKey behaviors:\n- Uses nested IpoptAlgorithm to solve restoration NLP\n- eq_mult_calculator_ reinitializes multipliers after restoration\n- bound_mult_reset_threshold_: Limits post-restoration bound multipliers\n- count_restorations_: Tracks restoration phase calls", "algorithm": "Restoration Phase (ℓ₁ Feasibility Minimization):\nSolve modified NLP to find feasible point when main algorithm stuck:\n1. Formulate restoration NLP (RestoIpoptNLP):\n   - Add slack pairs (p,n) to each constraint: c(x) = p - n\n   - Minimize ρ·(Σp + Σn) (penalized infeasibility)\n   - Add proximity term η·||D(x - x_ref)||² (stay near reference)\n2. Run nested Ipopt on restoration NLP:\n   - Uses its own filter, barrier parameter, line search\n   - Converges when infeasibility sufficiently reduced\n3. Post-processing:\n   - Reset constraint multipliers via least-squares (eq_mult_calculator_)\n   - Cap bound multipliers at threshold to prevent blow-up\n   - Return to main algorithm with new feasible iterate", "math": "Restoration NLP (smooth ℓ₁ via slack decomposition):\n  min_{x,p,n} ρ·eᵀ(p+n) + (ζ/2)·||D_R·(x - x_R)||²\n  s.t. c(x) - p + n = 0\n       x_L ≤ x ≤ x_U\n       p ≥ 0, n ≥ 0\n\nwhere D_R = diag(min(1, 1/|x_R|)) provides scaling,\nρ penalizes constraint violation, ζ prevents wandering.\n\nℓ₁ equivalence: At solution, either p_i=0 or n_i=0 for each i,\nso ||p+n||₁ = ||c(x)||₁ at optimum (complementarity).", "complexity": "Same as main Ipopt: O(n³) per iteration for linear algebra.\nRestoration may require many iterations; worst case certifies infeasibility.", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Section 3.3: Restoration phase]"], "see": ["IpRestoPhase.hpp for the base interface", "IpRestoIpoptNLP.hpp for the restoration NLP formulation", "IpRestoFilterConvCheck.hpp for restoration convergence criteria"], "has_pass2": true}, "src/Algorithm/IpBacktrackingLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpBacktrackingLSAcceptor.hpp", "filename": "IpBacktrackingLSAcceptor.hpp", "file": "IpBacktrackingLSAcceptor.hpp", "brief": "Strategy interface for step acceptance in backtracking line search\n\nBacktrackingLSAcceptor defines the interface for step acceptance\ntests used by BacktrackingLineSearch. Implementations decide whether\na trial point should be accepted based on various criteria.", "algorithm": "Backtracking Step Acceptance Interface:\n  CheckAcceptabilityOfTrialPoint(α) tests if x + α·Δx is acceptable:\n  1. Evaluate merit function φ(x+α·Δx) or filter (θ,φ) pair.\n  2. Compare to reference value (Armijo) or filter (envelope check).\n  3. If rejected: caller backtracks α ← ρ·α and retries.\n  4. TrySecondOrderCorrection(): If rejected, improve c(x+α·Δx).\n  5. PrepareRestoPhaseStart(): Augment filter before restoration.", "math": "Acceptance criteria options:\n  Filter: Accept if (θ,φ) not dominated by any filter entry.\n  Armijo: Accept if φ(x+α·Δx) ≤ φ(x) - η·α·(∇φ^T·Δx).\n  Penalty: Accept if merit l(x+α·Δx) ≤ l(x) - η·pred.\n  Watchdog: Non-monotone acceptance allowing temporary increases.\n\nKey responsibilities:\n- CheckAcceptabilityOfTrialPoint(): Main acceptance test\n- CalculateAlphaMin(): Minimum step size before restoration phase\n- TrySecondOrderCorrection(): Improve constraint satisfaction\n- TryCorrector(): Higher-order corrector for fast local convergence\n- PrepareRestoPhaseStart(): Augment filter before restoration\n\nImplementations:\n- FilterLSAcceptor: Wächter-Biegler filter method\n- PenaltyLSAcceptor: Penalty function approach\n\nWatchdog support:\n- StartWatchDog()/StopWatchDog(): Store/restore acceptor state\n- InitThisLineSearch(): Called at start of each line search", "see": ["IpBacktrackingLineSearch.hpp for the line search framework", "IpFilterLSAcceptor.hpp for filter-based implementation", "IpFilter.hpp for filter data structure"], "return": "a character for the info_alpha_primal_char field in IpData", "has_pass2": true}, "src/Algorithm/IpPenaltyLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPenaltyLSAcceptor.hpp", "filename": "IpPenaltyLSAcceptor.hpp", "file": "IpPenaltyLSAcceptor.hpp", "brief": "Penalty function line search using Waltz-Morales-Nocedal-Orban method\n\nPenaltyLSAcceptor implements exact penalty function globalization\nas an alternative to filter line search. Based on the paper by\nWaltz, Morales, Nocedal, and Orban.\n\nMerit function: phi_nu(x) = f(x) + nu * ||c(x)||_1\n\nAcceptance criterion:\n  phi_nu(x + alpha*d) <= phi_nu(x) + eta * alpha * pred\nwhere pred is the predicted reduction.\n\nPenalty parameter nu:\n- Automatically updated to ensure descent direction\n- Increased by nu_inc_ when direction is not descent\n- nu_init_ provides starting value\n\nKey parameters:\n- nu_init_, nu_inc_: Penalty parameter initialization/increment\n- eta_: Sufficient decrease parameter (Armijo)\n- rho_: Parameter for predicted reduction calculation\n- max_soc_: Maximum second order correction steps", "algorithm": "Exact Penalty Line Search (Waltz-Morales-Nocedal-Orban):\nGlobalization using ℓ₁ exact penalty merit function:\n1. Compute merit: φ_ν(x) = f(x) + ν·||c(x)||₁\n2. Compute predicted reduction: pred = -∇f·d - ν·(||c+J·d||₁ - ||c||₁)\n3. Update penalty ν if d is not descent for current ν:\n   ν ← max(ν, (∇f·d + ρ·dᵀWd) / ((1-ρ)·(||c||₁ - ||c+Jd||₁)))\n4. Armijo backtracking: accept α if φ_ν(x+αd) ≤ φ_ν(x) + η·α·pred\n5. Optional: Second order correction for constraint violation", "math": "Exact penalty merit function:\n  φ_ν(x) = f(x) + ν·||c(x)||₁\n\nDescent condition for direction d:\n  D_d φ_ν(x) = ∇f(x)ᵀd - ν·||c(x)||₁ < 0\n(if ||c|| > 0 and d reduces linearized constraints)\n\nPenalty update ensures descent:\n  ν ≥ ||∇L(x)||∞ / (1 - ε) for some ε > 0", "complexity": "O(n + m) per function evaluation. Backtracking may require\nmultiple evaluations. Second order correction adds one linear solve.", "ref": ["Waltz, Morales, Nocedal, Orban (2006). \"An interior algorithm for\n  nonlinear optimization that combines line search and trust region steps\".\n  Mathematical Programming 107(3):391-408."], "see": ["IpFilterLSAcceptor.hpp for filter-based alternative", "IpBacktrackingLSAcceptor.hpp for base interface"], "param": ["alpha_primal_test value of alpha that has been used for in the acceptence test ealier"], "has_pass2": true}, "src/Algorithm/IpRestoFilterConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoFilterConvCheck.hpp", "filename": "IpRestoFilterConvCheck.hpp", "file": "IpRestoFilterConvCheck.hpp", "brief": "Restoration phase convergence check for filter line search\n\nRestoFilterConvergenceCheck specializes restoration phase termination\nfor use with the filter globalization mechanism. It checks whether\nthe current restoration point would be acceptable to the original\nproblem's filter.", "algorithm": "Filter-Based Restoration Termination:\n  TestOrigProgress(φ_trial, θ_trial) from restoration iterate:\n  1. Query original filter acceptor: IsAcceptableToCurrentFilter(θ, φ).\n  2. Check: (θ_trial, φ_trial) not dominated by any filter entry.\n  3. Also check: IsAcceptableToCurrentIterate() for Armijo-like decrease.\n  4. If acceptable to original → exit restoration with CONVERGED.\n  5. Otherwise continue restoration iterations.", "math": "Filter acceptance from restoration:\n  Let F = {(θ_i, φ_i)} be the original filter.\n  Accept if: ∀i: θ_trial < θ_i OR φ_trial < φ_i - γ_φ·θ_i (envelope check).\n  Ensures restoration doesn't return to points worse than filter history.\n\nTermination criteria (via TestOrigProgress):\n1. Current point acceptable to original filter\n2. Current point shows sufficient decrease vs original iterate\n3. Constraint violation reduced below tolerance\n\nThe orig_filter_ls_acceptor_ reference is used to query:\n- IsAcceptableToCurrentIterate(): Armijo or θ-type improvement\n- IsAcceptableToCurrentFilter(): Point not dominated by filter\n\nImportant: SetOrigLSAcceptor() must be called before Initialize()\nto establish the link to the original problem's filter.\n\nNote: Uses raw pointer to avoid circular reference with line search.", "see": ["IpRestoConvCheck.hpp for the base restoration convergence class", "IpFilterLSAcceptor.hpp for filter acceptance methods", "IpRestoMinC_1Nrm.hpp for the restoration phase algorithm"], "has_pass2": true}, "src/Algorithm/IpBacktrackingLineSearch.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpBacktrackingLineSearch.hpp", "filename": "IpBacktrackingLineSearch.hpp", "file": "IpBacktrackingLineSearch.hpp", "brief": "General backtracking line search with filter/restoration support\n\nBacktrackingLineSearch is the main LineSearch implementation,\nproviding a flexible framework for globalization strategies:\n\nCore algorithm:\n1. Start with full step alpha = alpha_max (fraction-to-boundary)\n2. Test acceptability via BacktrackingLSAcceptor\n3. If rejected, reduce alpha *= alpha_red_factor and retry\n4. If alpha becomes too small, trigger restoration phase\n\nAdvanced features:\n- Watchdog mechanism: Accept poor steps temporarily to escape local minima\n- Second-order correction (SOC): Improve constraint satisfaction\n- Soft restoration phase: Accept steps that reduce primal-dual error\n- Magic steps: Improve slack/bound multiplier pairing\n- Corrector steps: Improve local convergence rate\n\nKey parameters:\n- alpha_for_y: How to step in equality multipliers\n- watchdog_trial_iter_max: Watchdog iteration limit\n- max_soc: Maximum second-order corrections", "algorithm": "Backtracking Line Search with Filter/Restoration:\nGlobalization for interior point ensuring convergence from arbitrary starts.\n1. Compute max step α_max from fraction-to-boundary rule: x+α*Δx ≥ τ*x\n2. Try full step α = α_max, check acceptability (filter or merit)\n3. If rejected, backtrack: α ← α * α_red (typically 0.5)\n4. If α < α_min, enter restoration phase (minimize infeasibility)\n5. Second-order correction: if step rejected due to constraints,\n   solve for correction Δx_soc to reduce linearization error\nWatchdog: temporarily accept steps failing filter to escape local minima.", "math": "Fraction-to-boundary: α_max = max{α : x + αΔx ≥ τx, s + αΔs ≥ τs}\nwhere τ ∈ (0,1) keeps iterates strictly interior (typically τ = 0.995).\nArmijo-like sufficient decrease: φ(x+αd) ≤ φ(x) + η*α*∇φ^T*d\nor filter acceptance: (θ,φ) not dominated by any filter entry.\nSOC correction: Δx_soc = -H^{-1}*A^T*(A*Δx - c(x+αΔx))", "complexity": "O(1) to O(k) acceptability tests per iteration where k ≤ log(α_max/α_min).\nEach test requires O(m) constraint evaluations. SOC adds O(m³) for linear solve.\nRestoration phase iterations: O(√m) typically for feasibility restoration.", "ref": ["Fletcher & Leyffer (2002). \"Nonlinear programming without a penalty function\".\n  Mathematical Programming 91:239-269. [Filter method]", "Wächter & Biegler (2005). \"Line search filter methods for nonlinear programming\".\n  SIAM J. Optim. 16(1):1-31. [Filter line search theory]"], "see": ["IpLineSearch.hpp for the interface", "IpBacktrackingLSAcceptor.hpp for acceptance test strategies", "IpFilterLSAcceptor.hpp for filter-based acceptance", "IpRestoPhase.hpp for restoration phase"], "return": "false, if that is not possible.", "has_pass2": true}, "src/Algorithm/IpEquilibrationScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpEquilibrationScaling.hpp", "filename": "IpEquilibrationScaling.hpp", "file": "IpEquilibrationScaling.hpp", "brief": "NLP scaling using MC19 matrix equilibration\n\nEquilibrationScaling computes scaling factors using the HSL MC19\nsymmetric indefinite matrix equilibration routine. This produces\nwell-conditioned scaling by analyzing the Jacobian structure.", "algorithm": "MC19-Based Matrix Equilibration Scaling:\n  DetermineScalingParametersImpl():\n  1. Perturb x₀ randomly within bounds (avoid singular points).\n  2. Evaluate Jacobians J_c, J_d at perturbed points.\n  3. Build \"equilibration matrix\" combining KKT structure.\n  4. Call MC19AD: computes D s.t. ||D·A·D||∞ ≈ 1 per row/col.\n  5. Extract dx from variable columns, dc/dd from constraint rows.\n  6. df computed separately from objective gradient.", "math": "Equilibration principle:\n  MC19 minimizes Σ(log|a_ij·d_i·d_j|)² for symmetric indefinite A.\n  Result: balanced row/column norms, improved KKT conditioning.\n  Better than gradient scaling for problems with structural imbalance.\n\nMC19 routine (HSL):\n- Computes diagonal scaling D such that |D*A*D|_ij ≈ 1\n- Uses infinity-norm equilibration\n- Loaded dynamically via LibraryLoader\n\nPointPerturber helper class:\n- Generates random perturbations within bounds\n- Used to avoid evaluating at singular points\n- MakeNewPerturbedPoint(): Returns randomly perturbed x", "see": ["IpNLPScaling.hpp for the scaling framework", "IpGradientScaling.hpp for gradient-based alternative", "IpMc19TSymScalingMethod.hpp for MC19 interface"], "has_pass2": true}, "src/Algorithm/IpRestoPenaltyConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoPenaltyConvCheck.hpp", "filename": "IpRestoPenaltyConvCheck.hpp", "file": "IpRestoPenaltyConvCheck.hpp", "brief": "Restoration phase convergence for penalty line search\n\nRestoPenaltyConvergenceCheck extends RestoConvergenceCheck for use\nwhen the original algorithm uses penalty function globalization\n(as opposed to filter method).", "algorithm": "Penalty-Based Restoration Termination:\n  TestOrigProgress(φ_trial, θ_trial) from restoration iterate:\n  1. Query original penalty acceptor: IsAcceptableToCurrentIterate(θ, φ).\n  2. Check Armijo-like decrease: φ_trial ≤ φ_ref - η·Δφ_pred.\n  3. Check infeasibility: θ_trial ≤ θ_ref (or sufficient reduction).\n  4. If acceptable to original penalty → exit restoration with CONVERGED.\n  5. Otherwise continue restoration iterations.", "math": "Penalty acceptance from restoration:\n  Merit function: ψ(x) = φ(x) + ν·θ(x) where ν = penalty parameter.\n  Accept if ψ_trial ≤ ψ_ref - η·α·∇ψ^T·d (Armijo on merit).\n  Penalty method has no filter history, simpler acceptance test.\n\nPurpose:\nDuring restoration phase, check if current point would be\nacceptable to the original penalty line search acceptor.\n\nTermination:\n- Restoration succeeds when IsAcceptableToCurrentIterate() returns\n  true for the original penalty acceptor\n- This tests if trial_barr and trial_theta would be accepted\n\nSetup requirement:\n- SetOrigLSAcceptor() must be called before Initialize()\n- Links to the PenaltyLSAcceptor from the original algorithm\n\nNote: Uses raw pointer (not SmartPtr) to avoid circular reference\nbetween restoration phase and original algorithm objects.", "see": ["IpRestoConvCheck.hpp for base restoration convergence", "IpRestoFilterConvCheck.hpp for filter-based restoration", "IpPenaltyLSAcceptor.hpp for penalty line search"], "has_pass2": true}, "src/Algorithm/IpRestoPhase.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoPhase.hpp", "filename": "IpRestoPhase.hpp", "file": "IpRestoPhase.hpp", "brief": "Strategy interface for restoration phase fallback\n\nRestorationPhase is the abstract base for the fallback mechanism\nwhen the line search cannot make progress. The restoration phase\nminimizes constraint violation to find a feasible point.\n\nTriggered when:\n- Line search step size becomes too small\n- No search direction can be computed (singular KKT system)\n- Algorithm explicitly requests fallback (ActivateFallbackMechanism)\n\nExceptions thrown on restoration failure:\n- RESTORATION_CONVERGED_TO_FEASIBLE_POINT: Success, found feasible point\n- RESTORATION_FAILED: Could not reduce infeasibility\n- RESTORATION_MAXITER_EXCEEDED: Hit iteration limit\n- RESTORATION_USER_STOP: User callback requested stop\n\nMain implementation: RestoIterationOutput minimizes ||c(x)||^2 + ||d(x)-s||^2\nusing the interior point method on a modified feasibility problem.", "algorithm": "Restoration Phase (Feasibility Recovery):\nWhen line search fails to find acceptable step, switch to minimizing\nconstraint violation using a modified interior point subproblem:\n1. Detect line search failure (step too small or filter blocking)\n2. Formulate feasibility problem: min Σ(p+n) s.t. c(x) = p-n, p,n ≥ 0\n3. Solve using nested Ipopt with modified NLP (RestoIpoptNLP)\n4. Return to original problem when feasible point found", "math": "Restoration problem formulation:\n  min_{x,p,n} ρ·(Σp_i + Σn_i) + ζ·||x - x_R||²\n  s.t. c(x) - p + n = 0,  p ≥ 0, n ≥ 0\nwhere x_R is reference point, ρ penalizes infeasibility,\nζ prevents wandering far from current iterate.\n\nFor inequality-constrained problems:\n  min Σ(p+n) s.t. c(x) = p-n, d_L ≤ d(x) ≤ d_U", "complexity": "Same as main Ipopt: O(n³) per iteration for linear algebra.\nMay require many iterations to find feasible point. Worst case: problem\nis infeasible and restoration certifies infeasibility.", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Section 3.3: Restoration phase]"], "see": ["IpRestoMinC_1Nrm.hpp for the main implementation", "IpRestoIpoptNLP.hpp for restoration phase NLP formulation", "IpBacktrackingLineSearch.hpp for restoration phase trigger"], "has_pass2": true}, "src/Algorithm/IpNLPScaling.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpNLPScaling.hpp", "filename": "IpNLPScaling.hpp", "file": "IpNLPScaling.hpp", "brief": "NLP scaling framework for problem conditioning\n\nNLPScalingObject is the abstract base for applying diagonal scaling\nto the NLP to improve numerical conditioning. Transforms:\n  min s_f*f(S_x^{-1}*x̃)  s.t.  s_c*c(S_x^{-1}*x̃)=0, ...", "algorithm": "NLP Diagonal Scaling:\n  DetermineScaling() computes scaling factors:\n  1. df: Objective scalar (scale f by df).\n  2. dx: Variable scaling D_x = diag(dx), x̃ = D_x·x.\n  3. dc, dd: Constraint scaling D_c, D_d.\n  Scaled problem: min df·f(D_x^{-1}·x̃) s.t. D_c·c(D_x^{-1}·x̃) = 0.\n  Applied automatically to all evaluations and derivatives.", "math": "Scaling transformations:\n  Gradient: ∇f̃ = df·D_x^{-1}·∇f (scaled gradient in x̃ space).\n  Jacobian: J̃ = D_c·J·D_x^{-1} (row and column scaling).\n  Hessian: W̃ = D_x^{-1}·W·D_x^{-1} (similarity transform).\n  Goal: Make ||∇f̃||, ||c̃||, ||J̃|| all O(1) for stability.\n\nProvides methods to:\n- apply_obj_scaling / unapply_obj_scaling: Scale objective\n- apply_vector_scaling_x/c/d: Scale primal/constraint vectors\n- apply_jac_c/d_scaling: Scale Jacobians (row/col scaling)\n- apply_hessian_scaling: Scale Hessian\n\nClass hierarchy:\n- NLPScalingObject: Abstract interface\n- StandardScalingBase: Common implementation with scaling vectors\n- NoNLPScalingObject: Identity (no scaling)\n- GradientScaling: Scale by gradient norm (nlp_scaling_method=gradient-based)\n- EquilibrationScaling: Row/column equilibration\n\nScaling matrices are represented via ScaledMatrix/SymScaledMatrix.", "see": ["IpGradientScaling.hpp for gradient-based implementation", "IpEquilibrationScaling.hpp for equilibration", "IpScaledMatrix.hpp for scaled matrix representation"], "has_pass2": true}, "src/Algorithm/IpFilterLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpFilterLSAcceptor.hpp", "filename": "IpFilterLSAcceptor.hpp", "file": "IpFilterLSAcceptor.hpp", "brief": "Filter line search acceptor using Wächter-Biegler method\n\nFilterLSAcceptor implements the filter globalization strategy where\nstep acceptance is based on improvement in either constraint violation\n(theta) or barrier objective (phi).\n\nAcceptance criteria:\n- Armijo sufficient decrease in barrier function, OR\n- Sufficient reduction in constraint violation (switching condition)\n- Point must be acceptable to current filter\n\nKey parameters:\n- theta_max_fact_: Upper bound factor on infeasibility\n- gamma_phi_, gamma_theta_: Margin parameters for filter\n- eta_phi_: Armijo parameter\n- s_phi_, s_theta_: Exponents in switching condition\n\nSecond-order correction (SOC):\n- When step is rejected, solves for constraint linearization error\n- Up to max_soc_ corrections tried\n- kappa_soc_ controls required constraint reduction between SOCs\n\nCorrector steps:\n- Affine or primal-dual corrector for fast local convergence\n- Applied even when step is acceptable\n\nFilter management:\n- Dominated entries removed on filter augmentation\n- Filter reset heuristic when repeatedly rejected by filter", "algorithm": "Filter Method for Nonlinear Optimization:\nBi-objective approach replacing traditional merit functions. Track pairs\n(θ, φ) where θ = constraint violation, φ = barrier objective.\nAccept trial point if:\n1. Sufficient φ decrease (Armijo): φ(x+αd) ≤ φ(x) - η*α*∇φ^T*d, OR\n2. Switching to feasibility: θ sufficient, ∇φ^T*d < 0, AND α*(-∇φ^T*d)^sφ > δ*θ^sθ\n3. Filter acceptable: (θ_new, φ_new) not dominated by any (θ_i, φ_i) in filter\nDominance: (θ,φ) dominates (θ',φ') if θ ≤ (1-γ_θ)θ' AND φ ≤ φ' - γ_φ*θ'", "math": "Filter entries F = {(θ_i, φ_i)}. Accept x_new if ∀(θ_i,φ_i) ∈ F:\nθ(x_new) ≤ (1-γ_θ)θ_i  OR  φ(x_new) ≤ φ_i - γ_φ*θ_i\nSwitching condition: if α*(-∇φ^T d)^s_φ > δ*θ^s_θ, prioritize optimality.\nOtherwise prioritize feasibility. Prevents Maratos effect via SOC.\nFilter augmented when accepting step improving only constraint violation.", "complexity": "O(|F|) per acceptance test where |F| = filter size.\nFilter size bounded by O(k) where k = iteration count; dominated entries pruned.\nSOC: O(m³) per correction for linear solve.", "ref": ["Fletcher & Leyffer (2002). \"Nonlinear programming without a penalty function\".\n  Mathematical Programming 91:239-269. [Filter method introduction]", "Fletcher, Leyffer & Toint (2002). \"On the global convergence of a filter-SQP\n  algorithm\". SIAM J. Optim. 13:44-59. [Global convergence theory]"], "see": ["IpBacktrackingLSAcceptor.hpp for the base interface", "IpFilter.hpp for the filter data structure", "IpBacktrackingLineSearch.hpp for usage"], "param": ["in_watchdog indicates if we are currently in an active watchdog procedure"], "has_pass2": true}, "src/Algorithm/IpRestoConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoConvCheck.hpp", "filename": "IpRestoConvCheck.hpp", "file": "IpRestoConvCheck.hpp", "brief": "Base class for restoration phase convergence checking\n\nRestoConvergenceCheck extends OptimalityErrorConvergenceCheck for\nthe restoration phase, adding termination criteria based on\nacceptability to the original problem's globalization mechanism.", "algorithm": "Restoration Phase Termination:\n  CheckConvergence() tests (in order):\n  1. Standard KKT convergence on restoration problem (inherited).\n  2. TestOrigProgress(): Is current (θ, φ) acceptable to original filter?\n  3. Sufficient infeasibility reduction: θ_new ≤ κ_resto·θ_start.\n  4. Iteration limits: iter ≤ maximum_iters_, resto_iter ≤ maximum_resto_iters_.\n  Returns CONVERGED if restoration found point acceptable to original.", "math": "Restoration exit conditions:\n  θ = ||c(x)||₁ (constraint violation / infeasibility).\n  φ = f(x) + barrier terms (merit / objective).\n  Accept if (θ, φ) not dominated by original filter AND θ sufficiently small.\n  kappa_resto_ controls required infeasibility reduction fraction.\n\nTermination conditions (checked in order):\n1. Standard optimality error convergence (inherited)\n2. Acceptable to original problem (via TestOrigProgress)\n3. Sufficient reduction in infeasibility (kappa_resto_)\n4. Iteration limits (maximum_iters_, maximum_resto_iters_)\n\nTestOrigProgress (pure virtual):\n- Implemented by subclasses for specific globalization\n- RestoFilterConvergenceCheck: Check filter acceptability\n- Returns CONVERGED if original algorithm can accept point\n\nKey parameters:\n- kappa_resto_: Required infeasibility reduction fraction\n- maximum_resto_iters_: Max successive restoration iterations\n- orig_constr_viol_tol_: Original constraint tolerance\n\nSetOrigLSAcceptor():\n- Must be called to link restoration check to original problem\n- Provides access to original filter/acceptor state", "see": ["IpOptErrorConvCheck.hpp for base convergence check", "IpRestoFilterConvCheck.hpp for filter implementation"], "has_pass2": true}, "src/Algorithm/IpDefaultIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpDefaultIterateInitializer.hpp", "filename": "IpDefaultIterateInitializer.hpp", "file": "IpDefaultIterateInitializer.hpp", "brief": "Standard initialization procedure for IPM iterates\n\nDefaultIterateInitializer computes starting points for all primal\nand dual variables based on user options and problem bounds.\n\nPrimal initialization (x, s):\n- Start from user-provided x0 or NLP default\n- Push away from bounds: x_new = max(x_L + ε, min(x, x_U - ε))\n- bound_push_, bound_frac_: Absolute/relative push parameters\n- least_square_init_primal_: Fit linearized constraints\n\nDual initialization:\n- Equality multipliers (y_c, y_d): Least-squares or zero\n- eq_mult_calculator_: Computes min ||y|| s.t. KKT gradient\n- constr_mult_init_max_: Reject large multiplier estimates\n- Bound multipliers (z_L, z_U, v_L, v_U):\n  - B_CONSTANT: bound_mult_init_val_\n  - B_MU_BASED: mu_init_ / slack\n\nWarm start:\n- warm_start_init_point_: Use warm_start_initializer_ instead\n- Delegates to WarmStartIterateInitializer\n\nStatic utilities:\n- push_variables(): Move point away from bounds\n- least_square_mults(): Compute y from gradient conditions", "algorithm": "IPM Iterate Initialization (Default):\nInitialize all primal and dual variables for interior point method:\n1. Primal variables (x):\n   - Start from user x0 or NLP default\n   - Push from bounds: x ← max(x_L + κ, min(x, x_U - κ))\n   - where κ = max(bound_push, bound_frac·|x_L|)\n   - Optional: least-squares fit to linearized constraints\n2. Slack variables (s):\n   - Initialize s = d(x) (constraint body values)\n   - Push from bounds similarly to x\n3. Constraint multipliers (y_c, y_d):\n   - Least-squares estimate: min ||∇f + Jᵀy - z||\n   - Reject if ||y||∞ > constr_mult_init_max (use 0 instead)\n4. Bound multipliers (z_L, z_U, v_L, v_U):\n   - B_CONSTANT: z = bound_mult_init_val\n   - B_MU_BASED: z = μ_init / slack (complementarity-based)", "math": "Push-from-bounds formula:\n  x_new = max(x_L + κ_abs, min(x, x_U - κ_abs))\n  where κ_abs = max(bound_push, bound_frac · max(1, |x_L|))\n\nBound multiplier initialization (μ-based):\n  z_L = μ_init / (x - x_L),  z_U = μ_init / (x_U - x)\nsatisfies complementarity x·z = μ at initialization.", "complexity": "O(n + m) for variable initialization.\nPlus one least-squares solve O(nnz·fill) if using LS multipliers.", "see": ["IpIterateInitializer.hpp for the base interface", "IpWarmStartIterateInitializer.hpp for warm start", "IpEqMultCalculator.hpp for multiplier computation"], "has_pass2": true}, "src/Algorithm/IpPDSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDSystemSolver.hpp", "filename": "IpPDSystemSolver.hpp", "file": "IpPDSystemSolver.hpp", "brief": "Abstract interface for solving the full primal-dual KKT system\n\nPDSystemSolver defines the interface for solving the 8x8 block\nprimal-dual linear system that arises at each Newton iteration:\n\n  [W    0   Jc^T  Jd^T  -Px_L  Px_U   0     0   ] [dx  ]   [rx  ]\n  [0    0    0    -I     0     0    -Pd_L  Pd_U ] [ds  ]   [rs  ]\n  [Jc   0    0     0     0     0     0     0   ] [dyc ] = [rc  ]\n  [Jd  -I    0     0     0     0     0     0   ] [dyd ]   [rd  ]\n  [Zl  Px_L^T ...                              ] [dzL ]   [rzL ]\n  [...]                                          [dzU ]   [rzU ]\n  [...]                                          [dvL ]   [rvL ]\n  [...]                                          [dvU ]   [rvU ]\n\nImplementations typically reduce this to the 4x4 augmented system\nby eliminating the bound multiplier equations, then solve via\nAugSystemSolver. The key implementation is PDFullSpaceSolver.", "algorithm": "Primal-Dual Interior Point Newton System:\nThis 8×8 block system represents one Newton step for the perturbed\nKKT conditions of the barrier problem. The system encodes:\n- Rows 1-2: Gradient of Lagrangian (stationarity)\n- Rows 3-4: Constraint feasibility (primal feasibility)\n- Rows 5-8: Complementarity conditions (XZe = μe perturbed)", "math": "Full KKT system for barrier NLP min f(x) s.t. c(x)=0, x≥0:\n[∇²L   -Aᵀ   -I  ] [Δx]   [-∇f + Aᵀλ + μ/x]\n[ A     0    0  ] [Δλ] = [-c(x)          ]\n[ Z     0    X  ] [Δz]   [-XZe + μe      ]\n\nwhere L = f - λᵀc - zᵀx is the Lagrangian, Z = diag(z), X = diag(x).\nThe 8×8 form handles inequality constraints and slack variables.\n\nStandard reduction: eliminate Δz = X⁻¹(μe - ZΔx - XZe) to get\naugmented system with (∇²L + X⁻¹Z) in the (1,1) block.", "complexity": "Per Newton iteration: O(n³) for dense factorization,\nO(nnz + n·fill) for sparse factorization where fill depends on\nsparsity structure. Typically dominated by matrix factorization.", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Ipopt algorithm paper]", "Nocedal & Wright (2006). \"Numerical Optimization\". 2nd ed. Springer.\n  Chapter 19: Interior point methods. [Textbook treatment]"], "see": ["IpPDFullSpaceSolver.hpp for the main implementation", "IpAugSystemSolver.hpp for the reduced 4x4 system", "IpIteratesVector.hpp for the rhs/solution structure"], "return": "false, if a solution could not be computed (for\n example, when the Hessian regularization parameter becomes too\n large)", "has_pass2": true}, "src/Algorithm/IpMuUpdate.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpMuUpdate.hpp", "filename": "IpMuUpdate.hpp", "file": "IpMuUpdate.hpp", "brief": "Strategy interface for updating barrier parameter mu\n\nMuUpdate is the abstract base for strategies that determine the\nbarrier parameter mu and fraction-to-boundary parameter tau for\neach iteration of the interior point method.", "algorithm": "Barrier Parameter Update Strategies:\n  Monotone (classical path-following):\n    μ_{k+1} = σ·μ_k where σ < 1 (typically σ = 0.1 after good progress).\n    Conservative: ensures superlinear convergence near solution.\n  Adaptive (LOQO-style / Mehrotra):\n    μ = (x^T z / n) · σ where σ from affine direction analysis.\n    Can increase μ if needed for centrality, more aggressive reduction.\n  Probing: test multiple μ values, select best progress.", "math": "Central path and barrier parameter:\n  Barrier problem: min f(x) - μ·Σlog(x_i) s.t. c(x) = 0.\n  Central path: x(μ)·z(μ) = μe for all μ > 0.\n  As μ → 0: x(μ) → x* (optimal), z(μ) → z* (optimal multipliers).\n  Complementarity measure: μ_avg = (x^T z + s^T v) / n.\n  τ = 1 - μ^θ (θ ≈ 1.5): fraction-to-boundary prevents hitting bounds.", "complexity": "O(1) for monotone update.\n  O(linear_solve) for predictor-corrector (affine direction needed).\n  Adaptive may require multiple KKT solves per iteration.", "see": ["IpMonotoneMuUpdate.hpp for monotone strategy", "IpAdaptiveMuUpdate.hpp for adaptive strategy", "IpMuOracle.hpp for mu computation oracles"], "has_pass2": true}, "src/Algorithm/IpNLPBoundsRemover.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpNLPBoundsRemover.hpp", "filename": "IpNLPBoundsRemover.hpp", "file": "IpNLPBoundsRemover.hpp", "brief": "NLP adapter that converts variable bounds to inequality constraints\n\nNLPBoundsRemover is an NLP adapter primarily used for inexact/iterative\nlinear solvers that require the KKT system to have a specific structure\nwithout bound constraints.", "algorithm": "Bounds-to-Inequality Transformation:\n  GetSpaces() transforms NLP structure:\n  1. Original: min f(x) s.t. c(x)=0, d_L ≤ d(x) ≤ d_U, x_L ≤ x ≤ x_U.\n  2. Remove x bounds: x now unbounded (no z_L, z_U multipliers).\n  3. Add new inequalities: d_new(x) = [d_orig(x); x].\n  4. New bounds: d_L_new = [d_L_orig; x_L], d_U_new = [d_U_orig; x_U].\n  5. Jacobian: J_d_new = [J_d_orig; I] (identity block for x bounds).\n  6. Hessian unchanged (new constraints linear in x).", "math": "KKT structure change:\n  Original: z_L, z_U appear in diagonal of reduced Hessian.\n  Transformed: Bound multipliers become v_L, v_U on new inequalities.\n  KKT system structure simplified (no variable bounds in main diagonal).\n  Cost: Larger J_d matrix, more inequality constraints.\n\nTransformation:\n  Original: x_L <= x <= x_U\n  Transformed: d(x) = x with d_L = x_L, d_U = x_U (added inequalities)\n  Result: No bounds on x, but equivalent inequality constraints\n\nThe resulting NLP has:\n- No variable bounds (x unbounded)\n- Additional inequality constraints d(x) = x\n- Corresponding d_L, d_U bounds\n\nKey restrictions:\n- Original inequalities must be one-sided (unless allow_twosided_inequalities_)\n- Jacobian structure changes (identity blocks added)\n\nMatrix structure modification:\n- J_d becomes [J_d_orig; I] (stacked)\n- Hessian unchanged (d=x is linear)\n\nUses:\n- Inexact Newton methods requiring bound-free formulation\n- Research into alternative KKT structures", "see": ["IpNLP.hpp for the base NLP interface"], "has_pass2": true}, "src/Algorithm/IpAlgorithmRegOp.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAlgorithmRegOp.hpp", "filename": "IpAlgorithmRegOp.hpp", "file": "IpAlgorithmRegOp.hpp", "brief": "Registration function for all Algorithm module options\n\nDeclares RegisterOptions_Algorithm() which registers all options\nfor the Ipopt algorithm components with the RegisteredOptions system.\n\nThis function is called during IpoptApplication initialization to\nmake all algorithm options available. It aggregates option registration\nfrom numerous strategy classes.\n\nRegistered option categories include:\n- Main algorithm options (IpoptAlgorithm)\n- Line search options (BacktrackingLineSearch, FilterLSAcceptor)\n- Barrier parameter options (MonotoneMuUpdate, AdaptiveMuUpdate)\n- Linear solver options (PDFullSpaceSolver, AugSystemSolver)\n- Initialization options (DefaultIterateInitializer)\n- Scaling options (NLPScaling, GradientScaling)\n- Convergence options (OptimalityErrorConvergenceCheck)\n- Output options (OrigIterationOutput)", "see": ["IpRegOptions.hpp for the options registration framework", "IpoptApplication::RegisterAllOptions"], "has_pass2": false}, "src/Algorithm/IpRestoIterationOutput.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoIterationOutput.hpp", "filename": "IpRestoIterationOutput.hpp", "file": "IpRestoIterationOutput.hpp", "brief": "Iteration output during restoration phase\n\nRestoIterationOutput provides per-iteration summary output while\nthe algorithm is in restoration phase. It displays metrics for the\nORIGINAL NLP (not the restoration feasibility problem).\n\nOutput format:\n- Iteration number marked with 'r' prefix to indicate restoration\n- Objective value from original NLP\n- Constraint violation (theta) for original constraints\n- Dual infeasibility for original problem\n\nDual output mode:\nIf resto_orig_iteration_output is provided, produces two lines:\n1. Restoration phase problem metrics\n2. Original NLP metrics (using original scaling)\n\nConfiguration:\n- print_info_string_: Whether to print info at end of line\n- inf_pr_output_: What to show in inf_pr column\n- print_frequency_iter_: Iteration print frequency\n- print_frequency_time_: Time-based print frequency", "algorithm": "Restoration Phase Output (Dual View):\nProvides visibility into restoration phase while tracking original NLP:\n1. Mark iteration with 'r' prefix to distinguish from normal iterations\n2. Report original NLP objective f(x) (not restoration objective)\n3. Report original constraint violation θ = ‖c(x)‖\n4. Optionally show both restoration and original metrics (dual output)\n\nUser sees: restoration working to reduce infeasibility, with\ntracking of how original problem metrics change.", "see": ["IpOrigIterationOutput.hpp for regular iteration output", "IpIterationOutput.hpp for base interface"], "has_pass2": true}, "src/Algorithm/IpAdaptiveMuUpdate.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAdaptiveMuUpdate.hpp", "filename": "IpAdaptiveMuUpdate.hpp", "file": "IpAdaptiveMuUpdate.hpp", "brief": "Adaptive (non-monotone) barrier parameter update strategy\n\nAdaptiveMuUpdate implements the free-mode/fixed-mode approach for\nbarrier parameter updates. In free mode, mu is computed adaptively\neach iteration. In fixed mode, monotone decrease is enforced.\n\nTwo operating modes:\n- Free mode: MuOracle suggests mu (e.g., Mehrotra predictor-corrector\n  or quality function minimization). Allows temporary mu increases.\n- Fixed mode: Monotone decrease enforced. Triggered when free mode\n  fails to make sufficient progress.\n\nGlobalization strategies (adaptive_mu_globalization_):\n- KKT_ERROR: Track reduction in primal-dual KKT error\n- FILTER_OBJ_CONSTR: Use filter on (theta, phi)\n- NEVER_MONOTONE_MODE: Always stay in free mode\n\nFree mode oracles (via MuOracle):\n- QualityFunctionMuOracle: Minimize quality function\n- ProbingMuOracle: Try candidate mu values\n- LoqoMuOracle: LOQO-style adaptive rule\n\nFixed mode behavior:\n- Uses fix_mu_oracle_ or average complementarity\n- restore_accepted_iterate_: Can restore last good free-mode point\n\nReference value tracking:\n- refs_vals_: List of recent KKT error values\n- refs_red_fact_: Required reduction factor\n- num_refs_max_: Maximum stored references", "algorithm": "Adaptive Barrier Parameter Update (Free/Fixed Mode):\nNon-monotone strategy allowing temporary μ increases for faster convergence.\n- Free mode: Oracle chooses μ (may increase). Globalization checks progress.\n- Fixed mode: Enforce μ_{k+1} ≤ κ_μ · μ_k until sufficient progress.\nSwitch to fixed mode when free mode stalls; return when progress resumes.", "math": "Barrier parameter update rules:\nFree mode (oracle-based): μ ← oracle(x,λ,z) minimizing quality function\n  Q(μ) = ||∇L||/μ + μ·||c|| + ||XZe - μe||\n\nFixed mode (monotone): μ_{k+1} = min(κ_μ · μ_k, μ_k^θ_μ) where\n  κ_μ ∈ (0,1) is linear decrease, θ_μ > 1 is superlinear power.\n\nGlobalization (KKT_ERROR mode): accept μ if\n  E(μ_new) ≤ max{E_j : j recent} · (1 - refs_red_fact_)\nwhere E = ||(∇L, c, XZe-μe)|| is primal-dual error.", "complexity": "O(n) per update: computing complementarity and KKT norms.\nOracle evaluation may involve trial solves (quality function search).", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Sections 3.1-3.2: μ update]", "Nocedal et al. (2009). \"An adaptive barrier update strategy for\n  nonlinear interior methods\". SIAM J. Optimization 19(4):1674-1693."], "see": ["IpMuUpdate.hpp for the base interface", "IpMuOracle.hpp for oracle interface", "IpQualityFunctionMuOracle.hpp for quality function approach"], "has_pass2": true}, "src/Algorithm/IpQualityFunctionMuOracle.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpQualityFunctionMuOracle.hpp", "filename": "IpQualityFunctionMuOracle.hpp", "file": "IpQualityFunctionMuOracle.hpp", "brief": "Barrier parameter selection via quality function minimization\n\nQualityFunctionMuOracle computes the barrier parameter by minimizing\na quality function that measures how close the predictor-corrector\nstep brings the iterate to optimality.\n\nAlgorithm:\n1. Compute affine scaling direction (mu = 0)\n2. Compute centering direction (pure mu term)\n3. For combined step (sigma * centering + affine), find optimal sigma\n4. Convert optimal sigma to mu = sigma * average_complementarity\n\nQuality function Q(sigma) measures (configurable):\n- Primal-dual error norm after step\n- Centrality deviation from mu-centered path\n- Balancing term for primal/dual progress\n\nConfiguration options:\n- quality_function_norm_: NM_NORM_1, NM_NORM_2, NM_NORM_MAX\n- quality_function_centrality_: CEN_NONE, CEN_LOG, CEN_RECIPROCAL\n- quality_function_balancing_term_: BT_NONE, BT_CUBIC\n\nGolden section search:\n- Finds optimal sigma in [sigma_min_, sigma_max_]\n- Tolerances: quality_function_section_sigma_tol_, _qf_tol_\n- Maximum iterations: quality_function_max_section_steps_", "algorithm": "Quality Function Barrier Parameter Oracle:\nChooses μ by minimizing predicted error after a predictor-corrector step:\n1. Solve affine direction: Newton step with μ=0 (predictor)\n2. Solve centering direction: pure μ contribution (corrector)\n3. Combined step: Δ = Δ_aff + σ·Δ_cen parameterized by σ ∈ [0,1]\n4. Golden section search minimizes Q(σ) over [σ_min, σ_max]\n5. Return μ = σ* · (average complementarity)", "math": "Quality function evaluates predicted iterate quality:\n  Q(σ) = ||∇L(x+Δx)||_p + C(x+Δx) + B(x+Δx)\nwhere:\n  ||·||_p is configurable norm (1, 2, ∞)\n  C(·) = centrality measure: none, Σlog(x_iz_i/μ), or Σ(1/(x_iz_i) - 1/μ)\n  B(·) = balancing term penalizing unequal primal/dual progress\n\nPredictor-corrector interpretation (Mehrotra):\n  σ = 0 → pure affine scaling (aggressive)\n  σ = 1 → pure centering (conservative)\n  σ* ∈ (0,1) → adaptive balance", "complexity": "Per oracle call: 2 linear solves (affine + centering) plus\nO(n · max_section_steps) quality function evaluations for golden section.", "ref": ["Nocedal et al. (2009). \"Adaptive Barrier Update Strategies for\n  Nonlinear Interior Methods\". SIAM J. Optim. 19(4):1674-1693.", "Mehrotra (1992). \"On the Implementation of a Primal-Dual Interior\n  Point Method\". SIAM J. Optim. 2(4):575-601. [Predictor-corrector]"], "see": ["IpMuOracle.hpp for the base interface", "IpAdaptiveMuUpdate.hpp for usage"], "has_pass2": true}, "src/Algorithm/IpAugRestoSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAugRestoSystemSolver.hpp", "filename": "IpAugRestoSystemSolver.hpp", "file": "IpAugRestoSystemSolver.hpp", "brief": "Augmented system solver exploiting restoration phase structure\n\nAugRestoSystemSolver is a decorator that exploits the known structure\nof the restoration phase problem to reduce the augmented system to\nthe original problem size.", "algorithm": "Restoration Augmented System Reduction:\n  Solve() reduces expanded restoration system to original size:\n  1. Restoration variables: (x, s, n_c, p_c, n_d, p_d).\n  2. n, p have simple structure: diagonal Σ blocks in Hessian.\n  3. Elimination: Solve for n, p analytically via:\n     n = Σ_n⁻¹·(rhs_n - contributions), p = Σ_p⁻¹·(rhs_p - contributions).\n  4. Reduced RHS: rhs_cR, rhs_dR incorporate n, p contributions.\n  5. Solve original-sized system, then back-substitute for n, p.", "math": "Schur complement structure:\n  Full system: [W_full, J_full^T; J_full, -D] [Δ_full; y] = [r_full].\n  Σ_n, Σ_p diagonal → closed-form elimination reduces to original (n_x + n_c + n_d) size.\n\nRestoration problem structure:\n- Variables: (x, s, n_c, p_c, n_d, p_d)\n- Constraints: c(x) - n_c + p_c = 0, d(x) - s - n_d + p_d = 0\n- Objective: penalty on ||n|| + ||p||\n\nThe Hessian and Jacobian have special structure that allows the\nn, p variables to be eliminated analytically. The resulting system\nhas the same dimension as the original problem.\n\nKey quantities computed and cached:\n- sigma_tilde_*_inv_: Inverse of perturbed bound multiplier ratios\n- Neg_Omega_*_plus_D_*: Combined diagonal terms\n- Rhs_*R: Reduced right-hand sides\n\nThe skip_orig_aug_solver_init flag allows reusing symbolic\nfactorization from the main algorithm.", "see": ["IpRestoIpoptNLP.hpp for restoration problem formulation", "IpAugSystemSolver.hpp for base interface"], "return": "true, if linear solver provides inertia", "has_pass2": true}, "src/Algorithm/IpAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAugSystemSolver.hpp", "filename": "IpAugSystemSolver.hpp", "file": "IpAugSystemSolver.hpp", "brief": "Abstract interface for solving the 4x4 augmented KKT system\n\nAugSystemSolver defines the interface for solving the reduced\naugmented system obtained by eliminating bound multiplier equations:\n\n  [W + Dx + δx*I      0        Jc^T      Jd^T  ] [sol_x]   [rhs_x]\n  [    0         Ds + δs*I     0         -I   ] [sol_s] = [rhs_s]\n  [   Jc             0      Dc - δc*I    0    ] [sol_c]   [rhs_c]\n  [   Jd            -I         0      Dd - δd*I] [sol_d]   [rhs_d]", "algorithm": "Augmented System Factorization:\n  1. Assemble symmetric indefinite matrix K from blocks.\n  2. Factor K = P·L·D·L^T·P^T (symmetric indefinite factorization).\n  3. Solve K·x = b using forward/back substitution.\n  Factorization reused for multiple RHS (predictor-corrector).\n  Inertia check: verify (n+, n-, n0) matches expected (nx+ns, nc+nd, 0).", "math": "Inertia correction via regularization:\n  If inertia incorrect: increase δx (primal reg) or δc,δd (dual reg).\n  Primal reg δx > 0 ensures positive inertia on (1,1) block.\n  Dual reg δc,δd < 0 controls negative inertia on (2,2),(3,3),(4,4) blocks.\n  Target: n+ = nx+ns (primal dim), n- = nc+nd (dual dim), n0 = 0.", "complexity": "O(nnz²/n) for sparse Cholesky-like factorization.\n  O(n²) worst case for dense submatrices.\n  Strategy pattern: different solvers (MA27, MA57, Pardiso, MUMPS).\n\nKey parameters:\n- W: Hessian of Lagrangian (or approximation)\n- Dx, Ds: Barrier diagonal contributions (Σx, Σs)\n- δx, δs, δc, δd: Regularization parameters for inertia control", "see": ["IpStdAugSystemSolver.hpp for the main implementation", "IpSymLinearSolver.hpp for the underlying sparse solver", "IpPDFullSpaceSolver.hpp for the caller"], "return": "return value of the linear solver object.", "has_pass2": true}, "src/Algorithm/IpPDPerturbationHandler.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDPerturbationHandler.hpp", "filename": "IpPDPerturbationHandler.hpp", "file": "IpPDPerturbationHandler.hpp", "brief": "Inertia correction via primal-dual perturbation (regularization)\n\nPDPerturbationHandler manages the regularization parameters\n(delta_x, delta_s, delta_c, delta_d) added to the KKT system to\nensure correct inertia (n positive, m+p negative eigenvalues).\n\nThe perturbed augmented system is:\n  [W + D_x + δ_x I   ...  ]\n  [  ...   D_s + δ_s I  ...]\n  [  ...   ... D_c - δ_c I]\n  [  ...   ...   D_d - δ_d I]\n\nPerturbation strategies:\n- ConsiderNewSystem(): Called for each new matrix, may add δ_c, δ_d\n  if structurally singular Jacobian is detected\n- PerturbForSingularity(): Handle numerically singular system\n- PerturbForWrongInertia(): Correct incorrect eigenvalue count\n\nHeuristics:\n- Track structural degeneracy (hess_degenerate_, jac_degenerate_)\n- Increase perturbation exponentially until acceptable\n- Decrease perturbation when not needed", "algorithm": "Inertia Correction via Perturbation:\nAdd regularization to KKT system diagonal for correct eigenvalue signature:\n1. ConsiderNewSystem: Check if structural degeneracy detected\n   - If jac_degenerate_: set δ_c = δ_d = δ_cd (constraint regularization)\n   - If hess_degenerate_: set δ_x, δ_s > 0 (primal regularization)\n2. PerturbForWrongInertia: Inertia mismatch after factorization\n   - First try: δ_x = delta_xs_init_ (small)\n   - Retry: δ_x ← δ_x · delta_xs_inc_fact_ (exponential increase)\n   - Stop when δ_x > delta_xs_max_ (failure)\n3. PerturbForSingularity: Numerical singularity detected\n   - Similar exponential increase strategy\n4. On success: δ_x_last_ ← δ_x · delta_xs_dec_fact_ (reduce for next time)", "math": "Perturbed augmented system:\n[H + Σ + δ_x·I     Aᵀ        ] [Δx]   [r_x]\n[    A        -δ_c·I         ] [Δλ] = [r_λ]\n\nRequired inertia for descent direction:\n- n + n_s positive eigenvalues (primal variables + slacks)\n- m_eq + m_ineq negative eigenvalues (dual variables)\n\nConstraint regularization: δ_c = δ_cd_val · μ^{δ_cd_exp}\n(decreases with barrier parameter for asymptotic accuracy)", "complexity": "O(1) per perturbation decision. Dominated by linear\nsolver factorization O(nnz(L)·fill) which may be retried multiple times.", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Section 3.1: Inertia correction]"], "see": ["IpPDFullSpaceSolver.hpp for usage", "IpAugSystemSolver.hpp for where perturbations are applied"], "return": "false, no suitable perturbation could be found.", "has_pass2": true}, "src/Algorithm/IpStdAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpStdAugSystemSolver.hpp", "filename": "IpStdAugSystemSolver.hpp", "file": "IpStdAugSystemSolver.hpp", "brief": "Standard augmented system solver forming explicit matrix\n\nStdAugSystemSolver is the main implementation of AugSystemSolver\nfor sparse triple-format matrices (SymTMatrix). It explicitly\nassembles the 4x4 augmented system as a CompoundSymMatrix:\n\n  [W + D_x + δ_x I      0         J_c^T      J_d^T  ]\n  [     0          D_s + δ_s I    0          -I    ]\n  [    J_c             0       D_c - δ_c I    0     ]\n  [    J_d            -I          0       D_d - δ_d I]\n\nImplementation details:\n- Uses CompoundSymMatrixSpace with SumSymMatrix for (1,1) block\n- DiagMatrix for D_x, D_s, D_c, D_d contributions\n- IdentityMatrix for scalar delta regularization\n- Tracks matrix tags to avoid unnecessary reassembly\n- Delegates factorization/solve to SymLinearSolver", "algorithm": "Augmented System Assembly (Standard):\nBuilds the 4×4 symmetric indefinite KKT matrix from components:\n1. Create matrix space once (CompoundSymMatrixSpace)\n2. Assemble (1,1) block: H = W·factor + D_x + δ_x·I using SumSymMatrix\n3. Set (2,2) block: D_s + δ_s·I\n4. Set (1,3): J_c^T, (1,4): J_d^T (Jacobians as GenTMatrix)\n5. Set (3,3): -D_c + δ_c·I, (4,4): -D_d + δ_d·I (regularization)\n6. Set (2,4): -I (slack-inequality coupling)\n7. Check matrix tags to avoid reassembly if unchanged\n8. Pass assembled CompoundSymMatrix to SymLinearSolver", "math": "Augmented system structure (symmetric indefinite):\n[H + Σ    0    Aᵀ   ] [Δx]   [r_x]\n[  0    D_s   -Eᵀ   ] [Δs] = [r_s]\n[  A    -E   -D_c   ] [Δλ]   [r_λ]\n\nwhere Σ = Pᵀ·S⁻¹·Z·P (bound multiplier contribution to diagonal),\nA = [J_c; J_d] (equality + inequality Jacobians),\nE = [0; I] (slack-inequality identity block).", "complexity": "Space: O(nnz(H) + nnz(J)) for sparse assembly.\nTime: O(1) for tag checking, O(nnz) for assembly,\ndominated by factorization O(nnz(L)·fill) in linear solver.", "see": ["IpAugSystemSolver.hpp for the interface", "IpSymLinearSolver.hpp for the underlying sparse solver", "IpCompoundSymMatrix.hpp for the matrix representation"], "return": "the number of negative eigenvalues of the\nmost recent factorized matrix", "has_pass2": true}, "src/Algorithm/IpIpoptAlg.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptAlg.hpp", "filename": "IpIpoptAlg.hpp", "file": "IpIpoptAlg.hpp", "brief": "Main Ipopt algorithm orchestrating the interior point method\n\nIpoptAlgorithm is the central class implementing the primal-dual\ninterior point method. Each iteration:\n1. UpdateHessian() - Evaluate or update Hessian approximation\n2. UpdateBarrierParameter() - Adjust mu using chosen strategy\n3. ComputeSearchDirection() - Solve KKT system for Newton step\n4. ComputeAcceptableTrialPoint() - Line search with filter/merit\n5. AcceptTrialPoint() - Update current iterate\n\nUses Strategy pattern: line search, mu update, convergence check\nare all pluggable components configured via AlgorithmBuilder.", "algorithm": "Primal-Dual Interior Point Method for NLP:\nSolves min f(x) s.t. c(x)=0, x≥0 by solving perturbed KKT conditions:\n∇f(x) - A^T y - z = 0,  c(x) = 0,  XZe = μe\nNewton direction from linearized KKT system (augmented system).\nFilter line search ensures global convergence. Barrier μ→0 drives\nsolution to boundary. Supports exact Hessian or L-BFGS approximation.", "math": "KKT system solved each iteration:\n[W    A^T  -I ] [Δx]   [∇f - A^T y - z    ]\n[A    0    0  ] [Δy] = [-c(x)             ]\n[Z    0    X  ] [Δz]   [μe - XZe          ]\nwhere W = ∇²L (Hessian of Lagrangian), A = ∇c (constraint Jacobian).\nReduced to augmented system: [W+Σ  A^T][Δx] = [r_d - X^{-1}(μe-XZe)]\n                             [A    0  ][Δy]   [-c(x)                ]\nwhere Σ = X^{-1}Z diagonal. Solved via symmetric indefinite factorization.", "complexity": "O(m²n + m³) per iteration: O(mn) to evaluate gradients,\nO(m²n) to form augmented matrix, O(m³) for factorization (sparse: O(nnz(L)²)).\nTotal iterations typically O(√n) to O(n) for well-conditioned problems.\nConvergence: locally quadratic near solution with exact Hessian.", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Main Ipopt paper]", "Nocedal & Wright (2006). \"Numerical Optimization\" 2nd ed. Springer.\n  Chapter 19. [Interior point methods for NLP]"], "see": ["IpAlgBuilder.hpp for algorithm construction", "IpIpoptData.hpp for iteration data storage", "IpBacktrackingLineSearch.hpp for filter line search", "IpPDSystemSolver.hpp for KKT system solution"], "return": "false, if the algorithm can't continue with the\n regular procedure and needs to revert to a fallback\n mechanism in the line search (such as restoration phase)", "has_pass2": true}, "src/Algorithm/IpRestoIterateInitializer.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpRestoIterateInitializer.hpp", "filename": "IpRestoIterateInitializer.hpp", "file": "IpRestoIterateInitializer.hpp", "brief": "Iterate initialization for restoration phase\n\nRestoIterateInitializer computes starting values for all variables\nin the restoration phase feasibility problem. This includes the\noriginal variables (x, s) and the slack variables (n_c, p_c, n_d, p_d).", "algorithm": "Restoration Phase Variable Initialization:\n  SetInitialIterates() initializes all restoration variables:\n  1. Copy x, s from point where main algorithm entered restoration.\n  2. Compute n_c, p_c satisfying: c(x) = p_c - n_c (equality residual split).\n  3. Compute n_d, p_d satisfying: d(x) - s + n_d - p_d ∈ [d_L, d_U].\n  4. solve_quadratic(): Finds n, p with n,p ≥ 0 minimizing ||n||+||p||.\n  5. Initialize z, v bound multipliers for n, p bounds (μ/bound).\n  6. Optionally: y_c, y_d via resto_eq_mult_calculator_ (least squares).", "math": "Slack initialization:\n  Given residual r = c(x): need n - p = -r with n, p ≥ 0.\n  If r ≥ 0: p = r + ε, n = ε (constraint violation positive).\n  If r < 0: n = -r + ε, p = ε (constraint violation negative).\n  ε = small positive to stay away from bounds.\n\nInitialization procedure:\n1. Take x, s from the point where main algorithm entered restoration\n2. Compute n, p slacks to satisfy: c(x) + n - p = 0\n3. Initialize bound multipliers (z, v) for new bounds\n4. Optionally compute equality multipliers via least squares\n\nThe slack computation solves quadratic equation v^2 + 2a*v - b = 0\nto find feasible n, p values.\n\nKey parameter:\n- constr_mult_init_max_: If initial multiplier estimate exceeds\n  this, reset to zero to avoid ill-conditioning", "see": ["IpDefaultIterateInitializer.hpp for standard cold start", "IpRestoIpoptNLP.hpp for restoration problem structure"], "has_pass2": true}, "src/Algorithm/IpIteratesVector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIteratesVector.hpp", "filename": "IpIteratesVector.hpp", "file": "IpIteratesVector.hpp", "brief": "Combined primal-dual iterate vector for IPM algorithm\n\nIteratesVector is a specialized CompoundVector with fixed 8 components\nrepresenting the complete primal-dual iterate of the interior point method:", "algorithm": "Primal-Dual Iterate Storage:\n  IteratesVector stores the complete IPM iterate:\n  - Primal: x (variables), s (inequality slacks).\n  - Constraint multipliers: y_c (equality), y_d (inequality).\n  - Bound multipliers: z_L, z_U (on x), v_L, v_U (on s).\n  Used for curr_, trial_, delta_ in IpoptData.\n  Inherits Vector ops: Norm(), Axpy(), Copy() operate on full iterate.", "math": "KKT system variables:\n  Primal feasibility: c(x) = 0, d(x) - s = 0.\n  Dual feasibility: ∇f + J_c^T·y_c + J_d^T·y_d - z_L + z_U = 0.\n  Complementarity: z_L·(x-x_L) = μe, z_U·(x_U-x) = μe, etc.\n\nComponents (indices 0-7):\n- x (0): Primal variables\n- s (1): Inequality slacks\n- y_c (2): Equality constraint multipliers\n- y_d (3): Inequality constraint multipliers\n- z_L (4): Lower bound multipliers on x\n- z_U (5): Upper bound multipliers on x\n- v_L (6): Lower bound multipliers on s\n- v_U (7): Upper bound multipliers on s\n\nThis allows the iterate to participate in vector operations (norms,\nscaling, axpy) while providing type-safe accessors for each component.\nIpoptData stores curr_, trial_, delta_ as IteratesVectors.", "see": ["IpCompoundVector.hpp for the base class", "IpIpoptData.hpp for storage of current/trial/delta iterates"], "return": "NULL, if none is currently set", "has_pass2": true}, "src/Algorithm/IpPDFullSpaceSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpPDFullSpaceSolver.hpp", "filename": "IpPDFullSpaceSolver.hpp", "file": "IpPDFullSpaceSolver.hpp", "brief": "Full-space primal-dual system solver with inertia correction\n\nPDFullSpaceSolver is the main implementation of PDSystemSolver.\nIt reduces the 8x8 primal-dual system to the 4x4 augmented system\nby eliminating bound multiplier equations:\n  d_z = S^{-1}(rhs_z - Z*P^T*d_x)\n\nKey features:\n- Iterative refinement with quality monitoring (residual_ratio)\n- Inertia correction via PDPerturbationHandler (adds delta_x, delta_c)\n- Automatic retries with increased pivot tolerance\n- Handles singular systems by adding regularization\n\nParameters:\n- min/max_refinement_steps: Iterative refinement bounds\n- residual_ratio_max: Acceptable solution quality threshold\n- neg_curv_test_tol: Tolerance for inertia heuristics", "algorithm": "Full-Space Primal-Dual System Solver:\nReduces 8×8 KKT system to 4×4 augmented system by variable elimination:\n1. Eliminate bound multipliers: Δz = S⁻¹(r_z - Z·Pᵀ·Δx)\n2. Form augmented system with diagonal Σ = S⁻¹Z in (1,1) block\n3. Factor augmented system (via AugSystemSolver)\n4. Recover full solution by back-substitution", "math": "Variable elimination for bound multipliers:\nFrom complementarity rows: Z·Pᵀ·Δx + S·Δz = r_z\nSolve: Δz = S⁻¹(r_z - Z·Pᵀ·Δx)\n\nAugmented system after elimination:\n[W + Σ   Aᵀ ] [Δx]   [r_x - Pᵀ·S⁻¹·r_z]\n[  A   -δ_c·I] [Δy] = [r_c              ]\nwhere Σ = Pᵀ·S⁻¹·Z·P is diagonal and δ_c is constraint regularization.\n\nIterative refinement: solve, compute residual r = b - Ax̂,\nsolve for correction, repeat until ||r||/||b|| < tol.", "complexity": "Dominated by augmented system factorization:\nO(n³) dense, O(nnz + n·fill) sparse. Iterative refinement adds\nO(n²) per step for residual computation.", "ref": ["Wächter & Biegler (2006). \"On the implementation of an interior-point\n  filter line-search algorithm for large-scale nonlinear programming\".\n  Mathematical Programming 106(1):25-57. [Section 3.1: Linear algebra]"], "see": ["IpPDSystemSolver.hpp for the interface", "IpAugSystemSolver.hpp for the reduced system solver", "IpPDPerturbationHandler.hpp for inertia correction"], "return": "false, if for some reason the linear system\n could not be solved (e.g. when the regularization parameter\n becomes too large)", "has_pass2": true}, "src/Algorithm/IpTimingStatistics.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpTimingStatistics.hpp", "filename": "IpTimingStatistics.hpp", "file": "IpTimingStatistics.hpp", "brief": "Collection of timing statistics for algorithm profiling\n\nTimingStatistics aggregates TimedTask objects for all major\nalgorithm components, enabling performance profiling and\nbottleneck identification.\n\nTiming categories:\n- Algorithm phases: InitializeIterates, UpdateHessian, OutputIteration,\n  UpdateBarrierParameter, ComputeSearchDirection, etc.\n- Linear system: PDSystemSolverTotal, LinearSystemFactorization,\n  LinearSystemBackSolve, LinearSystemScaling\n- NLP evaluations: f_eval_time, grad_f_eval_time, c_eval_time,\n  jac_c_eval_time, d_eval_time, jac_d_eval_time, h_eval_time\n- Auxiliary: Task1-Task6 for ad-hoc profiling\n\nKey methods:\n- ResetTimes(): Clear all accumulated times\n- EnableTimes()/DisableTimes(): Control timing overhead\n- PrintAllTimingStatistics(): Output formatted timing report\n- TotalFunctionEvaluationCpuTime(): Sum of NLP evaluation times\n\nEach TimedTask tracks CPU time, system time, and wall-clock time.", "algorithm": "Performance Profiling Infrastructure:\nHierarchical timing collection for identifying bottlenecks:\n- OverallAlgorithm: total solve time\n- ComputeSearchDirection → PDSystemSolverTotal → LinearSystemFactorization\n- NLP evaluations: f, grad_f, c, jac_c, d, jac_d, h\n\nTypical breakdown for large NLPs:\n- 60-80%: LinearSystemFactorization (sparse direct solve)\n- 10-20%: NLP evaluations (function/Jacobian/Hessian)\n- 5-10%: ComputeAcceptableTrialPoint (line search)\n- Remainder: bookkeeping, output, convergence checks", "complexity": "O(1) overhead per task start/stop (clock queries).\nPrintAllTimingStatistics: O(# tasks) to format output.", "see": ["IpTimedTask.hpp for individual task timing", "IpIpoptAlg.hpp for timing during main loop"], "has_pass2": true}, "src/Algorithm/IpSearchDirCalculator.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpSearchDirCalculator.hpp", "filename": "IpSearchDirCalculator.hpp", "file": "IpSearchDirCalculator.hpp", "brief": "Strategy interface for computing the Newton search direction\n\nSearchDirectionCalculator is the strategy interface for computing\nthe search direction at each IPM iteration. The computed direction\nis stored in IpData().delta() (an IteratesVector).", "algorithm": "Newton Search Direction Computation:\n  Given current iterate w = (x, s, y_c, y_d, z_L, z_U, v_L, v_U):\n  1. Form KKT system with current W (Hessian), J_c, J_d (Jacobians).\n  2. Compute RHS from gradient, constraint residuals, complementarity.\n  3. Solve KKT system: K·Δw = -r (via PDSystemSolver).\n  4. Store Δw in IpData().delta() for line search.\n  For predictor-corrector: compute affine direction (μ=0), then corrected.", "math": "Newton direction satisfies linearized KKT conditions:\n  ∇²L·Δx + A^T·Δy + Δz = -∇L (stationarity)\n  A·Δx = -c(x) (feasibility)\n  X·Δz + Z·Δx = σμe - XZe (complementarity)\n  where σ ∈ (0,1) is centering parameter, μ is barrier parameter.", "complexity": "O(n²) to O(n³) per iteration depending on KKT structure.\n  Dominated by linear system solve (see PDSystemSolver, AugSystemSolver).", "see": ["IpPDSearchDirCalc.hpp for the standard implementation", "IpPDSystemSolver.hpp for the linear system solver", "IpIpoptAlg.hpp for where ComputeSearchDirection() is called"], "has_pass2": true}, "src/Algorithm/IpAlgBuilder.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAlgBuilder.hpp", "filename": "IpAlgBuilder.hpp", "file": "IpAlgBuilder.hpp", "brief": "Builder pattern for constructing configured IpoptAlgorithm\n\nAlgorithmBuilder assembles a complete IpoptAlgorithm from components\nbased on user options. This implements the Builder design pattern,\ncentralizing the complex construction logic.\n\nBuild order (with dependency chain):\n1. SymLinearSolver (MA27/57/77/86/97, MUMPS, Pardiso, WSMP, etc.)\n2. AugSystemSolver (wraps SymLinearSolver for augmented system)\n3. PDSystemSolver (solves full primal-dual system)\n4. SearchDirectionCalculator, EqMultiplierCalculator\n5. IterateInitializer, LineSearch, MuUpdate, ConvergenceCheck\n\nCustomization: Subclass and override virtual Build* methods,\nor provide custom_solver in constructor.", "algorithm": "Linear Solver Selection (SymLinearSolverFactory):\n  HSL solvers (preferred): MA27/57 (multifrontal), MA77/86/97 (out-of-core)\n  Alternatives: MUMPS (parallel), Pardiso (MKL/project), WSMP, SPRAL\n  Key requirement: handle indefinite symmetric matrices with inertia detection.", "math": "[W + Σ  A'] [Δx]   [r_x]\n        [A      0 ] [Δy] = [r_c]\n  where W = ∇²L (Hessian of Lagrangian), Σ = X⁻¹Z (diagonal),\n  A = ∇c(x)' (Jacobian). Solve via sparse symmetric factorization.", "ref": ["Nocedal, J. and Wright, S.J. (2006). \"Numerical Optimization\".\n       Springer, Chapter 19 (Interior-Point Methods).", "Fiacco, A.V. and McCormick, G.P. (1968). \"Nonlinear Programming:\n       Sequential Unconstrained Minimization Techniques\". Wiley.", "Mehrotra, S. (1992). \"On the implementation of a primal-dual\n       interior point method\". SIAM J. Optimization 2(4):575-601.", "Wächter, A. and Biegler, L.T. (2006). \"On the implementation\n       of an interior-point filter line-search algorithm for large-scale\n       nonlinear programming\". Math. Programming 106(1):25-57."], "complexity": "Per iteration: O(n³) for dense, O(nnz^{1.5-2}) for sparse\n  Total: O(iterations × linear_solve_cost) where iterations ~ O(√n) typical", "see": ["IpIpoptAlg.hpp for resulting algorithm", "IpIpoptApplication.hpp for high-level usage"], "has_pass2": true}, "src/Algorithm/IpIterationOutput.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIterationOutput.hpp", "filename": "IpIterationOutput.hpp", "file": "IpIterationOutput.hpp", "brief": "Strategy interface for per-iteration output display\n\nIterationOutput is the abstract base for strategies that produce\nthe iteration summary line and optional detailed output.\n\nStandard output line (from OrigIterationOutput):\n  iter  objective   inf_pr   inf_du   lg(mu)  ||d||  lg(rg) alpha_du alpha_pr ls\n\nInfPrOutput enum controls whether inf_pr shows internal (with slacks)\nor original NLP constraint violation.\n\nImplementations:\n- OrigIterationOutput: Standard one-line summary for original problem\n- RestoIterationOutput: Output during restoration phase\n\nOutput controlled by print_level option (0-12).", "algorithm": "Iteration Output Strategy (Observer Pattern):\nStrategy interface for algorithm progress reporting:\n- WriteOutput() called once per iteration after accept trial point\n- Displays convergence metrics: objective, primal/dual infeasibility\n- Can adapt output format for different problem phases (normal vs restoration)\n\nStandard output columns:\n- iter: iteration counter\n- objective: current f(x) value\n- inf_pr: primal constraint violation ‖c(x)‖\n- inf_du: dual infeasibility ‖∇L(x,λ,z)‖\n- lg(mu): log₁₀ of barrier parameter\n- ‖d‖: search direction norm\n- lg(rg): log₁₀ of regularization δ_w\n- alpha_du, alpha_pr: step sizes for dual and primal\n- ls: line search iteration count", "see": ["IpOrigIterationOutput.hpp for standard implementation", "IpRestoIterationOutput.hpp for restoration phase", "IpJournalist.hpp for output channel management"], "has_pass2": true}, "src/Algorithm/IpIpoptData.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpIpoptData.hpp", "filename": "IpIpoptData.hpp", "file": "IpIpoptData.hpp", "brief": "Central storage for all iteration data in Ipopt\n\nIpoptData holds the algorithmic state across iterations:\n- curr_: Current iterate (x, s, y_c, y_d, z_L, z_U, v_L, v_U)\n- trial_: Trial point from line search\n- delta_: Search direction from KKT solve\n- delta_aff_: Affine-scaling step (for Mehrotra predictor-corrector)\n- W_: Hessian or Hessian approximation", "algorithm": "IPM Iterate Management:\n  Per iteration: curr → trial (via delta + line search) → accept.\n  Predictor-corrector: delta_aff (μ=0 step), then delta (corrected).\n  Step: trial = curr + α·delta where α from line search.\n  AcceptTrialPoint: curr ← trial, reset delta flags.", "math": "Primal-dual iterate structure:\n  w = (x, s, y_c, y_d, z_L, z_U, v_L, v_U) ∈ ℝⁿ×ℝˢ×ℝᵐ×...\n  x: primal variables, s: slack variables\n  y_c, y_d: equality/inequality constraint multipliers\n  z_L, z_U: bound multipliers (x ≥ x_L, x ≤ x_U)\n  v_L, v_U: slack bound multipliers", "complexity": "O(1) storage per iterate beyond vectors themselves.\n  Smart pointer caching with tags for invalidation tracking.\n  Memento pattern: stores algorithm state for rollback.\n\nAlso tracks: barrier parameter mu, iteration count, timing stats,\nconvergence tolerance, and iteration output information.", "see": ["IpIteratesVector.hpp for iterate vector structure", "IpIpoptAlg.hpp for algorithm using this data"], "has_pass2": true}, "src/Algorithm/IpLowRankAugSystemSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpLowRankAugSystemSolver.hpp", "filename": "IpLowRankAugSystemSolver.hpp", "file": "IpLowRankAugSystemSolver.hpp", "brief": "Low-rank Hessian handling via Sherman-Morrison (multiple backsolves)\n\nLowRankAugSystemSolver handles LowRankUpdateSymMatrix Hessians\n(from L-BFGS quasi-Newton) using the Sherman-Morrison formula.\n\nL-BFGS Hessian: W = sigma*I + V*M*V^T (compact representation)\n- V: n x 2k matrix of gradient/step differences\n- M: 2k x 2k small dense matrix\n- k: number of stored corrections (limited memory)\n\nSherman-Morrison approach:\n(A + UV^T)^{-1} = A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1}\n\nImplementation:\n1. Solve diagonal system: Vtilde = A^{-1}*V (2k backsolves)\n2. Form small dense matrix: J = I + V^T*Vtilde\n3. Solve J*y = V^T*A^{-1}*rhs (small dense solve)\n4. Correct: x = A^{-1}*rhs - Vtilde*y\n\nStorage:\n- J1_, J2_: Dense matrices for correction\n- Vtilde1_, Utilde2_: MultiVectorMatrix backsolve results\n- Wdiag_: Diagonal part passed to base solver", "algorithm": "Sherman-Morrison Low-Rank Augmented System Solver:\nEfficiently solve augmented system when Hessian has low-rank update:\n1. Decompose Hessian: W = σI + V·M·Vᵀ (L-BFGS compact form)\n2. Solve 2k auxiliary systems: Ṽ = (A_diag)⁻¹·V\n   where A_diag is augmented system with σI Hessian\n3. Form small (2k × 2k) correction matrix: J = M⁻¹ + Vᵀ·Ṽ\n4. Solve base system: x₀ = A_diag⁻¹·b\n5. Solve correction: J·y = Vᵀ·x₀\n6. Return: x = x₀ - Ṽ·y (Sherman-Morrison correction)", "math": "Sherman-Morrison-Woodbury formula:\n(A + UCV^T)^{-1} = A^{-1} - A^{-1}U(C^{-1} + V^TA^{-1}U)^{-1}V^TA^{-1}\n\nFor L-BFGS: A = augmented(σI), U = V = [S, Y], C = M (2k × 2k).\n\nKey insight: 2k backsolves with simpler (diagonal Hessian) system\ncheaper than one solve with dense/indefinite L-BFGS Hessian.", "complexity": "2k augmented system solves with diagonal W: O(2k·nnz(J)·fill).\nPlus O(k³) for small dense correction system.\nTotal often cheaper than one solve with explicit L-BFGS matrix.", "ref": ["Nocedal & Wright (2006). Numerical Optimization. Chapter 7: Large-Scale\n  Unconstrained Optimization. [Limited-memory quasi-Newton methods]"], "see": ["IpLowRankSSAugSystemSolver.hpp for single-backsolve version", "IpLimMemQuasiNewtonUpdater.hpp for L-BFGS Hessian"], "return": "number of negative eigenvalues of the most recent factorized matrix", "has_pass2": true}, "src/Algorithm/IpOrigIpoptNLP.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpOrigIpoptNLP.hpp", "filename": "IpOrigIpoptNLP.hpp", "file": "IpOrigIpoptNLP.hpp", "brief": "Standard IpoptNLP implementation wrapping user's NLP\n\nOrigIpoptNLP is the concrete implementation of IpoptNLP for standard\noptimization problems. It wraps the user's NLP (via TNLP/TNLPAdapter)\nand applies scaling transformations:\n\n  min s_f·f(S_x^{-1}·x̃)  s.t.  s_c·c(S_x^{-1}·x̃)=0, ...", "algorithm": "NLP Adapter with Scaling and Caching:\n  1. Initialize: Create vector/matrix spaces from NLP structure info.\n  2. Evaluate with caching: f, ∇f, c, d, J_c, J_d, H via CachedResults.\n  3. Apply scaling: x̃ = S_x·x, f̃ = s_f·f, c̃ = S_c·c, etc.\n  4. Relax bounds: x_L → x_L - ε, x_U → x_U + ε for numerical safety.\n  5. Unscale solution: Convert internal x̃, ỹ back to user space.\n  Hessian modes: EXACT (user provides) or LIMITED_MEMORY (L-BFGS).", "math": "Scaling transformation (improves conditioning):\n  Scaled problem: min s_o·f(S_x⁻¹·x̃) s.t. S_c·c(S_x⁻¹·x̃) = 0.\n  Variable scaling: x = S_x⁻¹·x̃ where S_x = diag(s_x).\n  Objective scaling: f̃ = s_o·f (single scalar).\n  Constraint scaling: c̃ = S_c·c, d̃ = S_d·d (diagonal matrices).\n  Bound scaling: x̃_L = S_x·x_L, x̃_U = S_x·x_U.\n  Multiplier transformation: y_c = s_o⁻¹·S_c·ỹ_c, z_L = s_o⁻¹·S_x·z̃_L.", "complexity": "O(n + m_c + m_d) to apply/unapply scaling (diagonal operations).\n  NLP evaluations: O(user-defined), cached to avoid redundant calls.\n  Space allocation: O(n + m) for vector/matrix space objects.", "see": ["IpIpoptNLP.hpp for the abstract interface", "IpRestoIpoptNLP.hpp for restoration phase variant", "IpNLPScaling.hpp for scaling strategies"], "has_pass2": true}, "src/Algorithm/IpAlgStrategy.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpAlgStrategy.hpp", "filename": "IpAlgStrategy.hpp", "file": "IpAlgStrategy.hpp", "brief": "Base class for all pluggable algorithm components\n\nAlgorithmStrategyObject is the abstract base for Ipopt's Strategy pattern\nimplementation. All pluggable algorithm components inherit from this:\n- LineSearch, MuUpdate, ConvergenceCheck\n- SearchDirectionCalculator, HessianUpdater\n- PDSystemSolver, AugSystemSolver", "algorithm": "Strategy Pattern with Template Method:\n  Initialize() = template method with invariant structure:\n    1. Store references to shared objects (Journalist, NLP, Data, CQ).\n    2. Call InitializeImpl() (subclass-specific hook).\n    3. Return success/failure status.\n  Ensures consistent initialization across all algorithm components.\n  ReducedInitialize() variant for components not requiring full IPM context.", "math": "Dependencies injected at runtime:\n  - IpoptNLP: problem definition (f, c, d, bounds, Hessian).\n  - IpoptData: iterate storage (x, s, y, z) and algorithm state.\n  - IpoptCalculatedQuantities: cached derived values (∇f, J, residuals).\n  Separation allows testing and alternative problem formulations.", "complexity": "O(1) for Initialize() overhead (pointer copies).\n  Actual work delegated to InitializeImpl() in each strategy.\n  Builder (IpAlgBuilder) composes O(10-20) strategy objects per run.", "see": ["IpAlgBuilder.hpp for strategy composition", "IpIpoptAlg.hpp for strategy usage"], "has_pass2": true}, "src/Algorithm/IpOptErrorConvCheck.hpp": {"path": "layer-2/Ipopt/src/Algorithm/IpOptErrorConvCheck.hpp", "filename": "IpOptErrorConvCheck.hpp", "file": "IpOptErrorConvCheck.hpp", "brief": "Standard convergence check based on optimality error tolerances\n\nOptimalityErrorConvergenceCheck implements the standard termination\ncriteria for Ipopt based on KKT optimality conditions.\n\nOptimal convergence (tol):\n- Dual infeasibility <= dual_inf_tol_\n- Constraint violation <= constr_viol_tol_\n- Complementarity <= compl_inf_tol_\n- Overall scaled error <= tol\n\nAcceptable convergence (acceptable_tol):\n- Less stringent tolerances applied for acceptable_iter_ iterations\n- Terminates early if stuck near acceptable solution\n- acceptable_obj_change_tol_: Detects stagnation in objective\n\nFailure conditions:\n- max_iterations_: Iteration limit exceeded\n- max_wall_time_, max_cpu_time_: Time limits\n- diverging_iterates_tol_: Primal variables diverging\n- mu_target_: Target barrier parameter reached\n\nCurrentIsAcceptable():\n- Returns true if acceptable-level tolerances are met\n- Used by line search to decide on restoration phase", "algorithm": "KKT Optimality Convergence Check:\nTests first-order optimality conditions (scaled and unscaled):\n1. Optimal convergence (all must hold):\n   - ||∇L||∞ ≤ dual_inf_tol (dual feasibility)\n   - ||c(x)||∞ ≤ constr_viol_tol (primal feasibility)\n   - ||X·Z·e - μ·e||∞ ≤ compl_inf_tol (complementarity)\n   - Overall scaled error ≤ tol\n2. Acceptable convergence (fallback after acceptable_iter_ iterations):\n   - Same conditions with relaxed tolerances (acceptable_*)\n   - Plus objective stagnation check: |f_k - f_{k-1}| ≤ acceptable_obj_change_tol\n3. Failure conditions:\n   - iter > max_iterations_\n   - time > max_wall_time_ or max_cpu_time_\n   - ||x||∞ > diverging_iterates_tol (divergence)\n   - μ ≤ mu_target_ (target barrier reached)", "math": "First-order KKT conditions:\n  ∇f(x) + ∇c(x)ᵀλ + ∇g(x)ᵀν - z = 0  (stationarity)\n  c(x) = 0                             (equality constraints)\n  g(x) - s = 0, s ≥ 0                  (inequality slacks)\n  X·Z·e = μ·e                          (complementarity)\n  x ≥ 0, z ≥ 0                         (bounds)\n\nScaled error: E_μ(x,λ,z) = max(||∇L||/(s_d+1), ||c||/(s_c+1), ||XZe-μe||/s_c)\nwhere s_d, s_c are scaling factors from NLP scaling.", "complexity": "O(n + m) per check for computing norms of residuals.\nCalled once per iteration.", "see": ["IpConvCheck.hpp for the base interface", "IpRestoConvCheck.hpp for restoration phase convergence"], "has_pass2": true}, "src/LinAlg/IpExpansionMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpExpansionMatrix.hpp", "filename": "IpExpansionMatrix.hpp", "file": "IpExpansionMatrix.hpp", "brief": "Expansion/projection matrices for dimension mapping\n\nExpansionMatrix maps between vectors of different sizes:\n- MultVector: Embeds small vector into larger space (zero-fills)\n- TransMultVector: Projects large vector to smaller space (extracts)", "algorithm": "Index-Based Expansion/Projection:\n  E ∈ {0,1}^{m×n} is a 0-1 matrix with exactly one 1 per column.\n  Stored as index array: ExpPos[j] = i means E[i,j] = 1.\n  MultVector: y[ExpPos[j]] = x[j] (expand), others zero.\n  TransMultVector: y[j] = x[ExpPos[j]] (project/extract).", "math": "Mathematical interpretation:\n  E·x embeds x ∈ ℝⁿ into y ∈ ℝᵐ (m > n) at specified positions.\n  E^T·x extracts components of x ∈ ℝᵐ to y ∈ ℝⁿ.\n  E^T·E = I_n (projection property), E·E^T ≠ I_m (not full rank).", "complexity": "O(n) storage (index array), O(n) for matvec.\n  No explicit m×n matrix stored - perfect for problems with\n  many fixed variables or inequality constraints.\n\nUsed in Ipopt for:\n- Mapping slack variables to constraint space\n- Handling fixed variables (remove from optimization)\n- Extracting subsets of inequality constraints", "see": ["IpMatrix.hpp for base interface", "IpTNLPAdapter.hpp for primary usage"], "has_pass2": true}, "src/LinAlg/IpMultiVectorMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpMultiVectorMatrix.hpp", "filename": "IpMultiVectorMatrix.hpp", "file": "IpMultiVectorMatrix.hpp", "brief": "Tall-skinny matrix stored as collection of column Vectors\n\nMultiVectorMatrix represents an m x k matrix (k << m) where each\ncolumn is stored as a separate Vector. Efficient for:\n- Limited-memory quasi-Newton: store recent gradient differences\n- Low-rank updates: V*V^T matvec via two sequential operations", "algorithm": "Low-Rank Matrix-Vector Product:\n  V ∈ ℝ^{m×k} stored as k column vectors.\n  MultVector: y ← αVx + βy where x ∈ ℝᵏ (linear combination of cols).\n  TransMultVector: y ← αV^T x + βy where x ∈ ℝᵐ (k dot products).\n  LRMultVector: y ← αVV^T x + βy = α·V·(V^T·x) + βy (two-stage).", "math": "Limited-memory quasi-Newton storage:\n  L-BFGS: stores {sᵢ, yᵢ} pairs for i = k-m+1,...,k.\n  sᵢ = xᵢ₊₁ - xᵢ (step), yᵢ = ∇f(xᵢ₊₁) - ∇f(xᵢ) (gradient diff).\n  Compact form: B = B₀ - [B₀S Y][R⁻^T(D+Y^T B₀Y)R⁻ ...][...]^T.\n  Inner products V^T W computed via MultiVectorMatrix ops.", "complexity": "O(mk) storage for m×k matrix.\n  O(mk) for MultVector, O(mk) for TransMultVector.\n  O(m·k²) for high-rank update forming k×k inner product matrix.\n\nUsed in Ipopt for:\n- L-BFGS/L-SR1 Hessian approximations\n- Storing {s_i, y_i} pairs for quasi-Newton", "see": ["IpLowRankUpdateSymMatrix.hpp for matrix using this", "IpExpandedMultiVectorMatrix.hpp for row-vector variant"], "has_pass2": true}, "src/LinAlg/IpVector.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpVector.hpp", "filename": "IpVector.hpp", "file": "IpVector.hpp", "brief": "Abstract base class for all vector types in Ipopt\n\nVector is the foundation of Ipopt's linear algebra abstraction.\nProvides BLAS-1 style operations (Copy, Scal, Axpy, Dot, Nrm2, etc.)\nplus IPM-specific operations (AddOneVector, FracToBound, etc.).", "algorithm": "Template Method + Abstract Factory Pattern:\n  - Public methods (Axpy, Dot, etc.) delegate to protected *_Impl methods\n  - VectorSpace factory creates compatible vectors of same type\n  - TaggedObject inheritance enables automatic cache invalidation\n  This design allows DenseVector and CompoundVector to share interface\n  while providing type-specific optimized implementations.", "math": "IPM-specific operation - FracToBound:\n  α = max{ᾱ ∈ (0,1] : x + ᾱΔx ≥ (1-τ)x}\n  Computes maximum step length maintaining fraction-to-boundary τ.\n  Critical for ensuring positivity of slack variables in interior-point.\n\nKey design features:\n- Inherits TaggedObject for automatic cache invalidation\n- Uses VectorSpace factory pattern for creating compatible vectors\n- Template Method pattern: public methods call protected *_Impl methods\n- Implementations: DenseVector, CompoundVector\n\nDerived classes override protected *_Impl methods to provide\nactual computation (e.g., CopyImpl, AxpyImpl, DotImpl).", "see": ["IpDenseVector.hpp for primary implementation", "IpCompoundVector.hpp for composite vectors", "IpMatrix.hpp for matrix abstraction"], "has_pass2": true}, "src/LinAlg/IpBlas.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpBlas.hpp", "filename": "IpBlas.hpp", "file": "IpBlas.hpp", "brief": "C++ wrappers for BLAS (Basic Linear Algebra Subprograms)\n\nProvides platform-independent access to BLAS Level 1, 2, and 3 routines.", "math": "BLAS Level 3 (matrix-matrix, O(n³)):\n  - Gemm: C ← αA·B + βC\n  - Syrk: C ← αA·A^T + βC (rank-k update)\n  - Trsm: B ← α·L⁻¹·B (triangular solve)", "complexity": "Level 1: O(n), Level 2: O(n²), Level 3: O(n³)\n  All operations are cache-optimized in vendor BLAS libraries.", "ref": ["Lawson et al. (1979). \"Basic Linear Algebra Subprograms for Fortran Usage\".\n     ACM TOMS 5(3):308-323."], "see": ["IpLapack.hpp for higher-level linear algebra (factorization, solve)"], "has_pass2": true}, "src/LinAlg/IpCompoundMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpCompoundMatrix.hpp", "filename": "IpCompoundMatrix.hpp", "file": "IpCompoundMatrix.hpp", "brief": "Block-structured matrix composed of sub-matrices\n\nCompoundMatrix implements the Composite pattern for matrices,\nrepresenting a block matrix: M = [M_00, M_01, ...; M_10, ...]", "algorithm": "Composite Pattern for Block Matrices:\n  CompoundMatrix stores a grid of sub-matrices M_{ij}. Operations like\n  MultVector are computed block-wise: y_i = Σ_j M_{ij}·x_j.\n  This recursive structure allows mixing different matrix types\n  (dense, sparse, diagonal) in a single unified representation.", "math": "Block matrix-vector multiplication:\n  For M = [M_{ij}] with i=0..r-1, j=0..c-1:\n  y = M·x computes y_i = Σ_j M_{ij}·x_j for each block row i.\n  NULL blocks M_{ij} are treated as zero: 0·x_j = 0.\n\nUsed in Ipopt for KKT system structure:\n  [W + Σ_x   A^T ] [Δx]   [r_d]\n  [A        -Σ_c ] [Δλ] = [r_p]\n  where W = Hessian, A = Jacobian, Σ = barrier regularization.\n\nNull components are treated as zero blocks. Operations recurse\nto sub-matrices. Each block can be from different MatrixSpace.", "see": ["IpMatrix.hpp for base interface", "IpCompoundSymMatrix.hpp for symmetric version", "IpCompoundVector.hpp for vector counterpart"], "has_pass2": true}, "src/LinAlg/IpIdentityMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpIdentityMatrix.hpp", "filename": "IpIdentityMatrix.hpp", "file": "IpIdentityMatrix.hpp", "brief": "Scalar multiple of the identity matrix (factor * I)\n\nIdentityMatrix represents matrices of the form factor*I where\nI is the identity. Stores only the scalar factor - O(1) storage.\nMatrix-vector multiply is just a scale operation: y = factor*x.", "algorithm": "Scalar Identity Representation:\n  M = σI where σ is a scalar factor.\n  MultVector: y ← α(σI)x + βy = ασx + βy (vector scaling).\n  No storage of matrix elements - only the scalar factor σ.", "math": "Properties of σI:\n  - Eigenvalues: all equal to σ (n-fold degeneracy)\n  - Determinant: det(σI) = σⁿ\n  - Inverse: (σI)⁻¹ = (1/σ)I\n  - Commutes with all matrices: (σI)A = A(σI)", "complexity": "O(1) storage, O(n) for matvec (single scale operation).\n  Most memory-efficient matrix representation possible.\n\nUsed in Ipopt for:\n- Regularization terms (δ_x·I, δ_c·I in augmented system)\n- Initial Hessian approximations in quasi-Newton (H₀ = γI)\n- Trust region constraints", "see": ["IpDiagMatrix.hpp for general diagonal matrices", "IpSymMatrix.hpp for base class"], "has_pass2": true}, "src/LinAlg/IpSumSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSumSymMatrix.hpp", "filename": "IpSumSymMatrix.hpp", "file": "IpSumSymMatrix.hpp", "brief": "Symmetric matrix as weighted sum: M = sum(alpha_i * M_i)\n\nSumSymMatrix represents a symmetric matrix as a sum of symmetric terms,\neach with its own scalar factor. Preserves symmetry of components.", "algorithm": "Symmetric Sum Matrix-Vector Product:\n  M = Σᵢ αᵢMᵢ where each Mᵢ is symmetric.\n  MultVector: y ← α(Σᵢ αᵢMᵢ)x + βy.\n  Computed as: y ← βy, then y += α·αᵢ·Mᵢ·x for each term.\n  Sum preserves symmetry: (Σᵢ αᵢMᵢ)^T = Σᵢ αᵢMᵢ^T = Σᵢ αᵢMᵢ.", "math": "Hessian of Lagrangian decomposition:\n  W = ∇²_xx L = ∇²f + Σᵢ λᵢ∇²gᵢ + Σⱼ νⱼ∇²hⱼ.\n  With barrier: W + μΣ_x where Σ_x = diag terms from bounds.\n  Each term can be different matrix type (dense, sparse, diagonal).", "complexity": "O(k × component matvec) for k terms.\n  Type-safe: all components must be SymMatrix.\n  Lazy evaluation: explicit sum never formed.\n\nUsed in Ipopt for KKT Hessian structure:\n- W + sum of barrier contributions + regularization\n- Combining user Hessian with barrier terms without explicit formation", "see": ["IpSumMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": true}, "src/LinAlg/IpLowRankUpdateSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpLowRankUpdateSymMatrix.hpp", "filename": "IpLowRankUpdateSymMatrix.hpp", "file": "IpLowRankUpdateSymMatrix.hpp", "brief": "Symmetric matrix as low-rank update: M = D + V*V^T - U*U^T\n\nLowRankUpdateSymMatrix represents matrices in factored form:\n  M = P_LR * (D + V*V^T - U*U^T) * P_LR^T  (if reduced_diag)\n  M = D + P_LR * (V*V^T - U*U^T) * P_LR^T  (otherwise)", "algorithm": "Limited-Memory Quasi-Newton Representation:\n  For L-BFGS with memory m, the inverse Hessian approximation H_k is:\n  H_k = (I - ρ_k s_k y_k^T)·H_{k-1}·(I - ρ_k y_k s_k^T) + ρ_k s_k s_k^T\n  This class stores the compact form: H = D + V·V^T - U·U^T\n  where V, U have at most m columns (typically m ≤ 20).", "math": "Efficient matrix-vector product without forming full matrix:\n  y = M·x = D·x + V·(V^T·x) - U·(U^T·x)\n  Cost: O(n·m) instead of O(n²) for full matrix multiply.\n  V^T·x and U^T·x are m-vectors computed first, then scaled.", "complexity": "O(n·m) for matvec where n = dimension, m = rank of update.\n  Memory: O(n·m) for V, U columns instead of O(n²) for dense matrix.", "ref": ["Nocedal (1980). \"Updating Quasi-Newton Matrices with Limited Storage\".\n     Mathematics of Computation 35(151):773-782.\n\nWhere D is diagonal, V and U are MultiVectorMatrices (few columns),\nand P_LR is an optional ExpansionMatrix for dimension lifting.\n\nThis representation is fundamental for limited-memory quasi-Newton:\n- L-BFGS: Hessian approximation as low-rank updates\n- L-SR1: Symmetric rank-1 updates\n- Efficient matvec without forming full matrix"], "see": ["IpMultiVectorMatrix.hpp for V, U storage", "IpExpansionMatrix.hpp for P_LR"], "has_pass2": true}, "src/LinAlg/IpScaledMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpScaledMatrix.hpp", "filename": "IpScaledMatrix.hpp", "file": "IpScaledMatrix.hpp", "brief": "Matrix decorator applying row and column scaling\n\nScaledMatrix wraps an unscaled matrix M to represent D_r * M * D_c\nwhere D_r and D_c are diagonal scaling matrices (stored as vectors).", "algorithm": "Scaled Matrix-Vector Multiplication:\n  Scaled matrix: M̃ = D_r · M · D_c where D_r, D_c are diagonal.\n  MultVector: y ← αM̃·x + βy = α·D_r·(M·(D_c·x)) + βy\n  Implementation: (1) z = D_c·x, (2) w = M·z, (3) y = αD_r·w + βy.\n  TransMultVector swaps row/column scaling order appropriately.", "math": "Scaling for numerical conditioning:\n  Row scaling normalizes constraint magnitudes (equilibration).\n  Column scaling balances variable contributions.\n  Goal: improve condition number κ(M̃) < κ(M).\n  Jacobian scaling: J̃ = D_c · J · D_g where D_c=constraint, D_g=gradient.", "complexity": "Same as underlying matrix M plus O(n+m) for scaling.\n  No additional matrix storage - scaling vectors only.\n  Decorator pattern: operations delegate to wrapped matrix.\n\nUsed in Ipopt for:\n- NLP scaling (user-provided or automatic gradient-based)\n- Constraint Jacobian scaling\n- Hessian scaling for numerical stability", "see": ["IpSymScaledMatrix.hpp for symmetric version", "IpMatrix.hpp for base interface"], "has_pass2": true}, "src/LinAlg/IpZeroMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpZeroMatrix.hpp", "filename": "IpZeroMatrix.hpp", "file": "IpZeroMatrix.hpp", "brief": "Matrix with all zero entries (null matrix)\n\nZeroMatrix represents a matrix of zeros with given dimensions.\nNo storage required. Matrix-vector multiply just scales y by beta.", "algorithm": "Zero Matrix Operations:\n  Z = 0 ∈ ℝ^{m×n} (all elements zero).\n  MultVector: y ← α·0·x + βy = βy (just scaling).\n  Row/ColNorms: no-op (norms are already zero or don't change).\n  No storage or computation proportional to dimensions.", "math": "Null Space Representation:\n  Z·v = 0 for all v (annihilator property).\n  rank(Z) = 0, null space = ℝⁿ.\n  Used as structural placeholder, not computed.", "complexity": "O(1) storage, O(n) for matvec (just scaling output).\n  Null Object pattern: provides interface without real computation.\n\nUsed in Ipopt for:\n- Placeholder in CompoundMatrix for absent blocks\n- Problems with no constraints (zero Jacobian)\n- Structural zeros in KKT system", "see": ["IpZeroSymMatrix.hpp for symmetric version", "IpCompoundMatrix.hpp where null blocks appear"], "has_pass2": true}, "src/LinAlg/IpDenseVector.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDenseVector.hpp", "filename": "IpDenseVector.hpp", "file": "IpDenseVector.hpp", "brief": "Dense vector implementation storing contiguous Number arrays\n\nDenseVector is the primary Vector implementation in Ipopt.\nTwo storage modes:\n- Heterogeneous: Full array of values (Values() method)\n- Homogeneous: Single scalar repeated for all elements (Scalar() method)", "algorithm": "Homogeneous Vector Optimization:\n  When all elements have same value (e.g., initialization to 1.0),\n  stores only scalar_ instead of n-element array. Transparently\n  expands to full storage when heterogeneous access needed.\n  Saves O(n) memory and enables O(1) operations on uniform vectors.", "math": "BLAS-backed operations for heterogeneous vectors:\n  - Axpy: y ← αx + y via DAXPY\n  - Dot: x^T·y via DDOT\n  - Nrm2: ||x||_2 via DNRM2\n  All other operations use optimized element-wise loops.", "complexity": "O(n) for most operations on heterogeneous vectors.\n  O(1) for operations on homogeneous vectors (scalar multiply, etc.).\n\nCheck IsHomogeneous() before accessing values to use appropriate method.\n\nFeatures:\n- Efficient BLAS operations via IpBlas wrappers\n- Metadata support for debugging (variable names)\n- DenseVectorSpace factory for creating compatible vectors\n\nOperations delegate to BLAS for performance when heterogeneous.", "see": ["IpVector.hpp for abstract interface", "IpCompoundVector.hpp for composite vectors"], "has_pass2": true}, "src/LinAlg/IpTransposeMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpTransposeMatrix.hpp", "filename": "IpTransposeMatrix.hpp", "file": "IpTransposeMatrix.hpp", "brief": "Matrix wrapper representing transpose of another matrix\n\nTransposeMatrix wraps an existing matrix M to represent M^T without\nexplicitly forming the transpose. MultVector becomes TransMultVector\non the original, and vice versa.", "algorithm": "Transpose via Operation Swapping:\n  Represents M^T by wrapping matrix M.\n  MultVector(x): y ← α(M^T)x + βy → calls M.TransMultVector(x).\n  TransMultVector(x): y ← α(M^T)^T x = αMx → calls M.MultVector(x).\n  ComputeRowAMax ↔ ComputeColAMax swapped similarly.", "math": "Transpose properties exploited:\n  (M^T)^T = M (involution)\n  (M^T)x = M^T x computed as transpose-mult on M\n  NRows(M^T) = NCols(M), NCols(M^T) = NRows(M)", "complexity": "Zero additional storage beyond pointer to original.\n  All operations delegate to original with same complexity.\n  Adapter pattern: interface transformation without data copy.\n\nUsed in Ipopt for:\n- Accessing Jacobian as J or J^T without storing both\n- Building KKT system blocks from constraint Jacobian\n- Symmetric operations that need both A and A^T", "see": ["IpMatrix.hpp for base interface"], "has_pass2": true}, "src/LinAlg/IpSymScaledMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSymScaledMatrix.hpp", "filename": "IpSymScaledMatrix.hpp", "file": "IpSymScaledMatrix.hpp", "brief": "Symmetric matrix with symmetric (congruent) scaling\n\nSymScaledMatrix wraps a symmetric matrix M to represent D * M * D\nwhere D is a diagonal scaling matrix. Unlike ScaledMatrix which has\nseparate row/column scaling, symmetric scaling uses the same vector.", "algorithm": "Symmetric Congruent Scaling:\n  M̃ = D · M · D where D = diag(d) and M = M^T.\n  MultVector: y ← αM̃x + βy = α·D·(M·(D·x)) + βy.\n  Implementation: (1) z = D·x, (2) w = M·z, (3) y = αD·w + βy.\n  Same diagonal for row and column ensures M̃ = M̃^T.", "math": "Congruence transformation properties:\n  Preserves symmetry: (DMD)^T = D^T M^T D^T = DMD.\n  Preserves definiteness: x^T(DMD)x = (Dx)^T M(Dx) ≥ 0 iff M ≥ 0.\n  Eigenvalues scaled: λ(DMD) related to λ(M) but not simple scaling.\n  Condition number: κ(M̃) can be much better than κ(M) with good D.", "complexity": "Same as underlying M plus O(n) for scaling.\n  Single scaling vector (not separate row/column).\n  Decorator pattern preserving SymMatrix type.\n\nUsed in Ipopt for:\n- Scaled Hessian of Lagrangian\n- Symmetrically scaled KKT system", "see": ["IpScaledMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": true}, "src/LinAlg/IpSumMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSumMatrix.hpp", "filename": "IpSumMatrix.hpp", "file": "IpSumMatrix.hpp", "brief": "Matrix representing weighted sum of matrices: M = sum(alpha_i * M_i)\n\nSumMatrix represents a matrix as a sum of terms, each with its own\nscalar factor: M = alpha_0*M_0 + alpha_1*M_1 + ... + alpha_n*M_n", "algorithm": "Distributed Matrix-Vector Multiplication:\n  M = Σᵢ αᵢMᵢ (weighted sum of k matrices).\n  MultVector: y ← α(Σᵢ αᵢMᵢ)x + βy\n  Distributes as: y ← βy, then y ← y + α·αᵢ·Mᵢ·x for each term.\n  No explicit sum formed - lazy evaluation through matvec.", "math": "Linear combination of matrices:\n  Each Mᵢ can have different sparsity structure but same dimensions.\n  Sum is implicitly represented; (M)ᵢⱼ = Σₖ αₖ(Mₖ)ᵢⱼ never computed.\n  Properties (rank, spectrum) not easily derived from components.", "complexity": "O(k × matvec cost) where k = number of terms.\n  Storage: k matrix pointers + k scalars.\n  Avoids O(nnz) memory for explicit sum of sparse matrices.\n\nUsed in Ipopt for:\n- Combining multiple contributions to Jacobians or Hessians\n- Building composite matrix structures lazily", "see": ["IpSumSymMatrix.hpp for symmetric version", "IpCompoundMatrix.hpp for block structure (vs linear combination)"], "has_pass2": true}, "src/LinAlg/IpZeroSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpZeroSymMatrix.hpp", "filename": "IpZeroSymMatrix.hpp", "file": "IpZeroSymMatrix.hpp", "brief": "Symmetric matrix with all zero entries\n\nZeroSymMatrix represents a symmetric zero matrix (n x n).\nNo storage required. Inherits from SymMatrix for type safety.", "algorithm": "Zero Symmetric Matrix:\n  Z = 0 ∈ ℝ^{n×n}, Z = Z^T trivially satisfied.\n  MultVector: y ← βy (no α term contributes).\n  Type-safe placeholder preserving symmetry structure.", "math": "Linear objective functions:\n  f(x) = c^T x has ∇²f = 0 (zero Hessian).\n  Hessian of Lagrangian: W = ∇²f + Σᵢ λᵢ∇²gᵢ.\n  When f linear and constraints linear: W = 0.", "complexity": "O(1) storage and O(n) for matvec scaling.\n  Null Object pattern for symmetric matrices.\n\nUsed in Ipopt for:\n- Problems with linear objectives (zero Hessian of objective)\n- Placeholder in CompoundSymMatrix for zero blocks\n- Default Hessian when user doesn't provide one", "see": ["IpZeroMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": true}, "src/LinAlg/IpMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpMatrix.hpp", "filename": "IpMatrix.hpp", "file": "IpMatrix.hpp", "brief": "Abstract base class for all (unsymmetric) matrix types\n\nMatrix provides the abstraction for general (non-symmetric) matrices.\nPrimary operations: MultVector (y = α*A*x + β*y) and TransMultVector.", "algorithm": "Template Method + Abstract Factory Pattern:\n  - MultVector/TransMultVector delegate to *_Impl for type-specific code\n  - MatrixSpace factories create compatible matrices of same structure\n  - Enables uniform interface across dense, sparse, structured matrices", "math": "BLAS Level-2 style operations on A ∈ ℝ^{m×n}:\n  - MultVector: y ← αAx + βy  (DGEMV with no transpose)\n  - TransMultVector: y ← αA^T x + βy  (DGEMV with transpose)\n  - AddMSinvZ: X ← X + α(A·S⁻¹·Z)  (specialized for ExpansionMatrix)\n  Row/column norms for scaling: ||A_{i,:}||_∞ and ||A_{:,j}||_∞\n\nKey design features:\n- MatrixSpace factory for creating compatible matrices\n- Template Method pattern: override *_Impl methods\n- Dimension information via OwnerSpace()\n- Optional methods: ComputeRowAMax, ComputeColAMax, HasValidNumbers\n\nImplementations include:\n- Dense: DenseGenMatrix\n- Structured: CompoundMatrix, SumMatrix, ScaledMatrix\n- Special: ExpansionMatrix, TransposeMatrix, DiagMatrix", "see": ["IpSymMatrix.hpp for symmetric matrices", "IpVector.hpp for vector abstraction"], "has_pass2": true}, "src/LinAlg/IpCompoundSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpCompoundSymMatrix.hpp", "filename": "IpCompoundSymMatrix.hpp", "file": "IpCompoundSymMatrix.hpp", "brief": "Block-structured symmetric matrix (lower triangle storage)\n\nCompoundSymMatrix implements a symmetric block matrix where only\nthe lower triangular blocks are stored: M[i][j] with j <= i.\nDiagonal blocks must themselves be SymMatrix types.", "algorithm": "Block Symmetric Matrix-Vector Multiply:\n  M = [M₀₀          ]    (symmetric block structure)\n      [M₁₀  M₁₁     ]\n      [M₂₀  M₂₁  M₂₂]\n  MultVector computes y_i = Σⱼ≤ᵢ Mᵢⱼ·xⱼ + Σⱼ>ᵢ Mⱼᵢ^T·xⱼ.\n  Lower blocks and their transposes both contribute.", "math": "KKT system structure in Ipopt:\n  K = [W + Σ_x   A^T] where W = ∇²L (Hessian of Lagrangian)\n      [  A      -D  ] where A = [J_c; J_d] (constraint Jacobians)\n  Σ_x = barrier Hessian diagonal, D = regularization diagonal.\n  Symmetric indefinite system requiring inertia-correcting factorization.", "complexity": "O(sum of block costs) for matvec.\n  Storage: pointers to block matrices only.\n  Null blocks treated as zero (no storage or computation).", "see": ["IpCompoundMatrix.hpp for non-symmetric version", "IpSymMatrix.hpp for base class"], "has_pass2": true}, "src/LinAlg/IpExpandedMultiVectorMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpExpandedMultiVectorMatrix.hpp", "filename": "IpExpandedMultiVectorMatrix.hpp", "file": "IpExpandedMultiVectorMatrix.hpp", "brief": "Short-fat matrix V^T*P^T with expansion for KKT construction\n\nExpandedMultiVectorMatrix represents a k x n matrix (k << n) as\nV^T * P^T where V is a MultiVectorMatrix-like collection of row\nvectors and P is an optional ExpansionMatrix.", "algorithm": "Expanded Row-Vector Matrix Operations:\n  M = V^T · P^T where V is k row vectors, P is expansion matrix.\n  MultVector: y ← αMx + βy = αV^T(P^T x) + βy.\n  TransMultVector: y ← αM^T x = αP(Vx) + βy.\n  If P is null, treated as identity (no expansion).\n  Null row vectors treated as zero (sparse row structure).", "math": "KKT system with low-rank Hessian:\n  When H ≈ D + VV^T (limited-memory), augmented system:\n  [D   V   A^T] [Δx]   [r_x]\n  [V^T -I   0 ] [w ] = [0  ]\n  [A   0   0  ] [Δy]   [r_c]\n  V^T block maps primal space to low-rank space (k dimensions).", "complexity": "O(k·m) storage for k row vectors of length m.\n  MultVector: O(k·m) if P present, otherwise O(k·n).\n  Sparse row support: null vectors cost nothing.\n\nUsed in Ipopt's KKT system construction for low-rank Hessian:\nWhen Hessian is D + V*V^T, the augmented system includes blocks\ninvolving V^T that map to different dimensions.", "see": ["IpMultiVectorMatrix.hpp for column-vector variant", "IpExpansionMatrix.hpp for P expansion", "IpLowRankUpdateSymMatrix.hpp for Hessian structure"], "has_pass2": true}, "src/LinAlg/IpCompoundVector.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpCompoundVector.hpp", "filename": "IpCompoundVector.hpp", "file": "IpCompoundVector.hpp", "brief": "Composite vector stacking multiple sub-vectors\n\nCompoundVector implements the Composite pattern for vectors,\nrepresenting: x_compound = [x_0; x_1; ...; x_{n-1}]", "algorithm": "Recursive Vector Operations:\n  Dot product: x^T y = Σᵢ xᵢ^T yᵢ (sum over components).\n  Axpy: y ← αx + y applied component-wise: yᵢ ← αxᵢ + yᵢ.\n  FracToBound: returns min over components (for step length).\n  All ops delegate to typed component implementations.", "math": "IPM iterate structure:\n  Primal-dual iterate: w = (x, s, y_c, y_d, z_L, z_U, v_L, v_U).\n  Search direction: Δw computed by KKT system solve.\n  Step length: α = min{1, τ·FracToBound} for bound constraints.", "complexity": "O(n) for n = total dimension across all components.\n  Storage: k pointers for k components.\n  Heterogeneous: each component can use different VectorSpace.\n\nUsed in Ipopt for structured iterate vectors combining:\n- Primal variables x, slack variables s\n- Dual variables (y_c, y_d, z_L, z_U)", "see": ["IpVector.hpp for base interface", "IpCompoundMatrix.hpp for matrix counterpart"], "has_pass2": true}, "src/LinAlg/IpDiagMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDiagMatrix.hpp", "filename": "IpDiagMatrix.hpp", "file": "IpDiagMatrix.hpp", "brief": "Diagonal matrix stored as a vector\n\nDiagMatrix efficiently represents diagonal matrices by storing only\nthe diagonal elements as a Vector. Matrix-vector multiply is O(n).", "algorithm": "Diagonal Matrix Representation:\n  D = diag(d₁, d₂, ..., dₙ) stored as n-vector.\n  Matrix-vector: y ← αD·x + βy computed as y_i = α·d_i·x_i + β·y_i.\n  Element-wise multiply avoids explicit matrix storage.", "math": "In IPM, diagonal matrices appear in the KKT system:\n  Σ_x = Z_L·(X-X_L)⁻¹ + Z_U·(X_U-X)⁻¹  (barrier Hessian contribution)\n  where Z_L, Z_U are dual variables and X_L, X_U are bounds.\n  These scale as 1/distance-to-bound, becoming large near bounds.", "complexity": "O(n) storage, O(n) for matvec, O(n) for row/col norms.\n  Far more efficient than storing n² elements for diagonal structure.\n\nUsed extensively in Ipopt's KKT system for:\n- Barrier Hessian contribution: diag(z_L/(x - x_L)) + diag(z_U/(x_U - x))\n- Slack variable Hessians\n- Scaling matrices", "see": ["IpIdentityMatrix.hpp for scalar multiple of identity", "IpSymMatrix.hpp for base class"], "has_pass2": true}, "src/LinAlg/IpSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpSymMatrix.hpp", "filename": "IpSymMatrix.hpp", "file": "IpSymMatrix.hpp", "brief": "Abstract base class for symmetric matrices\n\nSymMatrix extends Matrix for symmetric matrices (A = A^T).\nTransMultVector automatically delegates to MultVector since\ntranspose is a no-op for symmetric matrices.", "algorithm": "Symmetry Exploitation:\n  A = A^T implies TransMultVector ≡ MultVector.\n  ComputeColAMax ≡ ComputeRowAMax (row/col norms identical).\n  Subclasses only implement MultVector; transpose is automatic.", "math": "Symmetric matrices in optimization:\n  Hessian ∇²f symmetric by Schwarz's theorem (∂²f/∂xᵢ∂xⱼ = ∂²f/∂xⱼ∂xᵢ).\n  KKT matrix: indefinite symmetric (positive and negative eigenvalues).\n  Inertia (n₊, n₋, n₀): counts of positive/negative/zero eigenvalues.", "complexity": "Subclass-dependent. Symmetry halves storage for dense.\n  Template Method pattern: base provides transpose, subclass provides mult.\n\nImplementations include:\n- DenseSymMatrix: Full dense lower-triangular storage\n- CompoundSymMatrix, SumSymMatrix: Structured composites\n- LowRankUpdateSymMatrix: Limited-memory representations", "see": ["IpMatrix.hpp for general (unsymmetric) matrices", "IpDenseSymMatrix.hpp for dense implementation"], "has_pass2": true}, "src/LinAlg/IpDenseSymMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDenseSymMatrix.hpp", "filename": "IpDenseSymMatrix.hpp", "file": "IpDenseSymMatrix.hpp", "brief": "Dense symmetric matrix in BLAS lower-triangular storage\n\nDenseSymMatrix stores only the lower triangle in column-major format,\nfollowing BLAS/LAPACK conventions for symmetric matrices.", "algorithm": "Lower-Triangular Symmetric Storage:\n  Only A[i,j] for j ≤ i stored (lower triangle).\n  Column-major: A[i,j] at position i + j*ldim in array.\n  BLAS routines (DSYMV, DSYRK) access symmetric structure.\n  Half storage: n(n+1)/2 effective elements in n×n array.", "math": "Quasi-Newton update operations:\n  HighRankUpdate: B ← αVV^T + βB (BFGS outer product form).\n  SpecialAddForLMSR1: B ← B + D + L + L^T for SR1 representation.\n  SR1: Bₖ₊₁ = Bₖ + (y-Bs)(y-Bs)^T / (y-Bs)^T s (rank-1 update).", "complexity": "O(n²) storage (half of general n×n).\n  O(n²) for matvec via DSYMV. O(n²k) for rank-k update via DSYRK.\n\nUsed in Ipopt for:\n- Limited-memory quasi-Newton Hessian approximations (L-BFGS, L-SR1)\n- Dense KKT subsystems in reduced-space methods\n- Eigenvalue computations for inertia", "see": ["IpDenseGenMatrix.hpp for general (non-symmetric) variant", "IpMultiVectorMatrix.hpp for high-rank update operands"], "has_pass2": true}, "src/LinAlg/IpDenseGenMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpDenseGenMatrix.hpp", "filename": "IpDenseGenMatrix.hpp", "file": "IpDenseGenMatrix.hpp", "brief": "Dense general (non-symmetric) matrix with linear algebra operations\n\nDenseGenMatrix stores elements in column-major (Fortran) format\nand provides direct factorization capabilities:\n- Cholesky factorization (for positive definite matrices)\n- LU factorization with pivoting (for general matrices)\n- Forward/back substitution solves", "algorithm": "Dense Matrix Operations via LAPACK:\n  - Cholesky: A = L·L^T for SPD matrices (DPOTRF + DPOTRS)\n  - LU: A = P·L·U with partial pivoting (DGETRF + DGETRS)\n  - Eigendecomp: A = V·Λ·V^T for inertia computation (DSYEV)\n  Factorization state tracked to avoid redundant computation.", "math": "Column-major storage: A[i,j] at position i + j*nRows.\n  Matrix-vector: y ← αA·x + βy via DGEMV.\n  Matrix-matrix: C ← αA·B + βC via DGEMM.\n  High-rank update: C ← αV₁^T·V₂ + βC for L-BFGS inner products.", "complexity": "O(n³) for factorization, O(n²) for solve, O(n²) for matvec.\n  Only suitable for small dense subproblems (n ≤ few hundred).\n\nUsed in Ipopt for:\n- Small dense subsystems in limited-memory quasi-Newton\n- Eigenvalue decomposition for inertia correction\n- Dense reduced-space methods", "see": ["IpDenseSymMatrix.hpp for symmetric variant", "IpLapack.hpp for underlying LAPACK routines"], "return": "false if the factorization could not be done, e.g., when\n the matrix is not positive definite.", "has_pass2": true}, "src/LinAlg/IpLapack.hpp": {"path": "layer-2/Ipopt/src/LinAlg/IpLapack.hpp", "filename": "IpLapack.hpp", "file": "IpLapack.hpp", "brief": "C++ wrappers for LAPACK (Linear Algebra PACKage) routines\n\nProvides platform-independent access to LAPACK for dense matrices.", "algorithm": "Dense Matrix Factorization Methods:\n  - Cholesky: A = L·L^T for symmetric positive definite (SPD)\n  - LU: A = P·L·U with partial pivoting for general matrices\n  - Symmetric indefinite: A = L·D·L^T with Bunch-Kaufman pivoting", "math": "Eigenvalue decomposition (Syev):\n  For symmetric A, compute A = V·Λ·V^T where Λ = diag(λ_1,...,λ_n).\n  Returns eigenvalues in ascending order. Eigenvectors in columns of V.", "complexity": "Cholesky: O(n³/3), LU: O(2n³/3), Eigenvalues: O(n³)\n  Dense methods - use sparse solvers for large problems.", "ref": ["Anderson et al. (1999). \"LAPACK Users' Guide\". 3rd ed. SIAM."], "see": ["IpBlas.hpp for lower-level vector operations", "Linear solver interfaces for sparse factorization"], "has_pass2": true}, "src/Apps/AmplSolver/AmplTNLP.hpp": {"path": "layer-2/Ipopt/src/Apps/AmplSolver/AmplTNLP.hpp", "filename": "AmplTNLP.hpp", "file": "AmplTNLP.hpp", "brief": "AMPL interface for Ipopt via .nl files\n\nAmplTNLP: TNLP implementation reading AMPL .nl files via ASL library.\nHandles variable bounds, constraint types, sparse Jacobian/Hessian.\nSupports warm starting, dual initialization, and AMPL suffixes.\nPrimary interface for AMPL-based optimization models.", "return": "a pointer to a char* with the name of the stub", "has_pass2": false}, "src/contrib/CGPenalty/IpCGPenaltyLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyLSAcceptor.hpp", "filename": "IpCGPenaltyLSAcceptor.hpp", "file": "IpCGPenaltyLSAcceptor.hpp", "brief": "Chen-Goldfarb penalty function line search acceptor", "algorithm": "Chen-Goldfarb Penalty Line Search\n\nCGPenaltyLSAcceptor implements the backtracking line search acceptor\nbased on the Chen-Goldfarb penalty function approach, providing an\nalternative to the standard filter method.\n\n**Merit Function:**", "math": "φ_ρ(x) = f(x) + ρ · ‖c(x)‖₁\n\n**Acceptance criteria:**\n1. Armijo condition: φ_ρ(x + αd) ≤ φ_ρ(x) + η·α·∇φ_ρ(x)'d\n2. Piecewise linear penalty function (PLPF) acceptability\n\n**PLPF Non-monotone Acceptance:**\nMaintains envelope of (ρ, f, ‖c‖) triples defining acceptable region.\nAllows steps rejected by Armijo if they make progress toward feasibility,\navoiding the Maratos effect while ensuring global convergence.", "complexity": "O(1) per acceptance check; O(k) PLPF update where k = breakpoints", "ref": ["Chen & Goldfarb, \"Interior-point l2-penalty methods for nonlinear\n     programming with strong global convergence properties\", Math. Prog., 2006"], "see": ["IpPiecewisePenalty.hpp for the PLPF data structure", "IpCGSearchDirCalc.hpp for direction computation", "IpBacktrackingLSAcceptor.hpp for base interface"], "return": "false, if no such fall back option is available", "has_pass2": true}, "src/contrib/CGPenalty/IpPiecewisePenalty.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpPiecewisePenalty.hpp", "filename": "IpPiecewisePenalty.hpp", "file": "IpPiecewisePenalty.hpp", "brief": "Piecewise linear penalty function (PLPF) data structure", "algorithm": "Piecewise Linear Penalty Function (PLPF)\n\nPiecewisePenalty maintains a list of break points for the piecewise\nlinear penalty function used in the Chen-Goldfarb globalization.\n\n**Entry structure (PiecewisePenEntry):**\n- pen_r: Penalty parameter value at this break point\n- barrier_obj: Barrier objective function value\n- infeasi: Constraint violation (infeasibility)\n\n**Acceptability test:**\nTrial point (f_trial, θ_trial) is acceptable if it lies below the\nenvelope defined by existing breakpoints. This creates a non-monotone\nacceptance criterion similar to the filter method but using penalty.", "math": "Accept if: f_trial ≤ P(θ_trial) - γ_f · θ_trial", "complexity": "O(k) acceptability check where k = number of breakpoints", "ref": ["Chen & Goldfarb, \"Interior-point l2-penalty methods\", Math. Prog., 2006"], "see": ["IpCGPenaltyLSAcceptor.hpp for usage in line search", "IpFilterLSAcceptor.hpp for filter-based alternative"], "return": "true, if pair is acceptable", "has_pass2": true}, "src/contrib/CGPenalty/IpCGPerturbationHandler.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPerturbationHandler.hpp", "filename": "IpCGPerturbationHandler.hpp", "file": "IpCGPerturbationHandler.hpp", "brief": "Perturbation handler for Chen-Goldfarb penalty method", "algorithm": "KKT Perturbation for Chen-Goldfarb Method\n\nCGPerturbationHandler manages the diagonal perturbations (δ_x,\nδ_s, δ_c, δ_d) for the KKT system when using the\nChen-Goldfarb penalty function approach.\n\n**Perturbation purposes:**\n- Handle singular or nearly singular KKT matrices\n- Correct wrong inertia in factorization\n- Switch between pure Newton and penalty-regularized steps\n\n**Regularized KKT system:**", "math": "[   A       -I    -δ_c I  ] [Δy]   [r_c]\n\n**Adaptive strategy:**\nStart with δ = 0 for pure Newton. If factorization fails or has\nwrong inertia, increase perturbations geometrically until success.", "complexity": "O(1) per perturbation decision", "see": ["IpPDPerturbationHandler.hpp for base class", "IpCGSearchDirCalc.hpp for direction computation"], "return": "false, if no suitable perturbation could be found", "has_pass2": true}, "src/contrib/CGPenalty/IpCGPenaltyCq.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyCq.hpp", "filename": "IpCGPenaltyCq.hpp", "file": "IpCGPenaltyCq.hpp", "brief": "Calculated quantities for Chen-Goldfarb penalty method\n\nCGPenaltyCq provides cached computation of quantities specific to\nthe Chen-Goldfarb penalty function algorithm, extending the base\nIpoptCalculatedQuantities.\n\nKey computed quantities:\n- curr/trial_penalty_function(): phi_rho(x) = f(x) + rho*||c(x)||\n- curr_direct_deriv_penalty_function(): Directional derivative\n- curr_fast_direct_deriv_penalty_function(): For fast CG direction\n- curr_cg_pert_fact(): Perturbation factor for KKT regularization\n- compute_curr_cg_penalty(): Line search penalty parameter\n- compute_curr_cg_penalty_scale(): KKT scaling penalty\n\nAlso computes Jacobian norms and multiplier scaling factors\nneeded for the penalty parameter update rules.\n\nAll quantities use CachedResults for efficiency.", "see": ["IpIpoptCalculatedQuantities.hpp for base class", "IpCGPenaltyData.hpp for algorithm state"], "has_pass2": false}, "src/contrib/CGPenalty/IpCGSearchDirCalc.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGSearchDirCalc.hpp", "filename": "IpCGSearchDirCalc.hpp", "file": "IpCGSearchDirCalc.hpp", "brief": "Search direction calculator for Chen-Goldfarb penalty method", "algorithm": "Chen-Goldfarb Search Direction Computation\n\nCGSearchDirCalculator computes the search direction for the\nChen-Goldfarb penalty function algorithm. It produces two types\nof directions stored in CGPenaltyData:\n\n**Direction types:**\n1. delta_cgpen: Standard CG direction with penalty regularization\n2. delta_cgfast: \"Fast\" direction attempting pure Newton steps\n\n**Fast direction strategy:**\nWhen close to solution (measured by κ_x, κ_y criteria), attempt\npure Newton step for superlinear convergence. Fall back to\npenalty-regularized direction if Newton step fails quality tests.\n\n**Penalty parameter management:**", "math": "ρ updated: ensure sufficient descent in penalty function", "complexity": "Same as PDSystemSolver: O(n³) direct, O(n·k) iterative", "ref": ["Chen & Goldfarb, \"Interior-point l2-penalty methods\", Math. Prog., 2006"], "see": ["IpSearchDirCalculator.hpp for base interface", "IpCGPenaltyLSAcceptor.hpp for line search using these directions"], "has_pass2": true}, "src/contrib/CGPenalty/IpCGPenaltyData.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyData.hpp", "filename": "IpCGPenaltyData.hpp", "file": "IpCGPenaltyData.hpp", "brief": "Algorithm state data for Chen-Goldfarb penalty method\n\nCGPenaltyData extends IpoptAdditionalData to store all state\nspecific to the Chen-Goldfarb penalty function algorithm.\n\nStored directions:\n- delta_cgpen_: Standard Chen-Goldfarb search direction\n- delta_cgfast_: Fast (pure Newton) direction when near solution\n\nPenalty parameters:\n- curr_penalty_: Current line search penalty parameter (rho)\n- curr_kkt_penalty_: Penalty for KKT system scaling\n- curr_penalty_pert_: Current perturbation factor\n\nAlgorithm control:\n- never_try_pure_Newton_: Flag to disable fast direction\n- restor_iter_: Iteration count for restoration tracking\n- max_alpha_x_: Recorded primal step size\n\nThe data is accessed via IpData().AdditionalData() and cast\nto CGPenaltyData using the CGPenData() accessor pattern.", "see": ["IpIpoptData.hpp for base iteration data", "IpCGPenaltyCq.hpp for computed quantities"], "has_pass2": false}, "src/contrib/CGPenalty/IpCGPenaltyRegOp.hpp": {"path": "layer-2/Ipopt/src/contrib/CGPenalty/IpCGPenaltyRegOp.hpp", "filename": "IpCGPenaltyRegOp.hpp", "file": "IpCGPenaltyRegOp.hpp", "brief": "Registration of Chen-Goldfarb penalty method options\n\nDeclares RegisterOptions_CGPenalty(), which registers all options\nspecific to the Chen-Goldfarb penalty function globalization\nstrategy.\n\nOptions include:\n- Penalty parameter initialization and bounds\n- PLPF (piecewise linear penalty function) parameters\n- Fast direction control parameters\n- Armijo and sufficient decrease factors\n- Perturbation handling parameters\n\nCalled when Ipopt is configured to use the CG penalty method\ninstead of the default filter line search.", "see": ["IpAlgRegOp.hpp for main algorithm options", "IpCGPenaltyLSAcceptor.hpp for option usage"], "has_pass2": false}, "src/Algorithm/Inexact/IpInexactNewtonNormal.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactNewtonNormal.hpp", "filename": "IpInexactNewtonNormal.hpp", "file": "IpInexactNewtonNormal.hpp", "brief": "Newton normal step from slack-scaled augmented system\n\nInexactNewtonNormalStep computes the normal step component by\nsolving a reduced system derived from the slack-scaled KKT system.", "algorithm": "Newton Normal Step:\n  ComputeNewtonNormalStep(newton_x, newton_s):\n  1. Form reduced system in slack-scaled space:\n     [0, J_c^T, J_d^T; J_c, 0, 0; J_d, 0, -I] [Δx; Δy_c; Δy_d] = -[0; c; d-s].\n  2. Solve via AugSystemSolver (may be iterative).\n  3. Return slack-scaled step (caller transforms back).\n  Note: W=0 in normal step—only feasibility, no optimality.", "math": "Minimum-norm feasibility step:\n  Normal step minimizes ||Δx||² subject to J·Δx + c = 0 (linearized).\n  Closed form: Δx_n = -J^T·(J·J^T)^{-1}·c = -J^†·c.\n  Augmented system equivalent to computing pseudo-inverse times residual.\n  Slack scaling: Δs̃ = S^{-1}·Δs for better conditioning.", "complexity": "O(nnz·k) for iterative solve, O(m²n) worst-case direct.\n  Typically k iterations where k depends on preconditioner quality.", "see": ["IpInexactDoglegNormal.hpp for dogleg wrapper using this", "IpAugSystemSolver.hpp for the underlying solver interface"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactSearchDirCalc.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactSearchDirCalc.hpp", "filename": "IpInexactSearchDirCalc.hpp", "file": "IpInexactSearchDirCalc.hpp", "brief": "Search direction via normal-tangential decomposition\n\nInexactSearchDirCalculator computes the search direction using\niterative linear solvers by decomposing into normal and tangential\ncomponents, enabling inexact Newton methods.", "algorithm": "Normal-Tangential Step Decomposition:\n  ComputeSearchDirection():\n  1. Compute normal step Δx_n via InexactNormalStepCalculator:\n     - Solves: min ||Δx_n||² s.t. ||A·Δx_n + c|| ≤ κ·||c|| (feasibility).\n  2. Compute tangential step Δx_t via InexactPDSolver:\n     - Solves primal-dual system with modified RHS.\n     - Δx_t ∈ null(A) approximately (tangent to constraints).\n  3. Combine: Δx = Δx_n + Δx_t, store in InexactData.\n  4. Check local infeasibility: ||A·Δx_n||₂ ≤ local_inf_Ac_tol.", "math": "Decomposition strategy:\n  Normal step: Δx_n = -A^†·c(x) (minimum norm solution toward feasibility).\n  Tangential step: Δx_t = (I - A^†A)·d (projects optimality onto null(A)).\n  Full step: Δx = Δx_n + Δx_t achieves both feasibility and optimality.\n  Decomposition modes: ALWAYS, ADAPTIVE, SWITCH_ONCE (based on progress).", "complexity": "Normal step: O(nnz·k_n) for iterative solve.\n  Tangential step: O(nnz·k_t) for primal-dual solve.\n  Total: O(nnz·(k_n + k_t)), typically k_n, k_t << n.", "see": ["IpInexactNormalStepCalc.hpp for normal step interface", "IpInexactPDSolver.hpp for tangential step solver"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactRegOp.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactRegOp.hpp", "filename": "IpInexactRegOp.hpp", "file": "IpInexactRegOp.hpp", "brief": "Registration function for all inexact algorithm options\n\nRegisterOptions_Inexact() registers all configurable parameters\nfor the inexact Newton algorithm variant with the options system.", "algorithm": "Option Registration:\n  RegisterOptions_Inexact(roptions):\n  1. Calls RegisterOptions() on all Inexact* classes.\n  2. Registers termination test parameters (tcc_*, tt_*).\n  3. Registers Pardiso iterative parameters (dropping, fill, etc.).\n  4. Registers decomposition and line search options.\n  Centralizes registration for clean algorithm construction.", "see": ["IpAlgBuilder.hpp for how options are used during construction"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactAlgBuilder.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactAlgBuilder.hpp", "filename": "IpInexactAlgBuilder.hpp", "file": "IpInexactAlgBuilder.hpp", "brief": "Builder for inexact step computation algorithm variant\n\nInexactAlgorithmBuilder constructs the complete IpoptAlgorithm\nconfigured for inexact Newton methods using iterative linear solvers.", "algorithm": "Inexact Algorithm Construction:\n  BuildBasicAlgorithm():\n  1. Create InexactData and InexactCq for algorithm-specific storage.\n  2. Configure iterative linear solver (Pardiso iterative, GMRES).\n  3. Build InexactNormalStepCalculator (dogleg or Newton).\n  4. Build InexactPDSolver for tangential step.\n  5. Build InexactSearchDirCalculator combining normal + tangential.\n  6. Build InexactLSAcceptor for penalty line search.\n  7. Assemble into IpoptAlgorithm with inexact components.", "math": "Inexact vs standard Ipopt:\n  Standard: Direct LDL^T factorization, O(n³) per iteration.\n  Inexact: Iterative solves, O(nnz·k) per iteration, k << n.\n  Trade-off: Fewer flops per iteration, but possibly more iterations.\n  Best for: Large-scale problems where factorization is prohibitive.", "complexity": "Algorithm build: O(1) (just object construction).\n  Per-iteration: O(nnz·k) instead of O(n³) for direct methods.", "see": ["IpAlgBuilder.hpp for standard algorithm builder", "IpInexactSearchDirCalc.hpp for search direction component"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactPDSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactPDSolver.hpp", "filename": "IpInexactPDSolver.hpp", "file": "IpInexactPDSolver.hpp", "brief": "Primal-dual system solver for inexact Newton methods\n\nInexactPDSolver solves the primal-dual system using iterative linear\nsolvers, allowing for inexact solutions that don't fully satisfy\nthe linearized KKT conditions.", "algorithm": "Inexact Primal-Dual System Solution:\n  Solve(rhs, sol) for the tangential step:\n  1. Form augmented system: [W + δI, J^T; J, -δI] [Δx; Δy] = rhs.\n  2. Call iterative solver (e.g., GMRES, Pardiso iterative mode).\n  3. Check tangential component condition (TCC):\n     ||J·Δx_t||₂ ≤ ψ·θ·μ^θ_exp (controls constraint linearization error).\n  4. If TCC violated → increase regularization δ, retry.\n  5. Compute residual: resid = rhs - A·sol, verify acceptable accuracy.", "math": "Tangential Component Condition (TCC):\n  For step Δx = Δx_n + Δx_t (normal + tangential decomposition):\n  ||A_c·Δx_t||₂ ≤ ψ·θ·μ^exp ensures tangential step stays near null(J).\n  Parameters: tcc_psi (ψ), tcc_theta (θ), tcc_theta_mu_exponent (exp).\n  Allows inexact linear solves while maintaining convergence theory.", "complexity": "Per solve: O(nnz·k) for k iterative solver iterations.\n  k depends on conditioning and preconditioner quality.\n  Direct methods: O(n³), iterative: O(n·k) where k << n typically.", "see": ["IpInexactSearchDirCalc.hpp for search direction computation", "IpAugSystemSolver.hpp for base augmented system interface"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactLSAcceptor.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactLSAcceptor.hpp", "filename": "IpInexactLSAcceptor.hpp", "file": "IpInexactLSAcceptor.hpp", "brief": "Penalty function line search for inexact step algorithm\n\nInexactLSAcceptor implements backtracking line search using an\nexact penalty function, adapted for inexact Newton steps that\ndon't fully satisfy the linearized KKT conditions.", "algorithm": "Penalty Function Line Search:\n  CheckAcceptabilityOfTrialPoint(α):\n  1. Evaluate merit: φ(x+αΔx) = f(x+αΔx) + ν·||c(x+αΔx)||₁.\n  2. Compute predicted reduction: pred = -α·(∇f^T·Δx + ν·||c||₁).\n  3. Armijo condition: φ(x+αΔx) ≤ φ(x) - η·pred.\n  4. If not accepted: α ← ρ·α (backtrack), retry.\n  5. Update penalty ν if step rejected due to constraint violation.", "math": "Exact penalty function:\n  φ_ν(x) = f(x) + ν·||c(x)||₁ (ℓ₁ penalty on constraints).\n  ν updated adaptively: ν ← max(ν, ν_low + increment).\n  ν_low provides lower bound for penalty (Curtis-Nocedal flexible penalty).\n  Watchdog: stores reference point for non-monotone acceptance.", "complexity": "O(m) per merit evaluation (constraint norm).\n  No second-order correction (SOC) for inexact—too expensive.\n  Line search typically O(1) to O(log(1/α_min)) backtracks.", "see": ["IpBacktrackingLSAcceptor.hpp for base backtracking interface", "IpInexactSearchDirCalc.hpp for search direction computation"], "param": ["in_watchdog indicates if we are currently in an active watchdog procedure", "alpha_primal_test the value of alpha that has been used\n for in the acceptance test earlier."], "has_pass2": true}, "src/Algorithm/Inexact/IpIterativePardisoSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpIterativePardisoSolverInterface.hpp", "filename": "IpIterativePardisoSolverInterface.hpp", "file": "IpIterativePardisoSolverInterface.hpp", "brief": "Pardiso iterative solver interface for inexact Newton method\n\nIterativePardisoSolverInterface wraps the Pardiso sparse solver in\niterative (preconditioned Krylov) mode for use with inexact Newton.", "algorithm": "Pardiso Iterative Solve:\n  MultiSolve(A, b, x):\n  1. If new_matrix: SymbolicFactorization(A) → analyze sparsity.\n  2. Factorization(A): Compute incomplete LU preconditioner.\n     Dropping: factors with |L[i,j]| < drop_tol·||row||₂ dropped.\n  3. Solve(b, x): Preconditioned GMRES/BiCGSTAB iteration.\n     Iteration controlled by IterativeSolverTerminationTester.\n     Callback to TestTermination() after each Krylov iteration.\n  4. Return when termination test satisfied (TEST_1/2/3 or MODIFY_HESSIAN).", "math": "Preconditioned iterative method:\n  Solve Ax = b via x_{k+1} = x_k + M^{-1}·r_k where M ≈ A.\n  M = incomplete LU from Pardiso with controlled fill-in.\n  Parameters: dropping_factor, dropping_schur, max_row_fill.\n  Matching strategy: COMPLETE, COMPLETE2x2, CONSTRAINT for pivoting.", "complexity": "Per solve: O(nnz·k) for k Krylov iterations.\n  Preconditioner setup: O(nnz·fill) where fill controlled by parameters.\n  Much cheaper than direct O(n³) for large sparse systems.", "see": ["IpPardisoSolverInterface.hpp for direct Pardiso interface", "IpIterativeSolverTerminationTester.hpp for stopping criteria"], "return": "true, if linear solver provides inertia", "has_pass2": true}, "src/Algorithm/Inexact/IpInexactCq.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactCq.hpp", "filename": "IpInexactCq.hpp", "file": "IpInexactCq.hpp", "brief": "Cached quantities for inexact Newton / Chen-Goldfarb penalty method\n\nInexactCq provides precomputed and cached quantities specific to the\ninexact Newton algorithm, extending IpoptCalculatedQuantities.", "algorithm": "Cached Inexact Newton Quantities:\n  Each quantity computed on-demand and cached:\n  - curr_jac_cdT_times_curr_cdminuss(): J^T·c for gradient of infeasibility.\n  - curr_scaling_slacks(): Slack-based scaling factors for conditioning.\n  - curr_slack_scaled_d_minus_s(): Scaled inequality residuals.\n  - curr_scaled_Ac_norm(): ||J·c||₂ scaled for termination tests.\n  - curr_Wu_x/s(): W·u products for tangential step Hessian terms.\n  - curr_jac_times_normal_c/d(): J·Δx_n for normal step quality.\n  Caching via CachedResults with dependency-based invalidation.", "math": "Key quantities:\n  Infeasibility gradient: ∇θ = J_c^T·c + J_d^T·(d-s) (gradient of ||c||²).\n  Slack scaling: σᵢ = min(slack_scale_max, sᵢ) improves conditioning.\n  Scaled residuals: s⁻¹·(d-s) for slack-scaled linear systems.\n  W·u: Hessian-tangential product for curvature along step.", "complexity": "O(nnz) for each Jacobian-vector product.\n  Caching amortizes cost when same quantity needed multiple times.", "see": ["IpInexactData.hpp for storage of normal/tangential steps", "IpIpoptCalculatedQuantities.hpp for base cached quantities"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactData.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactData.hpp", "filename": "IpInexactData.hpp", "file": "IpInexactData.hpp", "brief": "Algorithm-specific data storage for inexact Newton method\n\nInexactData stores the decomposed step components and algorithm state\nrequired by the inexact Newton / Chen-Goldfarb penalty method.", "algorithm": "Inexact Algorithm Data Management:\n  AcceptTrialPoint() at end of each iteration:\n  1. Transfer trial point to current iterate (base class).\n  2. Reset compute_normal flag from next_compute_normal.\n  3. Clear normal/tangential step storage for fresh iteration.\n  4. Transfer full_step_accepted flag for trust region update.", "math": "Data stored:\n  Normal step: (normal_x_, normal_s_) = Δx_n toward feasibility.\n  Tangential step: (tangential_x_, tangential_s_) = Δx_t toward optimality.\n  Penalty parameter: curr_nu_ for ℓ₁ merit function.\n  Decomposition flags: compute_normal_, next_compute_normal_.\n  Step acceptance: full_step_accepted_ for trust region expansion.", "complexity": "O(n) storage for step vectors.\n  O(1) for scalar flags and parameters.", "see": ["IpInexactCq.hpp for computed quantities", "IpIpoptData.hpp for base algorithm data"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactTSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactTSymScalingMethod.hpp", "filename": "IpInexactTSymScalingMethod.hpp", "file": "IpInexactTSymScalingMethod.hpp", "brief": "Slack-based matrix scaling for inexact algorithm\n\nInexactTSymScalingMethod computes symmetric scaling factors for the\naugmented system, using current slack values for the inexact algorithm.", "algorithm": "Slack-Based Symmetric Scaling:\n  ComputeSymTScalingFactors(A) → diagonal scaling D:\n  1. For rows/cols corresponding to slack variables:\n     D[i] = 1/√(curr_scaling_slacks[i]) from InexactCq.\n  2. For other rows/cols: D[i] = 1 (no scaling).\n  3. Scale matrix: Ã = D·A·D (symmetric similarity transform).\n  Result: better conditioned system for iterative solver.", "math": "Slack scaling rationale:\n  Augmented system has (1,1) block W and (2,2) block with slack terms.\n  Slack values s can vary widely (near bounds vs interior).\n  Scaling by s^{-1/2} equilibrates the blocks.\n  Improves convergence of Krylov methods (GMRES, CG).", "complexity": "O(nnz) to apply scaling factors.\n  Scaling factors from cached InexactCq quantities.", "see": ["IpTSymScalingMethod.hpp for base scaling interface", "IpInexactCq.hpp for curr_scaling_slacks computation"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactDoglegNormal.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactDoglegNormal.hpp", "filename": "IpInexactDoglegNormal.hpp", "file": "IpInexactDoglegNormal.hpp", "brief": "Dogleg trust region method for normal step computation\n\nInexactDoglegNormalStep computes the normal step using a dogleg\napproach that combines steepest descent and Newton directions\nwithin a trust region.", "algorithm": "Dogleg Normal Step:\n  ComputeNormalStep():\n  1. Compute Cauchy point: x_c = x - α_c·A^T·c (steepest descent).\n     α_c = ||A^T·c||² / ||A·A^T·c||² (optimal step along gradient).\n  2. Compute Newton point: x_n via InexactNewtonNormalStep.\n  3. If ||x_c - x|| ≥ ω (trust region): return scaled Cauchy step.\n  4. If ||x_n - x|| ≤ ω: return Newton step (inside trust region).\n  5. Else: interpolate on dogleg path between Cauchy and Newton.\n     x = x_c + τ·(x_n - x_c) where τ chosen so ||x - x₀|| = ω.", "math": "Dogleg path geometry:\n  Dogleg: piecewise linear path from 0 → Cauchy → Newton.\n  Trust region: ||Δx|| ≤ ω (current trust radius).\n  ω updated based on ratio: ρ = actual_reduction / predicted_reduction.\n  If ρ high → expand ω; if ρ low → contract ω.", "complexity": "Cauchy point: O(nnz) for A^T·c and A·(A^T·c).\n  Newton point: O(nnz·k) for iterative solve.\n  Interpolation: O(n) for linear combination.", "see": ["IpInexactNewtonNormal.hpp for Newton step computation", "IpInexactNormalStepCalc.hpp for base normal step interface"], "has_pass2": true}, "src/Algorithm/Inexact/IpInexactNormalStepCalc.hpp": {"path": "layer-2/Ipopt/src/Algorithm/Inexact/IpInexactNormalStepCalc.hpp", "filename": "IpInexactNormalStepCalc.hpp", "file": "IpInexactNormalStepCalc.hpp", "brief": "Abstract base class for normal step computation\n\nInexactNormalStepCalculator defines the interface for computing\nthe normal step component in the inexact Newton decomposition.", "algorithm": "Normal Step Interface:\n  ComputeNormalStep(normal_x, normal_s):\n  1. Compute step toward constraint feasibility.\n  2. Return (Δx_n, Δs_n) satisfying feasibility reduction target.\n  3. Target: ||c(x) + J·Δx_n|| ≤ κ·||c(x)|| for some κ < 1.\n  Implementations: dogleg (trust region), Newton (direct solve).", "math": "Normal step goal:\n  Minimize ||Δx_n||² subject to linearized feasibility improvement.\n  Δx_n = argmin ||Δx||² s.t. ||J·Δx + c|| ≤ κ·||c|| (trust region).\n  Or: Δx_n = -J^†·c(x) (minimum norm, pseudo-inverse solution).\n  Normal step lies in range(J^T), orthogonal to null(J).", "complexity": "Implementation-dependent: O(nnz·k) for iterative.", "see": ["IpInexactDoglegNormal.hpp for dogleg trust-region implementation", "IpInexactNewtonNormal.hpp for Newton-based implementation"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpPardisoMKLSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpPardisoMKLSolverInterface.hpp", "filename": "IpPardisoMKLSolverInterface.hpp", "file": "IpPardisoMKLSolverInterface.hpp", "brief": "Interface to Intel MKL PARDISO sparse solver\n\nPardisoMKLSolverInterface wraps Intel's MKL implementation of PARDISO.\nWhile sharing the same API as pardiso-project.org's version, Intel MKL\nPARDISO has some differences in features and behavior.", "algorithm": "Intel MKL PARDISO LDL^T Factorization:\n  Multi-level nested dissection with supernodal blocking:\n  - Phase 11: Analysis - symbolic factorization, ordering\n  - Phase 22: Numerical factorization with threshold pivoting\n  - Phase 33: Forward/backward substitution\n  Automatic parallelism via Intel Threading Building Blocks (TBB).", "math": "Computes A = P·L·D·L^T·P^T for MTYPE=-2 (symmetric indefinite):\n  - P = METIS nested dissection + constrained AMD refinement\n  - L = unit lower triangular with supernodal blocking\n  - D = block diagonal (1×1 and 2×2 Bunch-Kaufman pivots)\n  Weighted matching preprocessor improves pivot stability.", "complexity": "O(n·f²/p) with p threads. Intel MKL provides highly\n  optimized BLAS3 kernels for dense supernode operations.\n  Typically 10-30% faster than HSL solvers on Intel CPUs.", "ref": ["Schenk & Gärtner (2004). \"Solving Unsymmetric Sparse Systems\n     of Linear Equations with PARDISO\". J. Future Gen. Comp. Sys.\n\nKey differences from pardiso-project.org version:\n- Linked directly with Intel MKL (no dynamic loading)\n- No DPARM array (only IPARM for parameters)\n- No iterative solver mode\n- Different default parameters in some cases\n\nInput format: CSR_Format_1_Offset (upper triangular)\nProvides inertia: Yes\nMTYPE=-2: Real symmetric indefinite\n\nMatching strategies same as pardiso-project.org version:\nCOMPLETE, COMPLETE2x2, CONSTRAINT\n\nIntroduced in Ipopt 3.14.0 as a distinct interface from the\npardiso-project.org version."], "see": ["IpPardisoSolverInterface.hpp for pardiso-project.org version", "IpMa97SolverInterface.hpp for HSL alternative"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpTSymLinearSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTSymLinearSolver.hpp", "filename": "IpTSymLinearSolver.hpp", "file": "IpTSymLinearSolver.hpp", "brief": "Driver connecting SymMatrix to sparse linear solver interfaces\n\nTSymLinearSolver is the main driver that connects Ipopt's SymMatrix\nobjects to concrete sparse linear solvers. It handles:\n- Matrix format conversion (SymMatrix to triplet/CSR)\n- Optional matrix scaling\n- Delegation to SparseSymLinearSolverInterface implementations", "algorithm": "Triplet Linear Solver Driver:\n  MultiSolve(A, rhs, sol):\n  1. Check if structure changed via atag_ (TaggedObject comparison).\n  2. If new structure: InitializeStructure() extracts triplet indices.\n  3. GiveMatrixToSolver(): Copy values, apply scaling if enabled.\n  4. Call solver_interface_->MultiSolve() for factorization + solve.\n  5. Unscale solution: x = D·x_scaled where D = diag(scaling_factors_).\n  On IncreaseQuality(): Enable scaling if linear_scaling_on_demand_.", "math": "Matrix scaling for conditioning:\n  Scaled system: D·A·D·(D^{-1}·x) = D·b where D = diag(d_i).\n  Goal: equilibrate rows/columns so max|A_scaled[i,j]| ≈ 1.\n  Improves numerical stability and convergence of iterative refinement.\n\nFormat handling:\n- Queries solver's MatrixFormat() preference\n- Uses TripletToCSRConverter if solver needs CSR format\n- Stores triplet indices in airn_/ajcn_ arrays\n\nScaling support:\n- Optional TSymScalingMethod for equilibration\n- Scales matrix, RHS, and solution: D*A*D, D*b, D^{-1}*x\n- linear_scaling_on_demand_: Enable scaling when quality increase requested\n\nThe tag system (atag_) tracks matrix changes to avoid redundant\nfactorizations when the matrix hasn't changed.", "see": ["IpSparseSymLinearSolverInterface.hpp for solver interface", "IpTripletToCSRConverter.hpp for format conversion", "IpTSymScalingMethod.hpp for scaling"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpTripletToCSRConverter.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTripletToCSRConverter.hpp", "filename": "IpTripletToCSRConverter.hpp", "file": "IpTripletToCSRConverter.hpp", "brief": "Convert sparse matrix from triplet to CSR format\n\nTripletToCSRConverter converts symmetric matrices from triplet\n(COO) format to Compressed Sparse Row (CSR) format. Handles both\nupper-triangular-only and full-matrix storage.", "algorithm": "Triplet-to-CSR Conversion with Duplicate Summation:\n  1. Sort triplet entries by (row, col) lexicographically\n  2. Scan sorted entries, merging duplicates to same CSR position\n  3. Build ia (row pointers) and ja (column indices) arrays\n  4. Record mapping for value conversion (ipos_first_, ipos_double_*)\n  Conversion separates structure (once) from values (each solve).", "math": "Duplicate handling: If triplet has entries (i,j,a_1), (i,j,a_2),...\n  the CSR value at (i,j) is sum: a_1 + a_2 + ... This is essential for\n  finite element assembly where element matrices overlap.", "complexity": "InitializeConverter: O(nnz·log(nnz)) for sort + O(nnz) scan.\n  ConvertValues: O(nnz) to copy and sum values.\n  Memory: O(nnz) for mapping arrays.\n\nTriplet format: (row[k], col[k], val[k]) for k=0..nnz-1\n- May contain duplicates (summed during conversion)\n- May have entries in either triangle (normalized to upper)\n\nCSR format: ia[0..n], ja[0..nnz_csr-1]\n- ia[i] = start of row i in ja\n- ja = column indices, sorted by row then column\n- Offset 0 (C-style) or 1 (Fortran-style) supported\n\nConversion process:\n1. InitializeConverter(): Build mapping structure (once per pattern)\n2. ConvertValues(): Apply mapping to convert values (each factorization)\n\nDuplicate handling:\n- ipos_first_[i]: First triplet entry contributing to CSR position i\n- ipos_double_*: Additional triplet entries to add", "see": ["IpTSymLinearSolver.hpp for usage"], "return": "number of nonzeros in the condensed matrix.\n (Since nonzero elements can be listed several times\n in the triplet format, it is possible that this value is\n different from the input value nonzeros.)\n\n This method must be called before IA, JA, or ConvertValues.", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMc19TSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMc19TSymScalingMethod.hpp", "filename": "IpMc19TSymScalingMethod.hpp", "file": "IpMc19TSymScalingMethod.hpp", "brief": "Matrix scaling using HSL MC19 equilibration\n\nMc19TSymScalingMethod uses the HSL subroutine MC19 to compute\nequilibration scaling factors for symmetric matrices.", "algorithm": "MC19 Iterative Row/Column Equilibration:\n  MC19 computes diagonal scaling D such that ||D·A·D||_∞ ≈ 1.\n  Algorithm iteratively updates scaling factors:\n  1. Compute r_i = max_j |a_ij| (row infinity norms)\n  2. Update D_i = D_i / sqrt(r_i)\n  3. Repeat until convergence (typically 3-5 iterations)\n  Result: scaled matrix has row/column norms near 1.", "math": "Equilibration reduces condition number κ(A):\n  For sparse symmetric A, equilibration typically reduces κ by factor\n  of 10-1000. Better conditioning → fewer delayed pivots, more accurate\n  factorization, better convergence of iterative refinement.\n  D returned as exp(R) where R is in single precision.", "complexity": "O(nnz) per iteration, typically 3-5 iterations = O(nnz).\n  Memory: O(n) for scaling factors.", "ref": ["Duff & Koster (1999). \"The design and use of algorithms for\n     permuting large entries to the diagonal of sparse matrices\".\n     SIAM J. Matrix Anal. Appl. 20(4):889-901.\n\nMC19 interface (Fortran):\n  MC19A(N, NZ, A, IRN, ICN, R, C, W)\nNote: R, C, W are single precision (float) even in double version.\n\nLibrary loading:\n- Can be linked at compile time\n- Or loaded dynamically via LibraryLoader at runtime\n- SetFunctions() allows setting function pointer globally\n\nThe scaling factors returned satisfy ||D*A*D||_inf ≈ 1 where\nD = diag(exp(R)) and the matrix is approximately equilibrated."], "see": ["IpTSymScalingMethod.hpp for base interface", "IpEquilibrationScaling.hpp for NLP-level scaling"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpWsmpSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpWsmpSolverInterface.hpp", "filename": "IpWsmpSolverInterface.hpp", "file": "IpWsmpSolverInterface.hpp", "brief": "Interface to IBM WSMP sparse symmetric direct solver\n\nWsmpSolverInterface wraps the Watson Sparse Matrix Package (WSMP),\na high-performance parallel direct solver developed at IBM for\nsparse symmetric indefinite linear systems.", "algorithm": "Parallel Sparse LDL^T with Multifrontal Method:\n  WSMP computes A = P·L·D·L^T·P^T using parallel multifrontal:\n  - Fill-reducing ordering: automatic selection of AMD/METIS/nested dissection\n  - Supernodal factorization with BLAS-3 dense operations\n  - Shared-memory parallelism via pthreads/OpenMP hybrid\n  - Optional positive-definite mode for faster factorization\n  Designed for shared-memory multi-processor systems.", "math": "Symmetric indefinite factorization with threshold pivoting:\n  - Static pivoting determines stable 1×1 and 2×2 pivot blocks\n  - Pivtol threshold: accept pivot if |d_kk| ≥ pivtol × ||A_:,k||\n  - Scaling options: equilibration, Hungarian matching, Ruiz iteration\n  - Singularity detection via threshold on pivot magnitudes", "complexity": "O(n·f²/p) with p threads for factorization.\n  Optimized for Intel architectures with cache-aware blocking.\n  WSMP often achieves 10-15% of peak FLOPS on dense supernodes.", "ref": ["Gupta (2002). \"Recent Advances in Direct Methods for Solving\n     Unsymmetric Sparse Systems of Linear Equations\". ACM TOMS 28(3).\n\nWSMP characteristics:\n- Parallel: Multi-threaded with shared memory\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n- Provides degeneracy detection\n- Supports both direct and iterative modes\n\nKey parameters:\n- wsmp_num_threads_: Number of threads for parallel execution\n- wsmp_pivtol_/wsmp_pivtolmax_: Pivot tolerance bounds\n- wsmp_scaling_: Scaling method selection\n- wsmp_singularity_threshold_: Singularity detection threshold\n- wsmp_no_pivoting_: Use positive definite mode (faster but unsafe)\n\nUses IPARM and DPARM arrays for parameter control, similar to PARDISO.\nPERM/INVP arrays store the fill-reducing ordering.\n\nCan optionally use PARDISO preprocessing for matching."], "see": ["IpIterativeWsmpSolverInterface.hpp for iterative mode", "IpPardisoSolverInterface.hpp for similar direct solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMumpsSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMumpsSolverInterface.hpp", "filename": "IpMumpsSolverInterface.hpp", "file": "IpMumpsSolverInterface.hpp", "brief": "Interface to MUMPS parallel sparse direct solver\n\nMumpsSolverInterface wraps MUMPS (MUltifrontal Massively Parallel\nsparse direct Solver), a freely available solver supporting MPI\nparallelism for distributed memory systems.", "algorithm": "Distributed-Memory Multifrontal LDL^T Factorization:\n  MUMPS computes A = P·L·D·L^T·P^T using multifrontal method with:\n  - MPI parallelism: distributes frontal matrices across processes\n  - Hybrid mode: MPI between nodes + OpenMP within nodes\n  - Out-of-core: stores factors on disk for very large problems\n  Supports both symmetric indefinite and positive definite modes.", "math": "Distributed factorization strategy:\n  - Master process coordinates analysis and distribution\n  - Worker processes hold portions of L and D factors\n  - Frontal matrices assembled from child contributions via MPI\n  - Parallel triangular solves with pipelined communication\n  Degeneracy detection: identifies nearly-zero pivots for constraint deletion.", "complexity": "O(n·f²/p) with p MPI processes. Communication: O(f²·log(p)).\n  Scales to hundreds of cores for large problems (n > 100,000).", "ref": ["Amestoy, Duff, L'Excellent & Koster (2001). \"A Fully Asynchronous\n     Multifrontal Solver Using Distributed Dynamic Scheduling\".\n     SIAM J. Matrix Anal. Appl. 23(1):15-41.\n\nMUMPS characteristics:\n- Parallel: MPI-based (also sequential mode)\n- Input format: Triplet_Format (lower triangular)\n- Provides inertia\n- Provides degeneracy detection (ProvidesDegeneracyDetection)\n- Open source (CeCILL-C license)\n\nJob codes in MUMPS:\n- Job 1: Analysis (symbolic factorization)\n- Job 2: Numerical factorization\n- Job 3: Solve phase\n\nKey parameters:\n- mumps_permuting_scaling_: Permutation and scaling method\n- mumps_pivot_order_: Pivot ordering (AMD, METIS, etc.)\n- mumps_scaling_: Additional scaling options\n- mumps_dep_tol_: Threshold for dependency detection\n\nOften the default linear solver for Ipopt when HSL is unavailable."], "see": ["IpMa57TSolverInterface.hpp for HSL alternative", "IpMa97SolverInterface.hpp for parallel HSL solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpTSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTSymScalingMethod.hpp", "filename": "IpTSymScalingMethod.hpp", "file": "IpTSymScalingMethod.hpp", "brief": "Base class for matrix scaling in triplet format\n\nTSymScalingMethod is the abstract base class for computing diagonal\nscaling factors for symmetric matrices. Scaling improves numerical\nconditioning of the linear system.\n\nScaling transformation:\n  Original: A * x = b\n  Scaled:   (D*A*D) * (D^{-1}*x) = D*b\nwhere D = diag(scaling_factors).\n\nThe ComputeSymTScalingFactors method takes:\n- n: matrix dimension\n- nnz: number of nonzeros\n- airn, ajcn: row/column indices (triplet format)\n- a: matrix values\n- scaling_factors: output array of length n\n\nImplementations:\n- Mc19TSymScalingMethod: HSL MC19 equilibration\n- SlackBasedTSymScalingMethod: Simple slack-based scaling", "algorithm": "Symmetric Matrix Scaling (Row/Column Equilibration):\nComputes diagonal D to improve κ(DAD) for better solver accuracy:\n1. Analyze matrix structure in triplet (i, j, a_ij) format\n2. Compute scaling factors d_i (implementation-specific)\n3. Apply: Ā = DAD, x̄ = D⁻¹x, b̄ = Db\nGoal: Reduce condition number and improve pivot selection.", "math": "Symmetric scaling with D = diag(d₁,...,dₙ):\n$$\\bar{A}_{ij} = d_i \\cdot A_{ij} \\cdot d_j$$\n\nEquilibration goal: make row/column norms approximately equal\n$$\\|(\\bar{A})_i\\| \\approx \\|(\\bar{A})_j\\| \\approx 1$$\n\nCondition number improvement: κ(DAD) << κ(A) typically.", "complexity": "O(nnz) for computing scaling factors.\nSingle pass through nonzero entries required.", "ref": ["Duff & Koster (2001). \"On algorithms for permuting large entries\n  to the diagonal of a sparse matrix\". SIAM J. Matrix Anal. Appl. 22(4).", "Curtis & Reid (1972). \"On the automatic scaling of matrices for\n  Gaussian elimination\". IMA J. Appl. Math. 10(1):118-124."], "see": ["IpMc19TSymScalingMethod.hpp for MC19 implementation", "IpTSymLinearSolver.hpp for usage"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpSlackBasedTSymScalingMethod.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSlackBasedTSymScalingMethod.hpp", "filename": "IpSlackBasedTSymScalingMethod.hpp", "file": "IpSlackBasedTSymScalingMethod.hpp", "brief": "Simple scaling based on current slack values\n\nSlackBasedTSymScalingMethod computes scaling factors using only\nthe current slack values, without requiring external HSL routines.\nDesigned for use with inexact/iterative linear solvers.\n\nUnlike MC19 which performs full equilibration, this method uses\na simpler heuristic based on:\n- Current slack variable values s\n- Diagonal elements of the KKT system\n\nBenefits:\n- No external library dependencies\n- Lightweight computation\n- Suitable when full equilibration is unnecessary\n\nLimitations:\n- May not achieve as good conditioning as MC19\n- Best for problems where slacks dominate scaling needs", "algorithm": "Slack-Based Scaling Heuristic:\nLightweight scaling using interior-point slack values:\n1. Extract diagonal elements from KKT matrix\n2. Scale based on current slack values s and multipliers\n3. Avoid expensive equilibration when simple scaling suffices\nTrade-off: faster but potentially worse conditioning than MC19.", "math": "Heuristic scaling for interior-point:\nIn barrier method, slacks s provide natural scaling information.\nFor KKT diagonals involving Z·S⁻¹ (where Z = dual slacks):\n$$d_i \\approx \\sqrt{s_i}$$ (simple heuristic)\n\nFull equilibration (MC19) minimizes max|D·A·D| but costs O(nnz·iter).\nSlack-based scaling: O(n) using readily available iterate values.", "complexity": "O(n) per call: single pass over slack variables.\nNo matrix traversal or iterative refinement required.", "see": ["IpTSymScalingMethod.hpp for base interface", "IpMc19TSymScalingMethod.hpp for full equilibration"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpGenKKTSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpGenKKTSolverInterface.hpp", "filename": "IpGenKKTSolverInterface.hpp", "file": "IpGenKKTSolverInterface.hpp", "brief": "Generic interface for iterative/matrix-free KKT solvers\n\nGenKKTSolverInterface provides an alternative to\nSparseSymLinearSolverInterface for solvers that work with the\nfull KKT structure rather than requiring explicit sparse matrices.\n\nThe 4x4 block KKT system:\n  [W + D_x + delta_x*I,  0,         J_c^T,  J_d^T ] [sol_x]   [rhs_x]\n  [0,          D_s + delta_s*I,   0,     -I     ] [sol_s] = [rhs_s]\n  [J_c,        0,         D_c - delta_c*I,  0  ] [sol_c]   [rhs_c]\n  [J_d,       -I,         0,    D_d - delta_d*I] [sol_d]   [rhs_d]\n\nInterface differences from SparseSymLinearSolverInterface:\n- Receives Matrix/SymMatrix objects directly (not sparse arrays)\n- Diagonal arrays (D_x, D_s, D_c, D_d) as Number* pointers\n- Better suited for iterative solvers (Krylov, GMRES)\n- Can use matrix-vector products without explicit assembly\n\nUsed by GenAugSystemSolver to adapt this interface to AugSystemSolver.", "algorithm": "Generic KKT System Solver Interface:\nDefines matrix-free/iterative solver interface for the primal-dual KKT system.\nUnlike sparse direct solvers, implementations can use:\n- Krylov subspace methods (CG, MINRES, GMRES)\n- Matrix-free products via Jacobian/Hessian callbacks\n- Preconditioners built from block structure", "math": "The regularized symmetric indefinite KKT system:\n$$\\begin{bmatrix} W + D_x + \\delta_x I & 0 & J_c^T & J_d^T \\\\\n                  0 & D_s + \\delta_s I & 0 & -I \\\\\n                  J_c & 0 & D_c - \\delta_c I & 0 \\\\\n                  J_d & -I & 0 & D_d - \\delta_d I \\end{bmatrix}\n\\begin{pmatrix} \\Delta x \\\\ \\Delta s \\\\ \\Delta y_c \\\\ \\Delta y_d \\end{pmatrix}\n= \\begin{pmatrix} r_x \\\\ r_s \\\\ r_c \\\\ r_d \\end{pmatrix}$$\n\nInertia requirement: n_x + n_s positive eigenvalues, n_c + n_d negative.\nRegularization δ_x, δ_s, δ_c, δ_d ensures correct inertia.", "complexity": "Iterative: O(k·nnz) where k = iterations to convergence.\nTypically k << n for well-preconditioned systems.\nStorage: O(nnz) vs O(n²) for dense factorizations.", "ref": ["Benzi et al. (2005). \"Numerical solution of saddle point problems\".\n  Acta Numerica 14:1-137. [Comprehensive survey of iterative methods]", "Wächter & Biegler (2006). Section 4: Linear algebra strategies."], "see": ["IpGenAugSystemSolver.hpp for adapter class", "IpSparseSymLinearSolverInterface.hpp for sparse direct solvers"], "return": "The number of negative eigenvalues of the most recent factorized matrix.\n\n This must not be called if the linear solver does not compute this quantities\n (see ProvidesInertia).", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpSpralSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSpralSolverInterface.hpp", "filename": "IpSpralSolverInterface.hpp", "file": "IpSpralSolverInterface.hpp", "brief": "Interface to SPRAL SSIDS sparse symmetric solver\n\nSpralSolverInterface wraps SPRAL (Sparse Parallel Robust Algorithms\nLibrary), an open-source alternative to HSL solvers developed by\nSTFC RAL. SSIDS is SPRAL's symmetric indefinite direct solver.", "algorithm": "Hybrid CPU/GPU Multifrontal LDL^T:\n  SPRAL SSIDS uses multifrontal method with hybrid parallelism:\n  - CPU path: OpenMP-parallel supernodal factorization\n  - GPU path: CUDA kernels for large frontal matrices\n  - Automatic work distribution between CPU and GPU\n  - Delayed pivots handled with 2×2 Bunch-Kaufman pivoting\n  Open-source BSD-3 license enables unrestricted use.", "math": "Computes A = P·L·D·L^T·P^T where:\n  - P = fill-reducing permutation (METIS or AMD)\n  - L = unit lower triangular, supernodal storage\n  - D = block diagonal with 1×1 and 2×2 blocks\n  Threshold pivoting with dynamic scaling for stability.", "complexity": "O(n·f²) factorization. GPU acceleration provides 2-5×\n  speedup on large problems with dense fronts. CPU fallback for\n  small problems or when GPU unavailable.", "ref": ["Hogg, Sherwood-Taylor, Scott (2021). \"SSIDS: A Sparse Symmetric\n     Indefinite Direct Solver for Modern Architectures\". ACM TOMS.\n\nSPRAL/SSIDS characteristics:\n- Open source (BSD-3-Clause license)\n- Parallel: GPU (CUDA) and multi-core CPU support\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n- Dynamic scaling strategies (same as MA97)\n\nScaling strategies (scaling_opts):\n- SWITCH_NEVER: No dynamic scaling\n- SWITCH_AT_START: Scale on first factorization\n- SWITCH_ON_DEMAND: Scale when needed\n- SWITCH_NDELAY: Scale based on delayed pivots\n\nSimilar API to MA97 with akeep/fkeep separation for\nanalysis and factorization phases.\n\nRecommended when HSL is unavailable and GPU acceleration is desired.\n\n@since 3.14.0"], "see": ["IpMa97SolverInterface.hpp for similar HSL solver", "IpMumpsSolverInterface.hpp for open-source CPU alternative"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMa86SolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa86SolverInterface.hpp", "filename": "IpMa86SolverInterface.hpp", "file": "IpMa86SolverInterface.hpp", "brief": "Interface to HSL MA86 parallel sparse symmetric solver\n\nMa86SolverInterface wraps the HSL MA86 solver, a DAG-based\nparallel direct solver for symmetric indefinite systems.", "algorithm": "DAG-Based Parallel Multifrontal LDL^T:\n  MA86 exploits task parallelism via directed acyclic graph (DAG):\n  - Elimination tree converted to task dependency graph\n  - Independent frontal matrices factored in parallel (OpenMP)\n  - Dynamic scheduling balances load across cores\n  - Supernodal blocking for BLAS3 efficiency\n  Scales well on shared-memory multi-core systems.", "math": "Computes A = P·L·D·L^T·P^T where:\n  - P = permutation from MC68 (AMD/METIS/automatic selection)\n  - L = unit lower triangular, stored by supernodes\n  - D = block diagonal (1×1 and 2×2 pivots)\n  Threshold pivoting ensures stability for indefinite systems.", "complexity": "O(n·f²/p) with p OpenMP threads. Parallel speedup limited\n  by critical path in elimination tree. Best for medium-large problems\n  (n ~ 10^4 to 10^6) on 4-32 cores.", "ref": ["Hogg, Reid & Scott (2010). \"Design of a Multicore Sparse\n     Cholesky Factorization Using DAGs\". SIAM J. Sci. Comput.\n\nMA86 characteristics:\n- Parallel: Uses OpenMP for multi-core parallelism\n- DAG-based scheduling for load balancing\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n\nUses MC68 for fill-reducing ordering (AUTO, AMD, or METIS).\n\nOperations:\n- ma86_analyse: Symbolic factorization with ordering\n- ma86_factor: Parallel numerical factorization\n- ma86_factor_solve: Combined factorization and solve\n- ma86_solve: Forward/backward solve\n- ma86_finalise: Release memory\n\nThe order_ array stores the fill-reducing permutation reused\nacross factorizations with the same sparsity pattern."], "see": ["IpMa77SolverInterface.hpp for out-of-core solver", "IpMa97SolverInterface.hpp for newer parallel solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMa28TDependencyDetector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa28TDependencyDetector.hpp", "filename": "IpMa28TDependencyDetector.hpp", "file": "IpMa28TDependencyDetector.hpp", "brief": "Dependency detector using HSL MA28 unsymmetric solver\n\nMa28TDependencyDetector uses the unsymmetric sparse solver MA28\nto detect linearly dependent rows in the constraint Jacobian.\nUnlike the symmetric solvers, MA28 handles general rectangular\nmatrices, making it suitable for analyzing the constraint Jacobian\ndirectly.\n\nThe detection works by attempting LU factorization with threshold\npivoting. When a pivot falls below tolerance (ma28_pivtol_), the\ncorresponding row is flagged as linearly dependent.\n\nInput format: Triplet (row, col, val) for general matrices\n\nUsed by Ipopt's constraint degeneracy detection mechanism.", "algorithm": "MA28 Threshold Pivoting Dependency Detection:\nDirect LU factorization approach for rectangular matrices:\n1. Factor J = P·L·U·Q (with row/column permutations P, Q)\n2. Use threshold pivoting: accept pivot if |u_ii| ≥ tol·max_row\n3. Rows with |u_ii| < ma28_pivtol_ flagged as dependent\n4. Return list of numerically rank-deficient rows", "math": "Sparse LU factorization with threshold pivoting:\nGiven J ∈ ℝ^{m×n}, compute P·J·Q = L·U where:\n- L: unit lower triangular (m×min(m,n))\n- U: upper triangular (min(m,n)×n)\n\nRow i is dependent if during elimination:\n$$|u_{ii}| < \\text{pivtol} \\cdot \\max_k |a_{ik}^{(i-1)}|$$\nwhere a^{(i-1)} denotes the matrix after i-1 elimination steps.", "complexity": "O(nnz + fill-in) for sparse LU.\nFill-in can be O(n²) worst case, typically O(nnz·log(n)).", "ref": ["Duff (1977). \"MA28 - A set of Fortran subroutines for sparse\n  unsymmetric linear equations\". AERE Harwell Report R 8730."], "see": ["IpTDependencyDetector.hpp for base interface", "IpTSymDependencyDetector.hpp for symmetric solver approach"], "return": "false if there was a problem with the underlying linear solver", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpTSymDependencyDetector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTSymDependencyDetector.hpp", "filename": "IpTSymDependencyDetector.hpp", "file": "IpTSymDependencyDetector.hpp", "brief": "Dependency detection using symmetric linear solver\n\nTSymDependencyDetector detects linearly dependent constraint rows\nby using a TSymLinearSolver that provides degeneracy detection.\n\nMethod:\nSome symmetric linear solvers (e.g., MA57 via ProvidesDegeneracyDetection)\ncan identify dependent rows during factorization. This class\nleverages that capability.\n\nAlgorithm:\n1. Form symmetric matrix J*J^T (or equivalent structure)\n2. Attempt factorization with the TSymLinearSolver\n3. If solver detects singularity, query dependent row indices\n4. Return list of dependent rows in c_deps\n\nRequirements:\n- The underlying linear solver must implement\n  ProvidesDegeneracyDetection() returning true\n- Must implement DetermineDependentRows() for the sparse format\n\nThis is preferred over MA28-based detection when using a solver\nthat already provides this capability.", "algorithm": "Symmetric Normal Equations Dependency Detection:\nUses symmetric indefinite factorization for rank detection:\n1. Form normal equations: A = J·Jᵀ (or augmented system)\n2. Attempt LDLᵀ or Bunch-Kaufman factorization\n3. Zero/small diagonal pivots indicate dependent rows\n4. Extract row indices from solver's singularity info", "math": "Normal equations approach: A = J·Jᵀ ∈ ℝ^{m×m}\n- rank(A) = rank(J·Jᵀ) = rank(J) (assuming full column rank)\n- A is symmetric positive semi-definite\n- A_{ii} = 0 ⟹ row i of J is zero (trivially dependent)\n- Pivot d_i ≈ 0 during LDLᵀ ⟹ row i depends on rows 1,...,i-1", "complexity": "O(m³) worst case for dense; O(fill-in) for sparse.\nSymmetric factorization typically faster than unsymmetric MA28.", "ref": ["Duff (2004). \"MA57 - a code for the solution of sparse symmetric\n  definite and indefinite systems\". ACM TOMS 30(2):118-144."], "see": ["IpTDependencyDetector.hpp for base interface", "IpTSymLinearSolver.hpp for symmetric solver"], "return": "false if there was a problem with the underlying linear solver", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpTDependencyDetector.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpTDependencyDetector.hpp", "filename": "IpTDependencyDetector.hpp", "file": "IpTDependencyDetector.hpp", "brief": "Base class for detecting linearly dependent constraint rows\n\nTDependencyDetector is the abstract base class for algorithms that\ndetect linearly dependent rows in the constraint Jacobian. This is\nneeded to handle degenerate problems where some constraints are\nredundant.\n\nPurpose:\nIf rank(J_c) < m_c, the KKT system is singular. Detecting and\nremoving dependent rows allows the solver to proceed.\n\nInterface:\n- DetermineDependentRows(): Takes Jacobian in triplet format,\n  returns list of dependent row indices in c_deps\n\nInput format (MA28 style triplet):\n- n_rows, n_cols: Jacobian dimensions\n- n_jac_nz: Number of nonzeros\n- jac_c_vals, jac_c_iRow, jac_c_jCol: Values and indices\n\nThe input arrays may be modified internally (working space).", "algorithm": "Linear Dependency Detection (Rank Deficiency):\nIdentifies redundant constraint rows to handle degenerate NLPs:\n1. Receive constraint Jacobian J ∈ ℝ^{m×n} in triplet format\n2. Apply numerical rank test (LU or SVD-based)\n3. Return indices of linearly dependent rows\n4. IPM removes these rows from active KKT system", "math": "Problem degeneracy: rank(J_c) < m_c creates singular KKT system.\nFor equality constraints c(x) = 0 with Jacobian J_c = ∇c(x)ᵀ:\n- Full rank: m_c independent constraints\n- Rank deficient: some rows J_i = ∑_{j≠i} α_j·J_j\n\nRemove row i if |pivot_i| < tol during factorization, indicating\nrow i is numerically dependent on rows 1,...,i-1.", "complexity": "O(m·n·nnz/n) for sparse LU ≈ O(nnz·m/n).\nMA28: O(nnz + fill-in) per factorization attempt.", "ref": ["Nocedal & Wright (2006). Section 18.4: Degenerate problems.", "Duff et al. (1989). \"Direct Methods for Sparse Matrices\"."], "see": ["IpTSymDependencyDetector.hpp for symmetric solver-based detection", "IpMa28TDependencyDetector.hpp for MA28-based detection"], "return": "false if there was a problem with the underlying linear solver", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpSymLinearSolver.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSymLinearSolver.hpp", "filename": "IpSymLinearSolver.hpp", "file": "IpSymLinearSolver.hpp", "brief": "Base class for symmetric indefinite linear solvers\n\nSymLinearSolver is the abstract base class for all symmetric linear\nsolvers used in Ipopt's augmented system. The solver must handle\nsymmetric indefinite matrices and optionally provide inertia.", "algorithm": "Symmetric Indefinite Linear Solve Interface:\n  MultiSolve(A, rhs, sol) factorizes and solves A·x = b:\n  1. If matrix structure changed: Re-analyze sparsity pattern.\n  2. Factorize: A = L·D·L^T (symmetric indefinite factorization).\n  3. Solve: x = L^{-T}·D^{-1}·L^{-1}·b for each right-hand side.\n  4. Check inertia: Count negative eigenvalues (diagonal of D).\n  5. Return status: SUCCESS, SINGULAR, WRONG_INERTIA, CALL_AGAIN.", "math": "Inertia and indefiniteness:\n  KKT system is symmetric indefinite: [W, J^T; J, 0] with W≻0, J Jacobian.\n  Correct inertia: n positive, m negative eigenvalues (for m constraints).\n  Wrong inertia → singular Hessian or constraint degeneracy.\n  IncreaseQuality() → tighten pivot tolerance for accuracy.", "complexity": "Sparse LDL^T: O(nnz·fill) where fill depends on ordering.\n\nReturn codes (ESymSolverStatus):\n- SYMSOLVER_SUCCESS: Solve completed successfully\n- SYMSOLVER_SINGULAR: Matrix detected as singular\n- SYMSOLVER_WRONG_INERTIA: Negative eigenvalue count doesn't match\n- SYMSOLVER_CALL_AGAIN: Solver needs reallocation (retry with new values)\n- SYMSOLVER_FATAL_ERROR: Unrecoverable error\n\nKey interface methods:\n- MultiSolve(): Factorize and solve for multiple RHS\n- NumberOfNegEVals(): Query inertia after factorization\n- ProvidesInertia(): Check if solver computes inertia\n- IncreaseQuality(): Request higher accuracy (e.g., larger pivots)\n\nThe matrix structure must remain fixed between Initialize and solve calls.", "see": ["IpSparseSymLinearSolverInterface.hpp for sparse format interface", "IpTSymLinearSolver.hpp for triplet format driver"], "return": "the number of negative eigenvalues of the most recent factorized matrix\n\n This must not be called if the linear solver does not compute this quantities\n (see ProvidesInertia).", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMa57TSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa57TSolverInterface.hpp", "filename": "IpMa57TSolverInterface.hpp", "file": "IpMa57TSolverInterface.hpp", "brief": "Interface to HSL MA57 sparse symmetric indefinite solver\n\nMa57TSolverInterface wraps the Harwell MA57 subroutine, an improved\nmultifrontal solver over MA27 with better memory management and\nnumerical stability.", "algorithm": "Improved Multifrontal LDL^T with Advanced Pivoting:\n  MA57 computes A = P·L·D·L^T·P^T using an improved multifrontal\n  method with better pivot handling than MA27:\n  - Static pivoting: pre-determined pivots from symbolic phase\n  - Dynamic pivoting: adjustment during numerical factorization\n  - 2×2 pivots for indefinite blocks with Bunch-Kaufman strategy", "math": "Stability improvements over MA27:\n  - Lookahead pivoting: examines future columns for better pivots\n  - Delayed pivots: postpones problematic pivots to later stages\n  - Pivot perturbation: adds ε to tiny pivots instead of failing\n  Solve modes: Ax=b, PLPTx=b, PDPTx=b, PLTPT x=b", "complexity": "Same asymptotic as MA27: O(n·f²) factorization, O(nnz(L)) solve.\n  Typically 2-3× faster than MA27 due to better memory access patterns\n  and BLAS-3 dense operations within frontal matrices.", "ref": ["Duff (2004). \"MA57 - A Code for the Solution of Sparse Symmetric\n     Definite and Indefinite Systems\". ACM TOMS 30(2):118-144.\n\nMA57 improvements over MA27:\n- Better pivot selection for numerical stability\n- More efficient memory management\n- Support for multiple right-hand sides\n- Better handling of dense row detection\n\nInput format: Triplet (lower triangular), same as MA27\n\nPhases (MA57A/B/C/E/I):\n- MA57I: Initialize control parameters\n- MA57A: Symbolic factorization\n- MA57B: Numerical factorization\n- MA57C: Solve (job selects A, PLP^T, PDP^T, or PL^T P^T)\n- MA57E: Copy/resize factor arrays\n\nKey parameters:\n- pivtol_/pivtolmax_: Threshold pivoting tolerance\n- ma57_pre_alloc_: Factor for initial workspace allocation"], "see": ["IpMa27TSolverInterface.hpp for simpler MA27", "IpMa97SolverInterface.hpp for modern parallel solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpIterativeWsmpSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpIterativeWsmpSolverInterface.hpp", "filename": "IpIterativeWsmpSolverInterface.hpp", "file": "IpIterativeWsmpSolverInterface.hpp", "brief": "Interface to IBM WSMP iterative (WISMP) solver\n\nIterativeWsmpSolverInterface wraps the iterative variant of WSMP\n(called WISMP), which uses incomplete LU factorization as a\npreconditioner for iterative refinement.", "algorithm": "Preconditioned Iterative Solver with Incomplete LDL^T:\n  WISMP uses incomplete factorization as preconditioner for iterative solve:\n  - ILU(k) or ILUT: incomplete factorization with level-k fill or threshold\n  - Drop tolerance controls fill-in vs preconditioner quality tradeoff\n  - Preconditioned conjugate gradient or GMRES iteration\n  - Memory-efficient for very large problems where exact factors don't fit", "math": "Incomplete factorization A ≈ L̃·D̃·L̃^T where:\n  - L̃ = unit lower triangular, entries dropped if |l_ij| < droptol\n  - Fill-in limited by fillin_limit factor relative to original nnz\n  - Preconditioner M = L̃·D̃·L̃^T applied in each iteration\n  - No inertia information available (pivots may be altered by dropping)", "complexity": "O(nnz(L̃)) per iteration, typically O(k·nnz(A)) total\n  where k = iteration count. Memory: O(fillin_limit × nnz(A)).\n  Suitable when direct factorization exceeds memory.", "ref": ["Gupta (2000). \"WSMP: Watson Sparse Matrix Package\".\n     IBM Research Technical Report RC 21886.\n\nKey characteristics:\n- ProvidesInertia: false (unlike direct WSMP)\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Uses incomplete factorization with drop tolerance\n\nIterative-specific parameters:\n- wsmp_inexact_droptol_: Drop tolerance for incomplete factorization\n- wsmp_inexact_fillin_limit_: Maximum fill-in allowed\n\nSince WISMP cannot determine inertia (negative eigenvalue count),\nit may not be suitable for all Ipopt applications that rely on\ninertia correction for KKT system validity.\n\nUseful for very large problems where direct factorization memory\nrequirements are prohibitive."], "see": ["IpWsmpSolverInterface.hpp for direct solver variant", "IpGenKKTSolverInterface.hpp for generic iterative interface"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpPardisoSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpPardisoSolverInterface.hpp", "filename": "IpPardisoSolverInterface.hpp", "file": "IpPardisoSolverInterface.hpp", "brief": "Interface to PARDISO sparse solver from pardiso-project.org\n\nPardisoSolverInterface wraps the PARDISO solver distributed by\npardiso-project.org (not to be confused with Intel MKL's PARDISO).\nPARDISO is a high-performance parallel direct solver for sparse\nsymmetric indefinite systems.", "algorithm": "Parallel Supernodal LDL^T with Weighted Matching:\n  PARDISO computes A = P·S·L·D·L^T·S·P^T using:\n  - Supernodal factorization: groups columns into dense supernode blocks\n  - Level-3 BLAS operations within supernodes for cache efficiency\n  - OpenMP parallelism over independent supernode subtrees\n  - Weighted matching preprocessing for indefinite stability", "math": "Matching strategies for saddle point systems:\n  - COMPLETE: Maximum weighted matching on |A|, produces permutation P\n    such that |P*A| has large diagonal entries\n  - COMPLETE2x2: 2×2 pivot matching for symmetric indefinite systems,\n    identifies pairs (i,j) where A[i,j] forms stable 2×2 pivot block\n  - CONSTRAINT: Preserves constraint structure in optimization problems", "complexity": "O(n·f²/p) with p threads, O(nnz(L)) memory.\n  Very efficient for medium-sized dense fronts (supernodes).\n  Iterative refinement: 1-2 iterations to reach working precision.", "ref": ["Schenk & Gärtner (2004). \"Solving Unsymmetric Sparse Systems of\n     Linear Equations with PARDISO\". Future Generation Computer Systems\n     20(3):475-487.\n\nPARDISO characteristics:\n- Parallel: Multi-threaded with OpenMP\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n- Supports iterative refinement\n\nMatching strategies (match_strat_):\n- COMPLETE: Full weighted matching\n- COMPLETE2x2: 2x2 matching for saddle point systems\n- CONSTRAINT: Constraint-preserving matching\n\nPhases (controlled by PHASE parameter):\n- Phase 11: Analysis (reordering, symbolic factorization)\n- Phase 22: Numerical factorization\n- Phase 33: Forward/backward solve\n- Phase -1: Release memory\n\nMTYPE=-2 indicates real symmetric indefinite matrix."], "see": ["IpPardisoMKLSolverInterface.hpp for Intel MKL version", "IpMa97SolverInterface.hpp for alternative parallel solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMa77SolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa77SolverInterface.hpp", "filename": "IpMa77SolverInterface.hpp", "file": "IpMa77SolverInterface.hpp", "brief": "Interface to HSL MA77 out-of-core sparse symmetric solver\n\nMa77SolverInterface wraps the HSL MA77 solver, designed for\nlarge-scale problems that may not fit entirely in memory.", "algorithm": "Out-of-Core Multifrontal LDL^T Factorization:\n  MA77 uses multifrontal method with disk-based factor storage:\n  - Elimination tree computed from sparsity pattern (MC68 ordering)\n  - Frontal matrices assembled and factored sequentially\n  - Factors written to 4 disk files to minimize memory footprint\n  - Solve phase streams factors back from disk as needed\n  Enables solving problems where L factor exceeds available RAM.", "math": "Computes A = P·L·D·L^T·P^T where:\n  - P = permutation from MC68 (AMD/METIS fill-reducing order)\n  - L = unit lower triangular\n  - D = block diagonal (1×1 and 2×2 blocks for indefinite systems)\n  Inertia (positive/negative eigenvalue counts) from D block signs.", "complexity": "O(n·f²) factorization where f = front size (fill-dependent).\n  I/O cost: O(nnz(L)) disk reads/writes. Suitable for n > 10^6 when\n  in-core solvers run out of memory.", "ref": ["Reid & Scott (2009). \"An Out-of-Core Sparse Cholesky Solver\".\n     RAL Technical Report RAL-TR-2009-004.\n\nMA77 characteristics:\n- Out-of-core capability: Uses files for factor storage\n- Input format: CSR_Full_Format_1_Offset (both triangles)\n- Provides inertia\n- Supports element-by-element assembly\n\nUses MC68 for fill-reducing ordering (AMD or METIS).\n\nMain operations:\n- ma77_open: Initialize solver and create temporary files\n- ma77_input_vars/reals: Provide matrix structure/values\n- ma77_analyse: Symbolic factorization with MC68 ordering\n- ma77_factor: Numerical factorization\n- ma77_solve: Forward/backward solve\n- ma77_finalise: Cleanup and delete files\n\nThe solver creates 4 temporary files (fname1-4) for factor storage."], "see": ["IpMa86SolverInterface.hpp for in-core parallel solver", "IpMa97SolverInterface.hpp for modern in-core solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpSparseSymLinearSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpSparseSymLinearSolverInterface.hpp", "filename": "IpSparseSymLinearSolverInterface.hpp", "file": "IpSparseSymLinearSolverInterface.hpp", "brief": "Interface for sparse symmetric indefinite linear solvers\n\nSparseSymLinearSolverInterface defines the interface that concrete\nlinear solvers (MA27, MA57, MUMPS, Pardiso, etc.) must implement.", "algorithm": "Strategy Pattern for Sparse Symmetric Indefinite Solvers:\n  All Ipopt linear solvers implement this interface for the KKT system:\n  [W + Σ_x    A^T   ] [Δx]   [r_d]\n  [A          -Σ_c  ] [Δλ] = [r_p]\n  where W is the Hessian, A is the Jacobian, and Σ are regularization.\n  The system is symmetric but indefinite (n positive, m negative eigenvalues\n  at a local minimum).", "math": "Inertia requirement: At a local minimum, the KKT matrix must have\n  inertia (n, m, 0) where n = number of variables, m = number of constraints.\n  If wrong inertia is detected, Ipopt adds regularization to correct it.\n\nMatrix formats (EMatrixFormat):\n- Triplet_Format: (row, col, val) triplets, lower triangular (MA27 style)\n- CSR_Format_0_Offset: CSR with 0-based indexing (C-style)\n- CSR_Format_1_Offset: CSR with 1-based indexing (Fortran-style)\n- CSR_Full_Format_*: Full matrix (both triangles) CSR\n\nUsage protocol:\n1. Initialize(): Set options, allocate internal structures\n2. MatrixFormat(): Query required input format\n3. InitializeStructure(): Provide sparsity pattern, do symbolic factorization\n4. GetValuesArrayPtr(): Get array for matrix values\n5. MultiSolve(): Factorize (if new_matrix) and solve\n\nDuplicate entries in triplet format are summed. Warm start support\nallows reusing symbolic factorization across optimizations.", "complexity": "Typical sparse direct solver complexity:\n  - Symbolic factorization: O(nnz) to O(n²) depending on fill-in\n  - Numerical factorization: O(nnz(L)²/n) for multifrontal methods\n  - Solve: O(nnz(L)) for triangular solves", "ref": ["Duff, Erisman & Reid (1986). \"Direct Methods for Sparse Matrices\".\n     Oxford University Press. [Foundational text on sparse direct methods]"], "see": ["IpMa27TSolverInterface.hpp for Harwell MA27", "IpMumpsSolverInterface.hpp for MUMPS", "IpPardisoSolverInterface.hpp for Pardiso"], "return": "SYMSOLV_SUCCESS if the factorization and\n solves were successful, SYMSOLV_SINGULAR if the linear system\n is singular, and SYMSOLV_WRONG_INERTIA if check_NegEVals is\n true and the number of negative eigenvalues in the matrix does\n not match numberOfNegEVals.  If SYMSOLV_CALL_AGAIN is\n returned, then the calling function will request the pointer\n for the array for storing a again (with GetValuesPtr), write\n the values of the nonzero elements into it, and call this\n MultiSolve method again with the same right-hand sides.  (This\n can be done, for example, if the linear solver realized it\n does not have sufficient memory and needs to redo the\n factorization; e.g., for MA27.)\n\n The number of right-hand sides is given by nrhs, the values of\n the right-hand sides are given in rhs_vals (one full right-hand\n side stored immediately after the other), and solutions are\n to be returned in the same array.\n\n check_NegEVals will not be chosen true, if ProvidesInertia()\n returns false.", "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMa27TSolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa27TSolverInterface.hpp", "filename": "IpMa27TSolverInterface.hpp", "file": "IpMa27TSolverInterface.hpp", "brief": "Interface to HSL MA27 sparse symmetric indefinite solver\n\nMa27TSolverInterface wraps the Harwell MA27 subroutine for solving\nsparse symmetric indefinite linear systems using multifrontal\nfactorization with threshold pivoting.", "algorithm": "Multifrontal LDL^T Factorization with Threshold Pivoting:\n  MA27 computes A = P·L·D·L^T·P^T where:\n  - P is a permutation matrix (determined by AMD-like ordering)\n  - L is unit lower triangular\n  - D is block diagonal (1×1 or 2×2 blocks for indefinite matrices)\n  Uses Duff-Reid multifrontal method with assembly tree traversal.", "math": "Threshold pivoting: Accept pivot a_kk if |a_kk| ≥ pivtol · max_i |a_ik|.\n  Larger pivtol → more stable but more fill-in. Default 1e-8, max 0.5.\n  2×2 pivots handle indefiniteness: [a b; b c] when ac-b² < 0.", "complexity": "O(n·f²) where f is the \"frontsize\" (width of elimination fronts).\n  For well-ordered sparse matrices, f ≪ n. Memory: O(nnz(L)).", "ref": ["Duff & Reid (1983). \"The Multifrontal Solution of Indefinite Sparse\n     Symmetric Linear Equations\". ACM TOMS 9(3):302-325.\n\nMA27 characteristics:\n- Input format: Triplet (lower triangular)\n- Provides inertia (negative eigenvalue count)\n- Classic serial solver, still widely used\n\nPhases (MA27A/B/C):\n- MA27A: Symbolic factorization, determines pivoting order\n- MA27B: Numerical factorization with threshold pivoting\n- MA27C: Forward/backward solve\n\nKey parameters:\n- pivtol_/pivtolmax_: Pivot tolerance (default 1e-8, max 0.5)\n- liw_init_factor_, la_init_factor_: Workspace sizing\n- meminc_factor_: Memory growth factor when reallocation needed\n\nThe solver may request memory reallocation (SYMSOLVER_CALL_AGAIN)\nif initial workspace estimates prove insufficient."], "see": ["IpMa57TSolverInterface.hpp for improved MA57", "IpSparseSymLinearSolverInterface.hpp for interface"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpMa97SolverInterface.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpMa97SolverInterface.hpp", "filename": "IpMa97SolverInterface.hpp", "file": "IpMa97SolverInterface.hpp", "brief": "Interface to HSL MA97 modern parallel sparse symmetric solver\n\nMa97SolverInterface wraps the HSL MA97 solver, the most modern\nHSL solver for symmetric indefinite systems with advanced\nparallelism and scaling strategies.", "algorithm": "Parallel Multifrontal LDL^T with DAG-based Scheduling:\n  MA97 computes A = P·S·L·D·L^T·S·P^T where S is diagonal scaling.\n  Uses directed acyclic graph (DAG) task scheduling for parallelism:\n  - Assembly tree decomposed into independent subtrees\n  - OpenMP task parallelism within and across fronts\n  - Out-of-core capability for very large problems", "math": "Scaling strategies for improved numerical stability:\n  - Hungarian matching + MC64 for row/column equilibration\n  - Ruiz scaling for matrix balancing: ||A_scaled||_∞ ≈ 1\n  - Dynamic scaling: recompute when delayed pivots exceed threshold\n  Ordering options: AMD (fast), METIS (better fill), matched variants.", "complexity": "O(n·f²/p) parallel factorization with p threads.\n  Scales well up to ~8-16 cores for typical optimization problems.\n  Memory: O(nnz(L)) with optional out-of-core for factors.", "ref": ["Hogg & Scott (2013). \"New parallel sparse direct solvers for\n     multicore architectures\". Algorithms 6(4):702-725.\n\nMA97 characteristics:\n- Modern parallel implementation using OpenMP\n- Advanced scaling options (dynamic, reuse, on-demand)\n- Input format: CSR_Format_1_Offset (upper triangular)\n- Provides inertia\n\nOrdering options: AUTO, BEST, AMD, METIS, MATCHED_*\n\nScaling strategies (scale_opts):\n- SWITCH_NEVER: No dynamic scaling\n- SWITCH_AT_START: Scale on first factorization\n- SWITCH_ON_DEMAND: Scale when needed\n- SWITCH_NDELAY: Scale based on delayed pivots\n\nKeeps separate akeep (analysis) and fkeep (factorization) pointers\nto efficiently handle re-factorization with unchanged structure.\n\nRecommended as default HSL solver for parallel systems."], "see": ["IpMa86SolverInterface.hpp for older parallel solver", "IpMa57TSolverInterface.hpp for serial solver"], "has_pass2": true}, "src/Algorithm/LinearSolvers/IpLinearSolversRegOp.hpp": {"path": "layer-2/Ipopt/src/Algorithm/LinearSolvers/IpLinearSolversRegOp.hpp", "filename": "IpLinearSolversRegOp.hpp", "file": "IpLinearSolversRegOp.hpp", "brief": "Registration of all linear solver options\n\nDeclares RegisterOptions_LinearSolvers(), which registers options\nfor all available linear solvers (MA27, MA57, MA77, MA86, MA97,\nMUMPS, Pardiso, WSMP, SPRAL, etc.).\n\nCalled during Ipopt initialization to make solver-specific options\navailable through the OptionsList mechanism. Each solver interface\nhas its own RegisterOptions() method that is invoked here.\n\nOptions typically include:\n- Pivot tolerance bounds\n- Ordering method selection\n- Scaling options\n- Memory allocation factors\n- Parallelism settings (thread counts)", "see": ["IpAlgRegOp.hpp for algorithm options registration", "IpInterfacesRegOp.hpp for interface options"], "has_pass2": false}, "src/LinAlg/TMatrices/IpTripletHelper.hpp": {"path": "layer-2/Ipopt/src/LinAlg/TMatrices/IpTripletHelper.hpp", "filename": "IpTripletHelper.hpp", "file": "IpTripletHelper.hpp", "brief": "Recursive conversion of abstract matrices to triplet format\n\nTripletHelper provides static methods for extracting COO (Coordinate)\nsparse format from Ipopt's abstract Matrix hierarchy. Used to interface\nwith external linear solvers expecting triplet format.", "algorithm": "Recursive Triplet Extraction:\n  GetNumberEntries(M): Recursively count nonzeros by matrix type.\n  FillRowCol(M, iRow, jCol): Fill structure arrays via type dispatch.\n  FillValues(M, values): Fill values array via type dispatch.\n\n  Supported types (polymorphic dispatch):\n  - GenTMatrix, SymTMatrix: Direct triplet access.\n  - DiagMatrix, IdentityMatrix: Generate diagonal entries.\n  - ExpansionMatrix: Permutation/selection entries.\n  - CompoundMatrix, CompoundSymMatrix: Recurse on blocks.\n  - SumMatrix, SumSymMatrix: Concatenate component entries.\n  - ScaledMatrix, SymScaledMatrix: Recurse with scaling.\n  - TransposeMatrix: Swap row/col in recursion.", "math": "COO (Coordinate) sparse format:\n  Matrix A stored as (iRow[k], jCol[k], values[k]) for k = 0..nnz-1.\n  A[iRow[k], jCol[k]] += values[k] (duplicate entries summed).\n  1-based indexing for Fortran compatibility (HSL solvers).", "complexity": "O(nnz) per traversal, where nnz = total nonzeros.\n  Type dispatch via overloaded private methods (compile-time).", "see": ["IpGenTMatrix.hpp for general triplet storage", "IpSymTMatrix.hpp for symmetric triplet storage"], "has_pass2": true}, "src/LinAlg/TMatrices/IpSymTMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/TMatrices/IpSymTMatrix.hpp", "filename": "IpSymTMatrix.hpp", "file": "IpSymTMatrix.hpp", "brief": "Symmetric sparse matrix in triplet (COO) format\n\nSymTMatrix stores symmetric sparse matrices using coordinate format:\nthree arrays (Irn, Jcn, Values) of length nnz representing nonzeros.", "algorithm": "Triplet/COO Format for Symmetric Matrices:\n  Only lower (or upper) triangle is stored due to symmetry.\n  Entry (i,j,v) represents A[i,j] = A[j,i] = v.\n  Duplicate entries at same (i,j) are summed - essential for\n  finite element assembly where element matrices overlap.", "math": "Symmetric matrix-vector product y = A·x:\n  For each triplet (i, j, a_ij):\n  - y[i] += a_ij · x[j]\n  - y[j] += a_ij · x[i]  (if i ≠ j, symmetry contribution)\n  Cost: O(nnz), no redundant storage of symmetric entries.", "complexity": "O(nnz) for matrix-vector product.\n  Memory: O(nnz) for values + O(nnz) for indices.\n  Note: 1-based indexing (Fortran-style) for HSL solver compatibility.", "see": ["IpGenTMatrix.hpp for general (non-symmetric) triplet format", "IpTripletToCSRConverter.hpp for CSR conversion"], "has_pass2": true}, "src/LinAlg/TMatrices/IpGenTMatrix.hpp": {"path": "layer-2/Ipopt/src/LinAlg/TMatrices/IpGenTMatrix.hpp", "filename": "IpGenTMatrix.hpp", "file": "IpGenTMatrix.hpp", "brief": "General sparse matrix in triplet (COO) format\n\nGenTMatrix stores a general (non-symmetric) sparse matrix using\ntriplet/coordinate format: three arrays for row indices, column\nindices, and values.", "algorithm": "Triplet Matrix-Vector Multiplication:\n  y ← αAx + βy computed as:\n  1. If β ≠ 1: y ← β·y (scale).\n  2. For each triplet (i, j, v): y[i] += α·v·x[j].\n  Transpose: swap roles of i and j.\n  Duplicate entries: values are implicitly summed.", "math": "COO sparse format properties:\n  Storage: O(3·nnz) (iRow, jCol, values arrays).\n  No ordering requirements (unlike CSR/CSC).\n  1-based indexing: iRow[k], jCol[k] ∈ {1, ..., n} (HSL convention).\n  Duplicates allowed: A[i,j] = Σ{v : (i,j,v) in triplets}.", "complexity": "MultVector: O(nnz) for nnz nonzeros.\n  SetValues: O(nnz) copy from user array.\n  Structure fixed at creation; only values change.\n\nUsed for Jacobians (J_c, J_d) in Ipopt's internal representation.\nStructure (sparsity pattern) is fixed; values updated per iteration.", "see": ["IpSymTMatrix.hpp for symmetric triplet storage", "IpTripletHelper.hpp for triplet extraction utilities"], "has_pass2": true}, "contrib/sIPOPT/AmplSolver/SensAmplTNLP.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/AmplSolver/SensAmplTNLP.hpp", "filename": "SensAmplTNLP.hpp", "file": "SensAmplTNLP.hpp", "brief": "sIPOPT AMPL interface with sensitivity support\n\nSensAmplTNLP extends AmplTNLP with sensitivity parameter handling.\nReads sens_init_constr suffix for constraint parameter indices.\nEnables parametric sensitivity analysis for AMPL models.", "has_pass2": false}, "contrib/sIPOPT/src/SensSuffixHandler.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensSuffixHandler.hpp", "filename": "SensSuffixHandler.hpp", "file": "SensSuffixHandler.hpp", "brief": "sIPOPT AMPL suffix handler\n\nSensSuffixHandler: processes AMPL suffixes for sensitivity analysis.\nReads parameter indices and perturbation values from .row/.col suffixes.\nEnables AMPL-based sensitivity specification.", "has_pass2": false}, "contrib/sIPOPT/src/SensAlgorithm.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensAlgorithm.hpp", "filename": "SensAlgorithm.hpp", "file": "SensAlgorithm.hpp", "brief": "sIPOPT sensitivity analysis main algorithm\n\nSensAlgorithm: controller for parametric sensitivity analysis in Ipopt.\nComputes directional derivatives and sensitivity matrices for solution\nw.r.t. parameter perturbations. Uses Schur complement decomposition\nto efficiently compute sensitivities after optimal solution found.", "has_pass2": false}, "contrib/sIPOPT/src/SensBuilder.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensBuilder.hpp", "filename": "SensBuilder.hpp", "file": "SensBuilder.hpp", "brief": "sIPOPT builder for sensitivity analysis components\n\nSensBuilder: factory for creating sIPOPT algorithm components.\nAssembles SchurDriver, StepCalculator, and Measurement objects\nbased on options. Configures sensitivity computation pipeline.", "has_pass2": false}, "contrib/sIPOPT/src/SensReducedHessianCalculator.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensReducedHessianCalculator.hpp", "filename": "SensReducedHessianCalculator.hpp", "file": "SensReducedHessianCalculator.hpp", "brief": "sIPOPT reduced Hessian computation\n\nSensReducedHessianCalculator: computes reduced Hessian of Lagrangian.\nProjects full Hessian onto null space of active constraints.\nUsed for computing second-order sensitivity information.", "has_pass2": false}, "contrib/sIPOPT/src/SensDenseGenSchurDriver.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensDenseGenSchurDriver.hpp", "filename": "SensDenseGenSchurDriver.hpp", "file": "SensDenseGenSchurDriver.hpp", "brief": "sIPOPT dense Schur complement driver\n\nSensDenseGenSchurDriver: dense matrix implementation of SchurDriver.\nComputes Schur complement S = C - B*K^{-1}*A for sensitivity analysis.\nSuitable for problems with moderate numbers of sensitivity parameters.", "has_pass2": false}, "contrib/sIPOPT/src/SensApplication.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensApplication.hpp", "filename": "SensApplication.hpp", "file": "SensApplication.hpp", "brief": "sIPOPT application wrapper for sensitivity analysis\n\nSensApplication wraps IpoptApplication with sensitivity extensions.\nEntry point for sIPOPT: runs optimization then sensitivity analysis.\nManages parameter perturbations and extracts sensitivity information.", "has_pass2": false}, "contrib/sIPOPT/src/SensMeasurement.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensMeasurement.hpp", "filename": "SensMeasurement.hpp", "file": "SensMeasurement.hpp", "brief": "sIPOPT measurement interface for parameter values\n\nSensMeasurement: abstract interface for providing parameter values.\nMeasurement objects supply perturbed parameter values to sensitivity\nanalysis. Enables real-time or batch parameter updates.", "has_pass2": false}, "contrib/sIPOPT/src/SensPCalculator.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensPCalculator.hpp", "filename": "SensPCalculator.hpp", "file": "SensPCalculator.hpp", "brief": "sIPOPT perturbation calculator interface\n\nSensPCalculator: abstract interface for computing perturbations.\nCalculates right-hand sides for sensitivity linear systems based\non how parameters affect KKT conditions.", "has_pass2": false}, "contrib/sIPOPT/src/SensUtils.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensUtils.hpp", "filename": "SensUtils.hpp", "file": "SensUtils.hpp", "brief": "sIPOPT utility functions and enums\n\nSensUtils: utility types for sIPOPT sensitivity analysis.\nDefines SensAlgorithmExitStatus enum for return codes.\nCommon helper functions used across sIPOPT components.", "has_pass2": false}, "contrib/sIPOPT/src/SensRegOp.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensRegOp.hpp", "filename": "SensRegOp.hpp", "file": "SensRegOp.hpp", "brief": "sIPOPT registration operations for options\n\nSensRegOp: registers sIPOPT-specific options with Ipopt.\nAdds sensitivity analysis parameters like n_sens_steps, sens_prefix,\nand computation modes to Ipopt option system.", "has_pass2": false}, "contrib/sIPOPT/src/SensSchurDriver.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensSchurDriver.hpp", "filename": "SensSchurDriver.hpp", "file": "SensSchurDriver.hpp", "brief": "sIPOPT Schur complement driver interface\n\nSensSchurDriver: abstract interface for Schur complement operations.\nManages factorization and solve phases for sensitivity analysis.\nImplementations handle dense, sparse, or iterative approaches.", "has_pass2": false}, "contrib/sIPOPT/src/SensIndexSchurData.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensIndexSchurData.hpp", "filename": "SensIndexSchurData.hpp", "file": "SensIndexSchurData.hpp", "brief": "sIPOPT index-based Schur data storage\n\nSensIndexSchurData: stores index-based data for Schur complement.\nTracks which rows/columns of KKT system correspond to parameters.\nManages sparse structure for efficient Schur complement computation.", "has_pass2": false}, "contrib/sIPOPT/src/SensStdStepCalc.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensStdStepCalc.hpp", "filename": "SensStdStepCalc.hpp", "file": "SensStdStepCalc.hpp", "brief": "sIPOPT standard sensitivity step calculator\n\nSensStdStepCalc: standard implementation of SensitivityStepCalculator.\nComputes sensitivity steps using Schur complement and back-solves.\nHandles bound corrections for perturbed solutions.", "has_pass2": false}, "contrib/sIPOPT/src/SensBacksolver.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensBacksolver.hpp", "filename": "SensBacksolver.hpp", "file": "SensBacksolver.hpp", "brief": "sIPOPT backsolver interface for linear systems\n\nSensBacksolver: abstract interface for solving linear systems in sIPOPT.\nUsed for Schur complement right-hand side solutions. Implementations\nleverage existing Ipopt linear solvers for efficient back-substitution.", "has_pass2": false}, "contrib/sIPOPT/src/SensSimpleBacksolver.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensSimpleBacksolver.hpp", "filename": "SensSimpleBacksolver.hpp", "file": "SensSimpleBacksolver.hpp", "brief": "sIPOPT simple backsolver implementation\n\nSensSimpleBacksolver: basic implementation using Ipopt linear solver.\nWraps existing factorized KKT system for back-substitution.\nReuses factorization from optimization for sensitivity solves.", "has_pass2": false}, "contrib/sIPOPT/src/SensSchurData.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensSchurData.hpp", "filename": "SensSchurData.hpp", "file": "SensSchurData.hpp", "brief": "sIPOPT Schur complement data structures\n\nSensSchurData: abstract base for Schur complement data storage.\nDefines interface for storing and accessing matrix blocks needed\nfor Schur complement S = C - B*K^{-1}*A computation.", "has_pass2": false}, "contrib/sIPOPT/src/SensStepCalc.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensStepCalc.hpp", "filename": "SensStepCalc.hpp", "file": "SensStepCalc.hpp", "brief": "sIPOPT sensitivity step calculator interface\n\nSensStepCalc: abstract interface for computing sensitivity steps.\nGiven parameter perturbation, computes resulting change in solution.\nCore component of sIPOPT sensitivity analysis pipeline.", "has_pass2": false}, "contrib/sIPOPT/src/SensIndexPCalculator.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensIndexPCalculator.hpp", "filename": "SensIndexPCalculator.hpp", "file": "SensIndexPCalculator.hpp", "brief": "sIPOPT index-based perturbation calculator\n\nSensIndexPCalculator: calculates perturbations for indexed parameters.\nIdentifies which variables/constraints are perturbed and computes\ncorresponding right-hand sides for sensitivity linear systems.", "has_pass2": false}, "contrib/sIPOPT/src/SensMetadataMeasurement.hpp": {"path": "layer-2/Ipopt/contrib/sIPOPT/src/SensMetadataMeasurement.hpp", "filename": "SensMetadataMeasurement.hpp", "file": "SensMetadataMeasurement.hpp", "brief": "sIPOPT metadata-based measurement\n\nSensMetadataMeasurement: reads parameter perturbations from NLP metadata.\nExtracts sensitivity parameters from AMPL suffixes or TNLP metadata.\nEnables automatic parameter identification from problem definition.", "has_pass2": false}, "contrib/RInterface/src/IpoptRNLP.hpp": {"path": "layer-2/Ipopt/contrib/RInterface/src/IpoptRNLP.hpp", "filename": "IpoptRNLP.hpp", "file": "IpoptRNLP.hpp", "brief": "R language interface for Ipopt TNLP\n\nIpoptRNLP derives from TNLP to bridge Ipopt with R. Handles callback\nto R functions for objective (eval_f), gradient (eval_grad_f),\nconstraints (eval_g), Jacobian (eval_jac_g), and Hessian (eval_h).\nManages R SEXP objects for environment, bounds, and sparse structures.\nSupports Hessian approximation mode. Part of ipoptr R package.", "has_pass2": false}, "contrib/RInterface/src/IpoptRJournal.hpp": {"path": "layer-2/Ipopt/contrib/RInterface/src/IpoptRJournal.hpp", "filename": "IpoptRJournal.hpp", "file": "IpoptRJournal.hpp", "brief": "R output journal for Ipopt progress messages\n\nIpoptRJournal derives from Journal to redirect Ipopt output to R.\nSends iteration progress and solver messages to R console via Rprintf.\nAllows R users to monitor optimization progress interactively.", "has_pass2": false}, "src/Interfaces/IpStdCInterface.h": {"path": "layer-2/Ipopt/src/Interfaces/IpStdCInterface.h", "filename": "IpStdCInterface.h", "return": "false, if the option could not be set (e.g., if keyword is unknown)", "param": ["ipopt_problem (in) Problem that is currently optimized.", "n       (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping x, z_L, and z_U", "scaled  (in)  whether to retrieve scaled or unscaled iterate", "x       (out) buffer to store value of primal variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving x", "z_L     (out) buffer to store the lower bound multipliers \\f$z_L\\f$, must have length at least n; pass NULL to skip retrieving z_L and Z_U", "z_U     (out) buffer to store the upper bound multipliers \\f$z_U\\f$, must have length at least n; pass NULL to skip retrieving z_L and Z_U", "m       (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping g and lambda", "g       (out) buffer to store the constraint values \\f$g(x)\\f$, must have length at least m; pass NULL to skip retrieving g", "lambda  (out) buffer to store the constraint multipliers \\f$\\lambda\\f$, must have length at least m; pass NULL to skip retrieving lambda", "ipopt_problem (in) Problem that is currently optimized.", "scaled     (in)  whether to retrieve scaled or unscaled violations", "n          (in)  the number of variables \\f$x\\f$ in the problem; can be arbitrary if skipping compl_x_L, compl_x_U, and grad_lag_x", "x_L_violation (out) buffer to store violation of original lower bounds on variables (max(orig_x_L-x,0)), must have length at least n; pass NULL to skip retrieving orig_x_L", "x_U_violation (out) buffer to store violation of original upper bounds on variables (max(x-orig_x_U,0)), must have length at least n; pass NULL to skip retrieving orig_x_U", "compl_x_L  (out) buffer to store violation of complementarity for lower bounds on variables (\\f$(x-x_L)z_L\\f$), must have length at least n; pass NULL to skip retrieving compl_x_L", "compl_x_U  (out) buffer to store violation of complementarity for upper bounds on variables (\\f$(x_U-x)z_U\\f$), must have length at least n; pass NULL to skip retrieving compl_x_U", "grad_lag_x (out) buffer to store gradient of Lagrangian w.r.t. variables \\f$x\\f$, must have length at least n; pass NULL to skip retrieving grad_lag_x", "m          (in)  the number of constraints \\f$g(x)\\f$; can be arbitrary if skipping lambda", "nlp_constraint_violation (out) buffer to store violation of constraints \\f$max(g_l-g(x),g(x)-g_u,0)\\f$, must have length at least m; pass NULL to skip retrieving constraint_violation", "compl_g    (out) buffer to store violation of complementarity of constraint (\\f$(g(x)-g_l)*\\lambda^+ + (g_l-g(x))*\\lambda^-\\f$, where \\f$\\lambda^+=max(0,\\lambda)\\f$ and \\f$\\lambda^-=max(0,-\\lambda)\\f$ (componentwise)), must have length at least m; pass NULL to skip retrieving compl_g"], "has_pass2": false}}}}}, "layer-3": {"name": "layer-3", "library_count": 10, "libraries": {"Bcp": {"name": "Bcp", "file_count": 92, "pass2_count": 92, "files": {"Applications/MaxCut/include/MC.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC.hpp", "filename": "MC.hpp", "file": "MC.hpp", "brief": "Max-cut problem data structures", "algorithm": "Max-Cut: Graph Partitioning Optimization\n\nMC_problem: graph representation for max-cut optimization.\nStores nodes, edges, adjacency lists. Supports Ising problem variant\nwith four-cycles and triangles. MC_feas_sol stores cut solutions.\n\n**Problem:** Partition nodes into two sets S and T to maximize\ntotal weight of edges crossing the cut (one endpoint in S, one in T).\n\n**Data structures:**\n- MC_graph_node/edge: Graph topology and edge weights\n- MC_adjacency_entry: Neighbor list for efficient enumeration\n- MC_switch_structure: Local search move support", "complexity": "NP-hard; solved via branch-and-cut with cycle inequalities", "has_pass2": true}, "Applications/MaxCut/include/MC_solution.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_solution.hpp", "filename": "MC_solution.hpp", "file": "MC_solution.hpp", "brief": "Max-cut solution representation", "algorithm": "Max-Cut Solution: Partition and Heuristic Improvement\n\nStores max-cut solution: node partition (sig vector) and cut value.\n\n**sig vector:** Sign assignment (+1 or -1) for each node.\nTwo nodes with different signs are on opposite sides of the cut.\n\n**Local search:** Constructor can apply edge-switch and structure-switch\nheuristics to improve initial partition.", "see": ["MC.hpp for MC_problem definition"], "has_pass2": true}, "Applications/MaxCut/include/MC_tm_param.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_tm_param.hpp", "filename": "MC_tm_param.hpp", "file": "MC_tm_param.hpp", "brief": "Max-cut TM parameters", "algorithm": "Max-Cut TM Params: Input/Output Configuration\n\nMC_tm_par defines parameters for max-cut Tree Manager:\n\n- DisplaySolutionSignature: Show partition signs in output\n- DigitsToLose: Numerical precision control\n- InputFile: Problem instance file\n- FeasSolFile: Initial solution file\n- SolutionFile: Output solution file", "has_pass2": true}, "Applications/MaxCut/include/MC_tm.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_tm.hpp", "filename": "MC_tm.hpp", "file": "MC_tm.hpp", "brief": "Max-cut tree manager for BCP", "algorithm": "Max-Cut TM: Search Tree Coordination\n\nTree manager process for coordinating max-cut search tree.\nInherits from BCP_tm_user to customize tree management.\n\n**Responsibilities:**\n- Pack/unpack MC_problem graph to LP processes\n- Manage best known solution (MC_solution)\n- Serialize/deserialize cycle cuts between processes", "see": ["MC_lp.hpp for LP node processing"], "has_pass2": true}, "Applications/MaxCut/include/MC_lp_param.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_lp_param.hpp", "filename": "MC_lp_param.hpp", "file": "MC_lp_param.hpp", "brief": "Max-cut LP parameters", "algorithm": "Max-Cut LP Params: Solver and Cut Configuration\n\nMC_lp_par defines parameters for max-cut LP process:\n\n**Solver selection:**\n- MC_UseVol: Volume algorithm (Lagrangean)\n- MC_UseClp: CLP simplex solver\n\n**Cycle cut generation:**\n- MC_SPCycleCutGen: Shortest-path cycle cuts (never/last resort/always)\n- MC_MstCycleCutGen: MST-based cycle cuts (never/last resort/always)\n\n**Heuristics:**\n- DoEdgeSwitchHeur: Edge swap local search\n- StructSwitchHeur: Structure-based improvement", "has_pass2": true}, "Applications/MaxCut/include/MC_init.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_init.hpp", "filename": "MC_init.hpp", "file": "MC_init.hpp", "brief": "Max-cut BCP initialization", "algorithm": "Max-Cut Init: Process Factory Methods\n\nUSER_initialize subclass for max-cut application.\nCreates MC_tm and MC_lp instances when BCP spawns processes.\n\n**Factory methods:**\n- tm_init(): Creates MC_tm, reads problem file\n- lp_init(): Creates MC_lp for node processing", "see": ["BCP_USER.hpp for initialization interface"], "has_pass2": true}, "Applications/MaxCut/include/MC_cut.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_cut.hpp", "filename": "MC_cut.hpp", "file": "MC_cut.hpp", "brief": "Max-cut cutting planes", "algorithm": "Cycle Inequalities: Max-Cut Valid Cuts\n\nCut generation for max-cut: cycle inequalities, odd-wheel cuts.\n\n**Cycle inequality:** For any cycle in the graph, at most k-1 edges\ncan be in the cut if the cycle has k edges. This is violated when\nfractional LP solution assigns >k-1 total to cycle edges.\n\n**MC_cycle_cut:** Stores cycle as edge list with memory pooling.\n\n**MC_EdgeOrdering:** MST-based cycle finding with different tie-breaking\nfor edge selection (prefer 0, prefer 1, prefer extreme).", "complexity": "Separation via shortest path in auxiliary graph", "has_pass2": true}, "Applications/MaxCut/include/MC_lp.hpp": {"path": "layer-3/Bcp/Applications/MaxCut/include/MC_lp.hpp", "filename": "MC_lp.hpp", "file": "MC_lp.hpp", "brief": "Max-cut LP relaxation for BCP", "algorithm": "Max-Cut LP: Relaxation and Cut Generation\n\nLP process implementation for max-cut branch-and-cut.\nInherits from BCP_lp_user to customize node processing.\n\n**Key components:**\n- Volume algorithm option for Lagrangean relaxation\n- Tailing off detection via objective history\n- Heuristic solution generation during cut separation\n- Adaptive cut generation (easy → hard cuts)", "see": ["MC_cut.hpp for cycle inequality separation"], "has_pass2": true}, "Applications/Mkc/include/MKC_knapsack.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_knapsack.hpp", "filename": "MKC_knapsack.hpp", "file": "MKC_knapsack.hpp", "brief": "MKC knapsack subproblem for column generation", "algorithm": "Knapsack Pricing: Column Generation Subproblem\n\nKnapsack subproblem solver for Multi-Knapsack Cover problem.\n\n**Role in column generation:**\nMaster problem has covering constraints (each item covered at least once).\nPricing subproblem finds knapsack packings with negative reduced cost.\n\n**MKC_knapsack_entry:**\n- orig_cost: Original item value\n- cost: Modified cost using dual values from master\n- ratio: cost/weight for greedy/DP ordering\n\n**Sorting functions:** Support greedy heuristics and DP bounds.", "see": ["MKC_vargen.hpp for variable generation implementation"], "has_pass2": true}, "Applications/Mkc/include/MKC_var.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_var.hpp", "filename": "MKC_var.hpp", "file": "MKC_var.hpp", "brief": "MKC variable/column definitions", "algorithm": "MKC Variable: Knapsack Packing Column\n\nMKC_var represents a column in the set covering formulation:\na feasible packing of orders into a single knapsack.\n\n**Data:**\n- cost: Column coefficient in objective\n- entries: First entry = knapsack index, rest = order indices packed\n- color[2]: For branching on color combinations\n\n**Types:**\n- MKC_RegularVar: Generated via pricing\n- MKC_BranchingVar: Created during branching", "has_pass2": true}, "Applications/Mkc/include/MKC_vargen.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_vargen.hpp", "filename": "MKC_vargen.hpp", "file": "MKC_vargen.hpp", "brief": "MKC variable generation (pricing)", "algorithm": "MKC Pricing: Column Generation via Knapsack\n\nFunctions for generating improving columns:\n\n**MKC_compute_ks_upper_bound:**\nCompute upper bound on reduced cost for early termination.\nUses knapsack relaxation with modified costs from duals.\n\n**MKC_generate_variables:**\nSolve pricing subproblems to find negative reduced cost columns.\nCan enumerate small knapsacks exactly or use heuristics.", "see": ["MKC_knapsack.hpp for knapsack data structures"], "has_pass2": true}, "Applications/Mkc/include/MKC_solution.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_solution.hpp", "filename": "MKC_solution.hpp", "file": "MKC_solution.hpp", "brief": "MKC solution representation", "algorithm": "MKC Solution: Selected Column Set\n\nStores integer solution: set of selected MKC_var columns\nthat cover all orders exactly once.", "has_pass2": true}, "Applications/Mkc/include/MKC_optim.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_optim.hpp", "filename": "MKC_optim.hpp", "file": "MKC_optim.hpp", "brief": "MKC optimization routines", "algorithm": "Greedy Knapsack: Fast Packing Heuristic\n\nMKC_greedy_knapsack: Greedy packing algorithm for knapsack\nsubproblems. Sorts items by cost/weight ratio, adds greedily.\n\nUsed for generating initial columns and as heuristic bound\nduring pricing.", "has_pass2": true}, "Applications/Mkc/include/MKC_lp_param.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_lp_param.hpp", "filename": "MKC_lp_param.hpp", "file": "MKC_lp_param.hpp", "brief": "MKC LP parameters", "algorithm": "MKC LP Params: Pricing and Solver Options\n\nParameters for LP process:\n- CheckForTailoff: Detect slow progress\n- DoLogicalFixing: Implied variable fixing\n- ExactFallbackAtVargen: Use exact pricing when heuristic fails\n- UseVolume/UseClp: Solver selection", "has_pass2": true}, "Applications/Mkc/include/MKC_lp.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_lp.hpp", "filename": "MKC_lp.hpp", "file": "MKC_lp.hpp", "brief": "MKC LP relaxation for BCP", "algorithm": "MKC LP: Set Covering with Column Generation\n\nLP process for multi-knapsack cover branch-and-price.\n\n**Components:**\n- kss: Knapsack set defining problem structure\n- ks_fixings: Variable fixing from branching\n- enumerated_ks: Pre-enumerated small knapsack solutions\n- generated_vars: Columns from pricing in this node\n\n**Role:** Solve LP relaxation, call MKC_generate_variables\nwhen pricing needed, apply branching decisions.", "has_pass2": true}, "Applications/Mkc/include/MKC_tm_param.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_tm_param.hpp", "filename": "MKC_tm_param.hpp", "file": "MKC_tm_param.hpp", "brief": "MKC TM parameters", "algorithm": "MKC TM Params: Solution and Input Configuration\n\nParameters: DetailedFeasibleSolution, SolveLpForLB,\nCreateRootFromInputVars, ReadFromVarFile, etc.", "has_pass2": true}, "Applications/Mkc/include/MKC_tm.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_tm.hpp", "filename": "MKC_tm.hpp", "file": "MKC_tm.hpp", "brief": "MKC tree manager for BCP", "algorithm": "MKC TM: Search Tree Coordination\n\nTree manager for multi-knapsack cover branch-and-price.\n\n**State:**\n- kss: Knapsack set (problem definition)\n- input_vars: Initial columns (greedy or heuristic)\n- clp: CLP instance for root processing\n\nCoordinates LP processes, manages incumbent solution.", "has_pass2": true}, "Applications/Mkc/include/MKC_init.hpp": {"path": "layer-3/Bcp/Applications/Mkc/include/MKC_init.hpp", "filename": "MKC_init.hpp", "file": "MKC_init.hpp", "brief": "MKC BCP initialization", "algorithm": "MKC Init: Process Factory Methods\n\nUSER_initialize subclass for MKC application.\nCreates MKC_tm and MKC_lp instances when BCP spawns processes.", "has_pass2": true}, "Applications/Csp/include/KS.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/KS.hpp", "filename": "KS.hpp", "file": "KS.hpp", "brief": "Knapsack solver for CSP column generation", "algorithm": "Horowitz-Sahni: Exact Knapsack Solver\n\nBranch-and-bound algorithm for 0-1 knapsack problems.\n\n**Method:**\n1. Sort items by profit/weight ratio (Dantzig ordering)\n2. Compute LP upper bound via greedy fractional solution\n3. Forward step: greedily add items while capacity permits\n4. Backtrack when bound proves no improvement possible", "ref": ["Martello & Toth, \"Knapsack Problems\" (1990), pp. 30-31"], "complexity": "O(2^n) worst case, but efficient with good bounds", "has_pass2": true}, "Applications/Csp/include/CSP_tm_param.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_tm_param.hpp", "filename": "CSP_tm_param.hpp", "file": "CSP_tm_param.hpp", "brief": "CSP tree manager parameters", "algorithm": "CSP TM Params: Cutting Strategy Options\n\nParameters: combineExclusionConstraints, addKnapsackMirConstraints,\naddKnifeMirConstraints for strengthening the formulation.", "has_pass2": true}, "Applications/Csp/include/CSP_var.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_var.hpp", "filename": "CSP_var.hpp", "file": "CSP_var.hpp", "brief": "CSP variable/pattern definitions", "algorithm": "CSP Variable: Cutting Pattern as Column\n\nEach CSP_var represents a cutting pattern (column in master problem).\nMultiple inheritance: BCP_var_algo (LP variable) + PATTERN (items cut).\n\nPattern cost = raw material cost; coefficients = items produced.", "has_pass2": true}, "Applications/Csp/include/CSP_userexits.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_userexits.hpp", "filename": "CSP_userexits.hpp", "file": "CSP_userexits.hpp", "brief": "CSP user exit callbacks", "algorithm": "CSP User Data: Solution Pool\n\nUserData maintains pool of diverse solutions found during pricing:\n- denseSols: Solution vectors\n- addSol: Deduplicates based on objective and full vector comparison", "has_pass2": true}, "Applications/Csp/include/CSP_init.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_init.hpp", "filename": "CSP_init.hpp", "file": "CSP_init.hpp", "brief": "CSP BCP initialization", "algorithm": "CSP Init: Process Factory\n\nUSER_initialize subclass creating CSP_tm and CSP_lp processes.", "has_pass2": true}, "Applications/Csp/include/CSP_tm.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_tm.hpp", "filename": "CSP_tm.hpp", "file": "CSP_tm.hpp", "brief": "CSP tree manager for BCP", "algorithm": "CSP Tree Manager: Search Coordination\n\nCoordinates branch-and-price search for cutting stock:\n- initialize_core: Set packing constraints (each demand met)\n- create_root: Initial patterns from enumeration\n- compare_tree_nodes: Best-first search by lower bound", "has_pass2": true}, "Applications/Csp/include/CSP.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP.hpp", "filename": "CSP.hpp", "file": "CSP.hpp", "brief": "Cutting stock problem definitions", "algorithm": "Cutting Stock: One-Dimensional Bin Packing\n\nCSP_problem: cutting stock data with stock widths and patterns.\nStores demands, pattern costs, column generation state.\n\n**Problem:** Cut standard-width stock into demanded pieces,\nminimizing number of stock pieces used (minimizing waste).\n\n**Column generation approach:**\n- Master: Select patterns to cover all demands\n- Pricing: Solve bounded knapsack to find improving patterns\n- PATTERN: Specifies how many of each piece width to cut from one stock\n\n**CSP_packedVector:** Sparse representation for patterns\n(most patterns use few piece types from many available).", "complexity": "Weakly NP-hard; solved well by column generation + branching", "has_pass2": true}, "Applications/Csp/include/CSP_lp.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_lp.hpp", "filename": "CSP_lp.hpp", "file": "CSP_lp.hpp", "brief": "CSP LP relaxation for BCP", "algorithm": "CSP LP Process: Solve and Price\n\nLP relaxation management for cutting stock:\n- compute_lower_bound: Solve LP, generate columns via knapsack pricing\n- generate_vars_in_lp: Call CSP_colgen for improving patterns\n- select_branching_candidates: Branch on fractional patterns\n\nUses Clp or Volume algorithm for LP solves.", "param": ["vars       (IN) The variables in the current formulation", "cuts       (IN) The cuts in the current formulation", "var_status (IN) The stati of the variables", "cut_status (IN) The stati of the cuts", "var_changed_pos (OUT) The positions of the variables whose\n       bounds should be tightened", "var_new_bd      (OUT) The new lb/ub of those variables", "cut_changed_pos (OUT) The positions of the cuts whose bounds\n       should be tightened", "cut_new_bd (OUT) The new lb/ub of those cuts", "lpres  the result of the most recent LP optimization", "vars   variables currently in the formulation", "cuts    the cuts currently in the relaxation (IN)", "vars    the variables to be converted (IN/OUT)", "cols    the colums the variables convert into (OUT)", "lpres   solution to the current LP relaxation (IN)", "origin  where the do the cuts come from (IN)", "allow_multiple whether multiple expansion, i.e., lifting, is\n       allowed (IN)\n\n       Default: throw an exception (if this method is invoked then the user\n       must have generated variables and BCP has no way to know how to convert\n       them).", "lpres    solution to the current LP relaxation (IN)", "vars     the variabless currently in the relaxation (IN)", "cuts     the cuts currently in the relaxation (IN)", "new_cuts the vector of generated cuts (OUT)", "new_rows the correspontding rows(OUT)", "lpres         solution to the current LP relaxation (IN)", "vars          the variabless currently in the relaxation (IN)", "cuts          the cuts currently in the relaxation (IN)", "before_fathom if true then BCP is about to fathom the node, so\n       spend some extra effort generating variables if\n       you want to avoid that...", "new_vars      the vector of generated variables (OUT)", "new_cols the correspontding columns(OUT)", "lpres the result of the most recent LP optimization,", "vars the variables in the current formulation,", "status the stati of the variables as known to the system,", "var_bound_changes_since_logical_fixing the number of variables\n       whose bounds have changed (by reduced cost fixing) since the\n       most recent invocation of this method that has actually forced\n       changes returned something in the last two arguments,", "changed_pos the positions of the variables whose bounds should\n       be changed", "new_bd the new bounds (lb/ub pairs) of these variables.", "slack_pool the pool of slacks. (IN)", "to_be_purged the indices of the cuts to be purged. (OUT)"], "has_pass2": true}, "Applications/Csp/include/CSP_colgen.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_colgen.hpp", "filename": "CSP_colgen.hpp", "file": "CSP_colgen.hpp", "brief": "CSP column generation", "algorithm": "CSP Pricing: Knapsack Column Generation\n\nGenerate improving cutting patterns by solving knapsack subproblems.\n\n**Pricing problem:** max {π·a - c : a valid pattern}\n- π = dual prices from demand constraints\n- c = pattern cost (raw material)\n- Solve bounded knapsack with binary expansion\n\n**Features:**\n- Perturbation for diversity (perturb_factor, perturb_num)\n- Exclusion constraints for branching", "has_pass2": true}, "Applications/Csp/include/CSP_lp_param.hpp": {"path": "layer-3/Bcp/Applications/Csp/include/CSP_lp_param.hpp", "filename": "CSP_lp_param.hpp", "file": "CSP_lp_param.hpp", "brief": "CSP LP parameters", "algorithm": "CSP LP Params: Solver and Heuristic Options\n\nParameters: LpSolver (Simplex/Volume), BranchingStrategy,\nHeurIpFrequency, PerturbFactor/PerturbNum for pricing diversity.", "has_pass2": true}, "Bcp/src/include/BCP_matrix.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_matrix.hpp", "filename": "BCP_matrix.hpp", "file": "BCP_matrix.hpp", "brief": "Matrix and vector representations for BCP LP relaxation", "algorithm": "LP Relaxation Matrix with Dynamic Column/Row Addition\n\nExtends CoinPackedVector/Matrix with bounds for LP formulation:\n\n**BCP_col (extends CoinPackedVector):**\n- Sparse column with objective coefficient + lb/ub\n- Used for column generation (new variables)\n- Default bounds: [0, +∞), obj = 0\n\n**BCP_row (extends CoinPackedVector):**\n- Sparse row with lb/ub (constraint bounds)\n- Used for cut generation (new constraints)\n- Default bounds: [-∞, +∞]\n\n**BCP_lp_relax (extends CoinPackedMatrix):**\n- Full LP relaxation: matrix + bounds + objective\n- _Objective: Variable costs\n- _ColLowerBound, _ColUpperBound: Variable bounds\n- _RowLowerBound, _RowUpperBound: Constraint bounds\n- Can be row-major or column-major ordered\n- pack()/unpack() for serialization\n\n**Usage:**\nCore matrix defined in initialize_core(), columns added\nvia column generation, rows added via cut generation.", "see": ["BCP_var.hpp for variable representation", "BCP_cut.hpp for cut representation", "CoinPackedMatrix for underlying sparse matrix"], "has_pass2": true}, "Bcp/src/include/BCP_indexed_pricing.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_indexed_pricing.hpp", "filename": "BCP_indexed_pricing.hpp", "file": "BCP_indexed_pricing.hpp", "brief": "Indexed pricing list for column generation", "algorithm": "Indexed Pricing: Partial Column Selection for VG\n\nBCP_indexed_pricing_list tracks which variable indices to price.\nAllows selective pricing rather than full pricing each iteration.\n\n**Note:** Currently disabled (#if 0) - placeholder for future use.", "see": ["BCP_vg.hpp for Variable Generator process"], "has_pass2": true}, "Bcp/src/include/BCP_enum_process_t.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_enum_process_t.hpp", "filename": "BCP_enum_process_t.hpp", "file": "BCP_enum_process_t.hpp", "brief": "Process type enumeration for BCP", "algorithm": "Process Type Enum: TM, LP, CG, VG, TS Identifiers\n\nBCP_process_t enumerates all BCP process types:\n- Any: Wildcard for message receiving\n- TM: Tree Manager (coordinator)\n- TS: Tree Storage (external node storage)\n- LP: Linear Programming worker\n- CG: Cut Generator\n- VG: Variable Generator\n- CP: Cut Pool (future)\n- VP: Variable Pool (future)\n- EndProcess: Termination indicator", "see": ["BCP_process.hpp for process base class"], "has_pass2": true}, "Bcp/src/include/BCP_lp_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_user.hpp", "filename": "BCP_lp_user.hpp", "file": "BCP_lp_user.hpp", "brief": "User customization interface for LP process", "algorithm": "LP User Hooks: Cuts, Branching, Feasibility, Heuristics\n\nBCP_lp_user is the main interface for user-defined LP behavior.\nMost BCP customization happens here.\n\n**Initialization:**\n- initialize_solver_interface(): Create OsiSolverInterface\n- initialize_int_and_sos_list(): Define branching objects\n- initialize_new_search_tree_node(): Preprocess before node\n- load_problem(): Load problem into solver\n\n**LP iteration:**\n- process_lp_result(): Main iteration callback (or use individual)\n- test_feasibility(): Check if solution is integer-feasible\n- generate_heuristic_solution(): Run primal heuristics\n- compute_lower_bound(): True lower bound for column generation\n\n**Cut/variable generation:**\n- generate_cuts_in_lp(): Local cut generation\n- generate_vars_in_lp(): Local column generation (pricing)\n- cuts_to_rows(): Convert BCP_cut to matrix rows\n- vars_to_cols(): Convert BCP_var to matrix columns\n\n**Branching:**\n- select_branching_candidates(): Choose branching objects\n- compare_branching_candidates(): Select best candidate\n- set_actions_for_children(): Fathom/Return/Keep decisions\n\n**Packing:**\n- pack_primal_solution(): Send LP solution to CG\n- pack_dual_solution(): Send duals to VG\n- pack_feasible_solution(): Send solution to TM", "see": ["BCP_lp.hpp for LP process", "BCP_tm_user for Tree Manager user interface"], "param": ["vars       (IN) The variables in the current formulation", "cuts       (IN) The cuts in the current formulation", "var_status (IN) The stati of the variables", "cut_status (IN) The stati of the cuts", "var_changed_pos (OUT) The positions of the variables whose\n\t                             bounds should be tightened", "var_new_bd      (OUT) The new lb/ub of those variables", "cut_changed_pos (OUT) The positions of the cuts whose bounds\n\t                             should be tightened", "cut_new_bd (OUT) The new lb/ub of those cuts", "lp_result the result of the most recent LP optimization (IN)", "vars      variables currently in the formulation (IN)", "cuts      variables currently in the formulation (IN)", "old_lower_bound the previously known best lower bound (IN)", "new_cuts  the vector of generated cuts (OUT)", "new_rows  the correspontding rows(OUT)", "new_vars      the vector of generated variables (OUT)", "new_cols the correspontding columns(OUT)", "lp_result the result of the most recent LP optimization", "vars      variables currently in the formulation", "cuts      variables currently in the formulation", "buf (OUT) the buffer to pack into", "sol (IN)  the solution to be packed", "buf       (OUT) the buffer to pack into", "lp_result (IN) the result of the most recent LP optimization", "vars      (IN) variables currently in the formulation", "cuts      (IN) cuts currently in the formulation", "buf       (OUT) the buffer to pack into", "lp_result (IN) the result of the most recent LP optimization", "vars      (IN) variables currently in the formulation", "cuts      (IN) cuts currently in the formulation", "lp_result (IN) the result of the most recent LP optimization", "vars      (IN) variables currently in the formulation", "final_lp_solution (IN) whether the lp solution is final or not.", "vars    the variables currently in the relaxation (IN)", "cuts    the cuts to be converted (IN/OUT)", "rows    the rows into which the cuts are converted (OUT)", "lpres   solution to the current LP relaxation (IN)", "origin  where the cuts come from (IN)", "allow_multiple whether multiple expansion, i.e., lifting, is\n\tallowed (IN)\n\n        Default: throw an exception (if this method is invoked then the user\n        must have generated cuts and BCP has no way to know how to convert\n        them).", "cuts    the cuts currently in the relaxation (IN)", "vars    the variables to be converted (IN/OUT)", "cols    the colums the variables convert into (OUT)", "lpres   solution to the current LP relaxation (IN)", "origin  where the do the cuts come from (IN)", "allow_multiple whether multiple expansion, i.e., lifting, is\n\tallowed (IN)\n\n        Default: throw an exception (if this method is invoked then the user\n        must have generated variables and BCP has no way to know how to convert\n        them).", "lpres    solution to the current LP relaxation (IN)", "vars     the variabless currently in the relaxation (IN)", "cuts     the cuts currently in the relaxation (IN)", "new_cuts the vector of generated cuts (OUT)", "new_rows the correspontding rows(OUT)", "lpres         solution to the current LP relaxation (IN)", "vars          the variabless currently in the relaxation (IN)", "cuts          the cuts currently in the relaxation (IN)", "before_fathom if true then BCP is about to fathom the node, so\n\tspend some extra effort generating variables if\n\tyou want to avoid that...", "new_vars      the vector of generated variables (OUT)", "new_cols the correspontding columns(OUT)", "lpres the result of the most recent LP optimization,", "vars the variables in the current formulation,", "status the stati of the variables as known to the system,", "var_bound_changes_since_logical_fixing the number of variables\n\twhose bounds have changed (by reduced cost fixing) since the\n\tmost recent invocation of this method that has actually forced\n\tchanges returned something in the last two arguments,", "changed_pos the positions of the variables whose bounds should\n\tbe changed", "new_bd the new bounds (lb/ub pairs) of these variables.", "lpres the result of the most recent LP optimization.", "vars the variables in the current formulation.", "cuts the cuts in the current formulation.", "local_var_pool the local pool that holds variables with negative\n\treduced cost. In case of continuing with the node the best so\n\tmany variables will be added to the formulation (those with the\n\tmost negative reduced cost).", "local_cut_pool the local pool that holds violated cuts. In case\n\tof continuing with the node the best so many cuts will be added\n\tto the formulation (the most violated ones).", "cands the generated branching candidates.", "force_branch indicate whether to force branching regardless\n\t       of the size of the local cut/var pools", "slack_pool the pool of slacks. (IN)", "to_be_purged the indices of the cuts to be purged. (OUT)"], "has_pass2": true}, "Bcp/src/include/BCP_lp_functions.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_functions.hpp", "filename": "BCP_lp_functions.hpp", "file": "BCP_lp_functions.hpp", "brief": "LP process internal function declarations", "algorithm": "LP Internal Functions: Node Processing Pipeline\n\nFunction declarations for LP process internals organized by module:\n\n**Main loop (BCP_lp_main_loop.cpp):**\n- BCP_lp_main_loop(): Core solve-cut-branch iteration\n\n**Fathoming (BCP_lp_fathom.cpp):**\n- BCP_lp_fathom(): Node pruning by bound\n- BCP_price_vars(): Variable pricing for column generation\n- BCP_restore_feasibility(): Handle dual infeasibility\n\n**Cut/Column generation:**\n- BCP_lp_generate_cuts(): Invoke separation routines\n- BCP_lp_generate_vars(): Invoke pricing routines\n\n**Matrix management (BCP_lp_colrow.cpp):**\n- BCP_lp_fix_vars(): Variable fixing by reduced costs\n- BCP_lp_delete_cols_and_rows(): Matrix cleanup\n- BCP_lp_add_from_local_*_pool(): Pool management\n\n**Branching (BCP_lp_branch.cpp):**\n- BCP_lp_branch(): Execute branching decision\n\n**Messaging (BCP_lp_msgproc.cpp):**\n- BCP_lp_check_ub(): Process upper bound updates\n- BCP_lp_send_cuts_to_cp(): Communicate with cut pool", "see": ["BCP_lp.hpp for LP process class", "BCP_lp_user.hpp for user customization"], "has_pass2": true}, "Bcp/src/include/BCP_vector_short.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_short.hpp", "filename": "BCP_vector_short.hpp", "file": "BCP_vector_short.hpp", "brief": "BCP_vec<short> specialization", "algorithm": "POD Vector Specialization: short\n\nTemplate specializations for BCP_vec<short> that skip\nconstruct/destroy overhead since short is a POD type.\nSame optimization pattern as BCP_vector_double.hpp.", "has_pass2": true}, "Bcp/src/include/BCP_vector_change.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_change.hpp", "filename": "BCP_vector_change.hpp", "file": "BCP_vector_change.hpp", "brief": "Delta-encoded vector storage for BCP", "algorithm": "Delta Vector: Explicit or Relative Storage\n\nBCP_vec_change<T> stores vectors either explicitly or as deltas\nfrom a parent vector. This is the core of BCP's compact tree storage.\n\n**Storage modes:**\n- Explicit: Full vector stored in _values\n- WrtParent: Delta encoding via:\n  - _del_pos: Positions to delete from parent\n  - _change_pos: Positions to modify after deletions\n  - _values: New values (changes first, then additions)\n\n**Key operations:**\n- update(): Apply delta to reconstruct explicit vector\n- as_change(): Compute delta between two vectors\n- pack/unpack(): Serialization for message passing", "complexity": "Delta update is O(n) where n = vector size", "see": ["BCP_warmstart.hpp for warmstart using delta vectors"], "param": ["new_vec the vector that should result after the change is\n\t       applied to", "old_vec the original vector", "del_pos specifies which entries are to be deleted from old_vec\n               before the change in this object can be applied."], "has_pass2": true}, "Bcp/src/include/BCP_lp_pool.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_pool.hpp", "filename": "BCP_lp_pool.hpp", "file": "BCP_lp_pool.hpp", "brief": "Cut and variable pools for LP process", "algorithm": "Cut/Variable Pooling: Violation Ranking and Selection\n\nManages waiting cuts and variables for addition to LP relaxation.\nTracks violation/reduced cost and supports ranked selection.\n\n**BCP_lp_waiting_row:** Pair of (BCP_cut, BCP_row) with violation.\n**BCP_lp_cut_pool:** Pool of waiting rows sorted by violation.\n**BCP_lp_var_pool:** Pool of waiting columns (variables).\n\n**Usage:** Generated cuts/vars queued here, then best ones added to LP.", "see": ["BCP_lp_user::select_cuts() for cut selection policy", "BCP_lp_user::select_vars() for variable selection policy"], "has_pass2": true}, "Bcp/src/include/BCP_lp_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_param.hpp", "filename": "BCP_lp_param.hpp", "file": "BCP_lp_param.hpp", "brief": "LP process parameters for BCP Branch-Cut-Price", "algorithm": "LP Parameters: Cut/Branch/Pricing Configuration\n\nBCP_lp_par defines all configurable parameters for LP processes.\nSet via parameter file: BCP_{param_name} {value}\n\n**Character (boolean) parameters:**\n- BranchOnCuts: Enable branching on slack cuts\n- CompareNew{Cuts,Vars}ToOldOnes: Duplicate detection\n- DoReducedCostFixing*: Variable fixing strategies\n- SendFathomedNodeDesc: Return fathomed nodes to TM\n- LpVerb_*: Verbosity flags for LP events\n\n**Integer parameters:**\n- SlackCutDiscardingStrategy: When to purge slack cuts\n- CutEffectiveCountBeforePool: Iterations before pooling cut\n- CutPoolCheckFrequency: How often to query cut pool\n- IneffectiveConstraints: Definition of ineffective rows\n- MaxCutsAddedPerIteration: Limit on cuts per round\n- MaxPresolveIter: Strong branching iteration limit\n- StrongBranchNum: Candidates for strong branching\n- BranchingObjectComparison: Branch selection rule\n\n**Double parameters:**\n- Granularity: Objective discretization\n- IntegerTolerance: Integer feasibility tolerance\n- *Timeout: Waiting times for cuts/vars\n- MaxRunTime: Time limit", "see": ["BCP_lp.hpp for LP process", "BCP_tm_param.hpp for TM parameters"], "has_pass2": true}, "Bcp/src/include/BCP_message_pvm.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_message_pvm.hpp", "filename": "BCP_message_pvm.hpp", "file": "BCP_message_pvm.hpp", "brief": "PVM message passing for BCP", "algorithm": "PVM Backend: BCP_pvm_environment Implementation\n\nParallel Virtual Machine (PVM) implementation of BCP_message_environment.\nRequires COIN_HAS_PVM to be defined.\n\n**Key methods:**\n- send/receive(): Point-to-point messaging with tags\n- multicast(): Broadcast to multiple targets\n- start_processes(): Spawn workers on machines\n- probe(): Non-blocking message check\n- alive(): Check remote process liveness\n\n**PVM specifics:**\nUses PVM's task spawning and heterogeneous network support.\nSuitable for clusters without MPI infrastructure.", "see": ["BCP_message.hpp for abstract interface", "BCP_message_mpi.hpp for MPI backend"], "has_pass2": true}, "Bcp/src/include/BCP_branch.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_branch.hpp", "filename": "BCP_branch.hpp", "file": "BCP_branch.hpp", "brief": "Internal branching object for BCP Branch-Cut-Price", "algorithm": "Internal Branching Representation: Bound Changes per Child\n\nRepresents the result of a branching decision as bound changes\non variables and cuts across children nodes.\n\n**BCP_internal_brobj:**\nCreated AFTER any cuts/variables from branching are added.\nStores only bound changes, not the branching object itself.\n\n**Data structure:**\n- _child_num: Number of children (typically 2 for binary branching)\n- _var_positions: Indices of variables with changed bounds\n- _cut_positions: Indices of cuts with changed bounds\n- _var_bounds: New [lb, ub] pairs for each child × each affected var\n- _cut_bounds: New [lb, ub] pairs for each child × each affected cut\n\n**Memory layout:**\nBounds are stored as: child0_var0_lb, child0_var0_ub, child0_var1_lb, ...\nthen child1_var0_lb, child1_var0_ub, ...\nTotal length: 2 × child_num × var_positions.size()\n\n**Usage:**\n- apply_child_bounds(): Modify LP solver for child's bounds\n- pack()/unpack(): Serialize for sending to TM", "see": ["BCP_lp_branch.hpp for LP-side branching object", "BCP_enum_branch.hpp for branching enumerations", "BCP_lp_user::select_branching_candidates() for branching logic"], "has_pass2": true}, "Bcp/src/include/BCP_cut.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cut.hpp", "filename": "BCP_cut.hpp", "file": "BCP_cut.hpp", "brief": "Cut (constraint) representation for BCP Branch-Cut-Price", "algorithm": "Cut Classes for Cutting Plane Generation in B&B\n\nDefines cut classes used throughout BCP:\n\n**Class hierarchy:**\n- BCP_cut: Abstract base with lb, ub, status, bcpind, effective_count\n- BCP_cut_core: Core cuts that always stay in LP (original constraints)\n- BCP_cut_algo: Algorithmic cuts from cut generation\n- BCP_cut_set: Collection with bulk operations\n\n**Effectiveness tracking:**\n- _eff_cnt: Positive = consecutive iterations cut was tight\n- Negative = consecutive iterations cut was slack\n- Cuts with low effectiveness may be removed from LP\n\n**Cut bounds:**\n- lb, ub: Constraint lb ≤ ax ≤ ub\n- Inactive cut: lb = -∞, ub = +∞ (free constraint)\n\n**Status flags (BCP_obj_status):**\n- BCP_ObjInactive: Cut is free (not binding)\n- BCP_ObjNotRemovable: Cannot be removed (e.g., branching cut)\n- BCP_ObjToBeRemoved: Marked for deletion\n- BCP_ObjDoNotSendToPool: Skip cut pool\n\n**Slack pool:**\nIneffective cuts are moved to slack_pool for potential\nbranching later (branching on cuts).", "see": ["BCP_var.hpp for symmetric variable representation", "BCP_cg.hpp for Cut Generator process", "BCP_lp.hpp for LP process using cuts"], "has_pass2": true}, "Bcp/src/include/BCP_vg.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vg.hpp", "filename": "BCP_vg.hpp", "file": "BCP_vg.hpp", "brief": "Variable Generator process for BCP Branch-Cut-Price (pricing)", "algorithm": "Variable Generator: Column Generation via Pricing Subproblems\n\nThe Variable Generator (VG) performs column generation:\ngenerates variables with negative reduced cost from dual solutions.\n\n**Column generation workflow:**\n1. LP process packs dual solution (cut duals, reduced costs)\n2. VG solves pricing subproblem to find negative reduced cost columns\n3. VG sends new variables back to LP for addition\n\n**BCP_vg_prob members:**\n- cuts, pi: Cuts and their dual values from LP\n- sender: Process ID of requesting LP\n- phase, node_level, node_index, node_iteration: Context info\n- upper_bound: Current best known solution\n- core: Problem core description\n\n**User customization:**\nOverride BCP_vg_user methods to implement pricing:\n- unpack_dual_solution(): Unpack dual values\n- generate_vars(): Main pricing subproblem logic\n- pack_var(): Send variables back to LP\n\n**Relation to LP process:**\nWhen LP becomes primal infeasible during column generation,\nit sends duals to VG. New columns restore feasibility.", "see": ["BCP_vg_user for user override points", "BCP_lp.hpp for LP process that requests columns", "BCP_var.hpp for variable representation"], "has_pass2": true}, "Bcp/src/include/BCP_message.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_message.hpp", "filename": "BCP_message.hpp", "file": "BCP_message.hpp", "brief": "Abstract message passing interface for BCP parallelism", "algorithm": "Message Passing: PVM/MPI/Single Process Abstraction\n\nBCP_message_environment is the base class for all message passing\nimplementations. Derived classes implement the protocol:\n\n**Implementations:**\n- BCP_message_single: Single-process (no parallelism)\n- BCP_message_pvm: PVM (Parallel Virtual Machine)\n- BCP_message_mpi: MPI (Message Passing Interface)\n\n**Core operations:**\n- register_process(): Get process ID\n- parent_process(): Get spawning process ID\n- alive(): Test if processes are running\n\n**Point-to-point:**\n- send(): Send message to specific process\n- receive(): Blocking receive with timeout\n- probe(): Non-blocking check for messages\n\n**Collective:**\n- multicast(): Send to multiple processes\n- start_process(): Spawn new process\n- start_processes(): Spawn multiple processes\n\n**Message tags (BCP_message_tag):**\nPredefined tags for different message types (node data,\nsolutions, cuts, variables, etc.)", "see": ["BCP_buffer.hpp for message serialization", "BCP_message_tag.hpp for tag definitions", "BCP_message_single.hpp for single-process implementation"], "has_pass2": true}, "Bcp/src/include/BCP_tm.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm.hpp", "filename": "BCP_tm.hpp", "file": "BCP_tm.hpp", "brief": "Tree Manager process for BCP Branch-Cut-Price framework", "algorithm": "Tree Manager: Parallel B&B Coordinator with Distributed Scheduling\n\nThe Tree Manager (TM) coordinates the entire BCP algorithm:\n- Manages the search tree and candidate node selection\n- Tracks global upper/lower bounds\n- Schedules work to LP processes\n- Stores variables/cuts both locally and distributed\n\n**Key classes:**\n- BCP_tm_prob: Central TM state (search tree, bounds, problem core)\n- BCP_tm_stat: Statistics (wait time, queue length by LP count)\n- BCP_slave_params: Parameter sets for LP/CG/VG processes\n\n**Process coordination:**\n- lp_procs: Vector of LP process IDs\n- active_nodes: Map from process ID to current node\n- candidate_list: CoinSearchTreeManager for node selection\n\n**Bound tracking:**\n- lower_bounds: Multiset of unexplored node bounds\n- upper_bound: Best known feasible solution value\n- lb_multiplier: Floor(lb*multiplier) to avoid rounding errors", "see": ["BCP_tm_user for user customization hooks", "BCP_lp.hpp for LP process that receives work"], "has_pass2": true}, "Bcp/src/include/BCP_vg_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vg_param.hpp", "filename": "BCP_vg_param.hpp", "file": "BCP_vg_param.hpp", "brief": "Variable Generator parameters for BCP", "algorithm": "VG Parameters: Process Priority and Logging\n\nBCP_vg_par defines parameters for Variable Generator processes:\n\n**Character parameters:**\n- MessagePassingIsSerial: Single-process vs. distributed mode\n- ReportWhenDefaultIsExecuted: Debug overridable methods\n\n**Integer parameters:**\n- NiceLevel: OS scheduling priority (-20 to 20)\n\n**String parameters:**\n- LogFileName: Per-process logging", "see": ["BCP_vg.hpp for Variable Generator process", "BCP_vg_user.hpp for user overridable methods", "the man page of the <code>setpriority</code> system function."], "has_pass2": true}, "Bcp/src/include/BCP_cg.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cg.hpp", "filename": "BCP_cg.hpp", "file": "BCP_cg.hpp", "brief": "Cut Generator process for BCP Branch-Cut-Price framework", "algorithm": "Cut Generator: Distributed Separation in Branch-Cut-Price\n\nThe Cut Generator (CG) runs as a separate process that generates\ncutting planes from LP solutions sent by the LP process.\n\n**Workflow:**\n1. LP process packs primal solution (fractional variables)\n2. CG unpacks and generates violated cuts\n3. CG sends cuts back to LP for addition to formulation\n\n**BCP_cg_prob members:**\n- vars, x: Variables and their primal values from LP\n- sender: Process ID of requesting LP\n- phase, node_level, node_index, node_iteration: Context info\n- upper_bound: Current best known solution\n- core: Problem core description\n\n**User customization:**\nOverride BCP_cg_user methods to implement cut generation:\n- unpack_primal_solution(): Unpack LP solution\n- generate_cuts(): Main cut generation logic\n- pack_cut(): Send cuts back to LP", "see": ["BCP_cg_user for user override points", "BCP_lp.hpp for LP process that requests cuts", "BCP_cut.hpp for cut representation"], "has_pass2": true}, "Bcp/src/include/BCP_vector_char.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_char.hpp", "filename": "BCP_vector_char.hpp", "file": "BCP_vector_char.hpp", "brief": "BCP_vec<char> specialization", "algorithm": "POD Vector Specialization: char\n\nTemplate specializations for BCP_vec<char> that skip\nconstruct/destroy overhead since char is a POD type.\nSame optimization pattern as BCP_vector_double.hpp.", "has_pass2": true}, "Bcp/src/include/BCP_warmstart_primaldual.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_warmstart_primaldual.hpp", "filename": "BCP_warmstart_primaldual.hpp", "file": "BCP_warmstart_primaldual.hpp", "brief": "Primal-dual warm start for BCP", "algorithm": "Primal-Dual Warmstart: Full Solution Restart\n\nBCP_warmstart_primaldual stores both primal and dual vectors.\nEach uses BCP_vec_change<double> for independent delta encoding.\n\n**Storage logic:**\nCombined storage type determined by 4x4 matrix lookup based on\nindividual primal/dual storage types. Returns best available mode.\n\n**Operations:**\n- convert_to_CoinWarmStart(): Creates CoinWarmStartPrimalDual\n- as_change(): Computes delta for both vectors independently\n- update(): Applies deltas to reconstruct full warmstart\n\n**Use case:** Interior point methods or problems where both primal\nand dual solutions are valuable for warmstarting.", "see": ["BCP_warmstart_basis.hpp for simplex basis warmstart", "BCP_warmstart_dual.hpp for dual-only warmstart"], "has_pass2": true}, "Bcp/src/include/BCP_os.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_os.hpp", "filename": "BCP_os.hpp", "file": "BCP_os.hpp", "brief": "OS abstraction layer for BCP", "algorithm": "OS Utilities: Memory and Process Info\n\nCross-platform wrappers for system-level operations:\n\n**Process management:**\n- setpriority(): Process scheduling priority (Unix)\n- gethostname(): Machine identification\n- GETPID macro: Process ID abstraction\n\n**Memory monitoring:**\n- BCP_free_mem(): System free RAM via sysinfo()\n- BCP_used_heap(): Heap usage via mallinfo()\n\nReturns -1 on platforms without required APIs.\nUsed for dynamic load balancing and memory-aware scheduling.", "has_pass2": true}, "Bcp/src/include/BCP_lp_main_loop.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_main_loop.hpp", "filename": "BCP_lp_main_loop.hpp", "file": "BCP_lp_main_loop.hpp", "brief": "LP process main loop entry point", "algorithm": "LP Main Loop: Node Processing Cycle\n\nBCP_lp_main_loop() is the top-level driver for LP process execution:\n\n1. Wait for node from Tree Manager\n2. Reconstruct node (apply deltas to get vars/cuts/warmstart)\n3. Solve LP relaxation\n4. Generate cuts/variables\n5. Check for integer feasibility\n6. Branch or fathom\n7. Report results to TM\n8. Repeat until termination message", "see": ["BCP_lp.hpp for BCP_lp_prob state", "BCP_lp_user.hpp for user callbacks invoked during loop"], "has_pass2": true}, "Bcp/src/include/BCP_tm_functions.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_functions.hpp", "filename": "BCP_tm_functions.hpp", "file": "BCP_tm_functions.hpp", "brief": "Tree Manager internal function declarations", "algorithm": "TM Internal Functions: Process and Tree Management\n\nFunction declarations for Tree Manager internals organized by module:\n\n**Initialization (BCP_tm_main.cpp):**\n- BCP_tm_do_one_phase(): Execute one phase of B&B\n- BCP_tm_create_core(): Initialize core problem\n- BCP_tm_create_root(): Create root node\n\n**Tree trimming (BCP_tm_trimming.cpp):**\n- BCP_tm_trim_tree_wrapper(): Prune dominated subtrees\n- BCP_tm_remove_explored(): Cleanup explored nodes\n\n**Process management (BCP_tm_msgproc.cpp):**\n- BCP_tm_start_processes(): Spawn LP/CG/VG processes\n- BCP_tm_stop_processes(): Shutdown workers\n- BCP_tm_remove_lp/cg/vg(): Handle worker failures\n\n**Node dispatch (BCP_tm_functions.cpp):**\n- BCP_tm_assign_processes(): Allocate workers to nodes\n- BCP_tm_start_new_nodes(): Dispatch ready nodes\n\n**Messaging (BCP_tm_msg_node_*.cpp):**\n- BCP_tm_send_node(): Pack and send node to LP\n- BCP_tm_unpack_node_*(): Receive processed nodes", "see": ["BCP_tm.hpp for Tree Manager process class"], "has_pass2": true}, "Bcp/src/include/BCP_warmstart_basis.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_warmstart_basis.hpp", "filename": "BCP_warmstart_basis.hpp", "file": "BCP_warmstart_basis.hpp", "brief": "BCP warm start basis implementation", "algorithm": "Simplex Basis Warm Start: 2-Bit Status Encoding\n\nBCP_warmstart_basis stores LP basis information (basic/on_upper/on_lower)\nusing 2 bits per variable/cut for compact storage.\n\n**Storage:**\n- _var_stat: BCP_vec_change<char> for variable statuses\n- _cut_stat: BCP_vec_change<char> for cut statuses\n\n**Features:**\n- convert_to_CoinWarmStart(): Creates CoinWarmStartBasis for OSI\n- as_change(): Compute delta from previous basis\n- Supports Explicit and WrtParent storage modes", "see": ["BCP_warmstart.hpp for base class", "CoinWarmStartBasis for OSI compatibility"], "has_pass2": true}, "Bcp/src/include/BCP_enum.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_enum.hpp", "filename": "BCP_enum.hpp", "file": "BCP_enum.hpp", "brief": "Core enumerations for BCP Branch-Cut-Price framework", "algorithm": "BCP Enumerations: Algorithm Control Types\n\nDefines fundamental types controlling BCP algorithm behavior:\n\n**Object types:**\n- BCP_object_t: Core vs. Algorithmic objects\n- BCP_obj_status: Object status flags (removable, inactive, etc.)\n- BCP_var_t: Variable integrality (Binary, Integer, Continuous)\n\n**Algorithm control:**\n- BCP_column_generation: Fathom, Send, or GenerateColumns\n- BCP_storage_t: NoData, Explicit, WrtParent, WrtCore\n- BCP_warmstart_info: None, Root, Parent inheritance\n\n**Feasibility:**\n- BCP_feasibility: NotFeasible, Feasible, HeuristicFeasible\n- BCP_feasibility_test: Binary, Integral, FullTest\n\n**Cut management:**\n- BCP_slack_cut_discarding: AtNewNode, AtNewIteration\n- BCP_CutViolationNorm: Plain, Distance, Directional\n- BCP_IneffectiveConstraints: None, NonzeroSlack, ZeroDualValue", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp.hpp for LP process using these enums"], "has_pass2": true}, "Bcp/src/include/BCP_process.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_process.hpp", "filename": "BCP_process.hpp", "file": "BCP_process.hpp", "brief": "BCP process base class and scheduler", "algorithm": "Process Management: Base Class and Load Scheduler\n\n**BCP_process:**\nAbstract base for all BCP processes (TM, LP, CG, VG, TS).\n- get_process_id(): Unique process identifier\n- get_parent(): Parent process that spawned this one\n- get_message_buffer(): Buffer for IPC\n- process_message(): Handle incoming messages\n\n**BCP_scheduler:**\nDynamic load balancer for allocating LP and strong-branching IDs.\nUses rate-based allocation when many processes are busy, static\noverestimation otherwise.\n\n**Scheduling strategies:**\n- Static: Allocate rho_static_ * requested IDs\n- Rate-based: Track request/release rates over time horizon\n- Switch when busy count exceeds switch_thresh_ * total\n\n**ID types:**\n- Node IDs: For processing search tree nodes\n- SB IDs: For strong branching evaluations", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp.hpp for LP process"], "has_pass2": true}, "Bcp/src/include/BCP_cg_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cg_param.hpp", "filename": "BCP_cg_param.hpp", "file": "BCP_cg_param.hpp", "brief": "Cut Generator parameters for BCP", "algorithm": "CG Parameters: Process Priority and Logging\n\nBCP_cg_par defines parameters for Cut Generator processes:\n\n**Character parameters:**\n- MessagePassingIsSerial: Single-process vs. distributed mode\n- ReportWhenDefaultIsExecuted: Debug overridable methods\n\n**Integer parameters:**\n- NiceLevel: OS scheduling priority (-20 to 20)\n\n**String parameters:**\n- LogFileName: Per-process logging with \"-cg-<id>\" suffix", "see": ["BCP_cg.hpp for Cut Generator process", "BCP_cg_user.hpp for user overridable methods", "the man page of the <code>setpriority</code> system function."], "has_pass2": true}, "Bcp/src/include/BCP_cg_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_cg_user.hpp", "filename": "BCP_cg_user.hpp", "file": "BCP_cg_user.hpp", "brief": "User customization interface for Cut Generator process", "algorithm": "Cut Generator User Interface for Custom Separation\n\nBCP_cg_user is the base class for user-defined cut generation in\na separate process. Users derive and override virtual methods.\n\n**Key methods to override:**\n- unpack_module_data(): Receive initialization from TM\n- unpack_primal_solution(): Receive LP solution\n- generate_cuts(): Main cut generation logic\n\n**Usage pattern:**\n1. LP sends primal solution to CG process\n2. CG unpacks solution via unpack_primal_solution()\n3. generate_cuts() finds violated inequalities\n4. For each cut found, call send_cut() to send to LP\n\n**Informational methods:**\n- upper_bound(): Current best solution value\n- current_phase/level/index/iteration(): Search position\n- get_param()/set_param(): BCP_cg_par parameters\n\n**When to use CG process:**\nUse when cut generation is computationally expensive and\nbenefits from separate process or distributed computation.\nFor cheap cuts, generate locally in BCP_lp_user instead.", "see": ["BCP_cg.hpp for Cut Generator process internals", "BCP_lp_user for LP-side cut generation", "BCP_cg_param.hpp for CG parameters"], "has_pass2": true}, "Bcp/src/include/BCP_obj_change.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_obj_change.hpp", "filename": "BCP_obj_change.hpp", "file": "BCP_obj_change.hpp", "brief": "Delta encoding for variable/cut bound changes", "algorithm": "Object Set Delta: Compact Var/Cut Change Encoding\n\nProvides compact representation of how objects (vars/cuts) change\nbetween parent and child nodes in the search tree.\n\n**BCP_obj_change:**\nSingle object's bound change:\n- lb, ub: New lower/upper bounds\n- stat: New BCP_obj_status\n\n**BCP_obj_set_change:**\nEncodes all changes to a set of objects (vars or cuts):\n- _storage: BCP_Storage_Explicit or BCP_Storage_WrtParent\n- _deleted_num: Count of deleted objects\n- _del_change_pos: Indices of deleted/changed objects\n- _change: New (lb, ub, status) for changed objects\n- _new_objs: bcpind values of added objects\n\n**Storage modes:**\n- Explicit: Full list of all objects (for root or checkpoints)\n- WrtParent: Delta from parent - deletions, changes, additions\n\n**Methods:**\n- update(): Apply delta to reconstruct full object set\n- pack()/unpack(): Serialization for inter-process transfer\n- pack_size(): Estimate serialized size\n\nSpace-efficient encoding crucial for large trees where\neach node may have thousands of variables/cuts.", "see": ["BCP_node_change.hpp for full node delta encoding", "BCP_var.hpp, BCP_cut.hpp for object types"], "has_pass2": true}, "Bcp/src/include/BCP_node_change.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_node_change.hpp", "filename": "BCP_node_change.hpp", "file": "BCP_node_change.hpp", "brief": "Complete node delta encoding for tree storage", "algorithm": "Node Delta Encoding: Compact Tree Storage via Parent Diffs\n\nBCP_node_change aggregates all changes between parent and child nodes\nfor compact tree storage.\n\n**Components:**\n- core_change: Changes to core var/cut bounds (BCP_problem_core_change)\n- var_change: Changes to algorithmic variables (BCP_obj_set_change)\n- cut_change: Changes to algorithmic cuts (BCP_obj_set_change)\n- warmstart: LP warm start information\n\n**Purpose:**\nEnables efficient tree storage by encoding only deltas between\nparent and child nodes instead of full formulations. Critical for\nscaling to large trees where each node may have thousands of vars/cuts.\n\n**Reconstruction:**\nTo reconstruct a node's formulation, start from root (or checkpoint)\nand apply node_change deltas along the path to the target node.\n\n**Serialization:**\npack()/unpack() methods handle inter-process transfer, using\nBCP_user_pack for user-defined warmstart serialization.", "see": ["BCP_problem_core.hpp for core formulation", "BCP_obj_change.hpp for object set deltas", "BCP_tm_node.hpp for TM node representation"], "has_pass2": true}, "Bcp/src/include/BCP_lp.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp.hpp", "filename": "BCP_lp.hpp", "file": "BCP_lp.hpp", "brief": "LP process for BCP Branch-Cut-Price framework", "algorithm": "LP Worker: Relaxation Solving with Cut/Column Generation\n\nThe LP process is the workhorse that actually solves subproblems:\n- Solves LP relaxations via OsiSolverInterface\n- Generates cuts locally or receives from Cut Generator\n- Generates variables locally or receives from Variable Generator\n- Tests feasibility and applies heuristics\n- Performs branching when stuck\n\n**Key classes:**\n- BCP_lp_prob: Central LP state (solver, pools, current node)\n- BCP_lp_statistics: Timing for feas/cut/var/heuristics/LP/branching\n\n**Problem structure:**\n- core: Variables/cuts that always stay in formulation\n- node/parent: Current search tree node and its parent\n- local_var_pool, local_cut_pool: Generated vars/cuts pending addition\n- slack_pool: Removed cuts kept for potential branching\n\n**LP solving:**\n- master_lp: Original LP solver (template)\n- lp_solver: Working LP solver for current node\n- lp_result: Most recent LP optimization result\n- warmstartRoot: Optional warm start from root for all nodes", "see": ["BCP_lp_user for user customization hooks", "BCP_tm.hpp for Tree Manager that assigns work"], "has_pass2": true}, "Bcp/src/include/BCP_message_single.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_message_single.hpp", "filename": "BCP_message_single.hpp", "file": "BCP_message_single.hpp", "brief": "Single-process message passing for BCP", "algorithm": "Single-Process Messaging: Sequential BCP Execution\n\nBCP_single_environment implements BCP_message_environment for\nsingle-process (non-parallel) execution. All processes run\nsequentially in one address space.\n\n**Features:**\n- Simulates message passing with direct function calls\n- Maintains process ID map for logical process identification\n- Useful for debugging and small problem instances\n\n**Process management:**\n- register_process(): Assign unique ID\n- start_process(): Fork logical process (no actual parallelism)\n- send()/receive(): Direct buffer passing", "see": ["BCP_message.hpp for abstract interface", "BCP_message_pvm.hpp for PVM implementation", "BCP_message_mpi.hpp for MPI implementation"], "has_pass2": true}, "Bcp/src/include/BCP_main_fun.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_main_fun.hpp", "filename": "BCP_main_fun.hpp", "file": "BCP_main_fun.hpp", "brief": "Process entry point functions for BCP", "algorithm": "Process Entry Points: Main Functions per Process Type\n\nEntry point functions for each BCP process type. Called by message\nenvironment when spawning workers:\n\n- BCP_tm_main(): Tree Manager (coordinator, command line parsing)\n- BCP_tmstorage_main(): Tree Storage (external node storage)\n- BCP_lp_main(): LP worker (relaxation solving)\n- BCP_cg_main(): Cut Generator (separation)\n- BCP_vg_main(): Variable Generator (pricing)\n\n**Parameters:**\n- msg_env: Message passing environment\n- user_init: User's initialization object\n- my_id: Process identifier\n- parent: Parent process ID\n- ub: Initial upper bound\n\n**Return:**\nBCP_process_t indicating process type or termination reason.", "see": ["BCP_USER.hpp for USER_initialize", "bcp_main() for top-level entry"], "has_pass2": true}, "Bcp/src/include/BCP_vg_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vg_user.hpp", "filename": "BCP_vg_user.hpp", "file": "BCP_vg_user.hpp", "brief": "User customization interface for Variable Generator process", "algorithm": "Variable Generator User Interface for Custom Pricing\n\nBCP_vg_user is the base class for user-defined column generation\n(pricing) in a separate process. Users derive and override virtual methods.\n\n**Key methods to override:**\n- unpack_module_data(): Receive initialization from TM\n- unpack_dual_solution(): Receive LP dual values\n- generate_vars(): Main pricing subproblem logic\n\n**Usage pattern:**\n1. LP sends dual solution (pi) to VG process\n2. VG unpacks duals via unpack_dual_solution()\n3. generate_vars() solves pricing problem for negative reduced cost columns\n4. For each variable found, call send_var() to send to LP\n\n**Informational methods:**\n- upper_bound(): Current best solution value\n- current_phase/level/index/iteration(): Search position\n- get_param()/set_param(): BCP_vg_par parameters\n\n**When to use VG process:**\nUse when pricing subproblem is computationally expensive (e.g.,\nshortest path, knapsack) and benefits from separate process.\nFor simple pricing, generate locally in BCP_lp_user instead.", "see": ["BCP_vg.hpp for Variable Generator process internals", "BCP_lp_user for LP-side column generation", "BCP_vg_param.hpp for VG parameters"], "has_pass2": true}, "Bcp/src/include/BCP_solution.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_solution.hpp", "filename": "BCP_solution.hpp", "file": "BCP_solution.hpp", "brief": "Solution representation for BCP Branch-Cut-Price", "algorithm": "MIP Solution Storage and Upper Bound Tracking\n\nDefines how MIP feasible solutions are stored and transmitted:\n\n**Class hierarchy:**\n- BCP_solution: Abstract base with objective_value() pure virtual\n- BCP_solution_generic: Default implementation storing (var, value) pairs\n\n**BCP_solution_generic:**\n- _objective: Objective function value\n- _vars: Vector of variables at nonzero level\n- _values: Corresponding values\n- _delete_vars: Whether to delete vars on destruction\n\n**Usage flow:**\n1. LP process finds integer feasible solution\n2. BCP_lp_user::pack_feasible_solution() serializes it\n3. Sent to Tree Manager\n4. BCP_tm_user::unpack_feasible_solution() deserializes\n5. Best solution tracked for upper bound\n\nUsers can derive from BCP_solution for custom solution\nstorage (e.g., to include dual values).", "see": ["BCP_lp_user for packing interface", "BCP_tm_user for unpacking interface"], "has_pass2": true}, "Bcp/src/include/BCP_error.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_error.hpp", "filename": "BCP_error.hpp", "file": "BCP_error.hpp", "brief": "Fatal error handling for BCP", "algorithm": "Error Handling: BCP_fatal_error Class\n\nSimple error handling via BCP_fatal_error exception-like class.\nConstruction triggers error message output and optional abort().\n\n**Behavior:**\n- Prints formatted error message to stdout\n- Flushes all buffers\n- Calls abort() if abort_on_error is true (creates core dump)\n\n**Control:**\n- BCP_fatal_error::abort_on_error: Static flag to enable/disable abort\n\n@note Not a proper C++ exception - uses abort() for distributed debugging.", "has_pass2": true}, "Bcp/src/include/BCP_functions.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_functions.hpp", "filename": "BCP_functions.hpp", "file": "BCP_functions.hpp", "brief": "Warmstart serialization utilities", "algorithm": "Warmstart Pack/Unpack: Polymorphic Serialization\n\nFunctions for serializing BCP_warmstart objects:\n\n- BCP_pack_warmstart(): Writes warmstart type tag + data to buffer\n- BCP_unpack_warmstart(): Reads type tag, creates correct subclass\n\nHandles polymorphism: packs type indicator so unpack creates\nthe correct BCP_warmstart subclass (basis, dual, primaldual).", "see": ["BCP_warmstart.hpp for base class", "BCP_buffer.hpp for serialization buffer"], "has_pass2": true}, "Bcp/src/include/BCP_vector_double.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_double.hpp", "filename": "BCP_vector_double.hpp", "file": "BCP_vector_double.hpp", "brief": "BCP_vec<double> specialization", "algorithm": "POD Vector Specialization: double\n\nTemplate specializations for BCP_vec<double> that skip\nconstruct/destroy overhead since double is a POD type.\n\n**Optimizations:**\n- destroy(): No-op (no destructor needed)\n- construct(): Direct assignment instead of placement new\n- Uses memcpy for bulk operations where applicable\n\nSame pattern applies to BCP_vector_int.hpp, BCP_vector_short.hpp, etc.\nfor all primitive types.", "has_pass2": true}, "Bcp/src/include/BCP_warmstart.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_warmstart.hpp", "filename": "BCP_warmstart.hpp", "file": "BCP_warmstart.hpp", "brief": "LP warm start information for BCP Branch-Cut-Price", "algorithm": "Warm Start: Delta-Encoded LP Basis for Fast Resolves\n\nBCP_warmstart is the abstract base for storing LP warm start data.\nEnables efficient LP resolves when moving between tree nodes.\n\n**Storage modes (BCP_storage_t):**\n- Explicit: Full warm start data\n- WrtParent: Changes relative to parent node\n- WrtCore: Changes relative to core formulation\n\n**Key methods:**\n- convert_to_CoinWarmStart(): Get Osi-compatible warm start\n- storage(): Query how data is stored\n- update(): Apply incremental changes\n- as_change(): Compute delta from previous warm start\n- clone(): Deep copy\n- storage_size(): Memory footprint for storage decisions\n\n**Node transitions:**\nWhen LP dives from parent to child, warm start may be stored\nas delta to save space. When backtracking, explicit form needed.\n\nImplementations: BCP_warmstart_basis (simplex basis),\nBCP_warmstart_primaldual (interior point)", "see": ["BCP_lp.hpp for LP process using warm starts", "CoinWarmStart for underlying OSI interface"], "param": ["old_ws the old warmstart info", "del_vars the indices of the variables that are deleted from the\n              formulation <code>old_ws</code> was created for", "del_cuts same for the cuts", "petol primal zero tolerance", "detol dual zero tolerance"], "has_pass2": true}, "Bcp/src/include/BCP_tm_user.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_user.hpp", "filename": "BCP_tm_user.hpp", "file": "BCP_tm_user.hpp", "brief": "User customization interface for Tree Manager process", "algorithm": "Tree Manager User Interface for Problem-Specific B&B\n\nBCP_tm_user is the base class for user-defined Tree Manager behavior.\nUsers derive their own class and override virtual methods.\n\n**Setup methods:**\n- initialize_core(): Define core variables, cuts, and matrix\n- create_root(): Set up the root node (extra vars/cuts, user data)\n- init_new_phase(): Initialize each algorithm phase\n\n**Solution handling:**\n- unpack_feasible_solution(): Deserialize solutions from LP\n- display_feasible_solution(): Output best solution\n- replace_solution(): Secondary objective tie-breaking\n\n**Tree management:**\n- change_candidate_heap(): Modify node selection strategy\n- display_node_information(): Status output before/after processing\n- display_final_information(): Summary after search completes\n\n**Communication:**\n- pack_module_data(): Send initialization to LP/CG/VG processes\n- process_message(): Handle user-defined messages\n\n**Parameter access:**\n- get_param()/set_param(): Query/modify BCP_tm_par parameters\n- upper_bound()/lower_bound(): Access global bounds", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp_user for LP process user interface"], "has_pass2": true}, "Bcp/src/include/BCP_math.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_math.hpp", "filename": "BCP_math.hpp", "file": "BCP_math.hpp", "brief": "Mathematical constants for BCP", "algorithm": "Math Constants: BCP_DBL_MAX\n\nDefines BCP_DBL_MAX = 1e100 as \"infinity\" for optimization bounds.\nUsed instead of DBL_MAX to avoid numerical issues with infinity\narithmetic in LP solvers.", "has_pass2": true}, "Bcp/src/include/BCP_var.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_var.hpp", "filename": "BCP_var.hpp", "file": "BCP_var.hpp", "brief": "Variable representation for BCP Branch-Cut-Price", "algorithm": "Variable Classes for Column Generation and B&B\n\nDefines variable classes used throughout BCP:\n\n**Class hierarchy:**\n- BCP_var: Abstract base with obj, lb, ub, type, status, bcpind\n- BCP_var_core: Core variables that always stay in LP\n- BCP_var_algo: Algorithmic variables from column generation\n- BCP_var_set: Collection with bulk operations\n\n**Variable types (BCP_var_t):**\n- BCP_BinaryVar: 0-1 variable\n- BCP_IntegerVar: General integer\n- BCP_ContinuousVar: Continuous\n\n**Status flags (BCP_obj_status):**\n- BCP_ObjInactive: Fixed to a bound\n- BCP_ObjNotRemovable: Cannot be removed (e.g., branched on)\n- BCP_ObjToBeRemoved: Marked for deletion\n- BCP_ObjDoNotSendToPool: Skip variable pool\n\n**Internal index (bcpind):**\nUnique global identifier assigned by TM. Negative bcpind indicates\nthe variable was deleted and should not be referenced.", "see": ["BCP_cut.hpp for symmetric cut representation", "BCP_vg.hpp for Variable Generator process", "BCP_lp.hpp for LP process using variables"], "has_pass2": true}, "Bcp/src/include/BCP_mempool.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_mempool.hpp", "filename": "BCP_mempool.hpp", "file": "BCP_mempool.hpp", "brief": "Memory pool allocator for BCP", "algorithm": "Free-List Allocator: Pooled Object Allocation\n\nBCP_MemPool implements a free-list memory pool for fixed-size objects.\nReduces malloc/free overhead for frequently created/destroyed objects.\n\n**Design:**\n- Pre-allocates blocks of BLOCK_SIZE entries\n- Maintains linked list of free entries (pointer embedded in unused space)\n- Falls back to ::operator new for wrong-sized allocations\n\n**Usage:**\n- alloc(n): Get memory (from pool if size matches, else new)\n- free(p, n): Return memory (to pool if size matches, else delete)", "complexity": "O(1) allocation when pool has free entries", "see": ["BCP_var.hpp, BCP_cut.hpp for pooled object classes"], "has_pass2": true}, "Bcp/src/include/BCP_enum_branch.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_enum_branch.hpp", "filename": "BCP_enum_branch.hpp", "file": "BCP_enum_branch.hpp", "brief": "Branching-related enumerations for BCP", "algorithm": "Branching Enums: Child Selection and Diving Control\n\nEnumerations controlling branching decisions and child management:\n\n**BCP_child_preference:**\nChild selection criteria for diving: LowBound, HighBound,\nMoreFractional, LessFractional, PreferDiveDown, PreferDiveUp.\n\n**BCP_branching_result:**\nOutcomes: FathomedThisNode, DivedIntoNewNode, ContinueThisNode.\n\n**BCP_branching_decision:**\nUser decisions: DoNotBranch_Fathomed, DoNotBranch, DoBranch.\n\n**BCP_branching_object_relation:**\nStrong branching comparison: OldPresolvedIsBetter,\nNewPresolvedIsBetter, NewPresolvedIsBetter_BranchOnIt.\n\n**BCP_branching_object_comparison:**\nComparison criteria using objective bounds and averages.\n\n**BCP_child_action:**\nPer-child: FathomChild, ReturnChild, KeepChild.\n\n**BCP_diving_status:**\nDiving control: DoNotDive, DoDive, TestBeforeDive.", "see": ["BCP_lp_user.hpp for branching hooks", "BCP_lp_branch.hpp for branching objects"], "has_pass2": true}, "Bcp/src/include/BCP_vector_general.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_general.hpp", "filename": "BCP_vector_general.hpp", "file": "BCP_vector_general.hpp", "brief": "Generic BCP_vec<T> method implementations", "algorithm": "Generic Vector: Template Methods for Non-POD Types\n\nDefault implementations of BCP_vec<T> methods using explicit\nconstruct/destroy via placement new and destructor calls.\n\n**Core operations:**\n- construct(): Placement new for default or copy construction\n- destroy(): Explicit destructor call\n- destroy_range(): Destructor calls in reverse order\n- allocate/deallocate(): Raw memory management via operator new/delete\n- insert_aux(): Growth with 2x+256 expansion strategy\n\nSpecialized versions exist for POD types (int, double, etc.) that\nskip construct/destroy overhead and use memcpy/memset.", "see": ["BCP_vector.hpp for class declaration", "BCP_vector_double.hpp for double specialization"], "has_pass2": true}, "Bcp/src/include/BCP_lp_branch.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_branch.hpp", "filename": "BCP_lp_branch.hpp", "file": "BCP_lp_branch.hpp", "brief": "LP-side branching objects for BCP Branch-Cut-Price", "algorithm": "Strong Branching with Presolved Candidates\n\nUser-facing branching object classes used during strong branching\nand branch selection in the LP process.\n\n**BCP_lp_branching_object:**\n- child_num: Number of children (usually 2)\n- vars_to_add, cuts_to_add: New objects for children\n- forced_var_pos/bd: Variable bounds changed by branching\n- forced_cut_pos/bd: Cut bounds changed by branching\n- implied_var_pos/bd: Additional fixings from logical implications\n- implied_cut_pos/bd: Additional cut bound changes\n\n**Position encoding:**\n- Non-negative: Index in current LP formulation\n- Negative (-i): Index (i-1) in added vars/cuts\n\n**BCP_presolved_lp_brobj:**\nWraps a branching object with strong branching results:\n- _lpres: LP results for each child after presolving\n- _child_action: What to do with each child\n  (BCP_ReturnChild, BCP_KeepChild, BCP_FathomChild)\n- _user_data: User data for child nodes\n\n**Strong branching flow:**\n1. User creates BCP_lp_branching_object candidates\n2. BCP wraps in BCP_presolved_lp_brobj\n3. Each child presolved, results stored in _lpres\n4. Best candidate selected based on results", "see": ["BCP_branch.hpp for internal branching representation", "BCP_lp_user::select_branching_candidates()", "OsiBranchingObject for OSI branching interface"], "has_pass2": true}, "Bcp/src/include/BCP_parameters.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_parameters.hpp", "filename": "BCP_parameters.hpp", "file": "BCP_parameters.hpp", "brief": "BCP parameters", "algorithm": "Parameter Management: Type-Safe Configuration System\n\nParameter handling for BCP framework configuration.\n\n**BCP_parameter_set<Par>:**\nTemplate class holding char, int, double, string, and string-array parameters.\nEach parameter type has its own typed array for efficient access.\n\n**Key features:**\n- Keyword-value file parsing with environment variable expansion\n- Type-safe entry() and set_entry() accessors\n- pack()/unpack() for inter-process parameter transfer\n- Support for ParamFile directive to include nested files\n\n**Usage:**\nDefine a struct with parameter enums, then instantiate BCP_parameter_set<>.\nImplement create_keyword_list() and set_default_entries() for the struct.", "see": ["BCP_tm_param.hpp, BCP_lp_param.hpp for TM/LP parameters"], "has_pass2": true}, "Bcp/src/include/BCP_set_intersects.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_set_intersects.hpp", "filename": "BCP_set_intersects.hpp", "file": "BCP_set_intersects.hpp", "brief": "Set intersection test utility", "algorithm": "Sorted Set Intersection: Linear Scan Test\n\nTemplate function `intersects()` tests whether two sorted ranges\nhave any common elements without computing the full intersection.\n\n**Algorithm:**\nDual-pointer linear scan - advance smaller element's pointer.\nReturns true immediately when match found.\n\n**Variants:**\n- operator< version: Uses default comparison\n- Comparator version: Custom ordering via function object", "complexity": "O(n + m) where n, m are range sizes\n@note Input ranges must be sorted in same order as comparator", "has_pass2": true}, "Bcp/src/include/BCP_warmstart_dual.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_warmstart_dual.hpp", "filename": "BCP_warmstart_dual.hpp", "file": "BCP_warmstart_dual.hpp", "brief": "Dual-only warm start for BCP", "algorithm": "Dual Warmstart: LP Restart from Dual Vector\n\nBCP_warmstart_dual stores just dual (row) values for LP warmstart.\nUses BCP_vec_change<double> for delta encoding (WrtParent/Explicit).\n\n**Operations:**\n- convert_to_CoinWarmStart(): Creates CoinWarmStartDual for LP solver\n- as_change(): Computes delta vs. parent (for tree storage)\n- update(): Applies delta to reconstruct full warmstart\n\n**Use case:** Problems where primal solution changes significantly\nbut dual solution is stable (constraint-focused problems).", "see": ["BCP_warmstart_basis.hpp for full basis warmstart", "BCP_warmstart_primaldual.hpp for primal+dual warmstart"], "has_pass2": true}, "Bcp/src/include/BCP_tmstorage.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tmstorage.hpp", "filename": "BCP_tmstorage.hpp", "file": "BCP_tmstorage.hpp", "brief": "BCP tree storage process", "algorithm": "Tree Storage: External Memory for Large Search Trees\n\nTree Storage (TS) process for storing search tree nodes externally.\n\n**BCP_ts_prob:**\nThe TS process manages external storage of tree nodes, variables,\nand cuts to allow the TM to handle trees larger than fit in memory.\n\n**Data structures:**\n- nodes: Map from node ID to BCP_ts_node_data (desc + user_data)\n- vars: Map from variable ID to BCP_var_algo*\n- cuts: Map from cut ID to BCP_cut_algo*\n\n**Usage:**\nTM sends storage/retrieval requests via messages. TS maintains\nthe mappings and serializes data to/from the message buffer.\n\n**Memory management:**\nMaxHeapSize parameter controls TS memory usage for cache decisions.", "see": ["BCP_tm.hpp for Tree Manager", "BCP_node_change.hpp for node delta encoding"], "has_pass2": true}, "Bcp/src/include/BCP_vector_int.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_int.hpp", "filename": "BCP_vector_int.hpp", "file": "BCP_vector_int.hpp", "brief": "BCP_vec<int> specialization", "algorithm": "POD Vector Specialization: int\n\nTemplate specializations for BCP_vec<int> that skip\nconstruct/destroy overhead since int is a POD type.\nSame optimization pattern as BCP_vector_double.hpp.", "has_pass2": true}, "Bcp/src/include/BCP_message_tag.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_message_tag.hpp", "filename": "BCP_message_tag.hpp", "file": "BCP_message_tag.hpp", "brief": "Message tag enumeration for BCP inter-process communication", "algorithm": "Message Tags: Protocol Types for Process Coordination\n\nBCP_message_tag defines all message types in BCP's protocol:\n\n**Control messages:**\n- CONFIG_*: Configurator ↔ TM for runtime changes\n- FinishedBCP, ProcessType, ProcessParameters, CoreDescription\n\n**TM ↔ Storage:**\n- NodeList*, VarList*, CutList*: Tree storage operations\n\n**TM ↔ LP:**\n- ActiveNodeData, WarmstartRoot, DivingInfo: Node dispatch\n- NodeDescription*: Node results (fathomed, branched, etc.)\n- FeasibleSolution, UpperBound: Incumbent updates\n\n**LP ↔ CG/VG:**\n- ForCG_*, ForVG_*: Primal/dual solutions for separation/pricing\n- CutDescription, VarDescription: Generated cuts/columns\n- NoMoreCuts, NoMoreVars: Generator completion", "see": ["BCP_message.hpp for message passing interface", "BCP_buffer.hpp for message serialization"], "has_pass2": true}, "Bcp/src/include/BCP_vector_bool.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_bool.hpp", "filename": "BCP_vector_bool.hpp", "file": "BCP_vector_bool.hpp", "brief": "BCP_vec<bool> specialization", "algorithm": "POD Vector Specialization: bool\n\nTemplate specializations for BCP_vec<bool> that skip\nconstruct/destroy overhead since bool is a POD type.\nSame optimization pattern as BCP_vector_double.hpp.", "has_pass2": true}, "Bcp/src/include/BCP_problem_core.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_problem_core.hpp", "filename": "BCP_problem_core.hpp", "file": "BCP_problem_core.hpp", "brief": "Core problem formulation for BCP Branch-Cut-Price", "algorithm": "Core Formulation: Immutable Base with Delta Tracking\n\nDefines the permanent part of the MIP formulation that never changes.\n\n**BCP_problem_core:**\nThe immutable core problem:\n- vars: Vector of BCP_var_core* (always in formulation)\n- cuts: Vector of BCP_cut_core* (always in formulation)\n- matrix: Constraint matrix for core vars × core cuts\n\nCreated once via BCP_tm_user::initialize_core() and shared\nacross all tree nodes.\n\n**BCP_problem_core_change:**\nTracks how core object bounds/stati differ from original:\n- _storage: Explicit, WrtCore, or WrtParent\n- var_pos, var_ch: Changed variable indices and new (lb,ub,status)\n- cut_pos, cut_ch: Changed cut indices and new (lb,ub,status)\n\n**Storage modes:**\n- Explicit: Full bounds for all core objects (root node)\n- WrtCore: Changes relative to original core (compact)\n- WrtParent: Incremental changes from parent (for tree traversal)\n\n**Key methods:**\n- ensure_explicit(): Convert delta to full representation\n- make_wrtcore_if_shorter(): Optimize storage size\n- update(): Apply incremental changes", "see": ["BCP_tm_user::initialize_core() for core setup", "BCP_node_change.hpp for full node deltas"], "has_pass2": true}, "Bcp/src/include/BCP_tm_node.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_node.hpp", "filename": "BCP_tm_node.hpp", "file": "BCP_tm_node.hpp", "brief": "Search tree node representation in Tree Manager", "algorithm": "Search Tree Management: Delta Encoding & Distributed Storage\n\nDefines how the TM represents and manages the branch-and-bound tree.\n\n**BCP_tm_node_status:**\n- BCP_DefaultNode: Initial state\n- BCP_ProcessedNode: Fully processed\n- BCP_ActiveNode: Currently being processed by LP\n- BCP_PrunedNode_*: Pruned (by bound, infeasibility, or discarded)\n- BCP_CandidateNode: In candidate queue\n- BCP_NextPhaseNode_*: Deferred to next phase\n\n**BCP_tm_node:**\nExtends CoinTreeNode with BCP-specific data:\n- status: Current node status\n- _index: Unique node identifier\n- _parent: Parent node pointer\n- _children: Child node pointers\n- lp, cg, cp, vg, vp: Process assignments\n- _*_storage: How vars/cuts/warmstart stored (explicit vs. delta)\n- _data: Node description and user data\n\n**BCP_tree:**\nContainer for all nodes with:\n- root(): Get root node\n- size(): Total nodes created\n- maxdepth(): Deepest level reached\n- true_lower_bound(): Global lower bound\n\n**BCP_tm_node_to_send:**\nHandles asynchronous node transmission to LP, collecting\nancestor data as needed for explicit storage reconstruction.", "see": ["BCP_tm.hpp for Tree Manager", "BCP_node_change.hpp for node delta encoding", "CoinSearchTree for node selection strategies"], "has_pass2": true}, "Bcp/src/include/BCP_enum_tm.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_enum_tm.hpp", "filename": "BCP_enum_tm.hpp", "file": "BCP_enum_tm.hpp", "brief": "Tree Manager enumerations for BCP", "algorithm": "TM Enums: Search Strategy and Node Dispatch\n\n**BCP_tree_search_method:**\nSearch order for tree exploration:\n- BestFirstSearch: Best bound first (minimize gap)\n- BreadthFirstSearch: Level-by-level\n- DepthFirstSearch: Dive to leaves (finds solutions fast)\n- PreferredFirstSearch: User-defined comparison\n\n**BCP_node_start_result:**\nOutcome of attempting to dispatch a node:\n- NoNode: No nodes available\n- Error: Dispatch failed\n- OK: Node successfully sent to LP", "see": ["BCP_tm.hpp for Tree Manager"], "has_pass2": true}, "Bcp/src/include/BCP_buffer.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_buffer.hpp", "filename": "BCP_buffer.hpp", "file": "BCP_buffer.hpp", "brief": "Message buffer for BCP inter-process communication", "algorithm": "Message Serialization: Pack/Unpack Buffer for IPC\n\nBCP_buffer is the serialization mechanism for all BCP messages.\nEach process uses a single buffer for both incoming and outgoing messages.\n\n**Packable types:**\n- POD types copyable via memcpy (int, double, etc.)\n- Structs of POD types\n- BCP_vec<T> of the above\n- BCP_string\n- Objects with pack(BCP_buffer&) method\n\n**Buffer structure:**\n- _data: Raw character array\n- _size: Current message size\n- _pos: Read position (incoming) or write position (outgoing)\n- _max_size: Allocated capacity (grows as needed, never shrinks)\n- _msgtag: Message type (for received messages)\n- _sender: Sender process ID (for received messages)\n\n**Usage pattern:**\n- pack(): Append data to buffer, advances _size\n- unpack(): Read data from buffer, advances _pos\n- clear(): Reset for new message", "see": ["BCP_message.hpp for message passing interface", "BCP_message_tag.hpp for message type tags"], "has_pass2": true}, "Bcp/src/include/BCP_vector_sanity.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector_sanity.hpp", "filename": "BCP_vector_sanity.hpp", "file": "BCP_vector_sanity.hpp", "brief": "Position vector validation utility", "algorithm": "Index Validation: BCP_vec_sanity_check\n\nValidates position vectors used in delta operations:\n- Positions must be strictly increasing (sorted, no duplicates)\n- No negative indices\n- All indices < maxsize\n\nThrows BCP_fatal_error on invalid input.\nUsed to catch programming errors in delta encoding early.", "see": ["BCP_vector_change.hpp for delta vector operations"], "has_pass2": true}, "Bcp/src/include/BCP_USER.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_USER.hpp", "filename": "BCP_USER.hpp", "file": "BCP_USER.hpp", "brief": "User initialization and packing interface for BCP", "algorithm": "User Entry Point: Initialization and Serialization\n\nDefines the entry points and serialization interfaces users must implement.\n\n**BCP_user_data:**\nBase class for user-defined data attached to tree nodes.\nDerive to store problem-specific information per node.\n\n**BCP_user_class:**\nEmpty base for all *_user classes (BCP_tm_user, BCP_lp_user, etc.).\n\n**BCP_user_pack:**\nSerialization interface for user-defined objects:\n- pack/unpack_warmstart(): Warm start information\n- pack/unpack_var_algo(): Algorithmic variables\n- pack/unpack_cut_algo(): Algorithmic cuts\n- pack/unpack_user_data(): Node-attached user data\n\n**USER_initialize:**\nMain entry point users must implement. Called by bcp_main():\n- msgenv_init(): Create message passing environment\n- tm_init(): Create Tree Manager user object\n- lp_init(): Create LP user object\n- cg_init(), vg_init(): Create CG/VG user objects\n- packer_init(): Create serialization object\n\n**bcp_main(argc, argv, user_init):**\nCall this from main() to start BCP with your USER_initialize.", "see": ["BCP_tm_user.hpp, BCP_lp_user.hpp for user hook classes"], "has_pass2": true}, "Bcp/src/include/BCP_lp_result.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_result.hpp", "filename": "BCP_lp_result.hpp", "file": "BCP_lp_result.hpp", "brief": "LP solve results for BCP Branch-Cut-Price", "algorithm": "LP Result Capture: Primal/Dual Solutions and Termination\n\nBCP_lp_result stores the output from solving an LP relaxation.\nUsed throughout BCP for accessing LP solution data.\n\n**BCP_termcode (LP termination codes):**\n- BCP_Abandoned: Solver gave up\n- BCP_ProvenOptimal: Optimal solution found\n- BCP_ProvenPrimalInf: Primal infeasible\n- BCP_ProvenDualInf: Dual infeasible (unbounded)\n- BCP_PrimalObjLimReached: Cutoff triggered\n- BCP_DualObjLimReached: Lower bound exceeded cutoff\n- BCP_IterationLimit: Iteration limit hit\n- BCP_TimeLimit: Time limit hit\n\n**BCP_lp_result data:**\n- _termcode: How LP terminated\n- _iternum: Iterations used\n- _objval: Objective value (or bound)\n- _x: Primal solution vector\n- _pi: Dual solution (row prices)\n- _dj: Reduced costs\n- _lhs: Left-hand side values (Ax)\n- _primal_tolerance, _dual_tolerance: Solver tolerances\n\n**Usage:**\nCall get_results(OsiSolverInterface&) after LP solve to\npopulate all fields from the solver.", "see": ["BCP_lp.hpp for LP process", "BCP_lp_branch.hpp for strong branching results", "OsiSolverInterface for LP solver interface"], "has_pass2": true}, "Bcp/src/include/BCP_message_mpi.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_message_mpi.hpp", "filename": "BCP_message_mpi.hpp", "file": "BCP_message_mpi.hpp", "brief": "MPI message passing for BCP", "algorithm": "MPI Backend: BCP_mpi_environment Implementation\n\nMPI implementation of BCP_message_environment interface.\nRequires COIN_HAS_MPI to be defined.\n\n**Key methods:**\n- is_mpi(): Detects MPI environment and process count\n- send/receive(): Point-to-point messaging with tags\n- multicast(): Broadcast to multiple targets\n- start_processes(): Spawn workers on machines\n- probe(): Non-blocking message check\n\n**Process management:**\nTracks process IDs, handles initialization/finalization,\nchecks liveness of remote processes.", "see": ["BCP_message.hpp for abstract interface", "BCP_message_pvm.hpp for PVM backend"], "has_pass2": true}, "Bcp/src/include/BCP_lp_node.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_lp_node.hpp", "filename": "BCP_lp_node.hpp", "file": "BCP_lp_node.hpp", "brief": "LP process node representation", "algorithm": "LP Node: Parent Info and Storage Strategy Tracking\n\nTracks node information within LP process including parent\ndescription and storage strategies for tree reconstruction.\n\n**BCP_node_storage_in_tm:** How each piece is stored (explicit vs delta).\n**BCP_lp_parent:** Parent node info (bounds, user data, diving info).\n**BCP_lp_node:** Current node being processed.", "see": ["BCP_tm_node.hpp for Tree Manager side node representation", "BCP_node_change.hpp for delta encoding"], "has_pass2": true}, "Bcp/src/include/BCP_tm_param.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_tm_param.hpp", "filename": "BCP_tm_param.hpp", "file": "BCP_tm_param.hpp", "brief": "Tree Manager parameters for BCP Branch-Cut-Price", "algorithm": "TM Parameters: Search Strategy and Resource Limits\n\nBCP_tm_par defines all configurable parameters for the Tree Manager.\nSet via parameter file: BCP_{param_name} {value}\n\n**Character (boolean) parameters:**\n- Debug*Processes: Enable debugging for LP/CG/VG/CP/VP\n- GenerateVars: Enable variable generation\n- MessagePassingIsSerial: Single-process mode\n- TrimTreeBeforeNewPhase: Clean tree between phases\n- RemoveExploredBranches: Free completed subtrees\n- TmVerb_*: Verbosity flags for various events\n\n**Integer parameters:**\n- WarmstartInfo: How to store warm start (None/Root/Parent)\n- MaxHeapSize: Memory limit in MB\n- TreeSearchStrategy: Best-first(0), Breadth-first(1), Depth-first(2)\n- LpProcessNum, CgProcessNum, etc.: Process counts\n- LPscheduler_*: Node scheduling parameters\n\n**Double parameters:**\n- UnconditionalDiveProbability: Dive vs. backtrack randomization\n- QualityRatioToAllowDiving_*: When diving is permitted\n- Granularity: Objective function discretization\n- MaxRunTime: Time limit\n- TerminationGap_*: Optimality gap tolerances\n- UpperBound: Initial upper bound\n\n**String parameters:**\n- SaveRootCutsTo/ReadRootCutsFrom: Root cut persistence\n- ExecutableName: Spawned process executable\n- *Machines: Machine lists for each process type", "see": ["BCP_tm.hpp for Tree Manager process", "BCP_lp_param.hpp for LP parameters"], "has_pass2": true}, "Bcp/src/include/BCP_string.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_string.hpp", "filename": "BCP_string.hpp", "file": "BCP_string.hpp", "brief": "Simple string class for BCP", "algorithm": "BCP_string: Safe Fixed-Length String\n\nMinimal string class avoiding std::string ABI issues across\ncompilers and avoiding C-string buffer overflow risks.\n\n**Features:**\n- Owns memory (RAII cleanup)\n- Copy construction/assignment\n- C-string interoperability via c_str()\n- Equality comparison operators\n\nUsed for executable names, machine names, and log file paths\nin message passing configuration.", "has_pass2": true}, "Bcp/src/include/BCP_vector.hpp": {"path": "layer-3/Bcp/Bcp/src/include/BCP_vector.hpp", "filename": "BCP_vector.hpp", "file": "BCP_vector.hpp", "brief": "Custom vector class for BCP with performance optimizations", "algorithm": "BCP Vector: Contiguous Array with Bulk Operations\n\nBCP_vec<T> is BCP's custom vector, similar to std::vector but\nguaranteed to be a contiguous memory array with extra methods.\n\n**Key advantages:**\n- Guaranteed contiguous memory (std::vector only \"likely\")\n- unchecked_* methods skip bounds checking for speed\n- Bulk operations: keep_by_index, erase_by_index, update\n- Direct memory access for serialization\n\n**Core interface:**\n- begin()/end(): Iterators (raw T* pointers)\n- size()/capacity()/empty(): Standard queries\n- operator[]: Element access\n- push_back()/pop_back(): End modification\n- insert()/erase(): Position modification\n- reserve()/swap(): Memory management\n\n**BCP-specific methods:**\n- entry(i): Get iterator to i-th element\n- unchecked_push_back(): Skip capacity check\n- keep_by_index(): Keep only specified indices\n- erase_by_index(): Erase specified indices\n- update(): Bulk update at positions\n\n**Helper functions:**\n- purge_ptr_vector(): Delete pointers and clear vector\n- keep_ptr_vector_by_index(): Keep selected, delete rest\n\nSpecializations included for bool, char, short, int, double.", "see": ["std::vector for standard interface reference"], "has_pass2": true}}}, "Bonmin": {"name": "Bonmin", "file_count": 89, "pass2_count": 41, "files": {"doc/BONMIN_ReferenceManual.hpp": {"path": "layer-3/Bonmin/doc/BONMIN_ReferenceManual.hpp", "filename": "BONMIN_ReferenceManual.hpp", "file": "BONMIN_ReferenceManual.hpp", "brief": "Bonmin Doxygen documentation main page\n\nMain page for Bonmin reference manual. Provides overview of Mixed\nInteger Nonlinear Programming solver combining B&B with Ipopt NLP.", "has_pass2": false}, "src/Interfaces/BonBranchingTQP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonBranchingTQP.hpp", "filename": "BonBranchingTQP.hpp", "file": "BonBranchingTQP.hpp", "brief": "QP adapter for strong branching in MINLP\nCopyright (C) International Business Machines Corporation and\nCarnegie Mellon University 2006, 2008. All Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "QP-Based Strong Branching for MINLP:\n  Accelerates strong branching by solving QP approximations:\n  1. **Reference point:** Store NLP solution x* with gradient g, Hessian H\n  2. **QP formulation:** For displacement d = x - x*:\n     min  g'd + ½d'Hd  (quadratic Taylor approximation)\n     s.t. J·d ≤ b - g(x*)  (linearized constraints at x*)\n  3. **Strong branching:** For each candidate variable x_i:\n     - Fix x_i at floor/ceil bounds\n     - Solve QP (much faster than full NLP)\n     - Estimate objective degradation\n  4. **Selection:** Branch on variable with best degradation estimate", "math": "Taylor approximation:\n  f(x) ≈ f(x*) + ∇f(x*)'(x-x*) + ½(x-x*)'∇²f(x*)(x-x*)\n  Constraints: g(x) ≈ g(x*) + J(x*)(x-x*) ≤ 0\n  Variables d represent displacement: d = x - x*", "complexity": "O(n³) for QP solve vs O(NLP iterations × n³) for full NLP.\n  Typically 10-100× faster per strong branching evaluation.\n  Accuracy degrades far from x*.\n\nAuthors: Andreas Waechter, IBM (derived from BonTMINLP2TNLP.hpp)\nDate: December 22, 2006", "see": ["TMINLP2TNLP for the wrapped MINLP interface", "BqpdSolver for QP solver used with this adapter"], "has_pass2": true}, "src/Interfaces/BonMsgUtils.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonMsgUtils.hpp", "filename": "BonMsgUtils.hpp", "file": "BonMsgUtils.hpp", "brief": "Message utilities and macros for Bonmin\nThis code is published under the Eclipse Public License.\n\nProvides ADD_MSG macro and message ID functions for standard,\nwarning, and error messages in the COIN message system.", "see": ["CoinMessages for the COIN messaging framework"], "has_pass2": false}, "src/Interfaces/BonTypes.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTypes.hpp", "filename": "BonTypes.hpp", "file": "BonTypes.hpp", "brief": "Type utilities for Bonmin (vector wrapper, reference counting)\nThis code is published under the Eclipse Public License.\n\nProvides:\n- Bonmin::vector: std::vector wrapper with pointer access for Fortran\n- SimpleReferenced: Wrapper to store objects in Coin::ReferencedObject\n- SimpleReferencedPtr: Wrapper to store pointers in ReferencedObject\n- make_referenced: Factory functions for creating referenced wrappers", "see": ["CoinSmartPtr for COIN reference counting framework"], "has_pass2": false}, "src/Interfaces/BonTMINLP2TNLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTMINLP2TNLP.hpp", "filename": "BonTMINLP2TNLP.hpp", "file": "BonTMINLP2TNLP.hpp", "brief": "Adapter converting TMINLP (MINLP) to TNLP (NLP) for Ipopt", "algorithm": "MINLP→NLP Adapter for B&B Node Processing\n\nThis adapter pattern allows an external caller to modify variable bounds,\nenabling treatment of binary/integer variables as either relaxed (continuous)\nor fixed. Essential for branch-and-bound where each node fixes or bounds\ninteger variables differently.\n\n**Key functionality:**\n- SetVariableBounds(): Modify bounds for branching\n- SetVariableType(): Change variable types (for probing/fixing)\n- x_sol()/duals_sol(): Access solution after optimization\n- get_starting_point(): Provide initial/warm start point\n\n**Usage in B&B:**\n1. Root node: All integers relaxed to continuous\n2. Branching: Fix or bound integer variables\n3. Each node: Solve NLP relaxation via Ipopt\n4. Warm start: Use parent's solution for faster convergence", "see": ["TMINLP for the original MINLP problem interface", "IpTNLP for Ipopt's NLP interface", "OsiTMINLPInterface for the higher-level solver interface"], "has_pass2": true}, "src/Interfaces/BonTMINLP2OsiLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTMINLP2OsiLP.hpp", "filename": "BonTMINLP2OsiLP.hpp", "file": "BonTMINLP2OsiLP.hpp", "brief": "Abstract transformer from MINLP to LP outer approximation\n\nBase class for building linear outer approximations of nonlinear problems.\nUsed to create MILP relaxations for OA-based algorithms.\n\n**Key methods:**\n- extract(): Build initial OA in an OsiSolverInterface\n- get_oas(): Generate OA cuts at a given point\n- get_refined_oa(): Get refined OA cuts\n\n**Coefficient cleaning (cleanNnz):**\nSmall coefficients are handled specially to avoid numerical issues:\n- Values < very_tiny_: Ignored completely\n- Values < tiny_: Try to absorb into RHS if bounds allow", "see": ["BonOaDecBase for OA algorithm driver", "BonOuterApprox for concrete implementation"], "has_pass2": false}, "src/Interfaces/BonColReader.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonColReader.hpp", "filename": "BonColReader.hpp", "file": "BonColReader.hpp", "brief": "Reader for AMPL .col and .row name files\nCopyright (C) Carnegie Mellon University 2005. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nNamesReader: Reads variable and constraint names from AMPL-generated\n.col (column names) and .row (row names) files. Provides bidirectional\nlookup between indices and names via hash map.\n\nAuthors: Pierre Bonami, Carnegie Mellon University\nDate: May 26, 2005", "see": ["OsiSolverInterface::OsiNameVec for name storage"], "has_pass2": false}, "src/Interfaces/BonExitCodes.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonExitCodes.hpp", "filename": "BonExitCodes.hpp", "file": "BonExitCodes.hpp", "brief": "Error codes for uncatchable errors in Bonmin\n\nDefines error codes for fatal errors that cannot be handled via exceptions.\n\n**Error codes:**\n- ERROR_IN_AMPL_SUFFIXES (111): Invalid AMPL suffix specification\n- UNSUPPORTED_CBC_OBJECT: CbcObject type not supported by Bonmin", "has_pass2": false}, "src/Interfaces/BonBoundsReader.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonBoundsReader.hpp", "filename": "BonBoundsReader.hpp", "file": "BonBoundsReader.hpp", "brief": "Reader for variable bound modification files\nCopyright (C) Carnegie Mellon University 2005. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nBoundsReader: Reads files containing variable bound changes in\nMPS-like format. Applies bound modifications to OsiTMINLPInterface\nfor scenario analysis or user-specified bound tightening.\n\nAuthors: Pierre Bonami, Carnegie Mellon University\nDate: May 26, 2005", "see": ["OsiTMINLPInterface for target solver interface"], "has_pass2": false}, "src/Interfaces/BonCutStrengthener.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonCutStrengthener.hpp", "filename": "BonCutStrengthener.hpp", "file": "BonCutStrengthener.hpp", "brief": "Strengthening OA cuts via NLP optimization\n\nImproves outer approximation cuts by solving auxiliary NLPs to find\ntighter bounds on linearizations. Also supports disjunctive cuts.", "algorithm": "OA Cut Strengthening via NLP:\n  Tightens outer approximation cuts by optimizing linearization RHS:\n  1. **Initial OA cut:** ∇g(x*)ᵀx ≤ ∇g(x*)ᵀx* - g(x*) (linearization at x*)\n  2. **Strengthening NLP:** min {∇g(x*)ᵀx : g(x) ≤ 0, l ≤ x ≤ u}\n     - Finds point minimizing LHS over original feasible region\n  3. **Tightened cut:** ∇g(x*)ᵀx ≤ optimal_value (strictly tighter RHS)\n  4. **Disjunctive option:** Generate cuts for branching disjunction\n     - Split on most fractional integer, strengthen each branch", "math": "Strengthening validity:\n  If x' solves min{∇g(x*)ᵀx : g(x) ≤ 0, x ∈ bounds}, then\n  ∇g(x*)ᵀx ≥ ∇g(x*)ᵀx' for all feasible x\n  The strengthened RHS = ∇g(x*)ᵀx' cuts off more infeasible region.", "complexity": "O(NLP solve) per cut strengthened.\n  Trade-off: tighter LP vs NLP overhead.\n  Most effective for cuts active at optimum.\n\n**Cut strengthening types:**\n- CS_None: No strengthening\n- CS_StrengthenedGlobal: Strengthen cuts globally\n- CS_UnstrengthenedGlobal_StrengthenedLocal: Global weak, local strong\n- CS_StrengthenedGlobal_StrengthenedLocal: Both global and local strong", "see": ["BonOaDecBase for OA cut generation", "StrengtheningTNLP for the internal NLP formulation"], "has_pass2": true}, "src/Interfaces/BonStrongBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonStrongBranchingSolver.hpp", "filename": "BonStrongBranchingSolver.hpp", "file": "BonStrongBranchingSolver.hpp", "brief": "Abstract base class for strong branching NLP solves", "algorithm": "Hot-Start Strong Branching for MINLP (LP/QP/Curvature Methods)\n\nProvides interface for efficiently solving modified NLPs during strong\nbranching evaluation. Supports hot-start capability to reuse factorizations.\n\n**Hot-start protocol:**\n1. markHotStart(): Save current solver state before branching evaluations\n2. solveFromHotStart(): Solve with changed bounds (fast, reuses state)\n3. unmarkHotStart(): Restore original state after all evaluations\n\n**Implementations:**\n- LpBranchingSolver: LP-based strong branching\n- QpBranchingSolver: QP approximation for strong branching\n- CurvBranchingSolver: Curvature-based branching", "see": ["BonChooseVariable for the variable selection driver", "LpBranchingSolver, QpBranchingSolver for implementations"], "has_pass2": true}, "src/Interfaces/BonStartPointReader.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonStartPointReader.hpp", "filename": "BonStartPointReader.hpp", "file": "BonStartPointReader.hpp", "brief": "Reader for Ipopt initialization point files\nCopyright (C) Carnegie Mellon University 2005. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nStartPointReader: Reads .initP files containing starting point\ninformation for Ipopt initialization. File format is: number of\nprimals, number of duals, then values. Variables ordered as:\nprimals, dual lower bounds, dual upper bounds, constraint duals.\n\nAuthors: Pierre Bonami, Carnegie Mellon University\nDate: May 26, 2005", "see": ["OsiTMINLPInterface for applying the starting point"], "has_pass2": false}, "src/Interfaces/BonTNLPSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTNLPSolver.hpp", "filename": "BonTNLPSolver.hpp", "file": "BonTNLPSolver.hpp", "brief": "Abstract interface for NLP solvers used in branch-and-bound", "algorithm": "NLP Solver Abstraction for B&B (Ipopt/FilterSQP Backend)\n\nProvides a generic interface for calling NLP solvers to solve continuous\nrelaxations of MINLPs. This abstraction allows Bonmin to use different\nNLP solvers (Ipopt, FilterSQP) interchangeably.\n\n**Key methods:**\n- OptimizeTNLP(): Solve an NLP from scratch\n- ReOptimizeTNLP(): Resolve with warm start (for B&B efficiency)\n- setWarmStart()/getWarmStart(): Manage warm start information\n- enableWarmStart()/disableWarmStart(): Control warm starting\n\n**Return statuses:**\n- solvedOptimal: Problem solved to optimality\n- provenInfeasible: Infeasibility proven\n- iterationLimit/timeLimit: Resource limits reached\n- computationError/exception: Solver failures\n\nImplementations: BonIpoptSolver (Ipopt), BonFilterSolver (FilterSQP)", "see": ["BonIpoptSolver.hpp for Ipopt-specific implementation", "BonFilterSolver.hpp for FilterSQP implementation", "IpTNLP.hpp for the NLP problem interface"], "has_pass2": true}, "src/Interfaces/BonRegisteredOptions.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonRegisteredOptions.hpp", "filename": "BonRegisteredOptions.hpp", "file": "BonRegisteredOptions.hpp", "brief": "Extended option registration with algorithm validity tracking\n\nExtends Ipopt's RegisteredOptions to track which options are valid for\nwhich Bonmin algorithms, enabling algorithm-specific option filtering.\n\n**Algorithm validity bits (ExtraOptInfosBits):**\n- validInHybrid (1): B-Hyb algorithm\n- validInQG (2): Quesada-Grossmann B-QG\n- validInOA (4): Pure outer approximation B-OA\n- validInBBB (8): Pure NLP-based branch-and-bound B-BB\n- validInEcp (16): Extended cutting plane B-Ecp\n- validIniFP (32): Iterated feasibility pump B-iFP\n- validInCbc (64): Cbc parallel mode\n\n**Category types (ExtraCategoriesInfo):**\n- BonminCategory, IpoptCategory, FilterCategory, BqpdCategory, CouenneCategory", "see": ["BonBabSetupBase for algorithm configuration", "Ipopt::RegisteredOptions for base functionality"], "has_pass2": false}, "src/Interfaces/BonCurvatureEstimator.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonCurvatureEstimator.hpp", "filename": "BonCurvatureEstimator.hpp", "file": "BonCurvatureEstimator.hpp", "brief": "Curvature estimation for branching decisions (NOT SUPPORTED)", "algorithm": "Null-Space Curvature for Branching (d^T H_Lag d Projection)\n\n@warning This file is not currently supported (#error directive).\n\nWould compute null-space curvature (d^T H_Lag d) for branching variable\nselection, projecting directions onto constraint null space.\n\n**Intended functionality:**\n- ComputeNullSpaceCurvature(): Project direction, compute Hessian product\n- Handle active bound and constraint detection\n- Use linear solvers for projection systems", "see": ["CurvBranchingSolver for curvature-based branching (if enabled)"], "has_pass2": true}, "src/Interfaces/BonOsiTMINLPInterface.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonOsiTMINLPInterface.hpp", "filename": "BonOsiTMINLPInterface.hpp", "file": "BonOsiTMINLPInterface.hpp", "brief": "OsiSolverInterface wrapper for TMINLP problems", "algorithm": "NLP-Based OsiSolverInterface with Outer Approximation\n\nProvides an OsiSolverInterface that wraps a TMINLP problem, enabling\nBonmin to use Cbc's branch-and-bound framework. The continuous relaxations\nare solved by an NLP solver (Ipopt or FilterSQP) rather than LP.\n\n**Key functionality:**\n- initialSolve()/resolve(): Solve NLP relaxation via TNLPSolver\n- extractLinearRelaxation(): Build LP outer approximation (OA)\n- getOuterApproximation(): Generate OA cuts at current point\n- solveFeasibilityProblem(): Find nearest feasible point\n\n**Outer Approximation (OA) support:**\n- Linearizes nonlinear constraints at solution points\n- Generates cutting planes for MIP approximation\n- Enables hybrid NLP-BB / OA-BB algorithms\n\n**Warm starting:**\n- Supports warm starts from previous NLP solutions\n- Multiple retry strategies for failed NLP solves\n\n@note Many OsiSolverInterface methods throw since an NLP lacks an LP matrix", "see": ["BonTMINLP.hpp for the underlying MINLP problem interface", "BonTNLPSolver.hpp for the NLP solver abstraction", "OsiSolverInterface for the base class interface"], "has_pass2": true}, "src/Interfaces/BonStdInterfaceTMINLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonStdInterfaceTMINLP.hpp", "filename": "BonStdInterfaceTMINLP.hpp", "file": "BonStdInterfaceTMINLP.hpp", "brief": "C-callback based TMINLP implementation for standard interface\nCopyright (C) Shaurya Sharma. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nStdInterfaceTMINLP: TMINLP implementation using C function pointers\nfor callbacks (eval_f, eval_g, eval_grad_f, etc.). Enables use of\nBonmin from C or languages with C FFI. Mirrors Ipopt's StdCInterface.\n\nAuthors: Shaurya Sharma\nDate: June 7, 2015", "see": ["TMINLP for base interface", "BonStdCInterface.h for C callback type definitions"], "has_pass2": false}, "src/Interfaces/BonTMINLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTMINLP.hpp", "filename": "BonTMINLP.hpp", "file": "BonTMINLP.hpp", "brief": "Base class for Mixed-Integer Nonlinear Programs (MINLP)", "algorithm": "MINLP Problem Interface (Binary/Integer Variables with NLP Structure)\n\nDefines the TMINLP interface for specifying optimization problems of the form:\n@f[\n\\begin{array}{rl}\n  \\min & f(x) \\\\\n  \\text{s.t.} & g^L \\le g(x) \\le g^U \\\\\n              & x^L \\le x \\le x^U \\\\\n              & x_i \\in \\{0,1\\} \\text{ for } i \\in \\mathcal{B} \\\\\n              & x_i \\in \\mathbb{Z} \\text{ for } i \\in \\mathcal{I}\n\\end{array}\n@f]\n\nUsers implement this interface to define their MINLP problem. Key methods:\n- get_nlp_info(): Problem dimensions and sparsity\n- get_variables_types(): Identify binary/integer/continuous variables\n- get_bounds_info(): Variable and constraint bounds\n- eval_f(), eval_grad_f(): Objective and gradient\n- eval_g(), eval_jac_g(): Constraints and Jacobian\n- eval_h(): Hessian of the Lagrangian\n\nThe TMINLP2TNLP adapter converts this to an Ipopt-solvable TNLP by\nrelaxing or fixing integer variables.", "see": ["BonTMINLP2TNLP.hpp for TMINLP to TNLP conversion", "BonOsiTMINLPInterface.hpp for OsiSolverInterface wrapper", "IpTNLP.hpp for Ipopt's NLP interface (parent of continuous relaxation)"], "has_pass2": true}, "src/Interfaces/BonTNLP2FPNLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonTNLP2FPNLP.hpp", "filename": "BonTNLP2FPNLP.hpp", "file": "BonTNLP2FPNLP.hpp", "brief": "Adapter for Feasibility Pump NLP formulation\n\nWraps a TNLP to create Feasibility Pump subproblems by modifying the\nobjective to minimize distance to a target integer point.\n\n**Modified objective:**\nf_FP(x) = lambda * ||x_I - x̃_I||_p + (1-lambda) * sigma * f(x)\nwhere x̃_I is the target integer point, p is norm (1 or 2).\n\n**Optional constraints:**\n- Cutoff constraint: f(x) <= cutoff (improve on incumbent)\n- Local branching: sum |x_i - x̃_i| <= rhs (neighborhood search)\n\n**Key parameters:**\n- lambda_: Weight on distance (vs original objective)\n- sigma_: Scaling for original objective\n- norm_: L1 or L2 distance metric", "see": ["HeuristicFPump for the Feasibility Pump driver", "HeuristicLocalBranching for local branching heuristic"], "has_pass2": false}, "src/Interfaces/BonAuxInfos.hpp": {"path": "layer-3/Bonmin/src/Interfaces/BonAuxInfos.hpp", "filename": "BonAuxInfos.hpp", "file": "BonAuxInfos.hpp", "brief": "Auxiliary information passing between Bonmin B&B components\n\nExtends OsiBabSolver to pass solver-specific information during\nbranch-and-cut, including NLP solutions found by heuristics.\n\n**Key functionality:**\n- nlpSolution_: Store solutions from NLP solves/heuristics\n- infeasibleNode_: Flag for cut generator detected infeasibility\n- bestSolution2_: Alternate objective tracking (for bi-objective)\n\n**Usage pattern:**\nCut generators and heuristics store found solutions here;\nthe B&B framework queries for incumbents.", "see": ["BabInfo for extended version with Bab pointer", "OsiBabSolver for the base class interface"], "has_pass2": false}, "src/CbcBonmin/BonGuessHeuristic.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonGuessHeuristic.hpp", "filename": "BonGuessHeuristic.hpp", "file": "BonGuessHeuristic.hpp", "brief": "Pseudocost-based initial guess heuristic for MINLP\nCopyright (C) International Business Machines 2007. All Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "Pseudocost-Guided Initial Guess Heuristic", "math": "Uses accumulated pseudocost information to guess integer values:\n      For each fractional integer variable x_j with value v:\n        If ψ_j^- < ψ_j^+: guess x̃_j = ⌊v⌋ (round down is cheaper)\n        If ψ_j^+ < ψ_j^-: guess x̃_j = ⌈v⌉ (round up is cheaper)\n      where ψ_j^-/ψ_j^+ are pseudocosts for down/up branches.\n\nGuessHeuristic: Generates initial feasible solution guesses based on\npseudocost information. Used early in the branch-and-bound process\nto provide good starting points.", "complexity": "O(n) for generating guess from pseudocosts\n\nAuthors: Andreas Waechter, IBM\nDate: September 1, 2007", "see": ["CbcHeuristic for base heuristic interface"], "has_pass2": true}, "src/CbcBonmin/BonBabInfos.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonBabInfos.hpp", "filename": "BonBabInfos.hpp", "file": "BonBabInfos.hpp", "brief": "Information passing between Bonmin B&B components\n\nExtends OsiAuxInfo to pass Bonmin-specific information between components\nduring branch-and-cut. Provides access to the Bab solver and solution info.\n\n**Key functionality:**\n- babPtr_: Access to the Bab object (and via it, CbcModel)\n- hasSolution(): Check if incumbent exists\n- bestSolution_: Current best known solution (from AuxInfo)\n\nUsed by cut generators and heuristics to query current B&B state.", "see": ["AuxInfo for the base class", "Bab for the branch-and-bound driver"], "has_pass2": false}, "src/CbcBonmin/BonCbcNode.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonCbcNode.hpp", "filename": "BonCbcNode.hpp", "file": "BonCbcNode.hpp", "brief": "Extended node information classes for MINLP branch-and-bound\n\nExtends Cbc's node information to track MINLP-specific data like sequences\nof infeasible or unsolved subproblems along a branch.\n\n**Classes:**\n- BonCbcFullNodeInfo: Complete node recreation from scratch\n- BonCbcPartialNodeInfo: Incremental node changes from parent\n\n**MINLP-specific tracking:**\n- sequenceOfInfeasiblesSize_: Consecutive infeasible NLP subproblems\n- sequenceOfUnsolvedSize_: Consecutive NLP solver failures\n\nThese counts help detect problematic branches where NLP solves consistently\nfail, enabling early pruning heuristics to avoid wasted computation.", "see": ["CbcFullNodeInfo for base class", "CbcPartialNodeInfo for base class"], "has_pass2": false}, "src/CbcBonmin/BonCbc.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonCbc.hpp", "filename": "BonCbc.hpp", "file": "BonCbc.hpp", "brief": "Main branch-and-bound driver for Bonmin using Cbc\n\nThe Bab class is the entry point for solving MINLPs with Bonmin. It wraps\nCbcModel to provide branch-and-bound with NLP solvers for subproblems.\n\n**Key method:**\n- branchAndBound(BabSetupBase&): Main solve routine\n\n**Return statuses (MipStatuses):**\n- FeasibleOptimal: Optimal solution found and proven\n- ProvenInfeasible: Problem proven infeasible\n- Feasible: Integer solution found (not proven optimal)\n- UnboundedOrInfeasible: Continuous relaxation unbounded\n- NoSolutionKnown: No feasible solution found\n\n**Solution access:**\n- bestSolution(): Primal solution vector\n- bestObj(): Objective value of best solution\n- bestBound(): Best known lower bound", "see": ["BabSetupBase for algorithm configuration", "CbcModel for the underlying MIP framework"], "has_pass2": false}, "src/CbcBonmin/BonDiver.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonDiver.hpp", "filename": "BonDiver.hpp", "file": "BonDiver.hpp", "brief": "Diving-based tree traversal strategies for MINLP branch-and-bound\n\nImplements tree traversal strategies that \"dive\" down branches before\nbacktracking, often finding feasible solutions faster than pure best-bound.", "algorithm": "Diving Tree Search:\n  Depth-first exploration with controlled backtracking:\n  1. **Dive phase:** Follow current branch down (one child per node)\n     - CbcDiver: Always take first child\n     - CbcProbedDiver: Evaluate both children, take better one\n     - CbcDfsDiver: Follow DFS with backtrack limits\n  2. **Termination conditions:**\n     - Hit leaf (feasible solution or pruned)\n     - maxDiveDepth_ exceeded\n     - maxDiveBacktracks_ exhausted\n     - Guessed objective ≥ incumbent (stop_diving_on_cutoff_)\n  3. **Backtrack phase:** Pop from dive stack or return to best-bound heap\n  4. **Mode switching (CbcDfsDiver):**\n     - Enlarge: Build initial tree (BFS-like)\n     - FindSolutions: Aggressive diving\n     - CloseBound: Best-bound focus\n     - LimitTreeSize: Depth-first to control memory", "math": "Diving vs best-bound trade-off:\n  Best-bound: guarantees smallest gap but may miss feasible solutions\n  Diving: finds incumbents fast, enables early pruning\n  Hybrid: start diving, switch to best-bound after n solutions found\n  DiverCompare.newSolution() triggers mode transitions", "complexity": "Diving: O(depth) memory for dive stack.\n  Hybrid manages heap of O(nodes) with dive stack overlay.\n  Mode switches trigger O(dive_stack) heap insertions.\n\n**Strategies:**\n- CbcDiver: Simple diving to leaves before returning to heap\n- CbcProbedDiver: Two-child look-ahead for dive direction\n- CbcDfsDiver: Configurable DFS with mode-based behavior\n- DiverCompare: Adaptive comparison switching modes on events", "see": ["CbcTree for the base tree class", "BabSetupBase for tree traversal configuration"], "has_pass2": true}, "src/CbcBonmin/BonCbcNlpStrategy.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/BonCbcNlpStrategy.hpp", "filename": "BonCbcNlpStrategy.hpp", "file": "BonCbcNlpStrategy.hpp", "brief": "NLP failure handling strategy for Cbc in MINLP context\nCopyright (C) International Business Machines Corporation and\nCarnegie Mellon University 2006. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nCbcNlpStrategy: Strategy class for handling NLP solve failures during\nbranch-and-bound. Tracks consecutive failures and infeasibilities,\noptionally treating failures as infeasible to continue search.\n\nAuthors: John J. Forrest, IBM; Pierre Bonami, CMU\nDate: March 15, 2006", "see": ["CbcStrategy for base strategy interface"], "has_pass2": false}, "src/Algorithms/BonBonminSetup.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonBonminSetup.hpp", "filename": "BonBonminSetup.hpp", "file": "BonBonminSetup.hpp", "brief": "Main Bonmin algorithm configuration and initialization\n\nExtends BabSetupBase with Bonmin-specific algorithm selection and\ninitialization for all MINLP algorithms.\n\n**Algorithm enum:**\n- B_BB (0): NLP-based branch-and-bound\n- B_OA (1): Pure Outer Approximation decomposition\n- B_QG (2): Quesada-Grossmann branch-and-cut\n- B_Hyb (3): Hybrid OA with NLP at nodes (default)\n- B_Ecp (4): Extended Cutting Plane (FilMINT-style)\n- B_IFP (5): Iterated Feasibility Pump\n\n**Initialization:**\n- initializeBBB(): Pure B&B with NLP at every node\n- initializeBHyb(): Hybrid with OA cuts + occasional NLP", "algorithm": "MINLP Solver Framework:\nBonmin implements multiple algorithms for convex MINLP:\nmin f(x,y) s.t. g(x,y) ≤ 0, x ∈ ℝⁿ, y ∈ {0,1}ᵐ\n\nB_BB: Solve NLP relaxation at every B&B node. Most robust, slowest.\nB_OA: Outer Approximation - alternates MILP master and NLP subproblems.\nB_QG: Single-tree OA - generates OA cuts within one B&B tree.\nB_Hyb: Hybrid - OA cuts + NLP solves at key nodes (default, often fastest).\nB_Ecp: Extended Cutting Plane - linear cuts only, no NLP solves in tree.", "math": "Outer Approximation generates cuts from NLP solutions x*:\nFor convex constraint g(x) ≤ 0 at point x*:\n  g(x*) + ∇g(x*)ᵀ(x - x*) ≤ 0\nThis linearization is valid for convex g. Collect cuts from multiple\nNLP solves to approximate the feasible region from outside.", "complexity": "Per-node: O(NLP) for B_BB, O(LP) for B_Ecp.\nHybrid reduces total NLP solves while maintaining solution quality.\nTotal complexity problem-dependent; convex MINLP is NP-hard.", "ref": ["Bonami et al. (2008). \"An algorithmic framework for convex mixed\n  integer nonlinear programs\". Discrete Optimization 5(2):186-204.", "Duran & Grossmann (1986). \"An outer-approximation algorithm for\n  a class of mixed-integer nonlinear programs\". Math. Prog. 36:307-339."], "see": ["BabSetupBase for common configuration", "BonminAmplSetup for AMPL-based initialization"], "has_pass2": true}, "src/Algorithms/BonSubMipSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonSubMipSolver.hpp", "filename": "BonSubMipSolver.hpp", "file": "BonSubMipSolver.hpp", "brief": "Unified interface for solving MILP subproblems in OA decomposition\n\nProvides a common interface for solving MILP subproblems using either\nCbc (via OsiClpSolverInterface) or CPLEX (via OsiCpxSolverInterface).\nUsed by OA decomposition algorithms to solve the linearized master problem.", "algorithm": "Lazy Constraint Callback (CPLEX):\nGenerate OA cuts on-the-fly during MILP solve.\n\n  optimize_with_lazy_constraints():\n    CPLEX calls back when integer solution found\n    Check NLP feasibility, add OA cut if violated\n    More efficient than iterative OA for some problems", "complexity": "MILP solve dominates. Branch-and-cut complexity depends\non problem structure. Master grows with accumulated linearizations.", "see": ["OaDecompositionBase for OA algorithm using this solver", "OACutGenerator2 for classical OA implementation"], "has_pass2": true}, "src/Algorithms/BonSolverHelp.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonSolverHelp.hpp", "filename": "BonSolverHelp.hpp", "file": "BonSolverHelp.hpp", "brief": "Utility functions for OA and Feasibility Pump algorithms\nCopyright (C) International Business Machines (IBM) 2006.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nHelper functions for manipulating solver state during OA decomposition\nand Feasibility Pump: checking integer feasibility, fixing/relaxing\ninteger variables, comparing solutions, and installing cuts.\n\nAuthors: Pierre Bonami, IBM\nDate: December 7, 2006", "see": ["OaDecompositionBase for main OA algorithm", "MinlpFeasPump for Feasibility Pump using these utilities"], "has_pass2": false}, "src/Algorithms/BonBabSetupBase.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonBabSetupBase.hpp", "filename": "BonBabSetupBase.hpp", "file": "BonBabSetupBase.hpp", "brief": "Base configuration class for branch-and-bound MINLP solvers\n\nAggregates all components needed to run branch-and-bound: NLP solver,\ncut generators, heuristics, branching rules, and tree traversal strategy.\nServes as the central configuration point for Bonmin algorithms.\n\n**Key components:**\n- nonlinearSolver_: OsiTMINLPInterface for NLP subproblems\n- cutGenerators_: List of CglCutGenerator (OA, ECP, Gomory, etc.)\n- heuristics_: List of CbcHeuristic (feasibility pump, rounding, etc.)\n- branchingMethod_: OsiChooseVariable for variable selection\n\n**Tree search strategies:**\n- NodeComparison: bestBound, DFS, BFS, dynamic, bestGuess\n- TreeTraversal: HeapOnly, DiveFromBest, ProbedDive, DfsDiveFromBest\n\n**Branching strategies (VarSelectStra_Enum):**\n- MOST_FRACTIONAL: Simple, fast, often poor\n- STRONG_BRANCHING: Evaluate candidates via LP/NLP solves\n- RELIABILITY_BRANCHING: Use pseudo-costs when trusted", "algorithm": "Branch-and-Bound for MINLP:\nSystematic enumeration via tree search with NLP relaxations at nodes.\nAt each node: solve continuous relaxation, branch on fractional integer\nvariable if not integer-feasible, prune if relaxation bound ≥ incumbent.", "math": "Node selection affects convergence rate:\n- Best-bound: Select node with lowest relaxation value (best global bound)\n- Depth-first: Minimize memory, find feasible solutions quickly\n- Best-estimate: Use pseudo-costs to estimate integer solution quality\nHybrid strategies (DiveFromBest) combine: dive from best-bound node\nuntil leaf, then return to best remaining node.", "complexity": "O(2^m) worst case where m = number of binary variables.\nIn practice, pruning via bounds and cuts dramatically reduces tree size.\nStrong branching: O(k · NLP) per node where k = candidates evaluated.", "ref": ["Belotti et al. (2013). \"Mixed-integer nonlinear optimization\".\n  Acta Numerica 22:1-131. [Comprehensive MINLP survey]", "Achterberg, Koch & Martin (2005). \"Branching rules revisited\".\n  Operations Research Letters 33(1):42-54. [Branching strategies]"], "see": ["BonminSetup for the concrete Bonmin configuration", "CbcBonmin for the main branch-and-bound driver"], "has_pass2": true}, "src/Algorithms/BonCbcLpStrategy.hpp": {"path": "layer-3/Bonmin/src/Algorithms/BonCbcLpStrategy.hpp", "filename": "BonCbcLpStrategy.hpp", "file": "BonCbcLpStrategy.hpp", "brief": "Cbc strategy for configuring cut generators in OA\nCopyright (C) Carnegie Mellon University 2006. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nCbcStrategyChooseCuts: Strategy class extending CbcStrategyDefault\nto configure cut generators (Gomory, MIR, etc.) for use in Outer\nApproximation LP subproblems. Sets generator frequencies based on\nBabSetupBase configuration.\n\nAuthors: Pierre Bonami, Carnegie Mellon University\nDate: March 15, 2006", "see": ["CbcStrategyDefault for base strategy", "BabSetupBase for cut generator configuration"], "has_pass2": false}, "src/Interfaces/Ipopt/BonIpoptSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ipopt/BonIpoptSolver.hpp", "filename": "BonIpoptSolver.hpp", "file": "BonIpoptSolver.hpp", "brief": "Ipopt implementation of TNLPSolver for NLP subproblems", "algorithm": "Ipopt Interior Point Solver for MINLP Node Relaxations\n\nWraps Ipopt's IpoptApplication to solve continuous NLP relaxations within\nBonmin's branch-and-bound framework. Provides warm starting capabilities\nessential for efficient node processing.\n\n**Key features:**\n- OptimizeTNLP()/ReOptimizeTNLP(): Solve NLPs via Ipopt\n- Warm start strategies: none, optimal point, interior point\n- Status translation: Maps Ipopt return codes to TNLPSolver::ReturnStatus\n\n**Warm start strategies (warmStartStrategy_):**\n- 0: No warm start\n- 1: Simple warm start from optimal point\n- 2: Interior point warm start (preserves barrier info)", "see": ["TNLPSolver for the abstract interface", "IpoptWarmStart for warm start data storage", "IpoptInteriorWarmStarter for advanced interior point warm start"], "has_pass2": true}, "src/Interfaces/Ipopt/BonIpoptWarmStart.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ipopt/BonIpoptWarmStart.hpp", "filename": "BonIpoptWarmStart.hpp", "file": "BonIpoptWarmStart.hpp", "brief": "Warm start storage for Ipopt within branch-and-bound\n\nStores primal and dual solution values for warm starting subsequent NLP\nsolves. Inherits from both CoinWarmStartPrimalDual (for actual data) and\nCoinWarmStartBasis (for Cbc integration compatibility).\n\n**Data layout (dual part has 2*n + m values):**\n- [0, n): Dual multipliers for lower bounds (z_L for x >= l)\n- [n, 2n): Dual multipliers for upper bounds (z_U for x <= u)\n- [2n, 2n+m): Dual multipliers for constraints (lambda for g(x))\n\n**Classes:**\n- IpoptWarmStart: Main warm start container\n- IpoptWarmStartDiff: Differential update for tree traversal", "see": ["IpoptInteriorWarmStarter for interior point specific warm start", "CoinWarmStartPrimalDual for base class data storage"], "has_pass2": false}, "src/Interfaces/Ipopt/BonIpoptInteriorWarmStarter.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ipopt/BonIpoptInteriorWarmStarter.hpp", "filename": "BonIpoptInteriorWarmStarter.hpp", "file": "BonIpoptInteriorWarmStarter.hpp", "brief": "Interior point warm start for Ipopt in branch-and-bound\n\nStores interior point iterates from a solved NLP for warm starting child\nnodes. Unlike simple optimal-point warm start, this preserves barrier\nparameter (mu) and interior structure, enabling faster convergence.\n\n**Key methods:**\n- UpdateStoredIterates(): Called during optimization to save promising iterates\n- WarmStartIterate(): Computes initial point for new problem from stored data\n- Finalize(): Post-processing after parent NLP solve completes\n\n**Stored data per iterate:**\n- Primal/dual iterate vectors\n- Barrier parameter mu\n- NLP error, primal/dual infeasibility, complementarity", "see": ["IpoptWarmStart for simpler primal-dual warm start", "IpoptSolver for the solver that uses this warm starter"], "has_pass2": false}, "src/Interfaces/Filter/BonBqpdSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonBqpdSolver.hpp", "filename": "BonBqpdSolver.hpp", "file": "BonBqpdSolver.hpp", "brief": "Interface to BQPD quadratic programming solver\nCopyright (C) International Business Machines Corporation, 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nBqpdSolver: TNLPSolver implementation wrapping the BQPD Fortran solver\nfor solving quadratic programs during strong branching. Provides hot\nstart capability for efficient re-optimization with changed bounds.\n\nUses BranchingTQP to create QP approximations of the MINLP.\n\nAuthors: Andreas Waechter, IBM (based on BonFilterSolver.cpp)\nDate: July 9, 2007", "see": ["BranchingTQP for QP problem adapter", "BqpdWarmStart for warm start information"], "has_pass2": false}, "src/Interfaces/Filter/BonFilterWarmStart.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonFilterWarmStart.hpp", "filename": "BonFilterWarmStart.hpp", "file": "BonFilterWarmStart.hpp", "brief": "Warm start storage for FilterSQP solver\n\nStores warm start information for FilterSQP, including primal/dual values\nand FilterSQP's internal lws (integer working set) array. Inherits from both\nCoinWarmStartPrimalDual and CoinWarmStartBasis for Cbc compatibility.\n\n**Data stored:**\n- x: Primal variables (from CoinWarmStartPrimalDual)\n- lam: Dual multipliers (from CoinWarmStartPrimalDual)\n- lws: FilterSQP's integer working set array (active set info)\n- istat[14]: FilterSQP status array\n\n**Classes:**\n- FilterWarmStart: Main warm start container\n- FilterWarmStartDiff: Differential update for tree traversal", "see": ["FilterSolver for the solver that uses this warm start", "IpoptWarmStart for the Ipopt equivalent"], "has_pass2": false}, "src/Interfaces/Filter/BonFilterSolver.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonFilterSolver.hpp", "filename": "BonFilterSolver.hpp", "file": "BonFilterSolver.hpp", "brief": "FilterSQP implementation of TNLPSolver for NLP subproblems", "algorithm": "FilterSQP Sequential Quadratic Programming for MINLP Nodes\n\nWraps the FilterSQP solver (Fortran library) to solve continuous NLP\nrelaxations as an alternative to Ipopt. FilterSQP uses sequential quadratic\nprogramming with a filter-based trust region.\n\n**Key features:**\n- OptimizeTNLP()/ReOptimizeTNLP(): Solve NLPs via FilterSQP\n- cachedInfo: Caches problem structure for efficient re-optimization\n- TMat2RowPMat(): Converts sparse matrix formats for Fortran interface\n\n**Filter algorithm:**\nThe \"filter\" maintains pairs (constraint violation, objective value) and\naccepts steps that improve either measure - more robust than merit functions.", "see": ["TNLPSolver for the abstract interface", "FilterWarmStart for warm start data", "BqpdSolver for the QP subproblem solver used by FilterSQP"], "has_pass2": true}, "src/Interfaces/Filter/BonFilterTypes.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonFilterTypes.hpp", "filename": "BonFilterTypes.hpp", "file": "BonFilterTypes.hpp", "brief": "Fortran type definitions for FilterSQP interface\n\nDefines type aliases for interfacing with the Fortran FilterSQP library.\nUses FORTRAN_INTEGER_TYPE from Ipopt's configuration for portability\nacross different Fortran compilers (32-bit vs 64-bit integers).", "see": ["BonFilterSolver.hpp for the FilterSQP wrapper"], "has_pass2": false}, "src/Interfaces/Filter/BonBqpdWarmStart.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Filter/BonBqpdWarmStart.hpp", "filename": "BonBqpdWarmStart.hpp", "file": "BonBqpdWarmStart.hpp", "brief": "Warm start information for BQPD solver\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nBqpdWarmStart: Stores warm start information for BQPD QP solver.\nInherits from both CoinWarmStartPrimalDual (actual warm start data)\nand CoinWarmStartBasis (for Cbc integration compatibility).\n\nAuthors: Andreas Waechter, IBM (based on BonFilterWarmStart.hpp)\nDate: August 3, 2007", "see": ["BqpdSolver for the solver using this warm start", "CoinWarmStartPrimalDual for primal/dual storage"], "has_pass2": false}, "src/Interfaces/Ampl/BonSolReader.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ampl/BonSolReader.hpp", "filename": "BonSolReader.hpp", "file": "BonSolReader.hpp", "brief": "Reader for AMPL .sol solution files\nCopyright (C) CNRS 2011. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nSolReader: Reads solution files in AMPL .sol format containing\nvariable values. Used to load known solutions for warm starting\nor comparison with computed solutions.\n\nAuthors: Pierre Bonami, LIF, CNRS\nDate: March 1, 2011", "see": ["NamesReader for corresponding variable name reader"], "has_pass2": false}, "src/Interfaces/Ampl/BonAmplInterface.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ampl/BonAmplInterface.hpp", "filename": "BonAmplInterface.hpp", "file": "BonAmplInterface.hpp", "brief": "OsiSolverInterface for AMPL .nl file input\n\nExtends OsiTMINLPInterface to read problems from AMPL .nl files.\nProvides convenient entry point for AMPL-based modeling.\n\n**Usage:**\n@code\nAmplInterface solver;\nsolver.readAmplNlFile(argv, roptions, options, journalist);\n@endcode\n\n**Additional features:**\n- readNames(): Read variable/row names from .col/.row files\n- amplModel(): Access underlying AmplTMINLP", "see": ["AmplTMINLP for the TMINLP implementation", "OsiTMINLPInterface for the base class"], "has_pass2": false}, "src/Interfaces/Ampl/BonAmplTMINLP.hpp": {"path": "layer-3/Bonmin/src/Interfaces/Ampl/BonAmplTMINLP.hpp", "filename": "BonAmplTMINLP.hpp", "file": "BonAmplTMINLP.hpp", "brief": "AMPL interface for MINLP problems via .nl files\n\nImplements TMINLP by reading AMPL .nl files, extracting integer variable\ninformation, SOS constraints, branching priorities, and convexity markers.\n\n**AMPL suffix reading:**\n- read_priorities(): Variable branching priorities\n- read_sos(): SOS1/SOS2 constraint definitions\n- read_convexities(): Constraint convexity markers (for OA)\n- read_onoff(): Perspective reformulation indicators\n- read_obj_suffixes(): Upper bounding objective (UBObj prefix)\n\n**AMPL solve_result_num codes:**\n- 3: Integer optimal\n- 220: Proven infeasible\n- 421: Limit reached with feasible solution\n- 410: Limit reached, no feasible solution\n- 500: Error", "see": ["TMINLP for the base interface", "AmplInterface for the OsiSolverInterface wrapper"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicFPump.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicFPump.hpp", "filename": "BonHeuristicFPump.hpp", "file": "BonHeuristicFPump.hpp", "brief": "Feasibility Pump heuristic for MINLP\n\nImplements the Feasibility Pump algorithm adapted for nonlinear problems.\nAlternates between:\n1. Rounding to nearest integer solution\n2. Projecting back to feasible continuous space via NLP\n\n**Classes:**\n- HeuristicFPump: Main feasibility pump heuristic (CbcHeuristic)\n- RoundingFPump: Helper for intelligent rounding considering constraints", "algorithm": "Feasibility Pump for MINLP:\nFind feasible integer solution by alternating rounding and projection:\n1. Solve NLP relaxation: x* = argmin{f(x) : g(x) ≤ 0}\n2. Round integers: x̃_i = round(x*_i) for i ∈ I (integer variables)\n3. Projection NLP: x* = argmin{||x_I - x̃_I||_p : g(x) ≤ 0}\n   where p = objective_norm_ (1 or 2)\n4. If x*_I = x̃_I, return feasible solution\n5. Else repeat from step 2 (with cycle detection)\n\nRoundingFPump provides constraint-aware rounding:\n- Considers Jacobian structure for better rounding decisions\n- Respects integerTolerance for near-integer values", "math": "Projection objective (step 3):\n  L₁ norm: min Σᵢ |xᵢ - x̃ᵢ|  (reformulated with auxiliary variables)\n  L₂ norm: min Σᵢ (xᵢ - x̃ᵢ)²  (smooth, faster convergence)", "complexity": "O(k · NLP_solve) where k = iterations until convergence.\nTypically k = 10-100. Each NLP solve is O(n³) worst case for Newton.", "ref": ["Bonami, Cornuéjols, Lodi, Margot (2009). \"A feasibility pump for\n  mixed integer nonlinear programs\". Mathematical Programming 119(2):331-352."], "see": ["CbcHeuristic for the base class", "BonminSetup for configuration"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonHeuristicDiveMIPFractional.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDiveMIPFractional.hpp", "filename": "BonHeuristicDiveMIPFractional.hpp", "file": "BonHeuristicDiveMIPFractional.hpp", "brief": "MIP-based diving heuristic selecting most fractional variable\nCopyright (C) 2007, International Business Machines Corporation and others.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "MIP-Guided Most Fractional Diving", "math": "Combines MIP guidance with fractional selection:\n      1. Solve MIP subproblem to get candidate integer assignment\n      2. Among unfixed variables, select j* = argmax min(f_j, 1-f_j)\n      3. Rounding uses MIP solution when available, else nearest integer\n\nHeuristicDiveMIPFractional: Diving heuristic for MINLP that solves\nMIP subproblems during diving. Variable selection based on\nfractionality - picks variable closest to 0.5.\n\n**Hybrid approach:** Uses MIP solver's insight into integer feasibility\ncombined with simple fractional selection. More robust than pure NLP\ndiving when MIP relaxation provides good guidance.", "complexity": "O(MIP) + O(n) per step; MIP dominates", "ref": ["Bonami, P. et al. (2008). \"An algorithmic framework for convex\n     mixed integer nonlinear programs\". Discrete Optimization 5(2):186-204.\n\nAuthors: Joao P. Goncalves, IBM\nDate: November 12, 2007"], "see": ["HeuristicDiveMIP for base MIP-diving class", "HeuristicDiveFractional for NLP-based variant"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonLocalSolverBasedHeuristic.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonLocalSolverBasedHeuristic.hpp", "filename": "BonLocalSolverBasedHeuristic.hpp", "file": "BonLocalSolverBasedHeuristic.hpp", "brief": "Base class for heuristics using local NLP/MINLP solves", "algorithm": "Local Search Heuristic Framework\n\nProvides infrastructure for heuristics that find feasible solutions by\nsolving restricted MINLP subproblems (local searches) in a neighborhood\nof the current solution.\n\n**Key method:**\n- doLocalSearch(): Solve MINLP subproblem with modified bounds/cutoff\n\n**Parameters:**\n- time_limit_: Maximum time for local search\n- max_number_nodes_: Node limit for local search\n- max_number_solutions_: Stop after finding this many solutions\n\n**Subclasses:**\n- FixAndSolveHeuristic: Fix some variables, solve remaining MINLP\n- HeuristicRINS: Relaxation Induced Neighborhood Search\n- HeuristicLocalBranching: Local branching cuts", "see": ["CbcHeuristic for the base class", "BonminSetup for local search configuration"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonFixAndSolveHeuristic.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonFixAndSolveHeuristic.hpp", "filename": "BonFixAndSolveHeuristic.hpp", "file": "BonFixAndSolveHeuristic.hpp", "brief": "Fix-and-Solve heuristic for MINLP", "algorithm": "Fix-and-Solve MINLP Heuristic\n\nSimple heuristic that fixes some integer variables to their current\nvalues and solves the resulting smaller MINLP subproblem.\n\n**Algorithm:**\n1. Select subset of integer variables to fix (based on LP solution)\n2. Fix selected variables to their current integer values\n3. Solve reduced MINLP with remaining variables\n4. If feasible and improved, update incumbent\n\nSimpler than RINS (doesn't require incumbent for comparison).", "see": ["LocalSolverBasedHeuristic for base class", "HeuristicRINS for more sophisticated fixing strategy"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonHeuristicDiveMIPVectorLength.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDiveMIPVectorLength.hpp", "filename": "BonHeuristicDiveMIPVectorLength.hpp", "file": "BonHeuristicDiveMIPVectorLength.hpp", "brief": "MIP-based diving heuristic selecting by constraint participation\nCopyright (C) 2007, International Business Machines Corporation and others.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "MIP-Guided Vector Length Diving", "math": "Combines MIP guidance with column-density selection:\n      score(x_j) = min(f_j, 1-f_j) · columnLength_j\n      Select variable with maximum impact score, use MIP for rounding.\n\nHeuristicDiveMIPVectorLength: Diving heuristic for MINLP that solves\nMIP subproblems during diving. Variable selection based on column\nlength (number of constraints containing the variable).\n\n**Hybrid approach:** Uses MIP solver's insight combined with structural\ninformation (constraint participation). Variables in many constraints\npropagate more information when fixed.\n\nUses columnLength_ array for O(1) lookup after O(nnz) preprocessing.", "complexity": "O(MIP) + O(n) per step; O(nnz) setup for column lengths", "ref": ["Achterberg, T. (2007). \"Constraint Integer Programming\".\n     PhD thesis, TU Berlin. Section 7.4.\n\nAuthors: Joao P. Goncalves, IBM\nDate: November 12, 2007"], "see": ["HeuristicDiveMIP for base MIP-diving class", "HeuristicDiveVectorLength for NLP-based variant"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonDummyPump.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonDummyPump.hpp", "filename": "BonDummyPump.hpp", "file": "BonDummyPump.hpp", "brief": "Placeholder/testing heuristic for feasibility pump\nCopyright (C) CNRS\nThis code is published under the Eclipse Public License.\n\nDummyPump: Simple placeholder heuristic extending LocalSolverBasedHeuristic.\nUsed for testing or as template for new heuristics.\n\nProvides minimal implementation of solution() method.\n\nAuthors: Pierre Bonami, LIF Université de la Méditerranée-CNRS\nDate: 06/18/2008", "see": ["LocalSolverBasedHeuristic for base class", "PumpForMinlp for full feasibility pump implementation"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicLocalBranching.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicLocalBranching.hpp", "filename": "BonHeuristicLocalBranching.hpp", "file": "BonHeuristicLocalBranching.hpp", "brief": "Local Branching heuristic for MINLP improvement", "algorithm": "Local Branching via Hamming Distance (Fischetti-Lodi)\n\nSearches for improved solutions in a Hamming distance neighborhood\nof the current incumbent by adding a local branching constraint.\n\n**Algorithm:**\n1. Add constraint: sum |x_i - x*_i| <= k (Hamming distance <= k)\n2. Solve restricted MINLP in this neighborhood\n3. If improved solution found, update incumbent\n4. Optionally expand neighborhood if no improvement\n\n**Local branching constraint:**\nFor binary variables: sum_{x*_i=1}(1-x_i) + sum_{x*_i=0}(x_i) <= k", "see": ["LocalSolverBasedHeuristic for base class", "TNLP2FPNLP for local branching constraint implementation"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonPumpForMinlp.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonPumpForMinlp.hpp", "filename": "BonPumpForMinlp.hpp", "file": "BonPumpForMinlp.hpp", "brief": "Feasibility Pump heuristic adapted for MINLP\nCopyright (C) CNRS\nThis code is published under the Eclipse Public License.\n\nPumpForMinlp: Feasibility Pump heuristic for nonlinear mixed-integer\nprogramming. Extends the MIP Feasibility Pump to handle nonlinear\nconstraints by using NLP subproblem solves.\n\nAlternates between:\n- Rounding current NLP solution to integer\n- Solving NLP with objective minimizing distance to rounded point\n\nAuthors: Pierre Bonami, LIF Université de la Méditerranée-CNRS\nDate: 02/18/2009", "see": ["LocalSolverBasedHeuristic for base class", "CbcHeuristicFPump for MIP feasibility pump"], "has_pass2": false}, "src/CbcBonmin/Heuristics/BonHeuristicDive.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDive.hpp", "filename": "BonHeuristicDive.hpp", "file": "BonHeuristicDive.hpp", "brief": "Base class for diving heuristics in MINLP", "algorithm": "NLP Diving Heuristic Framework\n\nImplements diving heuristics that repeatedly fix integer variables and solve\nNLP relaxations until a feasible integer solution is found or infeasibility.\n\n**Algorithm:**\n1. Solve NLP relaxation\n2. Select fractional variable via selectVariableToBranch()\n3. Fix variable to floor or ceil\n4. Resolve NLP, repeat until all integers fixed\n\n**Subclasses implement different selection rules:**\n- HeuristicDiveFractional: Branch on most fractional\n- HeuristicDiveVectorLength: Use gradient information\n- HeuristicDiveMIP: Solve MIP subproblem for selection\n\n**Helper functions:**\n- isNlpFeasible(): Check NLP constraint satisfaction\n- adjustPrimalTolerance(): Handle near-feasible solutions", "see": ["CbcHeuristic for the base class", "HeuristicDiveFractional for a concrete implementation"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonHeuristicDiveFractional.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDiveFractional.hpp", "filename": "BonHeuristicDiveFractional.hpp", "file": "BonHeuristicDiveFractional.hpp", "brief": "NLP-based diving heuristic selecting most fractional variable\nCopyright (C) 2007, International Business Machines Corporation and others.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "Most Fractional Diving Heuristic", "math": "Variable selection score: s(x_j) = min(f_j, 1-f_j) where f_j = x_j - ⌊x_j⌋\n      The variable with largest s(x_j) (closest to 0.5) is selected.\n      Rounding direction: ceil if f_j ≥ 0.5, floor otherwise.\n\nHeuristicDiveFractional: Diving heuristic for MINLP that solves\nNLP subproblems during diving. Variable selection based on\nfractionality - picks integer variable closest to 0.5.\n\n**Algorithm:**\n1. Compute fractional part f_j = x_j - ⌊x_j⌋ for each integer variable\n2. Select j* = argmax_j min(f_j, 1-f_j)\n3. Round towards nearest integer (ceil if f_j ≥ 0.5)\n4. Fix variable and resolve NLP", "complexity": "O(n) per diving step for variable selection", "ref": ["Berthold, T. (2006). \"Primal Heuristics for Mixed Integer Programs\".\n     Diploma thesis, TU Berlin.\n\nAuthors: Joao P. Goncalves, IBM\nDate: November 12, 2007"], "see": ["HeuristicDive for base NLP-diving class", "HeuristicDiveMIPFractional for MIP-based variant"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonHeuristicDiveMIP.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDiveMIP.hpp", "filename": "BonHeuristicDiveMIP.hpp", "file": "BonHeuristicDiveMIP.hpp", "brief": "Base class for MIP-based diving heuristics in MINLP\nCopyright (C) 2007, International Business Machines Corporation and others.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "MIP-Guided Diving Heuristic Framework", "math": "Uses MIP subproblem to guide diving in MINLP:\n      Given MINLP relaxation solution x*, solve MIP:\n        min ||x - x*||  s.t. Ax ≤ b, x_j ∈ Z (integer vars)\n      MIP solution suggests which integers to fix and their values.\n\nHeuristicDiveMIP: Abstract base class for diving heuristics that solve\nMIP subproblems during the dive. Unlike pure NLP-based diving, this\nuses a SubMipSolver (mip_) to handle integer feasibility more robustly.\n\n**Key advantage over NLP diving:**\n- MIP solver handles combinatorial structure better\n- Can find feasible integer assignments that NLP diving misses\n- Useful when NLP relaxation is far from integer feasible\n\n**Trade-off:** More expensive per iteration (MIP solve vs O(n) selection)\nbut may require fewer total iterations.\n\nDerived classes implement:\n- setInternalVariables(): Setup for variable selection\n- selectVariableToBranch(): Variable and rounding direction choice", "complexity": "O(MIP) per diving step; MIP complexity depends on formulation", "ref": ["Bonami, P. et al. (2008). \"An algorithmic framework for convex\n     mixed integer nonlinear programs\". Discrete Optimization 5(2):186-204.\n\nAuthors: Joao P. Goncalves, IBM\nDate: November 12, 2007"], "see": ["HeuristicDiveMIPFractional for fractional selection", "HeuristicDiveMIPVectorLength for column-density selection", "HeuristicDive for NLP-only base class"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonHeuristicDiveVectorLength.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicDiveVectorLength.hpp", "filename": "BonHeuristicDiveVectorLength.hpp", "file": "BonHeuristicDiveVectorLength.hpp", "brief": "NLP-based diving heuristic selecting by constraint participation\nCopyright (C) 2007, International Business Machines Corporation and others.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "Vector Length Diving Heuristic", "math": "Variable selection combines fractionality and column density:\n      score(x_j) = min(f_j, 1-f_j) · columnLength_j\n      where columnLength_j = number of constraints containing x_j.\n      Selects variable with maximum score (fractional AND high-impact).\n\nHeuristicDiveVectorLength: Diving heuristic for MINLP that solves\nNLP subproblems during diving. Variable selection based on column\nlength (number of constraints containing the variable).\n\n**Rationale:** Variables appearing in many constraints have higher\nimpact when fixed. By weighting fractionality by column length,\nwe prioritize fixing variables that propagate more information.\n\n**Algorithm:**\n1. Precompute columnLength_j for all integer variables\n2. For each fractional x_j, compute score = min(f_j, 1-f_j) · columnLength_j\n3. Select j* = argmax_j score(x_j)\n4. Round towards nearest integer", "complexity": "O(n) per diving step; O(nnz) one-time setup for column lengths", "ref": ["Achterberg, T. (2007). \"Constraint Integer Programming\".\n     PhD thesis, TU Berlin. Section 7.4.\n\nAuthors: Joao P. Goncalves, IBM\nDate: November 12, 2007"], "see": ["HeuristicDive for base NLP-diving class", "HeuristicDiveMIPVectorLength for MIP-based variant"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonHeuristicRINS.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonHeuristicRINS.hpp", "filename": "BonHeuristicRINS.hpp", "file": "BonHeuristicRINS.hpp", "brief": "Relaxation Induced Neighborhood Search (RINS) heuristic for MINLP", "algorithm": "RINS Neighborhood Search (Danna-Rothberg-LePape)\n\nRINS finds improving solutions by solving restricted MINLP subproblems\nwhere variables agreeing between LP relaxation and incumbent are fixed.\n\n**Algorithm:**\n1. Compare current LP solution with incumbent\n2. Fix integer variables where both solutions agree\n3. Solve restricted MINLP with remaining variables free\n4. If solution improves incumbent, update\n\n**Parameters:**\n- howOften_: Frequency of heuristic application\n- numberSolutions_: Track solution count for triggering", "see": ["LocalSolverBasedHeuristic for base class", "HeuristicLocalBranching for related neighborhood search"], "has_pass2": true}, "src/CbcBonmin/Heuristics/BonMilpRounding.hpp": {"path": "layer-3/Bonmin/src/CbcBonmin/Heuristics/BonMilpRounding.hpp", "filename": "BonMilpRounding.hpp", "file": "BonMilpRounding.hpp", "brief": "MILP-based rounding heuristic for MINLP", "algorithm": "MILP Rounding with No-Good Cuts\n\nCopyright (C) 2010, International Business Machines Corporation and others.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nMilpRounding: Heuristic that constructs a MIP approximation of the MINLP,\nsolves it, and uses the solution as a starting point for NLP refinement.\n\nUses SubMipSolver (mip_) for the linear approximation and maintains\nnoGoods cuts to avoid revisiting previously explored regions.\n\nAuthors: Pierre Bonami, CNRS\nDate: May 26, 2010", "see": ["SubMipSolver for MIP subproblem solver", "HeuristicDiveMIP for diving-based approach"], "has_pass2": true}, "src/Algorithms/Branching/BonRandomChoice.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonRandomChoice.hpp", "filename": "BonRandomChoice.hpp", "file": "BonRandomChoice.hpp", "brief": "Random variable selection for branching\nCopyright (C) CNRS 2008. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nBonRandomChoice: Variable selection strategy that randomly picks from\nunsatisfied integer variables. Useful for diversification in search\nor as baseline for comparison with sophisticated selection rules.\n\nAuthors: Pierre Bonami, LIF, CNRS-Marseille Universites\nDate: March 17, 2008", "see": ["OsiChooseVariable for base variable chooser interface"], "has_pass2": false}, "src/Algorithms/Branching/BonPseudoCosts.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonPseudoCosts.hpp", "filename": "BonPseudoCosts.hpp", "file": "BonPseudoCosts.hpp", "brief": "Pseudo-cost storage and update for MINLP branching\n\nExtends OsiPseudoCosts to track branching history for integer variables.\nPseudo-costs estimate the objective change per unit change in a variable,\nenabling efficient branching decisions without expensive strong branching.", "algorithm": "Pseudo-Cost Branching:\nLearn branching quality from history to avoid repeated strong branching:\n1. Initialize: ψ⁺ᵢ = ψ⁻ᵢ = default_cost (or from first strong branch)\n2. After branching on xᵢ with fractional part fᵢ:\n   - Up branch (xᵢ ≥ ⌈xᵢ⌉): record Δobj⁺, update ψ⁺ᵢ\n   - Down branch (xᵢ ≤ ⌊xᵢ⌋): record Δobj⁻, update ψ⁻ᵢ\n3. Update formula:\n   ψ⁺ᵢ = (Σ Δobj⁺) / count(up branches)\n   ψ⁻ᵢ = (Σ Δobj⁻) / count(down branches)\n4. Score variable: score(i) = w·min(ψ⁺ᵢ·(1-fᵢ), ψ⁻ᵢ·fᵢ)\n                             + (1-w)·max(ψ⁺ᵢ·(1-fᵢ), ψ⁻ᵢ·fᵢ)", "math": "Pseudo-cost per unit change:\n  ψ⁺ᵢ ≈ E[Δobj | branch up on xᵢ] / (1 - fᵢ)\n  ψ⁻ᵢ ≈ E[Δobj | branch down on xᵢ] / fᵢ\nwhere fᵢ = xᵢ - ⌊xᵢ⌋ is the fractional part.", "complexity": "O(1) per variable selection (no solving needed).\nCompare to O(LP_solve) for strong branching per candidate.", "ref": ["Benichou et al. (1971). \"Experiments in mixed-integer linear programming\".\n  Mathematical Programming 1(1):76-94."], "see": ["BonChooseVariable for usage in branching decisions", "OsiPseudoCosts for the base class"], "has_pass2": true}, "src/Algorithms/Branching/BonQpBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonQpBranchingSolver.hpp", "filename": "BonQpBranchingSolver.hpp", "file": "BonQpBranchingSolver.hpp", "brief": "QP-based strong branching solver\n\nImplements strong branching by solving QP approximations of the NLP\nsubproblems. Faster than full NLP but more accurate than LP.", "algorithm": "QP Strong Branching for MINLP:\nUse second-order approximation to estimate branching impact.\n\nMOTIVATION:\n  Full NLP strong branching: accurate but O(k · NLP_solve) per node\n  LP strong branching: fast but ignores nonlinearity\n  QP strong branching: balance accuracy and speed\n\nAPPROXIMATION:\n  At current NLP solution x*, build QP:\n    min  f(x*) + ∇f(x*)ᵀ(x-x*) + ½(x-x*)ᵀH(x-x*)\n    s.t. g(x*) + ∇g(x*)ᵀ(x-x*) ≤ 0\n         l ≤ x ≤ u\n\n  where H = ∇²L(x*, λ*) is the Hessian of the Lagrangian\n\nSTRONG BRANCHING PROCEDURE:\n  markHotStart(): Build BranchingTQP from current NLP point\n  For each candidate variable x_j:\n    solveFromHotStart(x_j ≤ ⌊x*_j⌋): Get Δz⁻_j\n    solveFromHotStart(x_j ≥ ⌈x*_j⌉): Get Δz⁺_j\n  unmarkHotStart(): Clean up\n  Select j* = argmax score(Δz⁻_j, Δz⁺_j)", "math": "QP provides second-order Taylor approximation:\n  Error is O(||x - x*||³) vs O(||x - x*||²) for LP\n  Captures curvature that LP misses", "complexity": "O(k · QP) per node where k = #candidates.\nQP solve is O(n²) to O(n³) depending on structure (n = variables).\nMuch faster than O(k · NLP) for full NLP strong branching.", "ref": ["Requires FilterSQP or BQPD QP solver (BONMIN_HAS_FILTERSQP)"], "see": ["StrongBranchingSolver for base class", "BranchingTQP for the QP subproblem formulation", "BqpdSolver for the underlying QP solver"], "has_pass2": true}, "src/Algorithms/Branching/BonLpBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonLpBranchingSolver.hpp", "filename": "BonLpBranchingSolver.hpp", "file": "BonLpBranchingSolver.hpp", "brief": "LP-based strong branching solver using ECP cuts\n\nImplements strong branching by solving LP relaxations enhanced with\nExtended Cutting Plane (ECP) cuts, avoiding full NLP solves.", "algorithm": "LP-Based Strong Branching for MINLP:\nApproximate NLP strong branching using LP + ECP cuts:\n1. markHotStart():\n   a. Extract LP relaxation from current NLP solution\n   b. Generate initial ECP cuts at NLP solution\n   c. Store warm start basis\n2. For each branching candidate i:\n   a. Modify bounds: x_i ≥ ⌈x_i⌉ (up) or x_i ≤ ⌊x_i⌋ (down)\n   b. solveFromHotStart():\n      - Warm start LP from stored basis\n      - Run ECP iterations (up to maxCuttingPlaneIterations_)\n      - Record objective change Δobj_i\n3. unmarkHotStart(): Restore original bounds, clean up\n4. Select: i* = argmax{score(Δobj_i⁺, Δobj_i⁻)}\n\nWarm start modes:\n- Basis: Reuse LP basis (faster, less accurate)\n- Clone: Clone entire LP solver (slower, more accurate)", "complexity": "O(candidates · ECP_iters · LP_solve).\nMuch faster than O(candidates · NLP_solve) for true strong branching.", "see": ["StrongBranchingSolver for base class", "EcpCuts for the cut generator used"], "has_pass2": true}, "src/Algorithms/Branching/BonCurvBranchingSolver.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonCurvBranchingSolver.hpp", "filename": "BonCurvBranchingSolver.hpp", "file": "BonCurvBranchingSolver.hpp", "brief": "Curvature-based branching solver (DEPRECATED)\nCopyright (C) 2006, 2007 International Business Machines Corporation.\nAll Rights Reserved.\n\nCurvBranchingSolver: Strong branching solver that uses curvature\nestimation to predict bound changes. Uses second-order information\nfrom the Hessian to guide variable selection.\n\nNOTE: This code is no longer supported.", "see": ["StrongBranchingSolver for base interface", "CurvatureEstimator for curvature computation"], "has_pass2": false}, "src/Algorithms/Branching/BonChooseVariable.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Branching/BonChooseVariable.hpp", "filename": "BonChooseVariable.hpp", "file": "BonChooseVariable.hpp", "brief": "Strong branching and pseudo-cost based variable selection for MINLP\n\nImplements branching variable selection for nonlinear branch-and-bound.\nCombines strong branching (solving LP/NLP relaxations) with pseudo-costs\n(estimates from historical branching information).\n\n**Key classes:**\n- BonChooseVariable: Main branching decision maker (extends OsiChooseVariable)\n- HotInfo: Stores strong branching results for a candidate\n\n**Branching decision process:**\n1. setupList(): Identify fractional variables, rank by pseudo-costs\n2. doStrongBranching(): Evaluate top candidates via LP/NLP solves\n3. chooseVariable(): Select best candidate based on objective change\n\n**Pseudo-cost computation:**\n- Estimates objective change per unit change in variable value\n- Updated after each branching decision using actual results\n- Used to avoid expensive strong branching after trust is established", "algorithm": "Reliability Branching with Strong Branching Initialization:\nHybrid strategy combining strong branching accuracy with pseudo-cost speed.\nPhase 1: Use strong branching (solve relaxations) until pseudo-costs trusted.\nPhase 2: Use pseudo-costs when reliable, strong branch only untrusted vars.", "math": "Pseudo-cost for variable x_j with value f_j (fractional part):\n- Down branch: P⁻_j = Δz⁻ / f_j (objective change per unit down)\n- Up branch: P⁺_j = Δz⁺ / (1 - f_j) (objective change per unit up)\nScore function (product): score_j = max(P⁻_j · f_j, ε) · max(P⁺_j · (1-f_j), ε)\nAlternatively, weighted sum: score_j = (1-μ) · min + μ · max\nwhere μ varies based on solution status (no incumbent vs have incumbent).", "complexity": "Strong branching: O(k · NLP) per node where k = #candidates.\nPseudo-cost only: O(n) per node for n candidates.\nReliability reduces to pseudo-cost after O(η) branches per variable\nwhere η = minReliability parameter (typically 4-8).", "ref": ["Achterberg, Koch & Martin (2005). \"Branching rules revisited\".\n  Operations Research Letters 33(1):42-54. [Reliability branching]", "Benichou et al. (1971). \"Experiments in mixed-integer linear\n  programming\". Mathematical Programming 1:76-94. [Pseudo-costs]"], "see": ["OsiChooseVariable for the base class interface", "BonPseudoCosts for pseudo-cost storage and update"], "has_pass2": true}, "src/Algorithms/OaGenerators/BonOAMessages.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOAMessages.hpp", "filename": "BonOAMessages.hpp", "file": "BonOAMessages.hpp", "brief": "Message definitions for Outer Approximation algorithms\nCopyright (C) Carnegie Mellon University 2006. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nDefines message IDs and OaMessages class for output during\nOuter Approximation cutting plane algorithms. Messages include\nNLP solve status, bound updates, and iteration statistics.\n\nAuthors: Pierre Bonami, Carnegie Mellon University\nDate: July 15, 2005", "see": ["CoinMessages for COIN message framework", "OaDecompositionBase for OA algorithm using these messages"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonFpForMinlp.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonFpForMinlp.hpp", "filename": "BonFpForMinlp.hpp", "file": "BonFpForMinlp.hpp", "brief": "Feasibility Pump for MINLP via Outer Approximation\nCopyright (C) CNRS 2008. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nMinlpFeasPump: Feasibility Pump implementation for MINLP using OA\ndecomposition. Alternates between solving MIP with distance-to-integer\nobjective and NLP with fixed integers to find feasible solutions.", "algorithm": "Feasibility Pump for MINLP (OA-based):\n  Heuristic to find MINLP-feasible solutions via alternating projections:\n  1. Solve NLP relaxation → get (x*, y*) with y* fractional integers\n  2. Set MIP objective: min Σ_j |y_j - round(y_j*)| (distance to integrality)\n  3. Solve MIP with OA cuts → get integer ŷ\n  4. Fix integers: y = ŷ, solve NLP(y) for continuous x\n  5. If feasible: done. Else: add OA cuts, goto 2\n  6. Perturb objective if cycling detected", "math": "Distance-to-integer objective:\n  Δ(y, ŷ) = Σ_j (y_j - ŷ_j)² or L1 norm Σ_j |y_j - ŷ_j|\n  For binary: Δ = Σ_{ŷ_j=0} y_j + Σ_{ŷ_j=1} (1 - y_j)\n  OA linearization: g(x*) + ∇g(x*)ᵀ(x - x*) ≤ 0", "complexity": "O(k · (MIP + NLP)) where k = iterations until feasible.\n  Typically 5-50 iterations. Each MIP uses OA approximation.", "ref": ["Bonami, Cornuéjols, Lodi, Margot (2009). \"A Feasibility Pump for\n     Mixed Integer Nonlinear Programs\". Math. Programming 119:331-352.\n\nAuthors: Pierre Bonami, CNRS\nDate: February 13, 2009"], "see": ["OaDecompositionBase for base OA framework", "PumpForMinlp for alternative FP implementation"], "has_pass2": true}, "src/Algorithms/OaGenerators/BonEcpCuts.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonEcpCuts.hpp", "filename": "BonEcpCuts.hpp", "file": "BonEcpCuts.hpp", "brief": "Extended Cutting Plane (ECP) cut generator for MINLP\n\nGenerates OA cuts iteratively at LP solution points, refining the\nlinear approximation without requiring NLP solves at every iteration.", "algorithm": "Extended Cutting Plane (ECP) Method:\nIterative linearization without explicit NLP solves:\n1. Solve LP relaxation → x^k\n2. Evaluate nonlinear constraints: vᵢ = gᵢ(x^k) for all i\n3. Find most violated: i* = argmax{vᵢ : vᵢ > 0}\n4. If max violation < abs_violation_tol_: STOP (feasible)\n5. Add OA cut at x^k for constraint i*:\n   gᵢ*(x^k) + ∇gᵢ*(x^k)ᵀ(x - x^k) ≤ 0\n6. k ← k+1; if k < numRounds_: goto step 1\n\nRandomization via beta_:\n- Skip cut with probability (1 - β^violation_count)\n- Prevents overgeneration on nearly feasible constraints", "math": "Convergence for convex constraints:\nLP optimal value z^k → NLP optimal value z* as k → ∞\n(since linear underestimators converge to convex envelope)", "complexity": "O(k · LP_solve + k · nnz(∇g)) where k = numRounds_.\nMuch cheaper than OA since no NLP solves during iterations.", "ref": ["Kelley (1960). \"The cutting-plane method for solving convex programs\".\n  Journal of the SIAM 8(4):703-712."], "see": ["OaDecompositionBase for base class", "LpBranchingSolver for use in strong branching"], "has_pass2": true}, "src/Algorithms/OaGenerators/BonDummyHeuristic.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonDummyHeuristic.hpp", "filename": "BonDummyHeuristic.hpp", "file": "BonDummyHeuristic.hpp", "brief": "Simple placeholder heuristic wrapping NLP solver\nCopyright (C) Carnegie Mellon University 2005. All Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nDummyHeuristic: Minimal heuristic that calls NLP solver to provide\nsolutions. Used for testing or as template for new heuristics.\nAlways reports ready to run (shouldHeurRun returns true).\n\nAuthors: Pierre Bonami, Carnegie Mellon University\nDate: May 26, 2005", "see": ["CbcHeuristic for base heuristic interface", "OsiTMINLPInterface for NLP solver interface"], "has_pass2": false}, "src/Algorithms/OaGenerators/BonOaNlpOptim.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOaNlpOptim.hpp", "filename": "BonOaNlpOptim.hpp", "file": "BonOaNlpOptim.hpp", "brief": "NLP-based outer approximation cut generator\n\nGenerates OA cuts by solving NLP relaxations at B&B nodes, rather than\nat integer feasible points (as in classical OA). More expensive per cut,\nbut can improve bounds at fractional nodes.", "algorithm": "NLP-based OA Cut Generation:\n  Strengthens LP relaxation by solving NLP at fractional B&B nodes:\n  1. At B&B node (depth ≤ maxDepth_): get LP solution x_LP\n  2. Solve continuous NLP relaxation: min f(x) s.t. g(x) ≤ 0\n  3. At NLP solution x*: generate OA cuts g(x*) + ∇g(x*)ᵀ(x-x*) ≤ 0\n  4. Filter: keep only cuts violated by x_LP (if addOnlyViolated_)\n  5. Add cuts as global (valid in subtree) or local", "math": "Cut validity:\n  For convex g: linearization at any x* gives valid cut\n  Not restricted to integer points like classical OA\n  More cuts = tighter LP relaxation, but more NLP solves", "complexity": "O(NLP) per node up to maxDepth_ levels.\n  Trade-off: expensive per-node work vs. fewer total nodes.\n  Controlled by maxDepth_ and solves_per_level_.", "ref": ["Bonami et al. (2008). \"An algorithmic framework for convex MINLP\".\n     Discrete Optimization 5:186-204.\n\n**Parameters:**\n- maxDepth_: Maximum tree depth for NLP solves (default 10)\n- addOnlyViolated_: Only add cuts violated by LP solution\n- global_: Add cuts globally (valid throughout tree)\n- solves_per_level_: Control NLP solve frequency"], "see": ["OaDecompositionBase for the classical OA algorithm", "CglCutGenerator for the cut generator interface"], "has_pass2": true}, "src/Algorithms/OaGenerators/BonOaFeasChecker.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOaFeasChecker.hpp", "filename": "BonOaFeasChecker.hpp", "file": "BonOaFeasChecker.hpp", "brief": "OA-based feasibility checker for MILP solutions\n\nChecks if MILP solution is feasible for original MINLP by evaluating\nnonlinear constraints and generating OA/Benders cuts if infeasible.", "algorithm": "OA Feasibility Checking:\n  Validates MILP solution against nonlinear MINLP constraints:\n  1. Receive integer solution x̄ from MILP relaxation\n  2. Evaluate g(x̄) for all nonlinear constraints g(x) ≤ 0\n  3. If g_i(x̄) > ε (constraint violated):\n     - Generate OA cut: g_i(x̄) + ∇g_i(x̄)'(x - x̄) ≤ 0\n     - Or Benders cut (alternative formulation)\n  4. Add cuts to MILP, continue B&B\n  5. Cycle detection prevents infinite OA loops", "math": "OA cut validity:\n  For convex g: g(x) ≥ g(x̄) + ∇g(x̄)'(x - x̄) (supporting hyperplane)\n  Cut separates infeasible x̄ without excluding feasible region\n  Benders alternative uses optimality cut formulation", "complexity": "O(m·n) per feasibility check\n  m = nonlinear constraints, n = variables\n  Gradient evaluation dominates; cut generation O(n)\n\n**Cut types (CutsTypes):**\n- OA: Standard outer approximation cuts (gradient-based)\n- Benders: Benders optimality/feasibility cuts\n\n**Cut discard policies (CutsPolicies):**\n- DetectCycles: Detect and handle cycling\n- KeepAll: Keep all generated cuts\n- TreatAsNormal: Standard cut management", "see": ["OaDecompositionBase for base class", "OACutGenerator2 for main OA cut generator"], "has_pass2": true}, "src/Algorithms/OaGenerators/BonOaDecBase.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOaDecBase.hpp", "filename": "BonOaDecBase.hpp", "file": "BonOaDecBase.hpp", "brief": "Base class for Outer Approximation (OA) decomposition algorithms\n\nImplements the foundation for OA-based MINLP algorithms. OA iterates between\nsolving MILP subproblems and NLP subproblems, generating linear outer\napproximations of the nonlinear constraints.\n\n**OA algorithm outline:**\n1. Solve MILP relaxation → get integer solution x*\n2. Fix integers to x*, solve NLP → get nonlinear solution y*\n3. Generate linearization cuts at y* (gradient-based)\n4. Add cuts to MILP, repeat until convergence\n\n**Key components:**\n- solverManip: RAII helper to save/restore solver state\n- performOa(): Virtual method implementing specific OA variant\n- post_nlp_solve(): Handle NLP solution and update bounds\n\n**Parameters:**\n- global_: Add cuts globally (valid at all nodes)\n- addOnlyViolated_: Only add cuts violated by current solution\n- maxLocalSearch_: Limit on local search iterations", "algorithm": "Outer Approximation (OA) Decomposition:\nBenders-style decomposition for convex MINLP. Master problem is MILP\ncontaining linearizations of nonlinear constraints. Subproblem is NLP\nwith integers fixed. Iterate until master and sub agree.\n\nKey insight: For convex MINLP, linearization at any feasible point\nprovides valid outer approximation. Accumulating cuts from multiple\nNLP solves progressively tightens the MILP approximation.", "math": "Given convex MINLP: min f(x,y) s.t. g(x,y) ≤ 0, y ∈ {0,1}\n\nOA Master (MILP): min η s.t. η ≥ f(x^k) + ∇f(x^k)ᵀ(x-x^k) ∀k\n                           0 ≥ g(x^k) + ∇g(x^k)ᵀ(x-x^k) ∀k\n\nNLP Sub (y=y*): min f(x,y*) s.t. g(x,y*) ≤ 0\n\nConvergence: For convex problems, finite ε-convergence. Each NLP solve\neither provides improving incumbent or cut proving master solution\ninfeasible. MILP bound increases monotonically.", "complexity": "#iterations typically small (tens to hundreds).\nPer iteration: O(MILP) + O(NLP). MILP grows with accumulated cuts.\nTotal: problem-dependent, but often beats pure B&B for structured problems.", "ref": ["Duran & Grossmann (1986). \"An outer-approximation algorithm for\n  a class of mixed-integer nonlinear programs\". Math. Prog. 36:307-339.", "Fletcher & Leyffer (1994). \"Solving mixed integer nonlinear programs\n  by outer approximation\". Math. Prog. 66:327-349.", "Quesada & Grossmann (1992). \"An LP/NLP based branch and bound\n  algorithm for convex MINLP\". Comp. & Chem. Eng. 16:937-947."], "see": ["OACutGenerator2 for classical OA implementation", "BonOaNlpOptim for NLP-based OA variant", "CglCutGenerator for the cut generator interface"], "has_pass2": true}, "src/Algorithms/OaGenerators/BonOACutGenerator2.hpp": {"path": "layer-3/Bonmin/src/Algorithms/OaGenerators/BonOACutGenerator2.hpp", "filename": "BonOACutGenerator2.hpp", "file": "BonOACutGenerator2.hpp", "brief": "Classical Outer Approximation cut generator for MINLP\n\nImplements the standard OA algorithm of Duran & Grossmann (1986) for convex\nMINLP. Generates linearization cuts by Taylor expansion of nonlinear\nconstraints at NLP solutions.", "algorithm": "Outer Approximation (Duran-Grossmann):\nDecomposition method for convex MINLP via MILP master + NLP subproblems:\n1. Solve NLP relaxation: y⁰ = argmin{f(x): g(x) ≤ 0, x ∈ X}\n2. Generate OA cuts for each constraint i:\n   gᵢ(y⁰) + ∇gᵢ(y⁰)ᵀ(x - y⁰) ≤ 0  (linearization)\n3. Solve MILP master: min{cᵀx : OA cuts, x_I ∈ Z}\n4. Fix integers at x*: solve NLP(x*_I) → new point y*\n5. Add OA cuts at y*, update upper/lower bounds\n6. Terminate when UB - LB < ε or iteration limit", "math": "OA cut derivation (convex g):\nFor convex gᵢ: gᵢ(x) ≥ gᵢ(y) + ∇gᵢ(y)ᵀ(x-y)  (first-order condition)\nCut: ∇gᵢ(y)ᵀx ≤ ∇gᵢ(y)ᵀy - gᵢ(y)  (valid for feasible region)", "complexity": "O(k·(MILP + NLP)) where k = iterations.\nFinite convergence for convex MINLP. Typically k = O(2^|I|) worst case,\nbut often polynomial in practice for well-structured problems.", "ref": ["Duran, Grossmann (1986). \"An outer-approximation algorithm for a class\n  of mixed-integer nonlinear programs\". Mathematical Programming 36:307-339."], "see": ["OaDecompositionBase for the base class", "SubMipSolver for MILP subproblem solving"], "has_pass2": true}, "src/Algorithms/Ampl/BonAmplSetup.hpp": {"path": "layer-3/Bonmin/src/Algorithms/Ampl/BonAmplSetup.hpp", "filename": "BonAmplSetup.hpp", "file": "BonAmplSetup.hpp", "brief": "AMPL-specific Bonmin setup and initialization\n\nExtends BonminSetup to initialize from AMPL command-line arguments\nand .nl files. Handles reading options and model from AMPL.\n\n**Initialization methods:**\n- initialize(argv): Read from command line and files\n- initialize(argv, opt_content, nl_content): Read from strings (for Bcp)\n- fillOsiInterface(): Initialize existing interface from strings", "see": ["BonminSetup for algorithm configuration", "AmplInterface for the solver interface"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonOuterApprox.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonOuterApprox.hpp", "filename": "BonOuterApprox.hpp", "file": "BonOuterApprox.hpp", "brief": "Concrete outer approximation extractor for MINLP\n\nBuilds linear outer approximations of nonlinear constraints by\nlinearizing at a given point (typically NLP solution).", "algorithm": "Outer Approximation Linearization:\n  Constructs LP relaxation of convex MINLP via first-order Taylor:\n  1. At point x*, evaluate nonlinear constraint g(x)\n  2. Compute gradient ∇g(x*) via automatic differentiation\n  3. Add linearization: g(x*) + ∇g(x*)ᵀ(x - x*) ≤ 0\n  4. Clean coefficients: remove tiny coefficients, absorb into bounds\n  5. Repeat for each nonlinear constraint", "math": "First-order Taylor approximation:\n  For convex g: g(x) ≥ g(x*) + ∇g(x*)ᵀ(x - x*) (supporting hyperplane)\n  Linearization is valid cut: {x : g(x*) + ∇g(x*)ᵀ(x-x*) ≤ 0} ⊇ {x : g(x) ≤ 0}\n  For non-convex: linearization is relaxation, not exact", "complexity": "O(n) per constraint linearization where n = variables.\n  Gradient computation via AD is O(n) for each constraint.\n  Total O(m·n) for m constraints.", "ref": ["Duran & Grossmann (1986). \"An outer-approximation algorithm for a\n     class of mixed-integer nonlinear programs\". Math. Programming 36:307-339.\n\n**Coefficient cleaning (cleanNnz):**\nSmall coefficients are handled to avoid numerical issues:\n- >= tiny_: Keep as-is\n- < veryTiny_: Remove (absorb into RHS if possible)\n- Between: Try to absorb, else bump to tiny_"], "see": ["TMINLP2OsiLP for abstract base class", "BonOaDecBase for OA algorithm driver"], "has_pass2": true}, "src/Algorithms/QuadCuts/BonLinearCutsGenerator.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonLinearCutsGenerator.hpp", "filename": "BonLinearCutsGenerator.hpp", "file": "BonLinearCutsGenerator.hpp", "brief": "Composite cut generator managing multiple linear cut generators\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.", "algorithm": "Frequency Control:\nBalance cut generation cost vs. bound improvement.\n\n  frequency = 1: Every node (expensive, tight bounds)\n  frequency = 10: Every 10 nodes (cheaper, weaker bounds)\n  frequency = 0: Disabled\n  frequency = -1: Root node only\n\nCommon frequency settings:\n  GMI cuts: frequent (usually 1-5)\n  MIR cuts: moderate (5-10)\n  Lift-and-project: rare (10-100, expensive)", "complexity": "O(sum over generators of per-generator complexity)\nTotal work bounded by sum of individual generator complexities\nat their respective frequencies.\n\nAuthors: Pierre Bonami, IBM\nDate: October 6, 2007", "see": ["CglCutGenerator for individual cut generator interface", "BabSetupBase for cut generator configuration"], "has_pass2": true}, "src/Algorithms/QuadCuts/BonQuadRow.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonQuadRow.hpp", "filename": "BonQuadRow.hpp", "file": "BonQuadRow.hpp", "brief": "Quadratic row storage for NLP solver interface\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nQuadRow: Stores quadratic constraint l < c + ax + x'Qx < u with\nefficient gradient and Hessian evaluation. Provides methods for\nNLP solver interface: eval_f, eval_grad, eval_hessian.\n\nCan be initialized from QuadCut or linear OsiRowCut.\n\nAuthors: Pierre Bonami, IBM\nDate: October 6, 2007", "see": ["QuadCut for quadratic cut representation", "TMat for sparse triangular matrix storage"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonQuadCut.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonQuadCut.hpp", "filename": "BonQuadCut.hpp", "file": "BonQuadCut.hpp", "brief": "Quadratic cutting plane for MINLP\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nQuadCut: Extends OsiRowCut to include quadratic term x'Qx.\nStores c + ax + x'Qx with upper/lower storage modes for Q.\n\nCuts: Collection class extending OsiCuts to manage QuadCut objects\nalongside standard linear cuts.\n\nAuthors: Pierre Bonami, IBM\nDate: October 6, 2007", "see": ["OsiRowCut for linear cut base class", "QuadRow for quadratic constraint in NLP format"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonTMINLPLinObj.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonTMINLPLinObj.hpp", "filename": "BonTMINLPLinObj.hpp", "file": "BonTMINLPLinObj.hpp", "brief": "TMINLP adapter that linearizes the objective function\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nTMINLPLinObj: Transforms a TMINLP by moving the objective into a\nconstraint, adding epigraph variable η. Converts min f(x) to\nmin η s.t. f(x) - η ≤ 0. Useful for algorithms requiring linear\nobjectives (e.g., some OA variants).\n\nAuthors: Pierre Bonami, IBM\nDate: August 16, 2007", "see": ["TMINLP for base interface", "OaDecompositionBase for OA algorithms using this transformation"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonTMatrix.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonTMatrix.hpp", "filename": "BonTMatrix.hpp", "file": "BonTMatrix.hpp", "brief": "Sparse triangular matrix storage for quadratic forms\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nTMat: Sparse matrix storage in triplet format (iRow, jCol, value).\nProvides ordering by rows/columns, duplicate removal, and conversion\nto upper triangular form for quadratic forms x'Qx.\n\nUsed for efficient gradient and Hessian computation in QuadRow.\n\nAuthors: Pierre Bonami, IBM\nDate: October 6, 2007", "see": ["QuadRow for quadratic constraint using TMat", "CoinPackedMatrix for conversion source"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonArraysHelpers.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonArraysHelpers.hpp", "filename": "BonArraysHelpers.hpp", "file": "BonArraysHelpers.hpp", "brief": "Array resize and copy utilities for Bonmin\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nTemplate functions for resizing arrays while preserving content:\n- resizeAndCopyArray: Resize and copy up to min(old,new) elements\n- resizeAndCopyArray with capacity: Only reallocate if needed\n\nAuthors: Pierre Bonami, IBM\nDate: October 6, 2007", "see": ["TMat for primary user of these utilities"], "has_pass2": false}, "src/Algorithms/QuadCuts/BonTMINLP2Quad.hpp": {"path": "layer-3/Bonmin/src/Algorithms/QuadCuts/BonTMINLP2Quad.hpp", "filename": "BonTMINLP2Quad.hpp", "file": "BonTMINLP2Quad.hpp", "brief": "TMINLP2TNLP extension supporting quadratic cuts\nCopyright (C) International Business Machines Corporation 2007.\nAll Rights Reserved.\nThis code is published under the Eclipse Public License.\n\nTMINLP2TNLPQuadCuts: Extends TMINLP2TNLP to handle quadratic cuts\nin the NLP subproblem. Manages adding/removing cuts, updating\nJacobian and Hessian structures, and evaluating cut contributions.\n\nSupports both linear (OsiRowCut) and quadratic (QuadCut) cuts.\n\nAuthors: Pierre Bonami, IBM\nDate: October 6, 2007", "see": ["TMINLP2TNLP for base NLP adapter", "QuadRow for quadratic cut evaluation"], "has_pass2": false}, "experimental/RobotBonmin/BonNWayObject.hpp": {"path": "layer-3/Bonmin/experimental/RobotBonmin/BonNWayObject.hpp", "filename": "BonNWayObject.hpp", "file": "BonNWayObject.hpp", "brief": "Bonmin N-way branching object\n\nN-way branching object for robot planning applications.\nCreates multi-way branches instead of binary left/right.", "has_pass2": false}, "experimental/RobotBonmin/RobotSetup.hpp": {"path": "layer-3/Bonmin/experimental/RobotBonmin/RobotSetup.hpp", "filename": "RobotSetup.hpp", "file": "RobotSetup.hpp", "brief": "Bonmin robot planning setup\n\nConfiguration for robot path planning MINLP application.\nSets up Bonmin solver for robotics optimization problems.", "has_pass2": false}, "experimental/RobotBonmin/BonNWayChoose.hpp": {"path": "layer-3/Bonmin/experimental/RobotBonmin/BonNWayChoose.hpp", "filename": "BonNWayChoose.hpp", "file": "BonNWayChoose.hpp", "brief": "Bonmin N-way branching choice\n\nN-way branching variable selection for robot path planning.\nExperimental: selects among multiple branching candidates.", "has_pass2": false}, "experimental/Bcp/BB_cut.hpp": {"path": "layer-3/Bonmin/experimental/Bcp/BB_cut.hpp", "filename": "BB_cut.hpp", "file": "BB_cut.hpp", "brief": "Bonmin BCP cutting plane interface\n\nCutting plane interface for Bonmin integrated with BCP framework.\nExperimental: combines branch-cut-price with MINLP solving.", "has_pass2": false}, "experimental/Bcp/BM.hpp": {"path": "layer-3/Bonmin/experimental/Bcp/BM.hpp", "filename": "BM.hpp", "file": "BM.hpp", "brief": "Bonmin BCP main interface\n\nMain interface for experimental Bonmin-BCP integration.\nCombines branch-cut-price parallel framework with MINLP.", "has_pass2": false}, "experimental/Separable/SepaHeuristicInnerApproximation.hpp": {"path": "layer-3/Bonmin/experimental/Separable/SepaHeuristicInnerApproximation.hpp", "filename": "SepaHeuristicInnerApproximation.hpp", "file": "SepaHeuristicInnerApproximation.hpp", "brief": "Separable MINLP inner approximation\n\nInner approximation heuristic for separable MINLP problems.\nExploits problem structure for better approximations.", "has_pass2": false}, "experimental/Separable/BonHeuristicInnerApproximation.hpp": {"path": "layer-3/Bonmin/experimental/Separable/BonHeuristicInnerApproximation.hpp", "filename": "BonHeuristicInnerApproximation.hpp", "file": "BonHeuristicInnerApproximation.hpp", "brief": "Bonmin inner approximation heuristic for MINLP\n\nPrimal heuristic using inner approximation of feasible region.\nGenerates feasible MINLP solutions from LP relaxations.", "algorithm": "Inner Approximation Heuristic:\n  Constructs feasible MINLP solutions by solving restricted MILPs:\n  1. Build polyhedral inner approximation of nonlinear feasible region\n  2. Sample points on convex hull of nonlinear constraints\n  3. Solve MILP on inner approximation to get integer-feasible point\n  4. Project MILP solution to NLP feasible region via local NLP solve\n  5. Return best feasible solution found", "math": "Inner approximation construction:\n  For convex g(x) ≤ 0: sample boundary points x_1,...,x_k with g(x_i) = 0\n  Inner approx: conv{x_1,...,x_k} ⊆ {x : g(x) ≤ 0}\n  MILP feasible → can project to NLP feasible (convexity)", "complexity": "O(MILP_solve + k·NLP_project) per heuristic call.\n  k = number of sample points for inner approximation.\n  Typically called at root node and periodically during B&B.", "ref": ["Bonami et al. (2008). \"An algorithmic framework for convex mixed integer\n     nonlinear programs\". Discrete Optimization 5(2):186-204."], "see": ["HeuristicInnerApproximation::solution for main heuristic method", "BonOuterDescription for complementary outer approximation"], "has_pass2": true}, "experimental/Separable/SepaTMINLP2OsiLP.hpp": {"path": "layer-3/Bonmin/experimental/Separable/SepaTMINLP2OsiLP.hpp", "filename": "SepaTMINLP2OsiLP.hpp", "file": "SepaTMINLP2OsiLP.hpp", "brief": "Separable MINLP to LP conversion\n\nConverts separable TMINLP to Osi LP relaxation.\nCreates piecewise linear approximations for separable functions.", "has_pass2": false}, "experimental/Separable/SepaSetup.hpp": {"path": "layer-3/Bonmin/experimental/Separable/SepaSetup.hpp", "filename": "SepaSetup.hpp", "file": "SepaSetup.hpp", "brief": "Separable MINLP setup\n\nConfiguration for separable MINLP solver variant.\nSpecialized setup exploiting separability in objective/constraints.", "has_pass2": false}, "experimental/Separable/BonOuterDescription.hpp": {"path": "layer-3/Bonmin/experimental/Separable/BonOuterDescription.hpp", "filename": "BonOuterDescription.hpp", "file": "BonOuterDescription.hpp", "brief": "Bonmin outer approximation description for convex MINLP\n\nOuter approximation (OA) cut management for MINLP.\nLinearization-based approach for convex MINLP.", "algorithm": "Outer Approximation (OA) Cut Generation:\n  Linearizes nonlinear constraints at solution points:\n  1. Given NLP solution x*, for each nonlinear constraint g_i(x) ≤ 0:\n     Generate gradient cut: g_i(x*) + ∇g_i(x*)ᵀ(x - x*) ≤ 0\n  2. For objective f(x): linearize as f(x*) + ∇f(x*)ᵀ(x - x*) ≤ θ\n  3. Add cuts to MILP relaxation and resolve\n  4. Iterate until convergence (gap < tolerance)", "math": "OA cut validity (convexity required):\n  For convex g_i: g_i(x) ≥ g_i(x*) + ∇g_i(x*)ᵀ(x - x*) for all x\n  Cut is valid: any x violating g_i(x) ≤ 0 also violates linearization.\n  Multiple linearization points → polyhedral outer approximation.", "complexity": "O(m·n) per cut generation where m = constraints, n = variables.\n  Total iterations: typically O(log(1/ε)) for ε-optimal solution.", "ref": ["Duran & Grossmann (1986). \"An outer-approximation algorithm for a class\n     of mixed-integer nonlinear programs\". Math. Programming 36:307-339."], "see": ["OsiTMINLPInterface for NLP interface", "BonCutStrengthener for cut improvement"], "has_pass2": true}}}, "CHiPPS-ALPS": {"name": "CHiPPS-ALPS", "file_count": 21, "pass2_count": 0, "files": {"src/AlpsKnowledgeBrokerMPI.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgeBrokerMPI.h", "filename": "AlpsKnowledgeBrokerMPI.h", "file": "AlpsKnowledgeBrokerMPI.h", "brief": "MPI-based parallel implementation of ALPS knowledge broker\n\nAlpsKnowledgeBrokerMPI provides scalable parallel tree search using MPI.\nImplements hierarchical Master-Hub-Worker architecture.\n\n**Process hierarchy:**\n- Master (rank 0): Inter-cluster load balancing, termination detection\n- Hubs: Manage worker clusters, intra-cluster load balancing\n- Workers: Process subtrees, report status to hub\n\n**Core methods per process type:**\n- masterMain(): Balance hubs, check termination\n- hubMain(): Balance workers, report to master\n- workerMain(): Explore subtrees, request work\n\n**Load balancing:**\n- Inter-cluster: Master moves work between hubs\n- Intra-cluster: Hub moves work between workers\n- Work donation: Split subtrees when requested\n\n**Static initialization schemes:**\n- Root initialization: Master generates nodes, distributes\n- Spiral: Distribute initial work in spiral pattern\n\n**MPI communicators:**\n- MPI_COMM_WORLD: All processes\n- clusterComm_: Hub and its workers\n- hubComm_: Master and all hubs", "see": ["AlpsKnowledgeBroker for base class", "AlpsKnowledgeBrokerSerial for single-process version", "AlpsParams for parallel parameters (hubNum, etc.)"], "has_pass2": false}, "src/AlpsParameterBase.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsParameterBase.h", "filename": "AlpsParameterBase.h", "file": "AlpsParameterBase.h", "brief": "Generic parameter management infrastructure for ALPS\n\nBase classes for defining and parsing typed parameter sets.\nDerived from BCP_parameters.hpp design.\n\n**AlpsParameterT enum:**\n- AlpsBoolPar, AlpsIntPar, AlpsDoublePar, AlpsStringPar\n- AlpsStringArrayPar: Multiple values per key\n\n**AlpsParameter class:**\nIdentifies a single parameter by type and index within type.\n\n**AlpsParameterSet class:**\nContainer for typed parameter arrays with file parsing.\nSubclasses must implement:\n- createKeywordList(): Define keyword→parameter mappings\n- setDefaultEntries(): Set default values\n\n**Reading parameters:**\n- readFromFile(): Parse parameter file\n- readFromStream(): Parse from istream\n- readFromArglist(): Parse command-line args", "see": ["AlpsParams for ALPS-specific parameter set", "BcpsParams, BlisParams for higher-level parameters"], "has_pass2": false}, "src/Alps.h": {"path": "layer-3/CHiPPS-ALPS/src/Alps.h", "filename": "Alps.h", "file": "Alps.h", "brief": "Core definitions for ALPS parallel tree search framework\n\nCentral header defining enumerations, constants, and types for the\nAbstract Library for Parallel Search (ALPS).\n\n**Core enumerations:**\n- AlpsNodeStatus: Node states (Candidate, Evaluated, Pregnant, Branched, Fathomed)\n- AlpsSearchType: Search strategies (BestFirst, BreadthFirst, DepthFirst, Hybrid)\n- AlpsKnowledgeType: Knowledge categories (Model, Node, Solution, SubTree)\n- AlpsExitStatus: Termination status (Optimal, TimeLimit, NodeLimit, etc.)\n- AlpsPhase: Execution phase (Rampup, Search, Rampdown)\n\n**Key constants:**\n- ALPS_OBJ_MAX, ALPS_BND_MAX: Numerical bounds\n- ALPS_ZERO, ALPS_GEN_TOL: Numerical tolerances\n\n**Design philosophy:**\n- Subtree is the basic unit of work in parallel execution\n- Workers process subtrees autonomously\n- Hub brokers coordinate work distribution", "see": ["AlpsKnowledgeBroker for central search management", "AlpsTreeNode for node abstraction", "AlpsModel for problem definition"], "has_pass2": false}, "src/AlpsKnowledgePool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgePool.h", "filename": "AlpsKnowledgePool.h", "file": "AlpsKnowledgePool.h", "brief": "Abstract base class for knowledge storage pools\n\nAlpsKnowledgePool defines the API for all pool types in ALPS.\nKnowledge is stored with associated priority values for ordering.\n\n**Pool types (AlpsKnowledgePoolType):**\n- NodePool: Stores tree nodes awaiting processing\n- SolutionPool: Stores feasible solutions found\n- SubTreePool: Stores subtrees for parallel distribution\n\n**Core API:**\n- addKnowledge(kl, priority): Add with ordering priority\n- getKnowledge(): Peek at highest-priority item\n- popKnowledge(): Remove highest-priority item\n- getBestKnowledge(): Get best quality item\n- hasKnowledge(): Check if pool is non-empty", "see": ["AlpsNodePool for node storage", "AlpsSolutionPool for solution storage", "AlpsSubTreePool for subtree storage"], "has_pass2": false}, "src/AlpsModel.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsModel.h", "filename": "AlpsModel.h", "file": "AlpsModel.h", "brief": "Abstract base class for user problem data in ALPS tree search\n\nAlpsModel holds problem-specific data (constraints, objectives, variables).\nUsers inherit this class and implement virtual methods for their application.\n\n**Key virtual methods to implement:**\n- readInstance(): Read problem from file (master process only)\n- setupSelf(): Initialize model after data loaded/received\n- preprocess(): Preprocessing before search\n- createRoot(): Create the root tree node\n- encode()/decodeToSelf(): Serialize for parallel distribution\n\n**Parallel execution flow:**\n1. Master: readInstance() → setupSelf() → preprocess() → createRoot()\n2. Master: encode() model and send to workers\n3. Workers: decodeToSelf() → setupSelf() → begin processing nodes\n4. All: postprocess() after search completes", "see": ["AlpsKnowledge for serialization interface", "AlpsTreeNode for node processing", "AlpsKnowledgeBroker for search orchestration"], "has_pass2": false}, "src/AlpsEncoded.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsEncoded.h", "filename": "AlpsEncoded.h", "file": "AlpsEncoded.h", "brief": "Serialization buffer for ALPS knowledge objects\n\nAlpsEncoded is the binary buffer for packing/unpacking AlpsKnowledge\nobjects for network transmission in parallel search.\n\n**Buffer structure:**\n- type_: Integer identifying the knowledge type\n- size_: Current data size in bytes\n- representation_: Raw byte buffer\n- pos_: Current read position\n\n**Template methods for serialization:**\n- writeRep(T): Write single value\n- readRep(T): Read single value\n- writeRep(T*, len): Write array with length prefix\n- readRep(T*, len): Read array\n- writeRep/readRep for std::string, std::vector\n\n**Memory management:**\n- Default: 16KB initial allocation\n- make_fit(): Grows buffer as needed (4x growth)\n- clear(): Reset buffer to empty state\n\nBased on BCP_buffer and CoinEncoded designs.", "see": ["AlpsKnowledge::encode()/decode() for usage", "AlpsKnowledgeBroker::registerClass() for type registration"], "has_pass2": false}, "src/AlpsKnowledgeBrokerSerial.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgeBrokerSerial.h", "filename": "AlpsKnowledgeBrokerSerial.h", "file": "AlpsKnowledgeBrokerSerial.h", "brief": "Serial (single-process) implementation of ALPS knowledge broker\n\nAlpsKnowledgeBrokerSerial provides single-threaded tree search.\nUse this for debugging or when parallel execution is not needed.\n\n**Usage:**\n@code\nMyModel model;\nAlpsKnowledgeBrokerSerial broker(argc, argv, model);\nbroker.search(&model);\nbroker.printBestSolution();\n@endcode\n\n**Key methods:**\n- initializeSearch(): Read parameters and problem data\n- rootSearch(): Execute tree search from root\n- searchLog(): Print search statistics\n- printBestSolution(): Output best solution found\n\n**Differences from MPI version:**\n- Single subtree pool (no distribution)\n- No message passing or load balancing\n- All phases (rampup/search/rampdown) in one process", "see": ["AlpsKnowledgeBroker for base class", "AlpsKnowledgeBrokerMPI for parallel version"], "has_pass2": false}, "src/AlpsTreeNode.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsTreeNode.h", "filename": "AlpsTreeNode.h", "file": "AlpsTreeNode.h", "brief": "Search tree node abstraction for ALPS parallel tree search\n\nAlpsTreeNode represents a single node in the search tree. Users inherit\nthis class to implement application-specific node processing.\n\n**Node status lifecycle (AlpsNodeStatus):**\n- Candidate: Waiting in pool for processing\n- Evaluated: Processed, waiting for branching decision\n- Pregnant: Ready to branch, has child descriptions\n- Branched: Children created and added to pool\n- Fathomed: Pruned (by bound, infeasibility, or solution)\n\n**Key virtual methods:**\n- process(): Perform bounding operation (e.g., LP solve)\n- branch(): Create child nodes from pregnant node\n- createNewTreeNode(): Factory for child node creation\n\n**Node data:**\n- quality_: Node quality for selection (lower = better)\n- solEstimate_: Estimated solution quality\n- desc_: AlpsNodeDesc holding problem-specific data\n- explicit_: 1=full description, 0=relative/diff", "see": ["AlpsNodeDesc for node description", "AlpsSubTree for subtree management", "AlpsNodePool for node storage"], "has_pass2": false}, "src/AlpsSubTree.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSubTree.h", "filename": "AlpsSubTree.h", "file": "AlpsSubTree.h", "brief": "Subtree management for ALPS parallel tree search\n\nAlpsSubTree is the basic unit of work in parallel ALPS. Workers process\nentire subtrees autonomously, enabling scalable parallelism.\n\n**Key components:**\n- root_: Root node of this subtree\n- nodePool_: Leaf nodes awaiting processing\n- diveNodePool_: Nodes for diving exploration\n- activeNode_: Currently being processed\n\n**Core operations:**\n- exploreSubTree(): Process nodes up to limits\n- exploreUnitWork(): Process bounded amount of work\n- createChildren(): Branch and add children to pool\n- splitSubTree(): Split off portion for redistribution\n- rampUp(): Generate initial nodes for parallel start\n\n**Diving strategy:**\n- diveNodePool_ holds nodes for deep exploration\n- diveDepth_ tracks current dive depth\n- Helps find feasible solutions quickly\n\n**Dead node removal:**\n- removeDeadNodes(): Recursively remove fathomed branches\n- fathomAllNodes(): Clear entire subtree", "see": ["AlpsTreeNode for individual nodes", "AlpsNodePool for node storage", "AlpsSubTreePool for subtree collection"], "has_pass2": false}, "src/AlpsNodePool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsNodePool.h", "filename": "AlpsNodePool.h", "file": "AlpsNodePool.h", "brief": "Priority queue for tree nodes awaiting processing\n\nAlpsNodePool stores candidate nodes ordered by search strategy.\nUses AlpsPriorityQueue with AlpsSearchStrategy comparison.\n\n**Storage:**\n- candidateList_: Priority queue of AlpsTreeNode pointers\n- searchStrategy_: Comparison function for ordering\n\n**Key operations:**\n- addKnowledge(): Add node with quality as priority\n- getKnowledge()/popKnowledge(): Access/remove best node\n- setNodeSelection(): Change search strategy (re-heaps)\n- getBestNode(): Get node with best quality\n\n**Search strategies supported:**\n- BestFirst: Lowest quality value first\n- DepthFirst: Deepest node first\n- BreadthFirst: Shallowest node first\n- BestEstimate: Best estimated solution first", "see": ["AlpsKnowledgePool for base class", "AlpsPriorityQueue for heap implementation", "AlpsSearchStrategy for comparison classes"], "has_pass2": false}, "src/AlpsMessage.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsMessage.h", "filename": "AlpsMessage.h", "file": "AlpsMessage.h", "brief": "Log messages for ALPS search status and events\n\nDefines ALPS_Message enum for CoinMessageHandler logging.\nNOT the MPI message tags (see AlpsMessageTag.h).\n\n**Message categories:**\n- ALPS_DONATE_*: Work donation events\n- ALPS_LOADBAL_*: Load balancing status\n- ALPS_RAMPUP_*: Initial distribution phase\n- ALPS_TERM_*: Termination detection\n- ALPS_S_*: Serial-only messages\n- ALPS_T_*: Termination reasons\n\n**AlpsMessage class:**\nExtends CoinMessages with ALPS-specific message definitions.\nUsed with CoinMessageHandler for consistent logging.", "see": ["CoinMessageHandler for message output control", "AlpsParams::msgLevel for verbosity settings", "AlpsMessageTag.h for MPI message tags"], "has_pass2": false}, "src/AlpsKnowledge.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledge.h", "filename": "AlpsKnowledge.h", "file": "AlpsKnowledge.h", "brief": "Base class for all ALPS knowledge types (Model, Node, Solution, SubTree)\n\nAlpsKnowledge is the abstract base for all sharable data in parallel search.\nDerived classes must implement encode()/decode() for serialization.\n\n**Knowledge types (AlpsKnowledgeType):**\n- Model: Problem data shared at initialization\n- Node: Search tree nodes processed by workers\n- Solution: Feasible solutions found during search\n- SubTree: Collections of nodes for work distribution\n\n**Serialization for parallel execution:**\n- encode(): Pack object into AlpsEncoded buffer\n- decode(): Create new object from AlpsEncoded\n- decodeToSelf(): Unpack into existing object\n\nSimple contiguous classes can use default encode/decode.\nClasses with pointers or STL containers must override.", "see": ["AlpsEncoded for serialization buffer", "AlpsKnowledgeBroker for knowledge management", "AlpsModel, AlpsTreeNode, AlpsSolution for concrete types"], "has_pass2": false}, "src/AlpsSolution.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSolution.h", "filename": "AlpsSolution.h", "file": "AlpsSolution.h", "brief": "Base class for solutions found during ALPS tree search\n\nAlpsSolution is the abstract base class for feasible solutions.\nUsers inherit this class to store application-specific solution data.\n\n**Base class data:**\n- index_: Node index where solution was found\n- depth_: Tree depth where solution was found\n\n**Key virtual methods:**\n- print(): Output solution to stream\n- encode()/decodeToSelf(): Serialize for parallel sharing\n\nSolutions are stored in AlpsSolutionPool with their quality values.\nThe incumbent (best solution) is tracked and used for pruning.", "see": ["AlpsSolutionPool for solution storage", "AlpsKnowledge for serialization interface", "AlpsKnowledgeBroker::getBestKnowledge() for retrieving best solution"], "has_pass2": false}, "src/AlpsSolutionPool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSolutionPool.h", "filename": "AlpsSolutionPool.h", "file": "AlpsSolutionPool.h", "brief": "Storage for feasible solutions found during search\n\nAlpsSolutionPool stores solutions ordered by quality (priority).\nLower priority value = better solution (for minimization).\n\n**Storage:**\n- solutions_: multimap<double, AlpsSolution*> ordered by quality\n- maxNumSolutions_: Maximum solutions to retain\n\n**Key operations:**\n- addKnowledge(): Add solution, possibly discard worst if at limit\n- getBestKnowledge(): Get solution with lowest priority (best)\n- getAllKnowledges(): Get all solutions for output\n- clean(): Delete all solutions\n\n**AlpsSolutionInterface macro:**\nConvenience macro defining getNumSolutions, getBestSolution, etc.\nfor use in classes that contain a solution pool.", "see": ["AlpsSolution for solution base class", "AlpsKnowledgePool for base interface", "AlpsKnowledgeBroker::solPool_ for broker's solution pool"], "has_pass2": false}, "src/AlpsSubTreePool.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSubTreePool.h", "filename": "AlpsSubTreePool.h", "file": "AlpsSubTreePool.h", "brief": "Pool of subtrees for work distribution in parallel search\n\nAlpsSubTreePool stores subtrees for load balancing between processes.\nKey component for parallel scalability in ALPS.\n\n**Storage:**\n- subTreeList_: Priority queue of AlpsSubTree pointers\n- Ordering by subtree quality for work distribution\n\n**Key operations:**\n- addKnowledge(): Add subtree from work donation\n- getKnowledge()/popKnowledge(): Get subtree for processing\n- setComparison(): Set subtree selection strategy\n- getBestQuality(): Get quality of best subtree\n\n**Parallel use:**\n- Serial: Single subtree in pool\n- Parallel: Multiple subtrees for load balancing\n- Subtrees split/merged for work sharing", "see": ["AlpsSubTree for subtree structure", "AlpsKnowledgePool for base interface", "AlpsKnowledgeBroker::subTreePool_ for broker's pool"], "has_pass2": false}, "src/AlpsKnowledgeBroker.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsKnowledgeBroker.h", "filename": "AlpsKnowledgeBroker.h", "file": "AlpsKnowledgeBroker.h", "brief": "Central manager for parallel tree search knowledge and coordination\n\nAlpsKnowledgeBroker is the abstract base class for search coordination.\nIt manages knowledge pools, search strategy, and execution statistics.\n\n**Key responsibilities:**\n- Manage SubTreePool and SolutionPool\n- Register knowledge types for serialization\n- Track search statistics (nodes processed, branched, etc.)\n- Apply search strategy (node/tree selection)\n\n**Knowledge pool management:**\n- addKnowledge()/getKnowledge(): Store/retrieve knowledge\n- registerClass(): Register decode functions for parallel\n- decoderObject(): Get decoder for knowledge type\n\n**Search phases (AlpsPhase):**\n- Rampup: Initial node generation and distribution\n- Search: Main parallel exploration\n- Rampdown: Collect results and terminate\n\n**Implementations:**\n- AlpsKnowledgeBrokerSerial: Single-process execution\n- AlpsKnowledgeBrokerMPI: MPI-based parallel execution", "see": ["AlpsKnowledgeBrokerSerial for serial implementation", "AlpsKnowledgeBrokerMPI for parallel implementation", "AlpsSearchStrategy for node/tree selection"], "has_pass2": false}, "src/AlpsNodeDesc.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsNodeDesc.h", "filename": "AlpsNodeDesc.h", "file": "AlpsNodeDesc.h", "brief": "Node description holding subproblem-specific data\n\nAlpsNodeDesc holds problem data for a tree node separate from tree\nstructure data. Users inherit this class for application-specific data.\n\n**Design rationale:**\n- AlpsTreeNode: Tree structure (parent, children, status)\n- AlpsNodeDesc: Subproblem data (bounds, constraints, etc.)\n\nThis separation simplifies parallel communication: only the\ndescription needs to be encoded for node transfer.\n\n**Representation modes:**\n- Explicit: Full problem data stored\n- Relative: Differences from parent node", "see": ["AlpsTreeNode for tree structure", "AlpsKnowledge for serialization interface"], "has_pass2": false}, "src/AlpsTime.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsTime.h", "filename": "AlpsTime.h", "file": "AlpsTime.h", "brief": "Timer utilities for ALPS search (CPU and wall clock)\n\nProvides timing functions and AlpsTimer class for measuring execution time.\n\n**Free functions:**\n- AlpsCpuTime(): Alias for CoinCpuTime()\n- AlpsGetTimeOfDay(): Wall clock (MPI_Wtime if MPI, else CoinGetTimeOfDay)\n\n**AlpsTimer class:**\n- start()/stop(): Control timing interval\n- getCpuTime()/getWallClockTime(): Get elapsed time\n- getTime(): Returns CPU or wall based on clockType_\n- reachCpuLimit()/reachWallLimit(): Check time limits\n- setLimit(): Set time limit for checks\n\n**Clock types (AlpsClockType):**\n- AlpsClockTypeCpu: Process CPU time\n- AlpsClockTypeWallClock: Real elapsed time (default)", "see": ["AlpsParams::timeLimit for search time limit", "AlpsKnowledgeBroker timers for search statistics"], "has_pass2": false}, "src/AlpsParams.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsParams.h", "filename": "AlpsParams.h", "file": "AlpsParams.h", "brief": "Parameter set for ALPS tree search configuration\n\nAlpsParams extends AlpsParameterSet with ALPS-specific parameters.\nParameters control search strategy, limits, load balancing, and logging.\n\n**Boolean parameters (boolParams):**\n- deleteDeadNode: Remove fathomed nodes (default: true)\n- interClusterBalance: Master balances hubs (default: true)\n- intraClusterBalance: Hub balances workers (default: true)\n\n**Integer parameters (intParams):**\n- nodeLimit: Max nodes to process (default: INT_MAX)\n- solLimit: Max solutions to store (default: INT_MAX)\n- searchStrategy: 0=best, 1=estimate, 2=BFS, 3=DFS, 4=hybrid\n- msgLevel: Output verbosity (0=none, 1=summary, 2=moderate, 3=verbose)\n- hubNum: Number of hub processes\n- unitWorkNodes: Nodes per work unit\n\n**Double parameters (dblParams):**\n- timeLimit: Max search time in seconds\n- tolerance: Numerical tolerance (default: 1e-6)\n- donorThreshold/receiverThreshold: Load balancing thresholds", "see": ["AlpsParameterBase for parameter infrastructure", "AlpsModel::readParameters() for loading from file"], "has_pass2": false}, "src/AlpsSearchStrategyBase.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSearchStrategyBase.h", "filename": "AlpsSearchStrategyBase.h", "file": "AlpsSearchStrategyBase.h", "brief": "Template base class for search strategy comparison functions\n\nDefines the template interface for node and subtree comparison.\nUsed in priority queues to order items for selection.\n\n**AlpsSearchStrategy<T> template:**\n- compare(x, y): Return true if y preferred over x\n- operator(): Calls compare() for STL compatibility\n- weight_: Optional parameter for tuning\n- type_: Strategy identifier (BestFirst, DFS, etc.)\n\n**Virtual methods for advanced strategies:**\n- selectNextNode(): Custom node selection logic\n- createNewNodes(): Custom branching logic\n\n**AlpsCompare<T> wrapper:**\nAdapter holding strategy pointer for use with std::priority_queue.\nUsed in AlpsPriorityQueue.", "see": ["AlpsSearchStrategy.h for concrete implementations", "AlpsNodePool for node selection usage", "AlpsSubTreePool for subtree selection usage"], "has_pass2": false}, "src/AlpsSearchStrategy.h": {"path": "layer-3/CHiPPS-ALPS/src/AlpsSearchStrategy.h", "filename": "AlpsSearchStrategy.h", "file": "AlpsSearchStrategy.h", "brief": "Node and subtree selection strategies for ALPS tree search\n\nDefines comparison classes for selecting which node or subtree to explore next.\ncompare(x,y) returns true if y should be processed before x.\n\n**Subtree selection (AlpsTreeSelection):**\n- AlpsTreeSelectionBest: Best quality subtree first\n- AlpsTreeSelectionBreadth: Shallowest root first\n- AlpsTreeSelectionDepth: Deepest root first\n- AlpsTreeSelectionEstimate: Best estimated solution first\n\n**Node selection (AlpsNodeSelection):**\n- AlpsNodeSelectionBest: Best bound first (minimize dual gap)\n- AlpsNodeSelectionBreadth: BFS (shallowest first)\n- AlpsNodeSelectionDepth: DFS (deepest first, finds solutions fast)\n- AlpsNodeSelectionEstimate: Best estimated solution first\n- AlpsNodeSelectionHybrid: Combination strategy\n\n**Hybrid strategy:**\nCombines best-first selection with diving for solution finding.\nselectNextNode() and createNewNodes() can be overridden.", "see": ["AlpsSearchStrategyBase for template base class", "AlpsKnowledgeBroker::setNodeSelection() for configuration", "AlpsSubTree::setNodeSelection() for per-subtree override"], "has_pass2": false}}}, "CHiPPS-BLIS": {"name": "CHiPPS-BLIS", "file_count": 22, "pass2_count": 0, "files": {"src/BlisHeurRound.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisHeurRound.h", "filename": "BlisHeurRound.h", "file": "BlisHeurRound.h", "brief": "Simple rounding heuristic for MILP\n\nAttempts to find an integer-feasible solution by rounding\nfractional variables from the LP solution.\n\n**Algorithm:**\n1. Start with LP relaxation solution\n2. Round each integer variable to nearest integer\n3. Check if resulting solution is feasible\n4. If not, try to fix infeasibilities\n\n**Matrix storage:**\nMaintains both column-major (matrix_) and row-major (matrixByRow_)\nrepresentations for efficient access during rounding.\n\n**seed_:** Random seed for tie-breaking decisions.", "see": ["BlisHeuristic for the base class interface", "BlisModel::callHeuristics() for heuristic invocation"], "has_pass2": false}, "src/BlisBranchStrategyPseudo.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyPseudo.h", "filename": "BlisBranchStrategyPseudo.h", "file": "BlisBranchStrategyPseudo.h", "brief": "Pseudo-cost branching strategy based on historical LP degradation\n\nPseudo-cost branching uses learned estimates of objective degradation\nto select branching variables without solving child LPs.\n\n**Pseudo-cost definition:**\nFor variable xⱼ with fractional value fⱼ = xⱼ* - floor(xⱼ*):\n- ψⱼ⁻ = average (ΔObj / fⱼ) over down branches\n- ψⱼ⁺ = average (ΔObj / (1-fⱼ)) over up branches\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ with value xⱼ*:\n   - Estimate down degradation: Δ⁻ = ψⱼ⁻ · fⱼ\n   - Estimate up degradation: Δ⁺ = ψⱼ⁺ · (1-fⱼ)\n   - Score = μ·min(Δ⁻, Δ⁺) + (1-μ)·max(Δ⁻, Δ⁺)\n2. Select variable with highest score\n\n**Initialization:**\nBefore enough observations, uses default pseudo-costs or\nfalls back to other criteria (objective coefficient, etc.)\n\n**Complexity:** O(n) where n = number of integer variables\nMuch faster than strong branching but less accurate.", "see": ["BlisPseudo for pseudo-cost data structure", "BlisBranchStrategyRel for reliability branching (hybrid approach)", "BlisBranchStrategyStrong for strong branching"], "has_pass2": false}, "src/BlisBranchStrategyRel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyRel.h", "filename": "BlisBranchStrategyRel.h", "file": "BlisBranchStrategyRel.h", "brief": "Reliability branching strategy (strong + pseudo-cost hybrid)\n\nReliability branching combines the accuracy of strong branching with\nthe efficiency of pseudo-costs. It uses strong branching until\npseudo-cost estimates become \"reliable\" (observed enough times).\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ:\n   - If pseudo-costs for xⱼ are reliable (count ≥ relibility_):\n     Use pseudo-cost estimate for score\n   - Else:\n     Perform strong branching (solve child LPs)\n     Update pseudo-costs with observed degradation\n2. Select variable with highest score\n\n**relibility_ parameter:**\nNumber of observations required before trusting pseudo-costs.\nHigher values → more strong branching (accurate but slow)\nLower values → more pseudo-cost usage (fast but less accurate)\n\n**Advantages:**\n- Best of both worlds: accurate early, fast later\n- Pseudo-costs improve as search progresses\n- Default strategy in many modern MILP solvers", "see": ["BlisBranchStrategyStrong for pure strong branching", "BlisBranchStrategyPseudo for pure pseudo-cost branching", "BlisPseudo for pseudo-cost data structure"], "has_pass2": false}, "src/BlisConstraint.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisConstraint.h", "filename": "BlisConstraint.h", "file": "BlisConstraint.h", "brief": "Constraint (cut) representation for BLIS branch-and-cut\n\nRepresents a linear constraint (row) that can be added to the LP.\nUsed for both original constraints and generated cutting planes.\n\n**Data members:**\n- size_, indices_, values_: Sparse row representation\n- Bounds inherited from BcpsConstraint (lbHard, ubHard, lbSoft, ubSoft)\n\n**Key methods:**\n- createOsiRowCut(): Convert to OsiRowCut for LP solver\n- violation(): Compute constraint violation for given LP solution\n- hashing(): Compute hash key for duplicate detection\n\n**Usage in branch-and-cut:**\n1. CglCutGenerator produces OsiRowCut objects\n2. Converted to BlisConstraint for storage in pool\n3. Applied to LP via createOsiRowCut() when needed", "see": ["BlisConGenerator for cut generation management", "BlisVariable for the dual (column) representation", "BcpsConstraintPool for constraint storage"], "has_pass2": false}, "src/BlisMessage.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisMessage.h", "filename": "BlisMessage.h", "file": "BlisMessage.h", "brief": "Log messages for BLIS MILP solver events\n\nDefines BLIS_Message enum for CoinMessageHandler logging.\nSimilar to CoinMessages but specialized for BLIS output.\n\n**Message types:**\n- BLIS_CUTOFF_INC: Cutoff improvement\n- BLIS_CUT_STAT_*: Cut generation statistics\n- BLIS_GAP_*: Optimality gap status\n- BLIS_HEUR_*: Heuristic events and statistics\n- BLIS_ROOT_*: Root node processing info", "see": ["CoinMessageHandler for message output control", "BlisParams for verbosity settings"], "has_pass2": false}, "src/BlisConGenerator.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisConGenerator.h", "filename": "BlisConGenerator.h", "file": "BlisConGenerator.h", "brief": "Interface between BLIS and CGL cut generators\n\nWraps a CglCutGenerator with configuration controlling when and how\ncutting planes are generated during branch-and-cut.\n\n**Strategy options (BlisCutStrategy):**\n- None: Cut generator disabled\n- Root: Generate cuts only at root node\n- Auto: BLIS decides based on effectiveness\n- Periodic: Generate every cutGenerationFrequency_ nodes\n\n**Trigger conditions:**\n- normal_: Call in standard cut generation loop\n- atSolution_: Call when integer solution found\n- whenInfeasible_: Call when LP becomes infeasible\n\n**Statistics tracked:**\n- numConsGenerated_/numConsUsed_: Cut effectiveness\n- time_: CPU time spent in generator\n- calls_/noConsCalls_: Call frequency tracking\n\n**Usage:**\n1. Create: BlisConGenerator(model, new CglGomory(), \"Gomory\")\n2. Register: model->addCutGenerator(generator)\n3. BLIS calls generateConstraints() based on strategy", "see": ["BlisConstraint for the constraint representation", "CglCutGenerator for the CGL base class", "BlisModel::addCutGenerator() for registration"], "has_pass2": false}, "src/BlisHelp.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisHelp.h", "filename": "BlisHelp.h", "file": "BlisHelp.h", "brief": "Utility functions for BLIS solver operations\n\nHelper functions for common BLIS operations including cut conversion,\nstrong branching, warm start serialization, and parallel cut detection.\n\n**Cut conversion:**\n- BlisOsiCutToConstraint(): Convert OsiRowCut to BlisConstraint\n\n**Strong branching:**\n- BlisStrongBranch(): Evaluate branching on a single variable\n  Returns degradation estimates for up/down branches\n\n**Warm start serialization:**\n- BlisEncodeWarmStart(): Pack CoinWarmStartBasis to AlpsEncoded\n- BlisDecodeWarmStart(): Unpack CoinWarmStartBasis from AlpsEncoded\n\n**Cut management:**\n- BlisHashingOsiRowCut(): Compute hash for duplicate detection\n- BlisParallelCutCut(): Check if two cuts are parallel\n- BlisParallelCutCon(): Check cut vs. constraint parallelism\n- BlisParallelConCon(): Check constraint vs. constraint parallelism", "see": ["BlisConstraint for constraint representation", "BlisNodeDesc for warm start storage", "BlisBranchStrategyStrong for strong branching usage"], "has_pass2": false}, "src/BlisBranchStrategyMaxInf.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyMaxInf.h", "filename": "BlisBranchStrategyMaxInf.h", "file": "BlisBranchStrategyMaxInf.h", "brief": "Maximum infeasibility branching strategy\n\nThe simplest branching variable selection: branch on the integer\nvariable that is furthest from being integer.\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ with value xⱼ*:\n   - Compute infeasibility: inf = min(xⱼ* - floor(xⱼ*), ceil(xⱼ*) - xⱼ*)\n   - This is the distance to nearest integer\n2. Select variable with maximum infeasibility\n\n**Rationale:**\nVariables far from integrality are \"more fractional\" and branching\non them may force larger changes to the LP solution.\n\n**Trade-offs:**\n- Very fast: O(n) scan of integer variables\n- No learning or LP solves required\n- Generally produces larger search trees than pseudo-cost or strong\n- Good for initial exploration or when speed matters more than tree size", "see": ["BlisBranchStrategyPseudo for pseudo-cost branching (learned estimates)", "BlisBranchStrategyStrong for strong branching (accurate but slow)"], "has_pass2": false}, "src/BlisHeuristic.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisHeuristic.h", "filename": "BlisHeuristic.h", "file": "BlisHeuristic.h", "brief": "Base class for primal heuristics in BLIS\n\nPrimal heuristics search for integer-feasible solutions during\nbranch-and-cut. Good solutions improve the cutoff for pruning.\n\n**Strategy options (BlisHeurStrategy):**\n- None: Heuristic disabled\n- Root: Run only at root node\n- Auto: BLIS decides based on effectiveness\n- Periodic: Run every heurCallFrequency_ nodes\n- BeforeRoot: Run before solving first LP (e.g., feasibility pump)\n\n**Key virtual method:**\n- searchSolution(): Try to find a solution improving objectiveValue\n  Returns true if solution found, fills newSolution array\n\n**Statistics tracked:**\n- numSolutions_: Solutions found by this heuristic\n- time_: CPU time spent\n- calls_/noSolsCalls_: Success rate tracking", "see": ["BlisHeurRound for simple rounding heuristic", "BlisModel::addHeuristic() for registration", "BlisModel::callHeuristics() for invocation"], "has_pass2": false}, "src/BlisBranchObjectInt.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchObjectInt.h", "filename": "BlisBranchObjectInt.h", "file": "BlisBranchObjectInt.h", "brief": "Branching object for integer variable dichotomy\n\nRepresents a two-way branch on an integer variable.\nCreated when branching decisions are made at a tree node.\n\n**Branch structure:**\nFor variable xⱼ with fractional value v*:\n- Down branch: down_[0] ≤ xⱼ ≤ down_[1] = floor(v*)\n- Up branch: up_[0] = ceil(v*) ≤ xⱼ ≤ up_[1]\n\n**Direction semantics:**\n- direction_ = -1: Down branch executed first\n- direction_ = +1: Up branch executed first\n\n**Key methods:**\n- branch(): Apply bounds for current arm, advance to next\n- getDown()/getUp(): Access bound arrays\n\n**Serialization:**\nencode()/decode() for MPI transmission of branch decisions.", "see": ["BlisObjectInt for the integer variable object", "BlisBranchStrategy* for variable selection", "BlisTreeNode::branch() for branching implementation"], "has_pass2": false}, "src/BlisBranchStrategyBilevel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyBilevel.h", "filename": "BlisBranchStrategyBilevel.h", "file": "BlisBranchStrategyBilevel.h", "brief": "Branching strategy for bilevel programming problems\n\nSpecialized branching for bilevel optimization problems where there\nis a leader (upper-level) and follower (lower-level) decision maker.\n\n**Bilevel programming:**\n- Leader optimizes over (x, y) where y solves follower's problem\n- min_x { F(x, y) : G(x, y) ≥ 0, y ∈ argmin_y { f(x, y) : g(x, y) ≥ 0 } }\n- MILP reformulation uses complementarity or indicator constraints\n\n**Branching approach:**\nTypically branches on complementarity-related variables or\nuses specialized selection for leader vs. follower variables.", "see": ["BlisBranchObjectBilevel for bilevel branching objects", "BlisBranchStrategyStrong for standard strong branching"], "has_pass2": false}, "src/BlisModel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisModel.h", "filename": "BlisModel.h", "file": "BlisModel.h", "brief": "MILP model class extending BiCePS for branch-and-cut\n\nBlisModel is the main model class for the BLIS MILP solver.\nExtends BcpsModel with LP solver, cut generators, heuristics, and branching.\n\n**Initialization workflow:**\n1. readInstance() or importModel(): Load problem data\n2. readParameters(): Configure solver behavior\n3. setupSelf(): Initialize LP solver, objects, strategies\n4. preprocess(): Apply presolve transformations\n5. createRoot(): Create initial search tree node\n\n**LP Solver integration:**\n- origLpSolver_: User-provided OsiSolverInterface\n- presolvedLpSolver_: After presolve transformations\n- lpSolver_: Active solver (presolved or original)\n\n**Cut generation:**\n- addCutGenerator(): Register CglCutGenerator or BlisConGenerator\n- cutStrategy_: When to generate (Root, Auto, Periodic)\n- constraintPool_: Store generated cuts\n\n**Branching:**\n- branchStrategy_: BcpsBranchStrategy for variable selection\n- objects_: BcpsObject array (integer variables, SOS, etc.)\n- priority_: Branching priorities per object\n\n**Heuristics:**\n- addHeuristic(): Register BlisHeuristic instances\n- heurStrategy_: When to call (Root, Auto, Periodic, BeforeRoot)\n\n**Solution management:**\n- incumbent_: Best integer solution found\n- cutoff_: Objective bound for pruning\n- storeSolution(): Record new solutions\n\n**Parallel support:**\n- encode()/decodeToSelf(): Serialize model for MPI\n- packSharedKnowledge(): Share pseudo-costs, cuts, variables", "see": ["BlisTreeNode for node processing using this model", "BlisBranchStrategy* for branching strategies", "BlisConGenerator for cut generation wrapper", "BlisHeuristic for heuristic interface"], "has_pass2": false}, "src/BlisVariable.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisVariable.h", "filename": "BlisVariable.h", "file": "BlisVariable.h", "brief": "Variable representation for column generation in BLIS\n\nExtends BcpsVariable with sparse column representation for LP.\nUsed when variables are generated dynamically (column generation).\n\n**Data members:**\n- objCoef_: Objective coefficient for this variable\n- size_, indices_, values_: Sparse column in constraint matrix\n\n**Usage:**\nIn standard MILP solving, variables are implicit in the LP solver.\nBlisVariable is used when variables need to be:\n- Communicated between processes (parallel B&C)\n- Generated dynamically (column generation/branch-and-price)\n- Stored in variable pools for sharing", "see": ["BlisConstraint for the dual (row) representation", "BcpsVariablePool for variable storage"], "has_pass2": false}, "src/Blis.h": {"path": "layer-3/CHiPPS-BLIS/src/Blis.h", "filename": "Blis.h", "file": "Blis.h", "brief": "Core enumerations and constants for BLIS (BiCePS Linear Integer Solver)\n\nBLIS is a concrete implementation of the CHiPPS framework for MILP.\nHierarchy: ALPS (parallel search) → BiCePS (branch-cut-price) → BLIS (MILP).\n\n**BlisLpStatus enum:**\nLP relaxation solve outcomes (Optimal, Infeasible, IterLim, etc.)\n\n**BlisReturnStatus enum:**\nNode processing outcomes (Ok, ErrLp, Feasible, Branch, etc.)\n\n**BlisCutStrategy enum:**\n- NotSet, None: No cut generation\n- Root: Generate cuts only at root node\n- Auto: Automatic frequency selection\n- Periodic: Generate every cutGenerationFrequency_ nodes\n\n**BlisHeurStrategy enum:**\nSame as cut strategy plus BeforeRoot (e.g., feasibility pump)\n\n**BlisBranchingStrategy enum:**\n- MaxInfeasibility: Branch on most fractional variable\n- PseudoCost: Use pseudo-costs from LP degradation\n- Reliability: Pseudo-costs with strong branching fallback\n- Strong: Full strong branching (solve child LPs)\n- Bilevel: For bilevel programming problems\n\n**BlisBranchingObjectType enum:**\nInteger variables, SOS constraints, bilevel variables", "see": ["BlisModel for the MILP model class", "BlisTreeNode for node processing", "BlisParams for parameter configuration"], "has_pass2": false}, "src/BlisPseudo.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisPseudo.h", "filename": "BlisPseudo.h", "file": "BlisPseudo.h", "brief": "Pseudo-cost data structure for branching variable selection\n\nTracks historical LP objective degradation when branching on a variable.\nUsed to estimate branching quality without solving child LPs.\n\n**Pseudo-cost formula:**\nFor variable xⱼ branched with fractional part f:\n- Up cost: ψ⁺ = Δobj⁺ / (1 - f), averaged over up branches\n- Down cost: ψ⁻ = Δobj⁻ / f, averaged over down branches\n\n**Score calculation:**\nscore_ = weight_ * min(downCost_, upCost_) + (1 - weight_) * max(...)\nDefault weight_ = 1.0 emphasizes minimum degradation.\n\n**Key members:**\n- upCost_/downCost_: Average per-unit objective change\n- upCount_/downCount_: Number of observations (for reliability)\n- weight_: Weighting in score formula [0, 1]\n\n**Usage:**\n- update(): Called after branching to incorporate new observation\n- getScore(): Returns estimated branching quality\n- Serializable via AlpsKnowledge for parallel sharing", "see": ["BlisBranchStrategyPseudo for pseudo-cost branching", "BlisBranchStrategyRel for reliability branching", "BlisObjectInt::pseudocost() which stores per-variable costs"], "has_pass2": false}, "src/BlisObjectInt.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisObjectInt.h", "filename": "BlisObjectInt.h", "file": "BlisObjectInt.h", "brief": "Integer variable object for branching decisions\n\nRepresents an integer-constrained variable in the MILP model.\nStores original bounds, breakeven point, and pseudo-costs.\n\n**Key attributes:**\n- columnIndex_: LP column index for this integer variable\n- originalLower_/Upper_: Bounds before any branching\n- breakEven_: Threshold for up vs. down preference (default 0.5)\n- pseudocost_: BlisPseudocost for branching decisions\n\n**Key methods:**\n- infeasibility(): Returns fractionality in [0.0, 0.5]\n- createBranchObject(): Creates BlisBranchObjectInt for branching\n- feasibleRegion(): Fixes variable to nearest integer\n- preferredNewFeasible(): Direction based on reduced cost\n\n**Pseudo-costs:**\nEach integer object maintains its own pseudo-cost history,\nenabling per-variable learning for branching decisions.", "see": ["BlisBranchObjectInt for the branching action", "BlisPseudocost for pseudo-cost data structure", "BlisBranchStrategyPseudo for pseudo-cost branching"], "has_pass2": false}, "src/BlisBranchStrategyStrong.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchStrategyStrong.h", "filename": "BlisBranchStrategyStrong.h", "file": "BlisBranchStrategyStrong.h", "brief": "Strong branching strategy for MILP variable selection\n\nStrong branching is the most accurate (but expensive) branching strategy.\nFor each candidate variable, it actually solves child LP relaxations\nto measure the objective degradation.\n\n**Algorithm:**\n1. For each fractional integer variable xⱼ:\n   - Create down branch (xⱼ ≤ floor(xⱼ*))\n   - Create up branch (xⱼ ≥ ceil(xⱼ*))\n   - Solve both LPs (limited iterations)\n   - Record objective change ΔDown, ΔUp\n2. Score = μ·min(ΔDown, ΔUp) + (1-μ)·max(ΔDown, ΔUp)\n3. Select variable with highest score\n\n**BlisStrong struct:**\nStores results for one candidate: objective changes, infeasibility counts,\nsolver completion status for up/down branches.\n\n**Trade-offs:**\n- Produces smallest search trees (best variable selection)\n- Very expensive at each node (many LP solves)\n- Typically used with iteration limits per candidate LP", "see": ["BlisBranchStrategyRel for reliability branching (strong + pseudo-cost hybrid)", "BlisBranchStrategyPseudo for pure pseudo-cost branching"], "has_pass2": false}, "src/BlisParams.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisParams.h", "filename": "BlisParams.h", "file": "BlisParams.h", "brief": "BLIS-specific parameter set for MILP solver configuration\n\nExtends AlpsParameterSet with parameters controlling branching,\ncut generation, heuristics, and optimality tolerances.\n\n**Parameter categories:**\n\n**chrParams (bool):**\n- cutRampUp: Generate cuts during parallel ramp-up\n- presolve: Apply presolve transformations\n- shareConstraints/Variables: Parallel knowledge sharing\n- sharePseudocostRampUp/Search: Share branching data\n\n**intParams:**\n- branchStrategy: 0=MaxInf, 1=Pseudo, 2=Reliability, 3=Strong, 4=Bilevel\n- cutStrategy, cut*Strategy: Cut generator control (-2=root, -1=auto, 0=off)\n- heurStrategy, heur*Strategy: Heuristic control\n- strongCandSize: Candidates for strong branching\n- pseudoRelibility: Observations before trusting pseudo-costs\n\n**dblParams:**\n- cutoff: Upper bound for pruning\n- cutoffInc: Cutoff increment for fathoming\n- integerTol: Tolerance for integrality\n- optimalRelGap/AbsGap: Termination gaps\n- pseudoWeight: Score formula weighting", "see": ["AlpsParameterSet for base class", "BlisModel which holds this parameter set"], "has_pass2": false}, "src/BlisSolution.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisSolution.h", "filename": "BlisSolution.h", "file": "BlisSolution.h", "brief": "Integer solution representation for BLIS\n\nExtends BcpsSolution with BLIS-specific solution handling.\nStores variable values and objective for integer-feasible solutions.\n\n**Key methods:**\n- print(): Output solution, showing near-integer values as integers\n- encode()/decode(): Serialization for parallel solution sharing\n\n**Usage:**\nCreated by heuristics or when LP solution is integer-feasible.\nStored in AlpsSolutionPool, shared across processes in parallel.", "see": ["BcpsSolution for base class with size_, values_, quality_", "AlpsSolutionPool for solution storage", "BlisHeuristic::searchSolution() which returns these"], "has_pass2": false}, "src/BlisBranchObjectBilevel.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisBranchObjectBilevel.h", "filename": "BlisBranchObjectBilevel.h", "file": "BlisBranchObjectBilevel.h", "brief": "Branching object for bilevel programming problems\n\nSpecialized branching object for bilevel optimization where\nbranching may involve multiple variables (a \"branching set\").\n\n**branchingSet_:**\nA deque of variable indices that are affected by this branch.\nUnlike standard integer branching (single variable), bilevel\nproblems may require coordinated changes to multiple variables.", "see": ["BlisBranchStrategyBilevel for bilevel branching strategy", "BlisBranchObjectInt for standard integer branching"], "has_pass2": false}, "src/BlisNodeDesc.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisNodeDesc.h", "filename": "BlisNodeDesc.h", "file": "BlisNodeDesc.h", "brief": "Node description storing LP basis and branching info\n\nExtends BcpsNodeDesc with simplex warm start and pseudo-cost data.\nContains the state needed to reconstruct a search tree node.\n\n**Warm start data:**\n- basis_: CoinWarmStartBasis for LP hot-starting\n- Stores structural and artificial variable status (basic/nonbasic)\n\n**Branching history (for pseudo-cost updates):**\n- branchedDir_: Direction of branch that created this node (-1/+1)\n- branchedInd_: Object index that was branched on\n- branchedVal_: Value at branching point (used in pseudo-cost update)\n\n**Serialization:**\nencode()/decode() serialize basis and branching info for MPI.\nUses BlisEncodeWarmStart/BlisDecodeWarmStart helpers.", "see": ["BlisTreeNode for the node using this description", "CoinWarmStartBasis for LP basis representation", "BlisPseudocost::update() which uses branching history"], "has_pass2": false}, "src/BlisTreeNode.h": {"path": "layer-3/CHiPPS-BLIS/src/BlisTreeNode.h", "filename": "BlisTreeNode.h", "file": "BlisTreeNode.h", "brief": "Concrete branch-and-bound tree node for MILP solving\n\nBlisTreeNode extends BcpsTreeNode with LP-based bounding and cut generation.\nImplements the main node processing loop for branch-and-cut.\n\n**Node processing (process()):**\n1. installSubProblem(): Set up LP with current bounds\n2. bound(): Solve LP relaxation\n3. callHeuristics(): Search for integer solutions\n4. generateConstraints(): Add cutting planes\n5. selectBranchObject(): Choose branching variable\n6. branch(): Create child nodes\n\n**Cut generation workflow:**\n- generateConstraints(): Call registered CglCutGenerators\n- getViolatedConstraints(): Check for violated cuts\n- applyConstraints(): Add cuts to LP and re-solve\n- parallel(): Remove duplicate/parallel cuts\n\n**Bound tightening:**\n- reducedCostFix(): Fix variables using reduced costs\n\n**Node descriptions:**\n- convertToExplicit(): Full bound/cut information\n- convertToRelative(): Delta from parent (space-efficient)\n\n**Branching:**\n- selectBranchObject(): Use model's BcpsBranchStrategy\n- branch(): Create children with modified bounds", "see": ["BlisModel for the model containing LP solver and generators", "BlisNodeDesc for node description structure", "BcpsBranchStrategy for branching variable selection"], "has_pass2": false}}}, "Couenne": {"name": "Couenne", "file_count": 104, "pass2_count": 32, "files": {"src/interfaces/BonCouenneInterface.hpp": {"path": "layer-3/Couenne/src/interfaces/BonCouenneInterface.hpp", "filename": "BonCouenneInterface.hpp", "file": "BonCouenneInterface.hpp", "brief": "Couenne's interface to Bonmin/AMPL\n\nExtends Bonmin's AmplInterface to read AMPL models and build\nCouenne's symbolic problem representation with convex relaxations.\n\n**Key methods:**\n- readAmplNlFile(): Read AMPL .nl file into Couenne structures\n- extractLinearRelaxation(): Build initial LP relaxation with OA cuts\n\n**Linear relaxation extraction:**\n1. Solve continuous NLP relaxation\n2. Generate linearization cuts at NLP optimum\n3. Load cuts into OsiSolverInterface for B&B", "see": ["CouenneProblem for the symbolic representation", "CouenneCutGenerator for cut generation"], "has_pass2": false}, "src/interfaces/CouenneUserInterface.hpp": {"path": "layer-3/Couenne/src/interfaces/CouenneUserInterface.hpp", "filename": "CouenneUserInterface.hpp", "file": "CouenneUserInterface.hpp", "brief": "Abstract base class for Couenne user interfaces\n\nDefines the interface contract for problem input sources.\nConcrete implementations read from AMPL, OSInstance, or custom formats.\n\n**Required methods (pure virtual):**\n- getCouenneProblem(): Return symbolic problem representation\n- getTMINLP(): Return Bonmin TMINLP interface for NLP solves\n\n**Optional hooks:**\n- setupJournals(): Configure output streams (default: stdout)\n- addBabPlugins(): Add cut generators, heuristics, bound tighteners\n- writeSolution(): Output final solution\n\n**Usage pattern:**\n```cpp\nCouenneAmplInterface interface(options, journalist);\nCouenneProblem* prob = interface.getCouenneProblem();\n// ... solve ...\ninterface.writeSolution(bab);\n```", "see": ["CouenneAmplInterface for AMPL .nl file input", "CouenneOSInterface for Optimization Services input"], "has_pass2": false}, "src/interfaces/CouenneTNLP.hpp": {"path": "layer-3/Couenne/src/interfaces/CouenneTNLP.hpp", "filename": "CouenneTNLP.hpp", "file": "CouenneTNLP.hpp", "brief": "Ipopt TNLP interface for Couenne problems\n\nWraps CouenneProblem as an Ipopt::TNLP for NLP solving.\nComputes derivatives using Couenne's symbolic expression DAG.\n\n**Purpose:**\nAllows Ipopt to solve the continuous relaxation or NLP\nsubproblems arising during Couenne's spatial B&B.\n\n**Derivative computation:**\n- Gradient: Via expression differentiation (gradient_ vector)\n- Jacobian: Via ExprJac symbolic Jacobian structure\n- Hessian: Via ExprHess symbolic Hessian of Lagrangian\n\n**Key methods (Ipopt callbacks):**\n- get_nlp_info(): Returns dimensions and sparsity info\n- get_bounds_info(): Variable/constraint bounds\n- eval_f(), eval_grad_f(): Objective and gradient\n- eval_g(), eval_jac_g(): Constraints and Jacobian\n- eval_h(): Hessian of Lagrangian\n- finalize_solution(): Store optimal solution\n\n**Hessian storage:**\noptHessian_ can store the optimal Hessian for later use\n(e.g., in distance computations for Feasibility Pump).", "see": ["CouenneProblem which provides the expression DAG", "ExprJac for Jacobian computation", "ExprHess for Hessian computation"], "has_pass2": false}, "src/interfaces/CouenneMINLPInterface.hpp": {"path": "layer-3/Couenne/src/interfaces/CouenneMINLPInterface.hpp", "filename": "CouenneMINLPInterface.hpp", "file": "CouenneMINLPInterface.hpp", "brief": "OsiSolverInterface for MINLP via Couenne expressions\n\nProvides an Osi-compatible interface for solving MINLP continuous\nrelaxations using NLP solvers (Ipopt or FilterSQP). Gradients and\nJacobians are computed from the expression DAG.\n\n**Solver enum:**\n- EIpopt: Interior-point via Ipopt\n- EFilterSQP: Sequential quadratic programming\n- EAll: Use multiple solvers for robustness\n\n**Key methods:**\n- initialSolve(): Solve initial continuous relaxation\n- resolve(): Re-solve after bound changes\n- getOuterApproximation(): Generate OA cuts at current point\n- extractLinearRelaxation(): Build LP relaxation from NLP\n\n**Outer approximation support:**\n- getOuterApproximation(): Generate linearizations at solution\n- getConstraintOuterApproximation(): Single constraint OA\n- getBendersCut(): Benders decomposition cut\n- getFeasibilityOuterApproximation(): Cuts for feasibility pump\n\n**Strong branching:**\nUses StrongBranchingSolver for approximate LP-based evaluation\nduring variable selection.", "see": ["CouenneProblem for the symbolic representation", "CouenneTNLP for the Ipopt TNLP adapter"], "has_pass2": false}, "src/disjunctive/CouenneDisjCuts.hpp": {"path": "layer-3/Couenne/src/disjunctive/CouenneDisjCuts.hpp", "filename": "CouenneDisjCuts.hpp", "file": "CouenneDisjCuts.hpp", "brief": "Disjunctive cut generator for nonconvex MINLP\n\nGenerates lift-and-project style disjunctive cuts by solving a\nCut-Generating Linear Program (CGLP) for each disjunction.", "algorithm": "Disjunctive Cut Generation via CGLP:\nGenerate valid inequalities from (P₁ ∨ P₂) disjunctions:\n1. For disjunction (Ax ≤ b) ∨ (Cx ≤ d):\n   Represent as union: x ∈ conv(P₁ ∪ P₂)\n2. CGLP formulation:\n   Variables: (α, β₀) for cut αᵀx ≤ β₀\n   Constraints:\n   - αᵀx ≤ β₀ valid for P₁: ∃u≥0: α = uᵀA, β₀ ≥ uᵀb\n   - αᵀx ≤ β₀ valid for P₂: ∃v≥0: α = vᵀC, β₀ ≥ vᵀd\n   Objective: max αᵀx̄ - β₀ (separation depth at LP solution x̄)\n3. Solve CGLP → optimal (α*, β₀*) gives deepest cut\n4. Add cut α*ᵀx ≤ β₀* to LP relaxation\n\nDisjunction sources:\n- Simple branching: xᵢ ≤ k ∨ xᵢ ≥ k+1\n- Split from strong branching candidates\n- Convexification disjunctions from nonlinear terms", "math": "Lift-and-project derivation:\nFor split π x ≤ π₀ ∨ π x ≥ π₀+1:\n  Cut: (α - λπ)ᵀx ≤ β₀ - λ(π₀+1) for any α·x ≤ β₀ valid, λ ≥ 0", "complexity": "CGLP has O(m₁ + m₂) constraints, O(n) variables.\nTotal: O(numDisjunctions_ · LP_solve(m,n)).", "ref": ["Balas (1979). \"Disjunctive programming\". Annals of Discrete Math 5:3-51.", "Ceria, Soares (1999). \"Convex programming for disjunctive convex\n  optimization\". Mathematical Programming 86(3):595-614."], "see": ["CglLandP for related lift-and-project cuts", "CouenneCutGenerator for convexification cuts"], "has_pass2": true}, "src/readnl/CouenneAmplInterface.hpp": {"path": "layer-3/Couenne/src/readnl/CouenneAmplInterface.hpp", "filename": "CouenneAmplInterface.hpp", "file": "CouenneAmplInterface.hpp", "brief": "AMPL .nl file reader for Couenne\n\nReads optimization problems from AMPL Solver Library (ASL)\nformat and converts to Couenne's expression DAG representation.\n\n**Key methods:**\n- getCouenneProblem(): Parse .nl file → CouenneProblem\n- getTMINLP(): Wrap as Bonmin TMINLP for NLP solves\n- writeSolution(): Write .sol file back to AMPL\n\n**Internal conversion:**\n- readASLfg(): Read using ASL fg (function/gradient) reader\n- readnl(): Alternative .nl parsing\n- nl2e(): Convert ASL expr* to Couenne expression*\n\n**ASL integration:**\nUses the AMPL Solver Library (ASL) C structures to parse\nthe binary .nl format. The nl2e() method recursively converts\nASL expression trees to Couenne expression nodes.", "see": ["CouenneUserInterface base class", "CouenneProblem the target representation"], "has_pass2": false}, "src/expression/CouExpr.hpp": {"path": "layer-3/Couenne/src/expression/CouExpr.hpp", "filename": "CouExpr.hpp", "file": "CouExpr.hpp", "brief": "Expression container with operator overloading for algebraic construction\n\nProvides a user-friendly wrapper around the expression DAG with\noverloaded operators for building mathematical expressions in\nnatural algebraic notation.\n\n**CouExpr container:**\n- Wraps an expression* with value semantics (cloning on copy)\n- Allows algebraic expression construction: e1 + e2, sin(e), etc.\n\n**Supported operations:**\n- Arithmetic: +, -, *, /, %, ^ (power)\n- Trigonometric: sin, cos\n- Other: log, exp\n\n**Operand types:**\n- CouExpr & CouExpr → CouExpr\n- CouNumber & CouExpr → CouExpr\n- CouExpr & CouNumber → CouExpr\n\n**Example:**\n```cpp\nCouExpr x(exprVar), y(exprVar);\nCouExpr f = sin(x) + x*y + exp(y);\nexpression* e = f.Expression();\n```", "see": ["expression which is the underlying representation", "CouenneProblem for high-level model construction"], "has_pass2": false}, "src/expression/CouenneDomain.hpp": {"path": "layer-3/Couenne/src/expression/CouenneDomain.hpp", "filename": "CouenneDomain.hpp", "file": "CouenneDomain.hpp", "brief": "Point and bounding box with save/restore stack\n\nManages the current solution point and variable bounds during\nexpression evaluation and bound tightening. Provides LIFO\nsave/restore functionality for backtracking.\n\n**DomainPoint:**\n- x_[]: Current variable values\n- lb_[]: Lower bounds\n- ub_[]: Upper bounds\n- copied_: True if data is owned (must free on destruction)\n- isNlp_: True if point came from NLP solver (feasible)\n\n**Domain:**\n- point_: Current active point\n- domStack_: Stack of saved points for restore\n\n**push()/pop() pattern:**\n```cpp\ndomain.push(newPoint);  // Save current, use new\n// ... work with new bounds ...\ndomain.pop();           // Restore previous\n```\n\n**Usage:**\nDuring B&B, each node has different bounds. push() saves the\nparent's bounds before processing a child; pop() restores them\nwhen backtracking.", "see": ["CouenneProblem which owns the Domain", "expression::domain_ pointer in each expression"], "has_pass2": false}, "src/expression/CouenneExpression.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExpression.hpp", "filename": "CouenneExpression.hpp", "file": "CouenneExpression.hpp", "brief": "Base class for expression DAG nodes in Couenne\n\nDefines the abstract interface for all expression types (constants,\nvariables, operators). Each expression can evaluate itself, generate\nconvex relaxation cuts, and participate in bound propagation.\n\n**Expression hierarchy:**\n- exprConst: Constant values\n- exprVar/exprAux: Original and auxiliary variables\n- exprOp: Operators (unary, binary, n-ary)\n- exprCopy/exprClone: References to other expressions\n\n**Key virtual methods:**\n- operator()(): Evaluate expression at current point\n- generateCuts(): Generate OA-style convexification cuts for w = f(x)\n- impliedBound(): Backward bound propagation (w bounds → x bounds)\n- getBounds(): Forward bound propagation (x bounds → w bounds)\n- standardize(): Convert to auxiliary variable form\n\n**auxSign enum:**\nDefines how auxiliary w relates to its expression f(x):\n- AUX_LEQ: w <= f(x) (under-estimator)\n- AUX_EQ: w = f(x) (equality definition)\n- AUX_GEQ: w >= f(x) (over-estimator)", "see": ["CouenneProblem for problem container", "exprOp for operator implementations"], "has_pass2": false}, "src/expression/CouenneExprBound.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprBound.hpp", "filename": "CouenneExprBound.hpp", "file": "CouenneExprBound.hpp", "brief": "Expressions representing variable bounds (l_i, u_i)\n\nExpression nodes that evaluate to the current lower/upper bound\nof a variable. Used in parametric convexification to express\nbound-dependent cuts.\n\n**exprLowerBound:**\n- Evaluates to domain_->lb(varIndex_)\n- Prints as \"l_i\" where i is variable index\n- Derivative is 0 (bounds are parameters, not variables)\n\n**exprUpperBound:**\n- Evaluates to domain_->ub(varIndex_)\n- Prints as \"u_i\" where i is variable index\n- Derivative is 0\n\n**Usage example:**\nFor convexification cut on f(x) with x ∈ [l, u]:\nThe secant line passes through (l, f(l)) and (u, f(u)).\nThe cut coefficients depend on l and u, represented\nas exprLowerBound and exprUpperBound.\n\n**Note:**\nThese behave like constants for differentiation purposes\nbut their values change during B&B as bounds tighten.", "see": ["CouenneDomain which provides the bound values", "exprVar for variable expressions"], "has_pass2": false}, "src/expression/CouenneExprVar.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprVar.hpp", "filename": "CouenneExprVar.hpp", "file": "CouenneExprVar.hpp", "brief": "Variable expression class for original decision variables\n\nRepresents decision variables in the expression DAG. All problem\nvariables (original and auxiliary) derive from this class.\n\n**Key attributes:**\n- varIndex_: Index in the problem's variable vector\n- domain_: Pointer to current point and bounds\n\n**Methods:**\n- operator()(): Return current variable value from domain\n- lb()/ub(): Access current lower/upper bounds\n- differentiate(): Returns 1 if differentiating w.r.t. this variable\n- impliedBound(): Propagate bounds from auxiliary to original variables\n\n**Linearity:**\nOriginal variables have Linearity() = LINEAR (affine in themselves).", "see": ["exprAux for auxiliary variables w = f(x)", "Domain for point/bounds container"], "has_pass2": false}, "src/expression/CouennePrecisions.hpp": {"path": "layer-3/Couenne/src/expression/CouennePrecisions.hpp", "filename": "CouennePrecisions.hpp", "file": "CouennePrecisions.hpp", "brief": "Numerical tolerances and constants for Couenne\n\nCentral definition of numerical tolerances used throughout\nCouenne for comparisons, cut generation, and bound handling.\n\n**General tolerances:**\n- COUENNE_EPS (1e-7): General numerical tolerance\n- COUENNE_BOUND_PREC (1e-5): Looser tolerance for bounds\n- COUENNE_EPS_INT (1e-9): Integrality check tolerance\n- COUENNE_EPS_SIMPL (1e-20): Simplification zero threshold\n\n**Infinity and bounds:**\n- COUENNE_INFINITY (1e50): Represents ±∞ in bounds\n- MAX_BOUND (1e45): Practical bound maximum\n- Couenne_large_bound (9.999e12): LP unbounded threshold\n\n**Cut coefficients:**\n- COU_MAX_COEFF (1e9): Maximum cut coefficient\n- COU_MIN_COEFF (1e-9): Minimum cut coefficient\n(Ensures numerical stability in LP solver)\n\n**Utility macros:**\n- COUENNE_round(x): Round to nearest integer\n- COUENNE_sign(x): Return +1 or -1", "see": ["CouenneCutGenerator which uses these tolerances"], "has_pass2": false}, "src/expression/CouenneExprConst.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprConst.hpp", "filename": "CouenneExprConst.hpp", "file": "CouenneExprConst.hpp", "brief": "Constant expression node\n\nRepresents a constant value in the expression DAG.\nLeaf node with fixed value that never changes.\n\n**Properties:**\n- Type: CONST (leaf node)\n- Linearity: ZERO if |value| < ε, else CONSTANT\n- Bounds: [value, value] (trivial)\n- Differentiation: Always returns 0\n- Rank: 0 (lowest priority for branching)\n\n**isInteger():**\nChecks if the constant value is an integer (within tolerance).\nUsed for determining integrality of expressions.\n\n**Usage:**\n- Constant terms in objective/constraints\n- Right-hand sides of constraints\n- Exponents in power expressions", "see": ["expression base class", "exprVar for variable expressions"], "has_pass2": false}, "src/expression/CouenneExprIVar.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprIVar.hpp", "filename": "CouenneExprIVar.hpp", "file": "CouenneExprIVar.hpp", "brief": "Integer variable expression node\n\nRepresents an integer-restricted decision variable in the\nexpression DAG. Inherits from exprVar with additional\nintegrality information.\n\n**Integer markers:**\n- isInteger(): Always returns true (variable is integer-valued)\n- isDefinedInteger(): Always returns true (defined as integer)\n\n**Printed notation:**\nPrints as \"y_i\" where i is the variable index, distinguishing\nfrom continuous variables which print as \"x_i\".\n\n**Usage:**\nInteger variables require special handling in:\n- Branch-and-bound (must branch to enforce integrality)\n- Feasibility checking (must satisfy x ∈ ℤ)\n- Solution rounding in heuristics", "see": ["exprVar base class for continuous variables", "CouenneProblem::addVariable() for variable creation"], "has_pass2": false}, "src/expression/CouenneExprStore.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprStore.hpp", "filename": "CouenneExprStore.hpp", "file": "CouenneExprStore.hpp", "brief": "Expression that returns previously stored value\n\nReturns the Value() of the pointed-to expression rather than\nre-evaluating it. Used for efficiency when an expression has\nalready been evaluated and the value cached.\n\n**Key difference from exprCopy/exprClone:**\n- exprCopy: Calls (*copy_)() to evaluate\n- exprClone: Calls (*copy_)() to evaluate\n- exprStore: Calls copy_->Value() to get cached value\n\n**Usage pattern:**\nWhen an expression tree is evaluated top-down, intermediate\nresults can be stored in value_ fields. exprStore allows\nretrieval of these stored values without re-computation.\n\n**Destructor:**\nLike exprClone, sets copy_ = NULL to prevent deletion.\nDoes NOT own the pointed-to expression.", "see": ["exprCopy which owns and evaluates", "exprClone which references and evaluates"], "has_pass2": false}, "src/expression/CouenneTypes.hpp": {"path": "layer-3/Couenne/src/expression/CouenneTypes.hpp", "filename": "CouenneTypes.hpp", "file": "CouenneTypes.hpp", "brief": "Core type definitions and enumerations for Couenne\n\nCentral header defining enums, types, and small classes used\nthroughout the Couenne codebase.\n\n**Key enumerations:**\n- nodeType: CONST, VAR, UNARY, N_ARY, COPY, AUX, EMPTY\n- linearity_type: ZERO, CONSTANT, LINEAR, QUADRATIC, NONLINEAR\n- convexity: CONVEX, CONCAVE, AFFINE, NONCONVEX\n- expr_type: Codes for each expression class (COU_EXPRSUM, etc.)\n- monotonicity: INCREAS, DECREAS, NONMONOTONE\n\n**t_chg_bounds class:**\nTracks whether lower/upper bounds have been:\n- UNCHANGED: No modification\n- CHANGED: Modified during bound tightening\n- EXACT: Bound is tight (equality holds)\n\n**CouNumber:**\ntypedef double CouNumber - the main floating-point type.\n\n**unary_function:**\nFunction pointer type for unary operators: CouNumber → CouNumber", "see": ["expression which uses these types", "CouenneProblem which uses t_chg_bounds"], "has_pass2": false}, "src/expression/CouenneExprOp.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprOp.hpp", "filename": "CouenneExprOp.hpp", "file": "CouenneExprOp.hpp", "brief": "Base class for n-ary operators in expression DAG\n\nBase for all non-leaf operators (sum, mul, pow, div, etc.).\nProvides argument list management and common functionality.\n\n**Key attributes:**\n- arglist_: Array of pointers to child expressions\n- nargs_: Number of arguments\n\n**Derived operator classes:**\n- exprSum, exprSub: Addition and subtraction\n- exprMul, exprDiv: Multiplication and division\n- exprPow: Power function x^k\n- exprLog, exprExp: Logarithm and exponential\n- exprSin, exprCos: Trigonometric functions\n- exprMin, exprMax: Min/max functions\n- exprAbs: Absolute value\n\n**Methods:**\n- standardize(): Replace with auxiliary variable w = f(x)\n- clonearglist(): Deep copy argument list for cloning\n- rank(): Maximum rank of arguments + 1", "see": ["exprUnary for unary operators (single argument)", "CouenneProblem for expression tree container"], "has_pass2": false}, "src/expression/CouenneExprAux.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprAux.hpp", "filename": "CouenneExprAux.hpp", "file": "CouenneExprAux.hpp", "brief": "Auxiliary variable class for reformulated nonlinear expressions\n\nAuxiliary variables replace nonlinear terms during standardization,\nenabling generation of convex relaxations. If w = f(x), then cuts\nare generated for the relation between w and f(x).", "algorithm": "Multiplicity-Based Selection:\nVariables appearing more often have higher branching value.\n\n  multiplicity_ counts occurrences in reformulated problem\n  Higher multiplicity → branching affects more constraints\n  Used to break ties in branching variable selection\n\n**Key attributes:**\n- image_: The expression f(x) this auxiliary represents\n- sign_: Relation type (AUX_LEQ: w <= f(x), AUX_EQ: w = f(x), AUX_GEQ)\n- rank_: Depth in expression DAG (used for branching priority)\n- multiplicity_: How many times this aux appears in reformulation", "see": ["exprVar for base class", "CouenneProblem::standardize() for auxiliary creation"], "has_pass2": true}, "src/expression/CouenneExprCopy.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprCopy.hpp", "filename": "CouenneExprCopy.hpp", "file": "CouenneExprCopy.hpp", "brief": "Reference copy of an expression\n\nPoints to another expression and delegates all operations to it.\nUsed to share expressions without duplicating the entire subtree.\n\n**Key concept:**\nexprCopy owns its copy_ pointer and destroys it in destructor.\nThis is the \"owning\" copy - use exprClone for non-owning.\n\n**Delegation pattern:**\nAll methods (evaluate, differentiate, bounds, etc.) forward\nto the underlying copy_ expression.\n\n**Original():**\nReturns the ultimate underlying expression, chasing through\nany chain of copies to find the actual expr.\n\n**value_:**\nCaches the last computed value for use by exprStore expressions.\n\n**Important warning:**\nThis destructor deletes copy_, unlike exprClone and exprStore.\nBe careful about ownership when using expression copies.", "see": ["exprClone for non-owning copy (different destructor behavior)", "exprStore for stored value version", "expression base class"], "has_pass2": false}, "src/expression/CouenneExprClone.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprClone.hpp", "filename": "CouenneExprClone.hpp", "file": "CouenneExprClone.hpp", "brief": "Non-owning reference to another expression\n\nPoints to another expression but does NOT own it - the destructor\nsets copy_ = NULL to prevent exprCopy's destructor from deleting it.\n\n**Key difference from exprCopy:**\n- exprCopy: Owns copy_, deletes it in destructor\n- exprClone: Does NOT own copy_, just references it\n\n**Usage:**\nUse exprClone when you need multiple references to the same\nexpression without duplicating memory or ownership.\n\n**getOriginal():**\nUsed in constructor to skip through chains of clones/copies\nto find the actual underlying expression.\n\n**Evaluation:**\noperator() calls (*copy_)() which evaluates the pointed-to\nexpression and returns its result.", "see": ["exprCopy which owns its referenced expression", "exprStore for storing computed values"], "has_pass2": false}, "src/expression/CouenneExprUnary.hpp": {"path": "layer-3/Couenne/src/expression/CouenneExprUnary.hpp", "filename": "CouenneExprUnary.hpp", "file": "CouenneExprUnary.hpp", "brief": "Base class for univariate function expressions\n\nProvides common framework for all single-argument functions like\nsin, cos, log, exp, sqrt, etc. Derived classes override F() to\ndefine the actual function.\n\n**Key virtual methods for derived classes:**\n- F(): Return the unary_function pointer for evaluation\n- generateCuts(): Create convexification cuts for w = f(x)\n- impliedBound(): Backward propagation (w bounds → x bounds)\n\n**Convexification approach:**\nUnivariate functions use tangent cuts (supporting hyperplanes) on\nconvex regions and secant cuts on concave regions.", "see": ["exprLog, exprExp, exprSin, exprCos for concrete implementations", "exprOp for n-ary operators"], "has_pass2": false}, "src/branch/CouenneChooseVariable.hpp": {"path": "layer-3/Couenne/src/branch/CouenneChooseVariable.hpp", "filename": "CouenneChooseVariable.hpp", "file": "CouenneChooseVariable.hpp", "brief": "Variable selection for branching in global optimization", "algorithm": "Variable Selection for Global Optimization\n\nExtends OsiChooseVariable to select branching variables based on\nnonconvexity-specific criteria. The goal is to choose variables\nthat most effectively reduce the relaxation gap.\n\n**setupList():**\nBuilds list of candidate branching objects, filtering by:\n- Infeasibility threshold (|w - f(x)| > tol)\n- Variable type and importance\n- Depth-based filtering\nReturns -1 if node is proven infeasible during setup.\n\n**feasibleSolution():**\nChecks if current solution satisfies all constraints including\nauxiliary variable definitions w = f(x) within tolerance.\n\n**Selection criteria:**\nUnlike MIP (which branches on integer violations), MINLP must\nalso consider nonconvex constraint violations.", "math": "Infeasibility measure: |w - f(x)| for auxiliary w = f(x)", "complexity": "O(n × m) where n = variables, m = auxiliaries per variable", "ref": ["Belotti et al., \"Branching and bounds tightening techniques\n     for non-convex MINLP\", Optimization Methods & Software, 2009"], "see": ["CouenneChooseStrong for strong branching variant", "CouenneObject for infeasibility computation"], "has_pass2": true}, "src/branch/CouenneThreeWayBranchObj.hpp": {"path": "layer-3/Couenne/src/branch/CouenneThreeWayBranchObj.hpp", "filename": "CouenneThreeWayBranchObj.hpp", "file": "CouenneThreeWayBranchObj.hpp", "brief": "Three-way spatial branching for continuous variables", "algorithm": "Three-Way Spatial Branching\n\nDivides a variable's domain into three parts instead of two,\nwhich can provide better convexification around the current\nLP solution point.\n\n**Three-way split:**\nGiven interval [l, u] and dividers lcrop_ and rcrop_:\n- Left branch:   [l, lcrop_]\n- Center branch: [lcrop_, rcrop_]\n- Right branch:  [rcrop_, u]\n\n**When useful:**\n- Current LP solution is interior to [l, u]\n- Better convexification needed around that point\n- Two-way split would create very unbalanced children\n\n**Branch order (firstBranch_):**\n- 0: left first\n- 1: center first (THREE_CENTER default)\n- 2: right first\n\nCenter-first often preferred since it contains the current point\nand may quickly find improving solutions.\n\n**Comparison to two-way:**\nThree-way creates more nodes but can reduce overall tree size\nby getting better bounds faster near the current solution.", "math": "Split: [l,u] → [l,lcrop_] ∪ [lcrop_,rcrop_] ∪ [rcrop_,u]", "complexity": "O(1) per branch operation; increases node count by 50%", "ref": ["Belotti, \"Couenne: a user's manual\", 2009"], "see": ["CouenneBranchingObject for standard two-way branching", "CouenneVarObject which can create three-way branches"], "has_pass2": true}, "src/branch/CouenneVarObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneVarObject.hpp", "filename": "CouenneVarObject.hpp", "file": "CouenneVarObject.hpp", "brief": "Variable-based branching object for MINLP", "algorithm": "Variable-Based Branching for Global Optimization\n\nBranching object that focuses on original problem variables rather\nthan auxiliary variables. Computes infeasibility by aggregating\nacross all auxiliaries that depend on this variable.\n\n**Infeasibility computation:**\nFor variable x, sum/min/max over all auxiliaries w = f(...,x,...)\nthe violation |w - f(...,x,...)|. This captures how much branching\non x could help close all related gaps.\n\n**Variable selection modes:**\n- OSI_SIMPLE: Use LP solution value directly\n- OSI_STRONG: Use strong branching estimate\n\n**Branch creation:**\nCan create either:\n- CouenneBranchingObject: standard two-way branch\n- CouenneThreeWayBranchObj: three-way spatial branch\n\n**isCuttable():**\nReturns whether we're on the \"bad\" side where cuts would help.\nIf not cuttable, branching is more likely to help.", "math": "inf(x) = aggregate_{w=f(...,x,...)} |w - f(...,x,...)|", "complexity": "O(deg(x)) where deg(x) = number of auxiliaries depending on x", "ref": ["Belotti et al., \"Branching and bounds tightening techniques\n     for non-convex MINLP\", Optimization Methods & Software, 2009"], "see": ["CouenneObject base class for auxiliary-based branching", "CouenneBranchingObject for branch execution"], "has_pass2": true}, "src/branch/CouenneComplBranchingObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneComplBranchingObject.hpp", "filename": "CouenneComplBranchingObject.hpp", "file": "CouenneComplBranchingObject.hpp", "brief": "Branching object for complementarity constraints", "algorithm": "Complementarity Branching for MPEC\n\nHandles branching on complementarity conditions x₁·x₂ = 0\n(or ≤ 0, ≥ 0 variants) arising in MPEC problems.\n\n**Branching strategy:**\nFor x₁·x₂ = 0, creates two children:\n- Left child: x₁ = 0 (fix first variable)\n- Right child: x₂ = 0 (fix second variable)\n\n**Sign variants:**\n- sign_ = 0: Classical x₁·x₂ = 0\n- sign_ = -1: x₁·x₂ ≤ 0 (opposite signs allowed)\n- sign_ = +1: x₁·x₂ ≥ 0 (same signs required)\n\n**MPEC context:**\nMathematical Programs with Equilibrium Constraints often have\ncomplementarity conditions like x ⊥ (Ax - b) ≥ 0, meaning\nx ≥ 0, Ax - b ≥ 0, and x·(Ax - b) = 0.", "math": "x₁ ⊥ x₂ ⟺ x₁ ≥ 0, x₂ ≥ 0, x₁·x₂ = 0", "complexity": "O(1) per branch operation", "ref": ["Bard, \"Practical Bilevel Optimization\", 1998", "Ferris & Pang, \"Engineering and Economic Applications of\n     Complementarity Problems\", SIAM Review, 1997"], "see": ["CouenneComplObject which creates these branching objects", "CouenneBranchingObject base class"], "has_pass2": true}, "src/branch/CouenneBranchingObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneBranchingObject.hpp", "filename": "CouenneBranchingObject.hpp", "file": "CouenneBranchingObject.hpp", "brief": "Spatial branching object for continuous and integer variables\n\nExecutes branching on a variable (which may be continuous) to\npartition the domain and tighten the convex relaxation.", "algorithm": "Branching Point Selection:\nWhere to split the variable domain matters significantly.\n\n  Common strategies:\n  - Midpoint: brpoint = (l + u) / 2\n  - LP solution: brpoint = x*_j (current relaxation value)\n  - Violation-based: Split where convexification error largest\n\n  Near-bound cropping (COUENNE_NEAR_BOUND):\n  - If brpoint near l or u, shift away to ensure meaningful split\n  - Prevents tiny subproblems that don't help convergence", "math": "For a convex envelope of f(x) on [l, u]:\n  Gap ≤ max_{x ∈ [l,u]} |f(x) - convenv(x)|\n  As (u - l) → 0, gap → 0 (envelope tightens)", "complexity": "O(propagation) + O(cut generation) per branch\nFBBT propagation is O(|expressions|), cut generation varies.", "ref": ["Belotti et al. (2009). \"Branching and bounds tightening techniques\n  for non-convex MINLP\". Optimization Methods & Software 24(4-5):597-634."], "see": ["CouenneObject for infeasibility computation", "CouenneChooseStrong for strong branching"], "has_pass2": true}, "src/branch/CouenneOrbitObj.hpp": {"path": "layer-3/Couenne/src/branch/CouenneOrbitObj.hpp", "filename": "CouenneOrbitObj.hpp", "file": "CouenneOrbitObj.hpp", "brief": "Orbital branching using symmetry detection (DISABLED)", "algorithm": "Orbital Branching via Nauty Symmetry Detection\n\nNOTE: This code is currently commented out/disabled.\n\nImplements orbital branching which exploits problem symmetry to\nreduce the search space. Uses nauty library to detect symmetries\nand organize variables into orbits.\n\n**Orbital branching concept:**\nIf variables {x1, x2, x3} are symmetric (permutable without\nchanging the problem), then branching on any one of them is\nequivalent. The orbit {x1, x2, x3} can be treated as a single\nbranching decision, pruning symmetric subtrees.\n\n**Node class:**\nStores variable metadata for symmetry detection:\n- index: variable index\n- coeff, lb, ub: bounds and coefficients\n- color: vertex coloring for nauty\n\n**Integration with nauty:**\n- Compute_Symmetry(): Calls nauty to find automorphisms\n- Print_Orbits(): Display detected orbits\n- ChangeBounds(): Update bounds respecting symmetry", "see": ["CouenneNauty for nauty integration", "CouenneObject base class"], "has_pass2": true}, "src/branch/CouenneVTObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneVTObject.hpp", "filename": "CouenneVTObject.hpp", "file": "CouenneVTObject.hpp", "brief": "Violation transfer branching for MINLP variables", "algorithm": "Violation Transfer Branching\n\nComputes variable infeasibility by aggregating violations from\nall auxiliary variables whose definitions depend on this variable.\n\n**Violation transfer concept:**\nFor variable x, measure infeasibility as:\nsum/min/max over all auxiliaries w where w = f(...,x,...):\n  |w - f(...,x,...)|\n\n**Rationale:**\nVariables appearing in many violated auxiliary definitions\nare good branching candidates since branching on them can\nsimultaneously reduce multiple auxiliary violations.\n\n**Comparison to CouenneVarObject:**\n- CouenneVarObject: Direct integrality/bound violation\n- CouenneVTObject: Aggregated auxiliary violations", "math": "VT_score(x) = Σ_{w: w=f(...,x,...)} |w - f(...,x,...)|", "complexity": "O(deg(x)) per variable where deg(x) = auxiliaries depending on x", "ref": ["Belotti et al., \"Branching and bounds tightening techniques\n     for non-convex MINLP\", Optimization Methods & Software, 2009"], "see": ["CouenneVarObject base class", "CouenneDepGraph for variable dependencies"], "has_pass2": true}, "src/branch/CouenneProjections.hpp": {"path": "layer-3/Couenne/src/branch/CouenneProjections.hpp", "filename": "CouenneProjections.hpp", "file": "CouenneProjections.hpp", "brief": "Point-to-segment projection utilities\n\nGeometric utilities for projecting points onto line segments,\nused in convexification cut generation and branching point selection.", "algorithm": "Point-to-Segment Projection:\n  Projects point p = (x₀,y₀) onto line segment S:\n  1. Compute foot of perpendicular to line: ax + by + c = 0\n     q = p - [(ap + c)/(a² + b²)]·(a,b)ᵀ (orthogonal projection)\n  2. If q_x ∈ [lb, ub]: projection is q (on segment interior)\n  3. Else: projection is nearest endpoint (lb,y(lb)) or (ub,y(ub))\n  4. Return Euclidean distance ||p - projection||", "math": "Projection formula:\n  Line L: ax + by + c = 0, point p = (x₀,y₀)\n  Distance to line: d = |ax₀ + by₀ + c|/√(a² + b²)\n  Projection: (xp,yp) = (x₀,y₀) - d·(a,b)/||(a,b)||\n  Segment clipping: xp = max(lb, min(ub, xp))", "complexity": "O(1) per projection - constant time operations\n  Used extensively in convexification and branching heuristics\n\n**Usage in branching:**\nHelps compute optimal branching points by finding closest\nfeasible points to current LP solution.", "see": ["CouenneBranchingObject for branching point selection"], "has_pass2": true}, "src/branch/CouenneComplObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneComplObject.hpp", "filename": "CouenneComplObject.hpp", "file": "CouenneComplObject.hpp", "brief": "Branching object for complementarity constraints", "algorithm": "Complementarity Object for MPEC Branching\n\nHandles branching on complementarity conditions x₁ · x₂ ≤ 0,\nx₁ · x₂ ≥ 0, or x₁ · x₂ = 0. Common in equilibrium problems,\nKKT conditions, and MPECs (Mathematical Programs with\nEquilibrium Constraints).\n\n**Complementarity branching:**\nFor x₁ · x₂ = 0, branch on either:\n- x₁ = 0 (one child node)\n- x₂ = 0 (other child node)\n\n**Sign handling (sign_):**\n- sign_ = 0: Classical x₁ · x₂ = 0\n- sign_ = +1: x₁ · x₂ ≤ 0\n- sign_ = -1: x₁ · x₂ ≥ 0\n\n**Infeasibility:**\nMeasures how much the complementarity is violated\nto determine branching priority.", "math": "inf(x₁,x₂) = max(0, sign_ × x₁ × x₂) for sign_ ∈ {-1,0,+1}", "complexity": "O(1) infeasibility check; creates CouenneComplBranchingObject", "ref": ["Ferris & Pang, \"Engineering and Economic Applications of\n     Complementarity Problems\", SIAM Review, 1997"], "see": ["CouenneObject base class", "CouenneComplBranchingObject for the branching execution"], "has_pass2": true}, "src/branch/CouenneNauty.hpp": {"path": "layer-3/Couenne/src/branch/CouenneNauty.hpp", "filename": "CouenneNauty.hpp", "file": "CouenneNauty.hpp", "brief": "Interface to nauty library for symmetry detection\n\nWraps the nauty graph automorphism library to detect symmetries\nin MINLP problems. Symmetry information enables orbital branching\nand isomorphism pruning to reduce the search space.", "algorithm": "Graph-Based Symmetry Detection (nauty):\n  Detects problem symmetries by computing graph automorphisms:\n  1. **Graph construction:** Build colored graph G from MINLP:\n     - Nodes: Variables (colored by type/bounds) + constraints (colored by type)\n     - Edges: Variable i appears in constraint j → edge (v_i, c_j)\n  2. **Partition refinement:** color_node() partitions by variable status\n     - Fixed variables form singleton cells\n     - Free variables may be permuted within same orbit\n  3. **Automorphism computation:** nauty computes Aut(G)\n     - Returns generators of symmetry group\n     - Orbits = equivalence classes of variables under Aut(G)\n  4. **Application:** Orbits enable orbital branching and pruning", "math": "Automorphism group:\n  Aut(G) = {π : V → V | π(E) = E and π preserves colors}\n  Orbit of v: Orb(v) = {π(v) : π ∈ Aut(G)}\n  |Aut(G)| can be exponentially large, stored via generators", "complexity": "O(n!) worst case, but practical: O(n²) for most graphs.\n  nauty uses canonical labeling with partition backtracking.\n  Preprocessing: Build graph O(nnz). Memory: O(n² + |generators|).", "ref": ["McKay & Piperno (2014). \"Practical graph isomorphism, II\".\n     J. Symbolic Computation 60:94-112.\n\n**Coloring (VarStatus):**\n- FIX_AT_ZERO, FIX_AT_ONE, FREE: Variable status for partitioning\n- color_node(): Assign color to node for refined symmetry"], "see": ["CouenneOrbitObj for orbital branching objects", "CouenneOrbitBranchingObj for orbital branch execution"], "has_pass2": true}, "src/branch/CouenneObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneObject.hpp", "filename": "CouenneObject.hpp", "file": "CouenneObject.hpp", "brief": "Branching object for auxiliary variables w = f(x)\n\nDefines branching for auxiliary variables based on their infeasibility\n|w - f(x)|. Creates branches to restore feasibility of the relation.", "algorithm": "Branching Point Selection Strategies:\nWhere to branch affects convergence significantly.\n\n  MID_INTERVAL: brpt = α·l + (1-α)·x* + α·u\n    - Default safe choice, always creates meaningful subproblems\n    - α parameter controls how much to pull toward midpoint\n\n  MIN_AREA: argmin_b Area(convenv on [l,b]) + Area(convenv on [b,u])\n    - Minimizes total relaxation gap over both children\n    - Requires knowledge of convex envelope structure\n\n  BALANCED: Balance improvement on both branches\n    - Aims for similar dual bound increase on both sides\n\n  LP_CENTRAL: brpt = x* (current LP solution value)\n    - Often good since LP suggests where \"action\" is\n\n  LP_CLAMPED: x* clamped to [l + ε(u-l), u - ε(u-l)]\n    - LP_CENTRAL but avoiding tiny subproblems near bounds", "math": "For convex envelope gap:\n  Area = integral_l^u |f(x) - convenv(x)| dx\n  MIN_AREA minimizes sum of child areas", "complexity": "infeasibility(): O(expression evaluation)\ngetBrPoint(): O(1) to O(envelope computation) depending on strategy", "see": ["CouenneBranchingObject for the actual branch creation", "CouenneChooseVariable for variable selection"], "has_pass2": true}, "src/branch/CouenneOrbitBranchingObj.hpp": {"path": "layer-3/Couenne/src/branch/CouenneOrbitBranchingObj.hpp", "filename": "CouenneOrbitBranchingObj.hpp", "file": "CouenneOrbitBranchingObj.hpp", "brief": "Orbital branching object using symmetry", "algorithm": "Orbital Branching for Symmetry Exploitation\n\nSpatial branching object that exploits problem symmetry via\norbital branching to prune symmetric subtrees.\n\n**Orbital branching:**\nWhen symmetry is detected (via nauty), branching on one variable\nin an orbit implicitly covers all symmetric variables in that orbit.\nThis avoids exploring symmetric portions of the search tree.\n\n**Key features:**\n- boundBranch(): Returns true if only bound changes (no cuts)\n- simulate_: Flag for simulating branch without execution\n- Integrates with CouenneNauty symmetry detection\n\n**Branch execution:**\nbranch() applies bound changes and optionally generates\nconvexification cuts, with symmetry-aware pruning.", "see": ["CouenneNauty for symmetry detection", "CouenneOrbitObj which creates these branching objects", "CouenneBranchingObject base class"], "has_pass2": true}, "src/branch/CouenneSOSObject.hpp": {"path": "layer-3/Couenne/src/branch/CouenneSOSObject.hpp", "filename": "CouenneSOSObject.hpp", "file": "CouenneSOSObject.hpp", "brief": "Special Ordered Set (SOS) branching for Couenne", "algorithm": "Special Ordered Set Branching with Bound Tightening\n\nExtends OsiSOS to include Couenne-specific functionality like\nbound tightening and convexification cut generation at branching.\n\n**SOS Types:**\n- SOS Type 1: At most one variable can be nonzero\n- SOS Type 2: At most two adjacent variables can be nonzero\n\n**CouenneSOSObject:**\nWraps OsiSOS with:\n- problem_: Link to CouenneProblem for bound tightening\n- reference_: Associated auxiliary variable\n- doFBBT_: Enable FBBT at branching\n- doConvCuts_: Add convexification cuts at branching\n\n**CouenneSOSBranchingObject:**\nExecutes the SOS branching, dividing variables into sets\nwhere the SOS constraint can be separately enforced.\n\n**TODO in code:**\nNotes extension to handle Σxᵢ ≤ k constraints with small k\nusing SOS-like branching instead of individual variable branching.", "math": "SOS2: at most 2 adjacent of {x₁,...,xₙ} nonzero", "complexity": "O(n log n) for initial ordering; O(log n) per branch", "ref": ["Beale & Tomlin, \"Special Facilities in a General Mathematical\n     Programming System for Non-convex Problems Using Ordered Sets\n     of Variables\", 1970"], "see": ["OsiSOS base class", "CbcSOS for CBC's SOS implementation"], "has_pass2": true}, "src/branch/CouenneChooseStrong.hpp": {"path": "layer-3/Couenne/src/branch/CouenneChooseStrong.hpp", "filename": "CouenneChooseStrong.hpp", "file": "CouenneChooseStrong.hpp", "brief": "Strong branching for global MINLP optimization\n\nExtends Bonmin's strong branching to handle nonconvex constraints\nby evaluating actual LP bound improvement from branching.", "algorithm": "Strong Branching for Nonconvex MINLP:\n  Selects branching variable by evaluating actual bound improvement:\n  1. Build candidate list from fractional/violated variables\n  2. For each candidate x_j in list (up to limit):\n     a. simulateBranch(down): Solve LP with x_j ≤ ⌊x_j*⌋\n     b. simulateBranch(up): Solve LP with x_j ≥ ⌈x_j*⌉\n     c. Record Δ_down, Δ_up = bound improvements\n  3. Score: s_j = (1-μ)·min(Δ_down,Δ_up) + μ·max(Δ_down,Δ_up)\n     or product: s_j = (ε + Δ_down)·Δ_up (reliability-like)\n  4. Select j* = argmax s_j, prune if both branches infeasible", "math": "Pseudocost updates from strong branching:\n  If pseudoUpdateLP_: update pseudocosts from LP improvement\n  ψ_j^- = Δ_down/(x_j* - ⌊x_j*⌋), ψ_j^+ = Δ_up/(⌈x_j*⌉ - x_j*)\n  These improve future branching decisions without strong branching", "complexity": "O(k · LP) where k = candidates evaluated\n  Expensive but critical for global optimization bound quality\n  Trade-off: evaluation cost vs. tree size reduction", "ref": ["Achterberg, Koch, Martin (2005). \"Branching rules revisited\".\n     Operations Research Letters 33:42-54.\n\n**Selection criteria (estimateProduct_):**\n- false: Convex combination of min/max estimates (classic)\n- true: Product (1e-6 + min) * max (reliability-like)\n\n**Pseudocost updates (pseudoUpdateLP_):**\nIf true, updates pseudocosts using distance between LP point\nand resulting branch LP solutions."], "see": ["CouenneChooseVariable for simpler selection", "CouenneBranchingObject for branch execution"], "has_pass2": true}, "src/convex/CouenneCutGenerator.hpp": {"path": "layer-3/Couenne/src/convex/CouenneCutGenerator.hpp", "filename": "CouenneCutGenerator.hpp", "file": "CouenneCutGenerator.hpp", "brief": "Main convexification cut generator for global MINLP optimization\n\nGenerates linear outer approximation cuts to build convex relaxations\nof nonconvex MINLPs. Works with the symbolic CouenneProblem representation.\n\n**Cut generation (generateCuts):**\n1. For each auxiliary variable w = f(x), generate linearization cuts\n2. Cuts are based on expression type (convex envelope, secants, tangents)\n3. Uses current LP solution as linearization point\n\n**Convexification types (conv_type):**\n- Current-point linearization (most common)\n- Multi-point sampling for tighter relaxations\n\n**Cut types generated:**\n- Tangent cuts for convex functions\n- Secant cuts for concave functions\n- McCormick envelope cuts for bilinear terms\n- Specialized cuts for sin, cos, exp, log, etc.\n\n**Helper methods:**\n- addEnvelope(): Generate convex/concave envelope for univariate function\n- addSegment(): Add secant line between two points\n- addTangent(): Add tangent at given point\n- createCut(): Build and validate OsiRowCut", "algorithm": "Spatial Branch-and-Bound with LP Relaxation:\nGlobal optimization via convex relaxation + branching. At each node:\n1. Solve LP relaxation (convex underestimator of nonconvex MINLP)\n2. If LP infeasible → prune node\n3. If LP solution is MINLP-feasible → update incumbent\n4. Otherwise: add convexification cuts and/or branch", "math": "Convex envelope for w = f(x) on [l,u]:\n- Convex f: tangent cuts w ≥ f(x̄) + f'(x̄)(x - x̄) at sample points\n- Concave f: secant cut w ≤ f(l) + (f(u)-f(l))/(u-l) · (x-l)\n- Bilinear w = xy: McCormick envelope\n  w ≥ l_y·x + l_x·y - l_x·l_y (underestimator)\n  w ≥ u_y·x + u_x·y - u_x·u_y (underestimator)\n  w ≤ u_y·x + l_x·y - l_x·u_y (overestimator)\n  w ≤ l_y·x + u_x·y - u_x·l_y (overestimator)", "complexity": "Cut generation: O(n) per node where n = #auxiliary variables.\nEach operator type has constant-time cut generation.\nLP solve: O(m³) worst case, typically much faster with warm starts.", "ref": ["McCormick (1976). \"Computability of global solutions to factorable\n  nonconvex programs\". Mathematical Programming 10:147-175.", "Al-Khayyal & Falk (1983). \"Jointly constrained biconvex programming\".\n  Mathematics of Operations Research 8(2):273-286.", "Tawarmalani & Sahinidis (2002). \"Convexification and Global\n  Optimization in Continuous and Mixed-Integer Nonlinear Programming\".\n  Kluwer Academic Publishers. [Convex envelopes for various operators]"], "see": ["CouenneProblem for the symbolic problem representation", "expression::generateCuts() for per-operator cut generation"], "has_pass2": true}, "src/bound_tightening/CouenneFixPoint.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneFixPoint.hpp", "filename": "CouenneFixPoint.hpp", "file": "CouenneFixPoint.hpp", "brief": "Fixpoint-based bound tightening via constraint propagation\n\nImplements Feasibility-Based Bound Tightening (FBBT) using fixpoint\niteration. Propagates bounds through expression DAG until no further\ntightening is possible.", "algorithm": "Feasibility-Based Bound Tightening (FBBT):\nPropagate variable bounds through constraint DAG to fixpoint:\n1. Initialize: [l⁰, u⁰] from problem definition\n2. Forward propagation for each expression w = f(x):\n   - Compute [w_L, w_U] from [x_L, x_U] using interval arithmetic\n3. Backward propagation (implied bounds):\n   - For w = f(x) with [w_L, w_U], derive tighter [x_L, x_U]\n   - Example: w = x·y, w_U = 10, y ∈ [2,5] ⟹ x ≤ 10/2 = 5\n4. Iterate until fixpoint: [l^k, u^k] = [l^{k-1}, u^{k-1}] or k > limit\n\nextendedModel_ creates auxiliary LP rows for tighter propagation.", "math": "Interval arithmetic rules (forward):\n  [a,b] + [c,d] = [a+c, b+d]\n  [a,b] · [c,d] = [min(ac,ad,bc,bd), max(ac,ad,bc,bd)]\n  exp([a,b]) = [exp(a), exp(b)]  (monotone functions)", "complexity": "O(k · |V| · |E|) where k = iterations, |V| = variables,\n|E| = expression nodes. Typically k = O(1) to O(depth of DAG).", "ref": ["Belotti et al. (2009). \"Branching and bounds tightening techniques for\n  non-convex MINLP\". Optimization Methods & Software 24(4-5):597-634."], "see": ["CouenneProblem::boundTightening() for main bound tightening entry", "expression::impliedBound() for backward propagation"], "has_pass2": true}, "src/bound_tightening/CouenneBTPerfIndicator.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneBTPerfIndicator.hpp", "filename": "CouenneBTPerfIndicator.hpp", "file": "CouenneBTPerfIndicator.hpp", "brief": "Performance metrics for bound tightening\n\nTracks effectiveness of bound tightening methods (FBBT, OBBT, etc.)\nby measuring how much bounds are reduced and how many variables are fixed.\n\n**Metrics tracked:**\n- nFixed_: Number of variables fixed (lb == ub)\n- boundRatio_: Average bound width shrinkage ratio\n- shrunkInf_: Bounds that became finite from infinite\n- shrunkDoubleInf_: [-inf,inf] that became [a,inf] or [-inf,b]\n- nProvedInfeas_: Number of infeasibility proofs\n\n**Usage:**\n1. Call setOldBounds() before bound tightening\n2. Run bound tightening\n3. Call update() with new bounds\n4. Metrics accumulated for end-of-run summary\n\n**Timing:**\n- totalTime_: CPU time spent in this bound tightener\n- nRuns_: Number of invocations", "see": ["CouenneFixPoint which uses this for FBBT stats", "CouenneAggrProbing which uses this for OBBT stats"], "has_pass2": false}, "src/bound_tightening/CouenneMultiVarProbe.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneMultiVarProbe.hpp", "filename": "CouenneMultiVarProbe.hpp", "file": "CouenneMultiVarProbe.hpp", "brief": "Multi-variable probing for bound tightening", "algorithm": "Multi-Variable Probing for OBBT\n\nExtension of single-variable probing that considers multiple\nvariables simultaneously for bound tightening. Can detect\nimplications that single-variable probing would miss.\n\n**Multi-variable probing:**\nInstead of fixing one variable at a time, considers combinations\nof variable settings to derive stronger implied bounds.\n\n**Example:**\nSingle-variable probing on x1 and x2 separately might not tighten bounds,\nbut probing x1=lb AND x2=ub together might prove infeasibility or\nderive new bounds.\n\n**Parameters:**\n- maxTime_: Maximum time for probing\n- numCols_: Number of columns to consider", "see": ["CouenneAggrProbing for single-variable OBBT", "CouenneFixPoint for FBBT"], "has_pass2": true}, "src/bound_tightening/CouenneSparseBndVec.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneSparseBndVec.hpp", "filename": "CouenneSparseBndVec.hpp", "file": "CouenneSparseBndVec.hpp", "brief": "Sparse vector with O(1) initialization for bound tracking\n\nEfficient sparse+dense hybrid data structure for tracking which\nvariables had bounds tightened. Avoids O(n) initialization cost.\n\n**Data structure (Briggs-Torczon):**\n- dInd_[0..n_-1]: Dense list of assigned indices\n- sInd_[i]: Position of index i in dense list (garbage if unset)\n- data_[i]: Value at index i (garbage if unset)\n\n**Key operations:**\n- operator[](i): O(1) access, auto-initializes new entries\n- reset(): O(1) clear (just set n_ = 0)\n- nElements(): Number of assigned entries\n- indices()/data(): Access for iteration\n\n**Why useful in FBBT:**\nIn bound tightening, typically k << n variables have bounds\ntightened. This structure allows O(k) iteration over tightened\nbounds without O(n) initialization/cleanup overhead.\n\n**Note:**\nValgrind will complain about uninitialized reads - this is expected\ndue to the lazy initialization design.", "see": ["CouenneFixPoint which uses this for bound change tracking"], "has_pass2": false}, "src/bound_tightening/CouenneInfeasCut.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneInfeasCut.hpp", "filename": "CouenneInfeasCut.hpp", "file": "CouenneInfeasCut.hpp", "brief": "Signal infeasibility via fictitious cut\n\nMechanism for bound tighteners to signal detected infeasibility\nto the node solver without early termination.\n\n**WipeMakeInfeas():**\nAdds a fictitious cut 1 ≤ x₀ ≤ -1 to the cut set. This\nimpossible constraint signals to subsequent components that\nthe node is infeasible.\n\n**isWiped():**\nChecks whether the cut set contains the infeasibility signal.\nCut generators should check this before expensive operations.\n\n**Why needed:**\nIn Cgl's cut generator framework, generators cannot directly\nprune nodes. This sentinel cut mechanism allows bound tighteners\nto signal infeasibility discovered during FBBT/OBBT.", "see": ["CouenneFixPoint which may detect infeasibility", "CouenneAggrProbing which may detect infeasibility"], "has_pass2": false}, "src/bound_tightening/CouenneAggrProbing.hpp": {"path": "layer-3/Couenne/src/bound_tightening/CouenneAggrProbing.hpp", "filename": "CouenneAggrProbing.hpp", "file": "CouenneAggrProbing.hpp", "brief": "Aggressive probing for bound tightening\n\nImplements Optimality-Based Bound Tightening (OBBT) through aggressive\nprobing. Temporarily fixes a variable bound and solves the resulting\nsubproblem to determine if a tighter bound is achievable.", "algorithm": "Optimality-Based Bound Tightening (OBBT):\nTighten bounds by solving auxiliary optimization problems:\nFor each variable xᵢ with current bounds [lᵢ, uᵢ]:\n1. Lower bound tightening:\n   lᵢ' = min{xᵢ : x ∈ P} where P is current relaxation\n   If lᵢ' > lᵢ: tighten to lᵢ ← lᵢ'\n2. Upper bound tightening:\n   uᵢ' = max{xᵢ : x ∈ P}\n   If uᵢ' < uᵢ: tighten to uᵢ ← uᵢ'\n3. Resource limits: maxTime_, maxNodes_ per probe\n4. Early termination: maxFailedSteps_ consecutive non-improvements\n\nParallelization: Variables can be probed independently.\nprobeVariable(index, probeLower) probes single variable.", "math": "OBBT guarantees:\nFor convex P: l' = optimal is exact lower bound.\nFor nonconvex: l' from relaxation may not be achievable,\nbut still provides valid (possibly weak) bound.", "complexity": "O(2n · subproblem_solve) for full OBBT.\nEach subproblem: NLP solve O(n³) per Newton iteration.\nVery expensive - use sparingly or with time limits.", "ref": ["Tawarmalani, Sahinidis (2005). \"A polyhedral branch-and-cut approach\n  to global optimization\". Mathematical Programming 103(2):225-249."], "see": ["CouenneFixPoint for cheaper FBBT alternative", "CouenneMultiVarProbe for multi-variable probing"], "has_pass2": true}, "src/main/CouenneOSInterface.hpp": {"path": "layer-3/Couenne/src/main/CouenneOSInterface.hpp", "filename": "CouenneOSInterface.hpp", "file": "CouenneOSInterface.hpp", "brief": "Optimization Services (OS) interface for Couenne\n\nReads optimization problems from OSInstance format (COIN-OR\nOptimization Services) and converts to Couenne representation.\n\n**OSInstance format:**\nXML-based representation for optimization problems defined by\nthe Optimization Services project. Supports linear, quadratic,\nand nonlinear constraints with expression trees.\n\n**Key methods:**\n- getCouenneProblem(): Convert OSInstance → CouenneProblem\n- getTMINLP(): Wrap as Bonmin TMINLP\n- writeSolution(): Output in OS solution format\n\n**Usage:**\nAlternative to AMPL interface for problems defined via\nOptimization Services XML or programmatic APIs.", "see": ["CouenneUserInterface base class", "CouenneAmplInterface alternative AMPL input"], "has_pass2": false}, "src/main/CouenneBab.hpp": {"path": "layer-3/Couenne/src/main/CouenneBab.hpp", "filename": "CouenneBab.hpp", "file": "CouenneBab.hpp", "brief": "Main Branch-and-Bound driver for Couenne\n\nExtends Bonmin::Bab to add Couenne-specific functionality\nfor spatial branch-and-bound on nonconvex MINLPs.\n\n**Inheritance:**\nCouenneBab → Bonmin::Bab → CbcModel\n\n**Key additions over Bonmin::Bab:**\n- Stores CouenneProblem pointer for access to expression DAG\n- Overrides bestSolution() and bestObj() for proper handling\n- bestBound() returns min of parent bound and best objective\n\n**Usage:**\nCalled from BonCouenneSetup after problem setup.\nThe branchAndBound() method runs the full spatial B&B algorithm,\nusing Couenne's convexification, bound tightening, and\nbranching strategies.\n\n**Solution retrieval:**\n- bestSolution(): Optimal variable values (or best known)\n- bestObj(): Objective value at best solution\n- bestBound(): Lower bound on optimal value", "see": ["BonCouenneSetup which configures and launches this", "Bonmin::Bab base class", "CouenneProblem for the problem representation"], "has_pass2": false}, "src/main/BonCouenneSetup.hpp": {"path": "layer-3/Couenne/src/main/BonCouenneSetup.hpp", "filename": "BonCouenneSetup.hpp", "file": "BonCouenneSetup.hpp", "brief": "Main setup class for Couenne global optimizer\n\nExtends Bonmin's setup to configure Couenne's global optimization\ncomponents including cut generators, bound tightening, and heuristics.\n\n**Initialization (InitializeCouenne):**\n1. Read AMPL model and options\n2. Create CouenneProblem representation\n3. Standardize problem (create auxiliary variables)\n4. Register cut generators (convexification, bound tightening)\n5. Configure branching and heuristics\n\n**Key components configured:**\n- CouenneCutGenerator: Convexification cuts\n- CouenneFixPoint: FBBT bound tightening\n- CouenneFeasPump: Feasibility pump heuristic\n- CouenneChooseVariable: Branching variable selection\n\n**SmartAsl:**\nReference-counted wrapper for ASL pointer (AMPL Solver Library).", "see": ["BonminSetup for base class", "CouenneProblem for the problem representation"], "has_pass2": false}, "src/main/BonCouenneInfo.hpp": {"path": "layer-3/Couenne/src/main/BonCouenneInfo.hpp", "filename": "BonCouenneInfo.hpp", "file": "BonCouenneInfo.hpp", "brief": "Information passing between B&B components\n\nExtends Bonmin's BabInfo with Couenne-specific information,\nparticularly storage of NLP solutions found during search.\n\n**NlpSolution class:**\nReference-counted storage for NLP solutions:\n- n_: Number of variables\n- sol_[]: Variable values\n- objVal_: Objective function value\n\n**CouenneInfo:**\n- nlpSols_: List of all NLP solutions found\n- addSolution(): Record a new NLP solution\n- NlpSolutions(): Access stored solutions\n\n**Usage:**\nWhen Couenne finds feasible NLP solutions (from heuristics\nor at B&B nodes), they are stored here for incumbent tracking\nand warm-starting purposes.", "see": ["Bonmin::BabInfo base class", "CouenneFeasPump which generates NLP solutions"], "has_pass2": false}, "src/problem/CouenneJournalist.hpp": {"path": "layer-3/Couenne/src/problem/CouenneJournalist.hpp", "filename": "CouenneJournalist.hpp", "file": "CouenneJournalist.hpp", "brief": "Couenne-specific logging categories using Ipopt's Journalist\n\nExtends Ipopt's Journalist logging facility with Couenne-specific\ncategories for selective debug output.\n\n**Log categories defined:**\n- J_BRANCHING: Branch-and-bound variable selection\n- J_BOUNDTIGHTENING: FBBT and OBBT progress\n- J_CONVEXIFYING: Cut generation and convexification\n- J_PROBLEM: Problem construction and reformulation\n- J_NLPHEURISTIC: NLP solve heuristics\n- J_DISJCUTS: Disjunctive cut generation (CGLP)\n- J_REFORMULATE: Expression reformulation\n- J_COUENNE: General Couenne messages\n\n**Usage:**\n```cpp\njnlst->Printf(J_SUMMARY, J_BRANCHING,\n              \"Selected var %d with score %.2f\\n\", idx, score);\n```\n\n**Verbosity levels (from Ipopt):**\n- J_NONE, J_ERROR, J_WARNING, J_SUMMARY, J_DETAILED, ...", "see": ["Ipopt::Journalist for full logging API"], "has_pass2": false}, "src/problem/CouenneSolverInterface.hpp": {"path": "layer-3/Couenne/src/problem/CouenneSolverInterface.hpp", "filename": "CouenneSolverInterface.hpp", "file": "CouenneSolverInterface.hpp", "brief": "OsiSolverInterface wrapper with bound tightening integration\n\nTemplate class wrapping any OsiSolverInterface (typically OsiClpSolverInterface)\nwith Couenne-specific enhancements for spatial B&B.\n\n**Key features:**\n1. Applies FBBT before resolve() to tighten bounds\n2. Uses expression-based isInteger() for auxiliary variables\n3. Integrates NLP solutions into branching decisions\n\n**Bound tightening integration:**\n- tightenBounds(): Apply FBBT before LP solve\n- tightenBoundsCLP(): Clp-specific variable bound analysis\n- tightenBoundsCLP_Light(): Lightweight version for hot starts\n\n**Infeasibility tracking:**\n- knowInfeasible_: Detected during bound tightening\n- knowDualInfeasible_: Continuous relaxation is unbounded\n- isProvenPrimalInfeasible(): May detect before LP solve\n\n**Template usage:**\nCouenneSolverInterface<OsiClpSolverInterface> wraps Clp\nwith Couenne's bound tightening infrastructure.", "see": ["CouenneCutGenerator for the convexification engine", "CouenneFixPoint for FBBT implementation"], "has_pass2": false}, "src/problem/CouenneRecordBestSol.hpp": {"path": "layer-3/Couenne/src/problem/CouenneRecordBestSol.hpp", "filename": "CouenneRecordBestSol.hpp", "file": "CouenneRecordBestSol.hpp", "brief": "Best solution recording with feasibility tracking\n\nMaintains the best feasible solution found during optimization,\nwith detailed violation tracking for debugging and reporting.\n\n**Initial domain info:**\n- cardInitDom: Problem dimension\n- initIsInt[]: Integer variable flags\n- listInt: Indices of integer variables\n- initDomLb[], initDomUb[]: Original bounds (for feasibility check)\n\n**Best solution state:**\n- hasSol: True if any solution has been recorded\n- sol[]: Current best solution vector\n- val: Objective value of best solution\n- maxViol: Maximum violation (bound, integrality, or constraint)\n\n**Modified solution workspace:**\n- modSol[], modSolVal, modSolMaxViol: Temporary storage for\n  checkNLP2 and update operations before committing\n\n**update() methods:**\n- update(sol, card, val, viol): Update if val < current val\n- update(): Commit modSol if modSolVal < current val\n\n**compareAndSave():**\nCompare two solutions, keep better one with finite value (<1e49).\nReturns: -1 if both infinite, 0 if solA saved, 1 if solB saved.", "see": ["CouenneProblem which owns the CouenneRecordBestSol", "checkNLP2 which uses modSol for temporary evaluation"], "has_pass2": false}, "src/problem/CouenneProblem.hpp": {"path": "layer-3/Couenne/src/problem/CouenneProblem.hpp", "filename": "CouenneProblem.hpp", "file": "CouenneProblem.hpp", "brief": "Central MINLP problem representation with expression DAG\n\nThe heart of Couenne's global optimization approach. Represents MINLPs\nsymbolically as expression trees, enabling automatic convexification,\nbound propagation, and reformulation.\n\n**Key data structures:**\n- variables_: Original, auxiliary, and defined variables\n- objectives_/constraints_: Symbolic expressions\n- graph_: Dependency graph for evaluation ordering\n- auxSet_: Set of auxiliary variables w = f(x) for linearization\n\n**Bound tightening methods:**\n- FBBT (doFBBT_): Feasibility-based bound tightening via constraint propagation\n- OBBT (doOBBT_): Optimality-based bound tightening via LP solves\n- RCBT (doRCBT_): Reduced-cost bound tightening\n- ABT (doABT_): Aggressive bound tightening via domain partitioning\n\n**Reformulation (standardize()):**\nConverts nonlinear expressions into auxiliary variable definitions\nw = f(x), enabling generation of convex relaxations for each operator.\n\n**Symmetry handling:**\nUses nauty library for graph automorphism detection and orbital branching.", "algorithm": "Factorable Programming / Auxiliary Variable Reformulation:\nTransform general nonconvex MINLP into standard form via introduction\nof auxiliary variables for each nonlinear operator. Example:\n  Original: min x·sin(y²)\n  Reformulated: min w₁  s.t. w₁ = x·w₂, w₂ = sin(w₃), w₃ = y²\nEach auxiliary w_i = f_i(x) gets individual convexification.", "math": "Bound propagation (FBBT) uses interval arithmetic:\nGiven w = f(x,y) with x ∈ [x_L, x_U], y ∈ [y_L, y_U]:\n- Forward: [w_L, w_U] = f([x_L,x_U], [y_L,y_U])\n- Backward: [x_L,x_U] ← [x_L,x_U] ∩ f⁻¹([w_L,w_U], [y_L,y_U])\nOBBT solves: min/max x_i s.t. LP relaxation ≤ cutoff.\nIterate until fixed point or iteration limit.", "complexity": "Reformulation: O(e) where e = expression size.\nFBBT: O(n·k) per pass where n = #variables, k = avg. constraint length.\nOBBT: O(2n·LP) per round, typically 1-2 rounds effective.", "ref": ["Belotti et al. (2009). \"Branching and bounds tightening techniques\n  for non-convex MINLP\". Optimization Methods & Software 24(4-5):597-634.", "Tawarmalani & Sahinidis (2005). \"A polyhedral branch-and-cut approach\n  to global optimization\". Mathematical Programming 103:225-249."], "see": ["expression for the expression base class", "CouenneCutGenerator for convexification cut generation"], "has_pass2": true}, "src/problem/CouenneProblemElem.hpp": {"path": "layer-3/Couenne/src/problem/CouenneProblemElem.hpp", "filename": "CouenneProblemElem.hpp", "file": "CouenneProblemElem.hpp", "brief": "Constraint and objective classes for Couenne problems\n\nDefines the building blocks for optimization problems:\nconstraints with expression bounds and objective functions.\n\n**CouenneConstraint:**\nRepresents lb_ <= body_ <= ub_ where all three are expressions:\n- body_: The constraint expression (e.g., exp(x1+x2))\n- lb_: Lower bound expression (defaults to 0 or -INFINITY)\n- ub_: Upper bound expression (defaults to 0 or +INFINITY)\n\n**Default bound handling:**\n- Both NULL → equality constraint (lb = ub = 0)\n- lb NULL, ub set → lb = -INFINITY (one-sided upper)\n- ub NULL, lb set → ub = +INFINITY (one-sided lower)\n\n**CouenneObjective:**\nMinimization objective wrapper. Maximization problems are\nconverted by negating the objective during problem input.\n\n**standardize():**\nDecomposes complex expressions into auxiliary variable\ndefinitions for reformulation-linearization.", "see": ["CouenneProblem which owns constraints and objectives", "expression for the expression tree classes"], "has_pass2": false}, "src/problem/CouenneGlobalCutOff.hpp": {"path": "layer-3/Couenne/src/problem/CouenneGlobalCutOff.hpp", "filename": "CouenneGlobalCutOff.hpp", "file": "CouenneGlobalCutOff.hpp", "brief": "Global best solution and cutoff value storage\n\nManages the incumbent (best known feasible solution) and its\nobjective value across the entire B&B search.\n\n**Role in B&B:**\n- Provides cutoff value for pruning nodes\n- Stores best solution found so far\n- Updated when heuristics or B&B find improvements\n\n**Members:**\n- cutoff_: Objective value of best solution (upper bound for min)\n- sol_: Variable values of best solution\n- valid_: Whether stored solution corresponds to cutoff\n\n**Thread safety note:**\nIn parallel environments, this would need synchronization.\nCurrently assumes sequential access.\n\n**setCutOff():**\nUpdates cutoff and optionally stores the solution vector.\nRequires CouenneProblem pointer for size information.", "see": ["CouenneProblem which owns the GlobalCutOff", "CouenneFeasPump which updates cutoff when finding solutions"], "has_pass2": false}, "src/standardize/CouenneLQelems.hpp": {"path": "layer-3/Couenne/src/standardize/CouenneLQelems.hpp", "filename": "CouenneLQelems.hpp", "file": "CouenneLQelems.hpp", "brief": "Linear and quadratic term storage for standardization\n\nData structures for collecting linear and quadratic terms during\nexpression standardization (conversion to auxiliary variable form).\n\n**quadElem:**\nSingle quadratic term c·xᵢ·xⱼ:\n- varI_, varJ_: Variable pointers\n- coeff_: Coefficient c\n\n**LinMap:**\nSparse map of linear terms: index → coefficient\n- insert(index, coe): Add/accumulate coefficient\n- Auto-removes zero entries\n\n**QuadMap:**\nSparse map of quadratic terms: (i,j) → coefficient\n- insert(indI, indJ, coe): Add/accumulate coefficient\n- Auto-removes zero entries\n\n**Usage:**\nDuring standardization, expressions like x² + 2xy + 3x + 4\nare decomposed into LinMap (3x + 4 constant) and QuadMap\n(x² + 2xy). These are then converted to auxiliary form.", "see": ["CouenneProblem::standardize() which uses these structures"], "has_pass2": false}, "src/heuristics/CouenneFeasPump.hpp": {"path": "layer-3/Couenne/src/heuristics/CouenneFeasPump.hpp", "filename": "CouenneFeasPump.hpp", "file": "CouenneFeasPump.hpp", "brief": "Feasibility Pump heuristic for nonconvex MINLP\n\nAlternates between NLP and MILP solves to find feasible solutions:\n- MILP phase: Find integer point closest to NLP solution\n- NLP phase: Find NLP-feasible point closest to integer solution", "algorithm": "Feasibility Pump for Nonconvex MINLP:\nAlternating projection between continuous and integer feasible sets:\n1. Solve NLP relaxation: nSol = argmin{f(x) : g(x) ≤ 0}\n2. MILP phase (milpPhase):\n   iSol = argmin{w_D·||x_I - nSol_I|| + w_H·xᵀHx + w_O·cᵀx : Ax ≤ b, x_I ∈ Z}\n   where w_D = multDistMILP_, w_H = multHessMILP_, w_O = multObjFMILP_\n3. NLP phase (nlpPhase):\n   nSol = argmin{w_D·||x_I - iSol_I|| + w_H·xᵀHx + w_O·f(x) : g(x) ≤ 0}\n4. If ||nSol_I - iSol_I|| < ε: return nSol (feasible found)\n5. Tabu management: If cycling detected, apply tabuMgt_ policy\n6. Fade weights: wᵢ ← fadeMult_·wᵢ, goto step 2\n\nDistance options (compDistInt_):\n- FP_DIST_INT: Only integer variables\n- FP_DIST_ALL: All variables\n- FP_DIST_POST: Post-process with LP", "math": "Weighted objective in MILP phase:\n  min α·Σᵢ|xᵢ - nSolᵢ| + β·xᵀHx + γ·cᵀx\nwhere α + β + γ = 1 and coefficients fade toward γ = 1 (original objective).", "complexity": "O(maxIter_ · (MILP_solve + NLP_solve)).\nEach iteration: MILP is O(2^|I|) worst, NLP is O(n³) per Newton step.", "ref": ["Bonami, Gonçalves (2012). \"Heuristics for convex mixed integer nonlinear\n  programs\". Computational Optimization and Applications 51(2):729-747."], "see": ["CouenneTNLP for the NLP interface", "CouenneFPpool for solution pool management"], "has_pass2": true}, "src/heuristics/CouenneIterativeRounding.hpp": {"path": "layer-3/Couenne/src/heuristics/CouenneIterativeRounding.hpp", "filename": "CouenneIterativeRounding.hpp", "file": "CouenneIterativeRounding.hpp", "brief": "Iterative rounding heuristic for nonconvex MINLP", "algorithm": "Iterative Rounding Heuristic (F-IR / I-IR)\n\nAlternates between MILP and NLP solves to find feasible solutions:\n1. Solve MILP relaxation to get integer assignment\n2. Fix integers, solve NLP for continuous variables\n3. Add cuts, iterate\n\n**Two modes:**\n- F-IR (Feasibility): Find first feasible solution using interior-point warmstart\n- I-IR (Improvement): Improve existing solution using local branching\n\n**Key parameters:**\n- maxRoundingIter_: Max MILP-NLP iterations per call\n- maxFirPoints_: Number of starting points in F-IR\n- omega_: Factor for log-barrier parameter (interior point)\n- baseLbRhs_: Local branching neighborhood size\n\n**Aggressiveness levels:**\n- 0: Light (5 iterations, 60s)\n- 1: Medium (10 iterations, 120s)\n- 2: Heavy (20 iterations, 300s)\n\n**CPLEX integration:**\nIf CPLEX available, uses it for MILP; otherwise uses CBC heuristics.", "see": ["CouenneFeasPump for the specialized feasibility pump", "CbcHeuristic base class"], "has_pass2": true}, "src/heuristics/BonInitHeuristic.hpp": {"path": "layer-3/Couenne/src/heuristics/BonInitHeuristic.hpp", "filename": "BonInitHeuristic.hpp", "file": "BonInitHeuristic.hpp", "brief": "Heuristic to inject initial NLP solution into Cbc\n\nCommunicates the initial NLP solution (computed before B&B starts)\nto Cbc as a known feasible solution.\n\n**Purpose:**\nWhen Couenne solves the root node NLP and finds a feasible MINLP\nsolution, this heuristic stores it so Cbc can use it as the\ninitial incumbent.\n\n**Usage:**\n```cpp\n// After initial NLP solve\nInitHeuristic heur(objValue, solution, problem);\ncbcModel.addHeuristic(&heur);\n```\n\n**solution() method:**\nReturns the stored initial solution on first call. Subsequent\ncalls return 0 (no new solution) since the initial solution\nhas already been provided.", "see": ["CouenneBab which adds this heuristic", "NlpSolveHeuristic for NLP solves during B&B"], "has_pass2": false}, "src/heuristics/CouenneFPpool.hpp": {"path": "layer-3/Couenne/src/heuristics/CouenneFPpool.hpp", "filename": "CouenneFPpool.hpp", "file": "CouenneFPpool.hpp", "brief": "Solution pool and tabu list for Feasibility Pump\n\nManages collections of solutions found during FP iterations,\nproviding restart points and avoiding revisiting similar solutions.\n\n**CouenneFPsolution:**\nWrapper for a solution with cached infeasibility metrics:\n- nNLinf_: Count of nonlinear infeasibilities\n- nIinf_: Count of integer infeasibilities\n- maxNLinf_/maxIinf_: Maximum violations\n- objVal_: Objective function value\n\n**Comparison modes (what_to_compare):**\n- SUM_NINF: Compare by sum of infeasibility counts\n- SUM_INF: Compare by sum of infeasibility magnitudes\n- OBJVAL: Compare by objective value\n- ALL_VARS: Compare all variable values (for tabu)\n- INTEGER_VARS: Compare only integer variables (for tabu)\n\n**CouenneFPpool:**\nSet of solutions with custom comparator. Used to:\n- Store good MILP solutions for NLP restarts\n- Implement tabu list to avoid cycling\n- Find closest solution in pool (findClosestAndReplace)", "see": ["CouenneFeasPump which uses this pool", "CouenneIterativeRounding for another heuristic"], "has_pass2": false}, "src/heuristics/BonNlpHeuristic.hpp": {"path": "layer-3/Couenne/src/heuristics/BonNlpHeuristic.hpp", "filename": "BonNlpHeuristic.hpp", "file": "BonNlpHeuristic.hpp", "brief": "NLP heuristic for near-integer B&B nodes", "algorithm": "Near-Feasibility NLP Heuristic\n\nCalls NLP solver when CouenneObjects are nearly satisfied to\nfind MINLP feasible solutions during branch-and-bound.\n\n**Triggering conditions:**\n- All CouenneObjects within maxNlpInf_ tolerance\n- Integer variables are rounded\n- No violated SOS constraints\n\n**Key parameters:**\n- maxNlpInf_ (default 1e-5): Max auxiliary violation to trigger\n- numberSolvePerLevel_: NLP calls allowed per tree level\n\n**Algorithm:**\n1. Check if current LP solution is nearly MINLP-feasible\n2. Round integer variables to nearest integer\n3. Call Ipopt to find local NLP solution\n4. If feasible, return as candidate incumbent\n\n**Comparison to FeasibilityPump:**\n- FeasPump: Alternates MILP/NLP from any point\n- NlpHeuristic: Only triggers when already near-feasible", "see": ["CouenneFeasPump for feasibility pump heuristic", "InitHeuristic for initial solution injection"], "has_pass2": true}, "src/util/CouenneSparseMatrix.hpp": {"path": "layer-3/Couenne/src/util/CouenneSparseMatrix.hpp", "filename": "CouenneSparseMatrix.hpp", "file": "CouenneSparseMatrix.hpp", "brief": "Sparse matrix storage for Feasibility Pump distance\n\nSimple COO (coordinate) format sparse matrix used to store\nthe Hessian of the Lagrangian at optimum for later use in\nmodified distance computations.\n\n**Storage format:**\nCOO (Coordinate list):\n- val_[k]: Value of k-th nonzero\n- row_[k]: Row index of k-th nonzero\n- col_[k]: Column index of k-th nonzero\n- num_: Total number of nonzeros\n\n**Usage in Feasibility Pump:**\nThe Hessian from an NLP solve can be saved and used to modify\nthe distance metric in subsequent MILP solves, biasing the\nsearch toward regions where the NLP objective improves.\n\n**Note:** This is a simple storage class, not a full sparse\nmatrix implementation. For operations, use CoinPackedMatrix.", "see": ["CouenneTNLP::optHessian_ which stores this", "CouenneFeasPump which uses modified distances"], "has_pass2": false}, "src/util/CouenneFunTriplets.hpp": {"path": "layer-3/Couenne/src/util/CouenneFunTriplets.hpp", "filename": "CouenneFunTriplets.hpp", "file": "CouenneFunTriplets.hpp", "brief": "Function triplets: f(x), f'(x), f''(x), and (f')⁻¹\n\nBundles a univariate function with its first two derivatives and\nthe inverse of the first derivative. Used in convexification to\ncompute tangent points and envelope parameters.\n\n**Abstract base: funtriplet**\n- F(x): Main function value\n- Fp(x): First derivative f'(x)\n- Fpp(x): Second derivative f''(x)\n- FpInv(x): Inverse of first derivative (f')⁻¹(x)\n\n**simpletriplet:**\nStores function pointers for each operation. Suitable for\nstandard functions like exp, log, sin, cos.\n\n**powertriplet:**\nSpecialized for xᵏ with analytical derivatives:\n- F(x) = xᵏ\n- Fp(x) = k·xᵏ⁻¹\n- Fpp(x) = k(k-1)·xᵏ⁻²\n- FpInv(y) = (y/k)^(1/(k-1))\n\n**kpowertriplet:**\nExtension for c·xᵏ with scalar multiplier.", "see": ["CouenneExprPow which uses these for convexification"], "has_pass2": false}, "src/util/CouenneRootQ.hpp": {"path": "layer-3/Couenne/src/util/CouenneRootQ.hpp", "filename": "CouenneRootQ.hpp", "file": "CouenneRootQ.hpp", "brief": "Root finding for Q^k(x) polynomials in power convexification\n\nFinds roots of the polynomial Q^k(x) = Σᵢ₌₁^{2k} i·x^{i-1} used in\nconvexification of odd powers. Based on Liberti & Pantelides (2003).\n\n**Mathematical background:**\nFor odd power expressions w = x^k, the convex/concave envelope\nrequires finding specific points where tangent lines from the\nboundary touch the curve. The Q^k polynomial characterizes these.\n\n**Usage:**\n- rootQ(k): Computes root for exponent 2k+1\n- Qroot class: Caches computed roots in static map for efficiency\n\n**Implementation:**\n- Odd k: Computed via Newton's method, cached in Qmap\n- Even k (2,4,6,8,10): Hardcoded analytical values\n  - k=2: -(√2 - 1) ≈ -0.414\n  - k=4: ≈ -0.560\n  - k=6: ≈ -0.641\n  - etc.", "see": ["CouenneExprOddPow which uses these roots", "CouenneExprSignPow for signed power convexification"], "has_pass2": false}, "src/expression/operators/CouenneExprMin.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMin.hpp", "filename": "CouenneExprMin.hpp", "file": "CouenneExprMin.hpp", "brief": "N-ary minimum operator w = min(x1, x2, ..., xn)\n\nRepresents the minimum of multiple expressions. The min function is\ncontinuous but nonsmooth (non-differentiable at kinks where arguments\nare equal).\n\n**Convexification challenge:**\nmin is neither convex nor concave. Standard approach:\n- Introduce auxiliary variables: w = min(x1,...,xn)\n- Add constraints: w <= xi for all i (overestimators)\n- Underestimator requires disjunction: w >= x_k for some k\n\n**Bound propagation:**\n- lb(w) = min(lb(x1), ..., lb(xn))\n- ub(w) = min(ub(x1), ..., ub(xn))\n\n**Implementation note:**\nUses copy/store pattern with 2n arguments: copies for evaluation,\nstores for caching evaluated values.", "see": ["exprMax for the dual operation", "CouenneExprAbs for another nonsmooth operator"], "has_pass2": false}, "src/expression/operators/CouenneExprLog.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprLog.hpp", "filename": "CouenneExprLog.hpp", "file": "CouenneExprLog.hpp", "brief": "Natural logarithm expression w = log(x)\n\nRepresents the natural logarithm function. This is a concave function\non its domain (x > 0), so convexification uses tangent cuts for\noverestimation and secant cuts for underestimation.\n\n**Convexification for w = log(x) on [l,u]:**\n- Overestimator (tangent): w <= log(x0) + (x - x0)/x0 at any x0 in (l,u)\n- Underestimator (secant): w >= log(l) + (log(u) - log(l))/(u - l) * (x - l)\n\n**Implied bounds:**\n- Given w in [wl, wu]: x in [exp(wl), exp(wu)]\n- Given x in [xl, xu] with xl > 0: w in [log(xl), log(xu)]\n\n**Bijective property:**\nLog is bijective with inverse exp(), enabling tighter bound propagation.", "see": ["CouenneExprExp for the inverse function", "CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprSignPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSignPow.hpp", "filename": "CouenneExprSignPow.hpp", "file": "CouenneExprSignPow.hpp", "brief": "Signed power function w = x * |x|^(k-1) = sign(x) * |x|^k\n\nGeneralizes power to handle negative arguments while preserving\nsign, creating a bijective function for any real k.\n\n**Definition:**\nsignpow(x, k) = x * |x|^(k-1) = sign(x) * |x|^k\n- For x >= 0: same as x^k\n- For x < 0: -|x|^k (preserves sign)\n\n**Key property:**\nUnlike standard power x^k which is undefined for x < 0 when k is\nnon-integer, signpow is defined and continuous for all x.\n\n**Convexification:**\n- For k > 1: convex for x > 0, concave for x < 0\n- For 0 < k < 1: concave for x > 0, convex for x < 0\n- Point x = 0 is an inflection point\n\n**Bijectivity:**\nInverse is sign(y) * |y|^(1/k), enabling exact reverse propagation.", "see": ["exprPow for standard power (handles only x >= 0 for non-integer k)", "exprOddPow for integer odd powers"], "has_pass2": false}, "src/expression/operators/CouenneExprMultiLin.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMultiLin.hpp", "filename": "CouenneExprMultiLin.hpp", "file": "CouenneExprMultiLin.hpp", "brief": "Multilinear product w = x1 * x2 * ... * xn\n\nProduct of n expressions. Generalizes bilinear (n=2) and trilinear (n=3)\nto arbitrary number of factors.\n\n**Standardization approaches:**\n1. Recursive bilinear: w = ((x1*x2)*x3)*... using nested aux variables\n2. Trilinear grouping: group into triplets when n >= 3\n3. Direct multilinear relaxation (more complex cuts)\n\n**Convexification complexity:**\n- Bilinear (n=2): 4 McCormick cuts\n- Trilinear (n=3): 8 Meyer-Floudas cuts\n- n-linear: 2^n cuts (exponential in n)\n\nFor large n, recursive bilinear decomposition is preferred despite\nintroducing auxiliary variables.\n\n**Bound propagation:**\nProduct of intervals requires considering all 2^n corner combinations\nto find the true bounds. Simplified by interval arithmetic.\n\n**Implied bounds (impliedBoundMul):**\nFrom w = x1*x2*...*xn and bounds on w, derive bounds on factors.\nMore complex than bilinear case.\n\n**Branching:**\nbalancedMul() selects branching point to balance the relaxation\nimprovement across all factors.", "see": ["exprMul for bilinear terms (base class)", "exprTrilinear for direct trilinear relaxation"], "has_pass2": false}, "src/expression/operators/CouenneExprSum.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSum.hpp", "filename": "CouenneExprSum.hpp", "file": "CouenneExprSum.hpp", "brief": "N-ary sum expression w = sum(f_i(x))\n\nRepresents sum of multiple expressions. Sums are linear in their\narguments if all arguments are linear.\n\n**Implied bounds for w = a0 + sum(a_i * x_i):**\nGiven bounds w in [l,u] and coefficients partitioned by sign:\n- I1: indices where a_i > 0\n- I2: indices where a_i < 0\n\nFor i in I1:\n- x_i >= (l - a0 - sum_{j!=i,j in I1}(a_j*u_j) - sum_{j in I2}(a_j*l_j)) / a_i\n- x_i <= (u - a0 - sum_{j!=i,j in I1}(a_j*l_j) - sum_{j in I2}(a_j*u_j)) / a_i\n\n**Quadratic detection:**\ncreateQuadratic() scans sum terms for products and creates exprQuad\nif enough quadratic terms exist for alpha-convexification.\n\n**Linearity:**\nReturns maximum linearity level among all arguments.", "see": ["CouenneExprGroup for constant + linear + nonlinear terms", "CouenneExprQuad for quadratic expressions"], "has_pass2": false}, "src/expression/operators/CouenneExprOddPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprOddPow.hpp", "filename": "CouenneExprOddPow.hpp", "file": "CouenneExprOddPow.hpp", "brief": "Odd integer power w = x^k where k is odd\n\nHandles x^1, x^3, x^5, etc. These have special structure:\n- Bijective (one-to-one) over all reals\n- Pass through origin with same sign as x\n- No symmetry about y-axis (unlike even powers)\n\n**Convexity structure:**\n- x^3: convex for x > 0, concave for x < 0 (S-shaped)\n- x^5, x^7, ...: similar S-shape, steeper for higher k\n- Inflection point at x = 0\n\n**Bound propagation:**\nSince bijective: lb(w) = lb(x)^k, ub(w) = ub(x)^k (sign preserved)\n\n**Implied bounds (inverse):**\nFrom w = x^k: x = sign(w) * |w|^(1/k)\n- lb(x) = sign(lb(w)) * |lb(w)|^(1/k)\n- ub(x) = sign(ub(w)) * |ub(w)|^(1/k)\n\n**Convexification:**\nFor w = x^k on [a, b]:\n- If both same sign: secant overestimator, tangent underestimators (or vice versa)\n- If spans zero: use separate relaxations for positive and negative parts", "see": ["exprEvenPow for even powers (x^2, x^4, ...)", "exprSignPow for non-integer signed powers"], "has_pass2": false}, "src/expression/operators/CouenneExprPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprPow.hpp", "filename": "CouenneExprPow.hpp", "file": "CouenneExprPow.hpp", "brief": "Power expression w = x^k with convexification", "algorithm": "Power Convexification with Newton Tangent Points\n\nHandles power functions with constant exponents. Special cases\n(odd/even integer powers) have specialized implementations.\n\n**Signed power (issignpower_):**\nsignpower(x,k) = sign(x) * |x|^k - useful for odd fractional powers\nof potentially negative bases.\n\n**Convexification depends on exponent k:**\n- k > 1: Convex on x > 0, tangent cuts below, secant above\n- 0 < k < 1: Concave on x > 0, secant cuts below, tangent above\n- k < 0: Convex on x > 0 (inverse), requires x > 0\n\n**Key functions:**\n- safe_pow(): Handles negative bases with odd/even exponent check\n- addPowEnvelope(): Generate envelope cuts for power functions\n- powNewton(): Find optimal tangent point using Newton's method", "see": ["CouenneExprEvenPow for x^(2k) - always convex, symmetric", "CouenneExprOddPow for x^(2k+1) - S-shaped, inflection at 0", "CouenneExprSignPow for signed power variant"], "has_pass2": true}, "src/expression/operators/CouenneExprCeil.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprCeil.hpp", "filename": "CouenneExprCeil.hpp", "file": "CouenneExprCeil.hpp", "brief": "Ceiling function w = ceil(x)\n\nReturns the smallest integer not less than x. Piecewise constant\nfunction with jump discontinuities at each integer.\n\n**Convexification challenge:**\nceil(x) is neither convex nor concave nor continuous. The function\nis constant on intervals (n-1, n] with value n.\n\n**Bound propagation:**\n- lb(w) = ceil(lb(x))\n- ub(w) = ceil(ub(x))\n\n**Integrality:**\nResult is always integer-valued. Can be used in MINLP constraints\nthat round up quantities.\n\n**Relationship to floor:**\nceil(x) = -floor(-x), so implementations can share code.", "see": ["exprFloor for the complementary floor function"], "has_pass2": false}, "src/expression/operators/CouenneExprMax.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMax.hpp", "filename": "CouenneExprMax.hpp", "file": "CouenneExprMax.hpp", "brief": "N-ary maximum operator w = max(x1, x2, ..., xn)\n\nRepresents the maximum of multiple expressions. The max function is\nconvex (pointwise supremum of linear functions) but nonsmooth.\n\n**Convexification:**\nmax is convex, so underestimators are straightforward:\n- w >= xi for all i (linear underestimators)\n\nOverestimators require disjunction:\n- w <= x_k for some k (one of the arguments achieves the max)\n\n**Bound propagation:**\n- lb(w) = max(lb(x1), ..., lb(xn))\n- ub(w) = max(ub(x1), ..., ub(xn))\n\n**Subgradient at nondifferentiable points:**\nAt kinks where multiple arguments are equal to max, any convex\ncombination of their gradients is a valid subgradient.", "see": ["exprMin for the dual operation", "exprAbs which can be expressed as max(x, -x)"], "has_pass2": false}, "src/expression/operators/CouenneExprIf.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprIf.hpp", "filename": "CouenneExprIf.hpp", "file": "CouenneExprIf.hpp", "brief": "Conditional (if-then-else) expression operator\n\nImplements the AMPL ifnl() operator:\n  if (condition) then expr1 else expr2\n\n**Status:** This is a placeholder/stub class. Full implementation\nwould require handling disjunctive convexification.\n\n**Convexification challenge:**\nConditional expressions create disjunctive feasible regions.\nStandard approach would use big-M formulations or disjunctive\nprogramming techniques to handle the branching logic.\n\n**Usage in AMPL:**\nModels with if-then-else create nonsmooth, potentially\ndiscontinuous functions that are difficult to convexify.", "see": ["CouenneDisjCuts for disjunctive cut generation", "exprOp base class"], "has_pass2": false}, "src/expression/operators/CouenneExprBinProd.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprBinProd.hpp", "filename": "CouenneExprBinProd.hpp", "file": "CouenneExprBinProd.hpp", "brief": "Product of binary variables (specialized multilinear)\n\nRepresents products of binary (0-1) variables: w = x₁ * x₂ * ... * xₙ\nwhere each xᵢ ∈ {0, 1}. Inherits from exprMul but exploits the\nbinary domain for tighter cuts.\n\n**Key simplification:**\nFor binary variables, w = 1 iff all xᵢ = 1, otherwise w = 0.\nThis is equivalent to logical AND.\n\n**Linearization (standard form):**\nFor n binaries, the product can be linearized exactly with:\n- w ≤ xᵢ for all i (w = 0 if any xᵢ = 0)\n- w ≥ Σxᵢ - (n-1) (w = 1 if all xᵢ = 1)\n\n**Bounds:**\n- Lower bound: 0 always (product of non-negative)\n- Upper bound: 1 always (product of binaries ≤ 1)\n\n**isCuttable():**\nReturns false - binary products are fully linearizable,\nno convexification cuts beyond the standard form needed.", "see": ["exprMul which this specializes", "CouenneExprMultiLin for general multilinear products"], "has_pass2": false}, "src/expression/operators/CouenneExprExp.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprExp.hpp", "filename": "CouenneExprExp.hpp", "file": "CouenneExprExp.hpp", "brief": "Exponential expression w = exp(x)\n\nRepresents the exponential function e^x. This is a convex function\nover all of R, so convexification uses secant cuts for overestimation\nand tangent cuts for underestimation.\n\n**Convexification for w = exp(x) on [l,u]:**\n- Underestimator (tangent): w >= exp(x0) + exp(x0)*(x - x0) at any x0 in [l,u]\n- Overestimator (secant): w <= exp(l) + (exp(u) - exp(l))/(u - l) * (x - l)\n\n**Implied bounds:**\n- Given w in [wl, wu] with wl > 0: x in [log(wl), log(wu)]\n- Given x in [xl, xu]: w in [exp(xl), exp(xu)]\n\n**Bijective property:**\nExp is bijective with inverse log(), enabling tighter bound propagation.\n\n**Convexity:** Convex everywhere, simplifying global optimization.", "see": ["CouenneExprLog for the inverse function", "CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprFloor.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprFloor.hpp", "filename": "CouenneExprFloor.hpp", "file": "CouenneExprFloor.hpp", "brief": "Floor function w = floor(x)\n\nReturns the largest integer not exceeding x. Piecewise constant\nfunction with jump discontinuities at each integer.\n\n**Convexification challenge:**\nfloor(x) is neither convex nor concave nor continuous. The function\nis constant on intervals [n, n+1) with value n.\n\n**Bound propagation:**\n- lb(w) = floor(lb(x))\n- ub(w) = floor(ub(x))\n\n**Integrality:**\nResult is always integer-valued, even when argument is continuous.\nThis can introduce implicit integrality constraints.\n\n**Not cuttable:**\nNo convex relaxation exists for piecewise constant functions.\nMust rely on branching to handle.", "see": ["exprCeil for the complementary ceiling function"], "has_pass2": false}, "src/expression/operators/CouenneExprNorm.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprNorm.hpp", "filename": "CouenneExprNorm.hpp", "file": "CouenneExprNorm.hpp", "brief": "p-norm expression: ||f(x)||_p\n\nRepresents the ℓₚ norm: ||f(x)||_p = (Σᵢ |fᵢ(x)|^p)^(1/p)\n\n**Status:** This is a placeholder/stub class.\n\n**Special cases:**\n- p = 1: Sum of absolute values (piecewise linear)\n- p = 2: Euclidean norm (convex, smooth except at origin)\n- p = ∞: Maximum absolute value (convex, nonsmooth)\n\n**Convexity:**\nAll ℓₚ norms with p ≥ 1 are convex functions.\nThis makes overestimation straightforward (tangent planes),\nbut underestimation requires handling the non-negativity\nand norm structure.\n\n**Common usage:**\n- Distance constraints: ||x - a||₂ ≤ r\n- Regularization: minimize f(x) + λ||x||₁", "see": ["exprAbs for absolute value (used in p=1 norm)", "exprOp base class"], "has_pass2": false}, "src/expression/operators/CouenneExprDiv.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprDiv.hpp", "filename": "CouenneExprDiv.hpp", "file": "CouenneExprDiv.hpp", "brief": "Division expression w = x / y\n\nRepresents division of two expressions. Division is reformulated as\nmultiplication: w = x/y becomes w*y = x, using bilinear convexification.\n\n**Standardization:**\nDivision x/y is converted to x * (1/y), where 1/y is handled by exprInv.\nThis allows reuse of McCormick envelope cuts for the product.\n\n**Linearity check:**\nIf denominator is constant, expression inherits linearity of numerator.\nOtherwise, expression is nonlinear.\n\n**Bound safety:**\n- is_boundbox_regular(): Checks if bounds are suitable for convexification\n- SAFE_COEFFICIENT (1e9): Maximum coefficient for OsiRowCut\n- BR_NEXT_ZERO: Safety margin near zero denominator\n\n**Not cuttable:**\nLike products, division is concave on both sides and cannot be\nfurther linearized - only bound tightening helps.", "see": ["CouenneExprMul for product convexification", "CouenneExprInv for 1/x handling"], "has_pass2": false}, "src/expression/operators/CouenneExprSin.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSin.hpp", "filename": "CouenneExprSin.hpp", "file": "CouenneExprSin.hpp", "brief": "Sine expression w = sin(x)\n\nRepresents the sine function. Sine is challenging for global optimization\ndue to its oscillatory nature - alternates between convex and concave\nregions every pi interval.\n\n**Convexification challenges:**\n- Convex on [2k*pi - pi, 2k*pi] for integer k\n- Concave on [2k*pi, 2k*pi + pi]\n- Multiple local optima on unbounded domains\n- Special handling needed when domain spans multiple periods\n\n**Implied bounds:**\n- w in [-1, 1] always\n- Given w bounds, x bounds via arcsin with period handling\n- trigImpliedBound(): Generalized procedure for sin/cos\n\n**Branching:**\ntrigSelBranch(): Selects branching points based on convexity regions\nand current LP solution position relative to sin curve.\n\n**Not cuttable:** Due to alternating convexity, cutting planes\nare less effective than branching.", "see": ["CouenneExprCos for cosine (uses same procedures)", "CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprOpp.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprOpp.hpp", "filename": "CouenneExprOpp.hpp", "file": "CouenneExprOpp.hpp", "brief": "Negation operator w = -x\n\nLinear operator that returns the opposite of an expression.\nPreserves linearity of the argument.\n\n**Convexification:**\n- Affine function: requires no convexification\n- Convex relaxation is exact\n- Linearity() returns same as argument's Linearity()\n\n**Bound propagation:**\n- lb(w) = -ub(x)\n- ub(w) = -lb(x)\nNote: bounds are swapped!\n\n**Implied bounds:**\nFrom w = -x: if w >= a then x <= -a, if w <= b then x >= -b\n\n**Integrality preservation:**\nIf x is integer, -x is also integer.\n\n**Simplification:**\n-(-x) simplifies to x (double negation elimination)", "see": ["exprSub for subtraction x - y = x + (-y)"], "has_pass2": false}, "src/expression/operators/CouenneExprEvenPow.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprEvenPow.hpp", "filename": "CouenneExprEvenPow.hpp", "file": "CouenneExprEvenPow.hpp", "brief": "Even integer power w = x^k where k is even\n\nHandles x^2, x^4, x^6, etc. These have special structure:\n- Always non-negative: w >= 0\n- Symmetric about y-axis: f(x) = f(-x)\n- NOT bijective: x and -x give same w\n\n**Convexity:**\nEven powers are convex everywhere (sum of squares is convex).\nThis greatly simplifies the convex relaxation.\n\n**Bound propagation:**\n- lb(w) = 0 if [lb(x), ub(x)] contains 0\n- lb(w) = min(lb(x)^k, ub(x)^k) otherwise\n- ub(w) = max(lb(x)^k, ub(x)^k)\n\n**Implied bounds (non-bijective!):**\nFrom w = x^k where k even:\n- |x| <= w^(1/k), so -w^(1/k) <= x <= w^(1/k)\n- Cannot determine sign of x from w alone\n\n**Convexification:**\nSince convex: tangent line underestimators everywhere.\nSecant line is the overestimator.\n\n**Special case x^2:**\nMost common even power, often handled separately for efficiency.", "see": ["exprOddPow for odd powers (bijective, S-shaped)", "exprQuad for sum of squares"], "has_pass2": false}, "src/expression/operators/CouenneExprAbs.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprAbs.hpp", "filename": "CouenneExprAbs.hpp", "file": "CouenneExprAbs.hpp", "brief": "Absolute value expression w = |x|\n\nRepresents the absolute value function. This is a convex function\nwith a non-differentiable point at x = 0.\n\n**Convexification for w = |x| on [l,u]:**\n- If l >= 0: w = x (identity)\n- If u <= 0: w = -x (negation)\n- If l < 0 < u: convex envelope is\n  - w >= x (right branch)\n  - w >= -x (left branch)\n  - w <= ((u+l)*x + u*(-l) - l*u) / (u - l) (secant from (l,-l) to (u,u))\n\n**Implied bounds:**\n- w >= 0 always\n- Given w in [0, wu]: x in [-wu, wu] ∩ [l, u]\n- Given x in [l, u]: w in [min(|l|,|u|)?, max(|l|,|u|)]\n\n**Integer preservation:**\nIf argument is integer, |x| is also integer.", "see": ["CouenneExprUnary for base class"], "has_pass2": false}, "src/expression/operators/CouenneExprMul.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprMul.hpp", "filename": "CouenneExprMul.hpp", "file": "CouenneExprMul.hpp", "brief": "N-ary multiplication expression with McCormick convexification\n\nRepresents products of the form w = x1 * x2 * ... * xn.\nDuring standardization, n-ary products are decomposed into\nbinary products: w1 = x1*x2, w2 = w1*x3, etc.", "algorithm": "McCormick Envelope for Bilinear Terms:\nConvex relaxation of w = x·y over [xL,xU] × [yL,yU]:\n1. Lower envelope (concave underestimator):\n   w ≥ xL·y + x·yL - xL·yL  (tangent at (xL, yL))\n   w ≥ xU·y + x·yU - xU·yU  (tangent at (xU, yU))\n2. Upper envelope (convex overestimator):\n   w ≤ xL·y + x·yU - xL·yU  (tangent at (xL, yU))\n   w ≤ xU·y + x·yL - xU·yL  (tangent at (xU, yL))\n3. These 4 planes form the convex envelope of x·y over the box.\n\nn-ary standardization:\n  x₁·x₂·x₃ → w₁ = x₁·x₂, w₂ = w₁·x₃\nintroduces O(n-1) auxiliary variables.", "math": "McCormick derivation:\nFor w = x·y, we have:\n  (x-xL)(y-yL) ≥ 0 ⟹ xy ≥ xL·y + x·yL - xL·yL\nSimilarly for other corners.\nEnvelope is tight at all 4 corners of the box.", "complexity": "O(1) to generate 4 cuts. Bound tightening improves\nrelaxation quality: gap ∝ (xU-xL)·(yU-yL).", "ref": ["McCormick (1976). \"Computability of global solutions to factorable\n  nonconvex programs\". Mathematical Programming 10(1):147-175."], "see": ["CouenneExprDiv for division (uses same convexification)", "CouenneExprBinProd for binary product specialization"], "has_pass2": true}, "src/expression/operators/CouenneExprCos.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprCos.hpp", "filename": "CouenneExprCos.hpp", "file": "CouenneExprCos.hpp", "brief": "Cosine function w = cos(x)\n\nBounded periodic function in [-1, 1]. Convexification follows the\nsame approach as exprSin with a phase shift.\n\n**Convexification (depends on bound interval):**\n- If interval < π: tangent line is overestimator on convex part,\n  underestimator on concave part\n- Secant line connects interval endpoints\n- Newton iteration (trigNewton) finds optimal tangent points\n\n**Implied bounds:**\nSince cos is bounded, w ∈ [-1, 1] always. Uses trigImpliedBound()\nfor reverse propagation when w bounds are known.\n\n**Derivative:**\nd(cos(x))/dx = -sin(x), used for gradient computations.\n\n**Period handling:**\nIf bound interval spans multiple periods, relaxation becomes\ntrivial: -1 <= w <= 1.", "see": ["exprSin for detailed convexification algorithm", "CouenneExprBCos for bound computation"], "has_pass2": false}, "src/expression/operators/CouenneExprInv.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprInv.hpp", "filename": "CouenneExprInv.hpp", "file": "CouenneExprInv.hpp", "brief": "Reciprocal function w = 1/x\n\nHyperbolic function that is convex on (0, ∞) and (-∞, 0) separately.\nSingular at x = 0, so domain handling is critical.\n\n**Convexification by domain:**\n- x > 0: 1/x is convex, use secant overestimator and tangent underestimators\n- x < 0: 1/x is convex, same approach\n- If x can cross zero: must handle carefully, possible branching\n\n**Tangent line at x = a:**\nw = 1/a - (x - a)/a² = 2/a - x/a²\n\n**Secant between (a, 1/a) and (b, 1/b):**\nw = 1/a + (1/b - 1/a)/(b - a) * (x - a) = 1/a - (x - a)/(ab)\n\n**Properties:**\n- Bijective: inverse of 1/x is 1/x (self-inverse)\n- Used in division: x/y reformulated as x * (1/y)\n- Derivatives: d(1/x)/dx = -1/x², d²(1/x)/dx² = 2/x³", "see": ["exprDiv which uses exprInv internally", "exprMul for the multiplication component"], "has_pass2": false}, "src/expression/operators/CouenneExprQuad.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprQuad.hpp", "filename": "CouenneExprQuad.hpp", "file": "CouenneExprQuad.hpp", "brief": "Quadratic expression with alpha-convexification", "algorithm": "Alpha-Convexification via Eigenvalue Analysis\n\nRepresents expressions of the form:\n  w = a0 + a'x + x'Qx + sum(nonlinear_terms)\n\nUses alpha-convexification (Adjiman-Floudas, LaGO/Nowak-Vigerske)\nto create convex/concave envelopes based on eigenvalue analysis.\n\n**Alpha-convexification algorithm:**\n1. Scale Q by variable bounds: A = Diag(u-l) * Q * Diag(u-l)\n2. Compute min/max eigenvalues lambda_min, lambda_max\n3. Under-estimator: x'Qx + sum(lambda_min_i * (x_i - l_i)(u_i - x_i))\n4. Over-estimator: x'Qx + sum(lambda_max_i * (x_i - l_i)(u_i - x_i))\n\n**Data structures:**\n- matrix_: Sparse Q matrix as vector of (variable, column) pairs\n- eigen_: Cached eigenvalues/eigenvectors for convexification\n- bounds_: Cached bounds to detect when recomputation needed\n\n**Key methods:**\n- alphaConvexify(): Compute eigenvalues for convexification\n- quadCuts(): Generate OA cuts from alpha-convexification", "see": ["CouenneExprGroup for linear + nonlinear base class", "CouenneExprMul for bilinear terms (McCormick)"], "has_pass2": true}, "src/expression/operators/CouenneExprTrilinear.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprTrilinear.hpp", "filename": "CouenneExprTrilinear.hpp", "file": "CouenneExprTrilinear.hpp", "brief": "Trilinear product expression w = x*y*z", "algorithm": "Trilinear Polyhedral Relaxation\n\nSpecialized handling for products of exactly three terms.\nTrilinear terms are common in pooling problems and arise\nfrom reformulation of bilinear products with parameters.\n\n**Standardization options:**\nA trilinear term x*y*z can be reformulated as:\n1. w1 = x*y, w = w1*z (two bilinear terms)\n2. Direct trilinear relaxation\n\n**Convexification:**\nUses direct trilinear relaxation when available, which can\nprovide tighter bounds than cascaded bilinear products.\nGenerates 8 linear constraints forming a polyhedral relaxation.\n\n**Implied bounds:**\nGiven w in [wl,wu] and bounds on x,y,z, tightens bounds\nby considering all 8 corners of the 3D box.", "see": ["CouenneExprMul for general n-ary products", "CouenneExprBinProd for binary products"], "has_pass2": true}, "src/expression/operators/CouenneExprSub.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprSub.hpp", "filename": "CouenneExprSub.hpp", "file": "CouenneExprSub.hpp", "brief": "Binary subtraction operator w = x - y\n\nLinear operator for computing the difference of two expressions.\nLinearity is the maximum of the two arguments' linearity.\n\n**Convexification:**\n- Affine in both arguments: no convexification needed\n- Linearity() = max(Linearity(x), Linearity(y))\n\n**Bound propagation:**\n- lb(w) = lb(x) - ub(y)\n- ub(w) = ub(x) - lb(y)\n\n**Implied bounds:**\nFrom w = x - y:\n- x >= lb(w) + lb(y)\n- x <= ub(w) + ub(y)\n- y >= lb(x) - ub(w)\n- y <= ub(x) - lb(w)\n\n**Standardization:**\nMay be rewritten as x + (-1)*y using exprSum and exprOpp\nduring problem transformation.", "see": ["exprSum for addition", "exprOpp for negation"], "has_pass2": false}, "src/expression/operators/CouenneExprPWLinear.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprPWLinear.hpp", "filename": "CouenneExprPWLinear.hpp", "file": "CouenneExprPWLinear.hpp", "brief": "Piecewise linear function expression\n\nRepresents piecewise linear (PWL) functions defined by breakpoints\nand slopes or by point sequences.\n\n**Status:** This is a placeholder/stub class.\n\n**Mathematical form:**\nf(x) = aᵢ * x + bᵢ  for  xᵢ ≤ x < xᵢ₊₁\n\n**Convexification:**\n- If convex (slopes increasing): trivial envelope\n- If concave (slopes decreasing): trivial envelope\n- If neither: requires disjunctive handling\n\n**Standard MIP formulation:**\nUses binary variables to select active segment:\n- λᵢ ∈ [0,1], Σλᵢ = 1, at most 2 adjacent λᵢ positive (SOS2)\n- x = Σλᵢxᵢ, y = Σλᵢyᵢ\n\n**Usage:**\nCommon in approximating nonlinear functions or modeling\ncosts with quantity discounts.", "see": ["CouenneExprAbs for a simple PWL function (V-shape)", "exprOp base class"], "has_pass2": false}, "src/expression/operators/CouenneExprGroup.hpp": {"path": "layer-3/Couenne/src/expression/operators/CouenneExprGroup.hpp", "filename": "CouenneExprGroup.hpp", "file": "CouenneExprGroup.hpp", "brief": "Mixed expression: constant + linear + nonlinear terms\n\nRepresents expressions of the form:\n  w = c0 + sum(a_i * x_i) + sum(f_j(x))\nwhere c0 is constant, a_i*x_i are linear terms, and f_j are nonlinear.\n\n**Data structures:**\n- c0_: Constant term\n- lcoeff_: Vector of (variable, coefficient) pairs for linear part\n- Inherits arglist_ from exprSum for nonlinear terms\n\n**Factory pattern:**\ngenExprGroup() checks parameters and returns:\n- exprConst if only constant\n- exprVar if single variable with coefficient 1\n- exprGroup otherwise\n\n**Evaluation:**\noperator() = c0 + sum(coefficient * variable_value) + sum(nonlinear)\n\n**Base for exprQuad:**\nexprQuad extends this to add x'Qx quadratic terms.", "see": ["CouenneExprSum for pure sum of expressions", "CouenneExprQuad for quadratic extension"], "has_pass2": false}, "src/expression/partial/CouenneExprJac.hpp": {"path": "layer-3/Couenne/src/expression/partial/CouenneExprJac.hpp", "filename": "CouenneExprJac.hpp", "file": "CouenneExprJac.hpp", "brief": "Symbolic Jacobian of constraints via expression DAG\n\nStores the constraint Jacobian as a sparse matrix of expression\npointers. Each nonzero ∂g_i/∂x_j is an expression node that can\nbe evaluated at any point.\n\n**Sparse structure:**\n- nnz_: Number of structural nonzeros\n- iRow_[], jCol_[]: COO format indices\n- expr_[k]: Expression for the k-th nonzero entry\n\n**Construction:**\nBuilt from CouenneProblem by differentiating each constraint\nbody expression with respect to each variable it depends on.\n\n**Usage:**\nCalled by Ipopt's eval_jac_g() callback to provide the\nconstraint Jacobian. Expression-based representation allows\nexact derivatives without finite differences.", "see": ["ExprHess for the Lagrangian Hessian", "CouenneTNLP which uses ExprJac for NLP solves"], "has_pass2": false}, "src/expression/partial/CouenneExprHess.hpp": {"path": "layer-3/Couenne/src/expression/partial/CouenneExprHess.hpp", "filename": "CouenneExprHess.hpp", "file": "CouenneExprHess.hpp", "brief": "Symbolic Hessian of Lagrangian via expression DAG\n\nStores the Hessian of the Lagrangian:\n∇²L(x,λ) = ∇²f(x) + Σᵢ λᵢ ∇²gᵢ(x)\n\n**Sparse structure:**\n- nnz_: Number of structural nonzeros (lower triangle)\n- iRow_[], jCol_[]: COO format indices\n- numL_[k]: How many λᵢ contribute to position k\n- lamI_[k][]: Indices of contributing λᵢ\n- expr_[k][j]: Expression for ∂²gⱼ/∂x_iRow[k]∂x_jCol[k]\n\n**Evaluation:**\nFor each nonzero position (i,j), sum over all constraints:\nH[i,j] = expr_obj[i,j] + Σₖ λ[lamI[k]] * expr[k][...]\n\n**Feasibility pump extension:**\nCan be extended with gg' term for feasibility pump objectives\nby augmenting the objective with ||x - x̄||² + gradient terms.", "see": ["ExprJac for the constraint Jacobian", "CouenneTNLP which uses ExprHess for NLP solves"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBCos.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBCos.hpp", "filename": "CouenneExprBCos.hpp", "file": "CouenneExprBCos.hpp", "brief": "Bound operators for cosine expressions\n\nComputes tight bounds for cos(x) given bounds [l, u] on x.\nUsed by FBBT (feasibility-based bound tightening) to propagate\nbounds through trigonometric expressions.\n\n**exprLBCos (lower bound):**\n- Returns -1 if interval spans full period (u - l >= 2π)\n- Returns -1 if interval contains π + 2kπ (minimum of cos)\n- Otherwise returns min(cos(l), cos(u))\n\n**exprUBCos (upper bound):**\n- Returns +1 if interval spans full period\n- Returns +1 if interval contains 2kπ (maximum of cos)\n- Otherwise returns max(cos(l), cos(u))\n\n**Period handling:**\nUses floor(l/2π - offset) < floor(u/2π - offset) to detect\nif critical points lie within [l, u].", "see": ["CouenneExprCos for the cosine expression itself", "CouenneExprBSin for sine bound computation"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBDiv.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBDiv.hpp", "filename": "CouenneExprBDiv.hpp", "file": "CouenneExprBDiv.hpp", "brief": "Bound operators for division expressions\n\nComputes tight bounds for n/d given bounds [n, N] on numerator\nand [d, D] on denominator. Handles singularity at d = 0.\n\n**safeDiv():**\nUtility for safe division avoiding NaN:\n- Returns 0 if numerator is ~0\n- Returns ±∞ if denominator is ~0 (sign-aware)\n- Otherwise returns a/b\n\n**exprLBDiv (lower bound):**\nCase analysis on signs:\n- d > 0: positive denominator → lb = n/D or n/d\n- d ≤ 0, D > 0: crosses zero → -∞ (unbounded)\n- d ≤ 0, D ≤ 0: negative denominator → lb = N/D or N/d\n\n**exprUBDiv (upper bound):**\nSymmetric case analysis:\n- d > 0: ub = N/D or N/d\n- d ≤ 0, D > 0: crosses zero → +∞\n- d ≤ 0, D ≤ 0: ub = n/D or n/d", "see": ["CouenneExprDiv for the division expression", "CouenneExprBMul for multiplication bounds"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBQuad.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBQuad.hpp", "filename": "CouenneExprBQuad.hpp", "file": "CouenneExprBQuad.hpp", "brief": "Bound expressions for quadratic forms\n\nExpression classes that compute bounds on quadratic forms\n∑ qᵢⱼ·xᵢ·xⱼ + ∑ bᵢ·xᵢ + c given variable bounds.\n\n**exprLBQuad:**\nReturns lower bound on the quadratic form by calling\ncomputeQBound(-1) on the referenced exprQuad.\n\n**exprUBQuad:**\nReturns upper bound on the quadratic form by calling\ncomputeQBound(+1) on the referenced exprQuad.\n\n**Bound computation (in exprQuad):**\nUses eigenvalue-based approach when Q is indefinite:\n- Decompose Q = ∑ λᵢ·vᵢ·vᵢᵀ\n- Bound each eigenterm using variable bounds\n- Sum contributions accounting for sign of λᵢ\n\n**Usage in FBBT:**\nThese expressions enable bound tightening on quadratic\nconstraints without explicitly enumerating all bilinear terms.", "see": ["CouenneExprQuad which implements computeQBound()", "CouenneExprBMul for bilinear bound operators"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBMul.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBMul.hpp", "filename": "CouenneExprBMul.hpp", "file": "CouenneExprBMul.hpp", "brief": "Bound operators for bilinear product expressions\n\nComputes tight bounds for x * y given bounds [n, N] on x\nand [d, D] on y. Used by FBBT for bound propagation.\n\n**safeProd():**\nUtility for safe multiplication avoiding overflow:\n- Handles ±∞ * 0 → 0\n- Handles ±∞ * finite → ±∞ (sign-aware)\n- Otherwise returns a * b\n\n**exprLBMul (lower bound):**\nFor bounds [n, N] × [d, D]:\n- d ≥ 0, n ≥ 0: lb = n * d\n- d ≥ 0, n < 0: lb = n * D\n- d < 0, N > 0: lb = min(N * d, n * D)\n- d < 0, N ≤ 0, D > 0: lb = n * D\n- d < 0, N ≤ 0, D ≤ 0: lb = N * D\n\n**exprUBMul (upper bound):**\nSymmetric case analysis yielding max of corner products.\n\n**Note:** The four corners {n, N} × {d, D} contain the\nextrema for bilinear functions over boxes.", "see": ["CouenneExprMul for the multiplication expression", "CouenneExprBDiv for division bounds"], "has_pass2": false}, "src/expression/operators/bounds/CouenneExprBSin.hpp": {"path": "layer-3/Couenne/src/expression/operators/bounds/CouenneExprBSin.hpp", "filename": "CouenneExprBSin.hpp", "file": "CouenneExprBSin.hpp", "brief": "Bound operators for sine expressions\n\nComputes tight bounds for sin(x) given bounds [l, u] on x.\nUsed by FBBT (feasibility-based bound tightening) to propagate\nbounds through trigonometric expressions.\n\n**exprLBSin (lower bound):**\n- Returns -1 if interval spans full period (u - l >= 2π)\n- Returns -1 if interval contains 3π/2 + 2kπ (minimum of sin)\n- Otherwise returns min(sin(l), sin(u))\n\n**exprUBSin (upper bound):**\n- Returns +1 if interval spans full period\n- Returns +1 if interval contains π/2 + 2kπ (maximum of sin)\n- Otherwise returns max(sin(l), sin(u))\n\n**Period handling:**\nUses floor(l/2π - offset) < floor(u/2π - offset) to detect\nif critical points lie within [l, u]:\n- offset = 0.75 for min (3π/2)\n- offset = 0.25 for max (π/2)", "see": ["CouenneExprSin for the sine expression itself", "CouenneExprBCos for cosine bound computation"], "has_pass2": false}, "src/bound_tightening/twoImpliedBT/CouenneTwoImplied.hpp": {"path": "layer-3/Couenne/src/bound_tightening/twoImpliedBT/CouenneTwoImplied.hpp", "filename": "CouenneTwoImplied.hpp", "file": "CouenneTwoImplied.hpp", "brief": "Bound tightening from pairs of linear constraints", "algorithm": "Two-Implied Bound Tightening via Convex Combinations\n\nDerives implied bounds by combining two linear inequalities via\nconvex combinations. Standard FBBT uses single constraints; this\nextends to pairs for tighter bounds.\n\n**Algorithm:**\nGiven constraints h and k, form convex combination with α ∈ [0,1]:\n  ℓ' ≤ Σᵢ bᵢ(α) xᵢ ≤ u'\n\nFor each variable xᵢ, the implied bounds are piecewise rational\nfunctions of α. Find optimal α to maximize lower bounds / minimize\nupper bounds.\n\n**Key insight:**\nDivide [0,1] into intervals at breakpoints cᵢ where coefficients\nchange sign. On each interval, functions are differentiable and\noptima can be found analytically.\n\n**Example:**\nFor x + y ≥ 2 and x - y ≥ 1 with x ∈ [0,4], y ∈ [0,1]:\n- Single-constraint FBBT: x ≥ 1\n- Two-implied (sum): x ≥ 1.5", "see": ["CouenneFixPoint for standard single-constraint FBBT", "CouenneAggrProbing for OBBT"], "has_pass2": true}, "src/problem/depGraph/CouenneDepGraph.hpp": {"path": "layer-3/Couenne/src/problem/depGraph/CouenneDepGraph.hpp", "filename": "CouenneDepGraph.hpp", "file": "CouenneDepGraph.hpp", "brief": "Dependency graph between auxiliary and original variables", "algorithm": "Dependency Graph with Topological Sort\n\nTracks which variables depend on which others, enabling:\n- Correct evaluation order (topological sort)\n- Cycle detection (would indicate model error)\n- Dependency analysis for bound propagation\n\n**DepNode:**\nA vertex in the dependency graph:\n- index_: Variable index\n- depList_: Forward star (variables this one depends on)\n- order_: Evaluation order from topological sort\n- color_: For DFS cycle detection (white/gray/black)\n\n**DepGraph:**\nThe full dependency DAG:\n- vertices_: Set of DepNodes\n- createOrder(): Topological sort for evaluation order\n- checkCycles(): Detect circular dependencies\n- depends(w, x): Check if w depends on x\n\n**Usage:**\nWhen w = f(x, y), node for w has edges to nodes for x and y.\nEvaluation must process x, y before w. Bound propagation\nfollows the reverse order.\n\n**replaceIndex():**\nUsed when redundant auxiliary w := x is found and w is\nreplaced by x throughout.", "see": ["exprAux which uses this for evaluation ordering", "CouenneProblem which owns the dependency graph"], "has_pass2": true}, "src/cut/crossconv/CouenneCrossConv.hpp": {"path": "layer-3/Couenne/src/cut/crossconv/CouenneCrossConv.hpp", "filename": "CouenneCrossConv.hpp", "file": "CouenneCrossConv.hpp", "brief": "Cuts from redundant relationships between auxiliary variables", "algorithm": "Cross-Convexification via Algebraic Identities", "math": "Exploits algebraic identities among auxiliary variables:\n      - Product chain: x_k = x_i·x_j, x_l = x_i·x_p ⟹ x_k·x_p = x_l·x_j\n      - Log sum: log(x_1) + log(x_2) = log(x_1·x_2)\n      - Power: x^α, x^β ⟹ x^β = (x^α)^(β/α)\n\nIdentifies and exploits algebraic relationships between auxiliary\nvariables that arise from the reformulation. These give valid\nequalities or inequalities that strengthen the relaxation.", "complexity": "O(n²) to identify relationships; O(1) per cut\n\n**Example relationships (commented out but informative):**\n\n1. **SumLogAuxRel**: x3 = log(x1), x4 = log(x2), x5 = x1*x2\n   Implies: x3 + x4 = log(x5), giving a valid cut.\n\n2. **MultiProdRel**: If x_k = x_i*x_j, x_l = x_i*x_p, x_q = x_k*x_p, x_r = x_l*x_j\n   Then x_q = x_r (both equal x_i*x_j*x_p).\n\n3. **BiProdDivRel**: Division chains that should be consistent.\n\n4. **PowRel**: If x_j = x_i^α and x_p = x_i^β, then x_p = x_j^(β/α).\n\n**How it works:**\n1. Analyze auxiliary variable definitions\n2. Detect patterns that imply redundant relationships\n3. Generate cuts enforcing these relationships\n\nThese cuts are \"free\" in the sense that they come from algebraic\nidentities, not from convexification of specific expressions.", "see": ["CouenneCutGenerator for standard convexification cuts", "CouenneDisjCuts for disjunctive cuts"], "has_pass2": true}, "src/cut/sdpcuts/CouennePSDcon.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/CouennePSDcon.hpp", "filename": "CouennePSDcon.hpp", "file": "CouennePSDcon.hpp", "brief": "Positive semidefinite constraint X ⪰ 0\n\nRepresents a constraint that a matrix of expressions must be\npositive semidefinite (all eigenvalues non-negative).\n\n**Mathematical meaning:**\nX ⪰ 0 is equivalent to:\n- All eigenvalues of X are non-negative\n- v'Xv >= 0 for all vectors v\n- X can be written as X = A'A for some matrix A\n\n**Common usage:**\n- Product matrices: X_ij = x_i * x_j must be PSD\n- Covariance matrices in statistics\n- Second-order cone constraints (via PSD embedding)\n\n**Handling in Couenne:**\nPSD constraints are not directly enforceable in MILP/NLP.\nInstead, they are enforced via:\n1. SDP cuts (see CouenneSdpCuts)\n2. Eigenvalue constraints (expensive)\n3. Principal minor constraints", "see": ["CouenneSdpCuts for cut generation from PSD constraints", "CouenneExprMatrix for matrix representation"], "has_pass2": false}, "src/cut/sdpcuts/CouenneSdpCuts.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/CouenneSdpCuts.hpp", "filename": "CouenneSdpCuts.hpp", "file": "CouenneSdpCuts.hpp", "brief": "SDP-based cutting planes using matrix positive semidefiniteness\n\nGenerates cuts exploiting that product matrices X = (x_ij) where x_ij = x_i*x_j\nmust be positive semidefinite. These cuts strengthen the LP relaxation beyond\nwhat McCormick envelopes provide.", "algorithm": "SDP Cut Generation (Qualizza-Belotti-Margot):\n  Eigenvalue-based separation for PSD constraint X ⪰ 0:\n  1. **Matrix construction:** Build X* from current LP (x_ij = w_ij values)\n  2. **Eigendecomposition:** Compute X* = VΛV' (symmetric EVD)\n  3. **Violation detection:** Find λ_min < 0 (X* not PSD)\n  4. **Cut generation:** For eigenvector v with λ < 0:\n     - Cut: v'Xv ≥ 0 (linear in original variables)\n     - Translates to: Σ v_i·v_j·x_ij ≥ 0\n  5. **Sparsification:** Remove small coefficients for stability\n  6. Add cuts to LP, iterate", "math": "PSD relaxation theory:\n  For X ⪰ 0: v'Xv ≥ 0 for all v (infinite constraints)\n  Current X* violates PSD if min eigenvalue λ < 0\n  Corresponding v gives most violated constraint v'Xv ≥ 0\n  These cuts are valid because X ⪰ 0 implies all v'Xv ≥ 0", "complexity": "O(n³) per eigendecomposition (n = matrix dimension)\n  Each minor generates up to numEigVec_ cuts\n  Sparsification is O(n²) but improves cut quality", "ref": ["Qualizza, Belotti, Margot (2012). \"Linear Programming Relaxations\n     of Quadratically Constrained Quadratic Programs\". Mixed Integer\n     Nonlinear Programming, Springer.\n\n**Parameters:**\n- numEigVec_: Number of eigenvectors to use (default: all)\n- onlyNegEV_: Only use negative eigenvalues (default: yes)\n- useSparsity_: Sparsify eigenvalues (default: no)\n- fillMissingTerms_: Add fictitious aux vars for denser minors"], "see": ["CouennePSDcon for explicit PSD constraints", "CouenneExprMatrix for sparse matrix representation"], "has_pass2": true}, "src/cut/sdpcuts/dsyevx_wrapper.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/dsyevx_wrapper.hpp", "filename": "dsyevx_wrapper.hpp", "file": "dsyevx_wrapper.hpp", "brief": "LAPACK dsyevx wrapper for symmetric eigenvalue computation\n\nC++ interface to LAPACK's dsyevx routine for computing selected\neigenvalues and eigenvectors of a real symmetric matrix.\n\n**dsyevx_interface parameters:**\n- n: Matrix dimension\n- A: Input symmetric matrix (upper or lower triangle)\n- m: [out] Number of eigenvalues found\n- w: [out] Eigenvalues in ascending order\n- z: [out] Eigenvectors (column-wise)\n- tolerance: Relative accuracy for eigenvalues\n- lb_ev, ub_ev: Eigenvalue range to compute (RANGE='V')\n- firstidx, lastidx: Index range to compute (RANGE='I')\n\n**Usage in Couenne:**\nUsed by CouenneSdpCuts to compute negative eigenvalues of the\nlifted matrix X - xxᵀ, where violations indicate cuts.\n\n**Algorithm:**\nDSYEVX uses bisection followed by inverse iteration for\nselected eigenvalues, more efficient than full diagonalization.", "see": ["CouenneSdpCuts which uses this for semidefinite cuts"], "has_pass2": false}, "src/cut/sdpcuts/CouenneMatrix.hpp": {"path": "layer-3/Couenne/src/cut/sdpcuts/CouenneMatrix.hpp", "filename": "CouenneMatrix.hpp", "file": "CouenneMatrix.hpp", "brief": "Sparse matrix and vector of expressions\n\nData structures for representing matrices of Couenne expressions,\nused primarily for SDP cuts and PSD constraints.\n\n**Classes:**\n- CouenneScalar: Single indexed expression element\n- CouenneSparseVector: Sparse vector of expressions (ordered set)\n- CouenneExprMatrix: Sparse matrix with row and column major storage\n\n**Storage:**\nUses std::set with custom comparators for O(log n) lookup.\nBoth row-major and column-major views maintained for efficient\nmatrix-vector and matrix-matrix operations.\n\n**Usage in SDP cuts:**\nX = (x_ij) where x_ij is the auxiliary for product x_i * x_j.\nvarIndices_ stores the original variables x_i used in products.\n\n**Operations:**\n- Vector dot product: v1 * v2\n- Matrix-vector product: M * v\n- Matrix-matrix product: M1 * M2\n- Threshold multiplication (early termination)", "see": ["CouenneSdpCuts which uses these for cut generation", "CouennePSDcon for PSD constraint using this matrix"], "has_pass2": false}, "src/cut/ellipcuts/CouenneEllipCuts.hpp": {"path": "layer-3/Couenne/src/cut/ellipcuts/CouenneEllipCuts.hpp", "filename": "CouenneEllipCuts.hpp", "file": "CouenneEllipCuts.hpp", "brief": "Ellipsoidal cuts for nonconvex quadratic constraints\n\nPlaceholder for ellipsoidal cutting planes derived from\nthe geometry of quadratic constraints.\n\n**Concept (not yet implemented):**\nFor a quadratic constraint xᵀQx + bᵀx ≤ c, the feasible\nregion forms an ellipsoid (if Q ≻ 0). Ellipsoidal cuts\nexploit this structure for tighter relaxations.\n\n@note Currently empty - functionality may be in development\n      or handled elsewhere (e.g., CouenneSdpCuts).", "see": ["CouenneSdpCuts for SDP-based cuts on quadratics", "CouenneExprQuad for quadratic expression handling"], "has_pass2": false}, "src/heuristics/cons_rowcuts.h": {"path": "layer-3/Couenne/src/heuristics/cons_rowcuts.h", "filename": "cons_rowcuts.h", "file": "cons_rowcuts.h", "brief": "constraint handler for rowcuts constraints\n        enables separation of convexification cuts during SCIP solution procedure", "author": "Timo Berthold\n@license This file is licensed under the Eclipse Public License (EPL)", "has_pass2": false}}}, "Dip": {"name": "Dip", "file_count": 25, "pass2_count": 9, "files": {"Dip/src/DecompApp.h": {"path": "layer-3/Dip/Dip/src/DecompApp.h", "filename": "DecompApp.h", "file": "DecompApp.h", "brief": "User application interface - derive to define your decomposition\n\nDecompApp is the main user-facing class. Derive from it to define:\n- Model decomposition (core vs relaxed constraints)\n- Subproblem solvers\n- Problem-specific heuristics", "algorithm": "Primal Heuristics (APPheuristics):\nGenerate feasible solutions during branch-and-price.\n\n  INPUT: Current fractional solution x̂\n  OUTPUT: Feasible integer solutions (incumbents)\n\n  Good incumbents:\n    - Improve global upper bound\n    - Enable more pruning in B&B tree\n    - Guide branching decisions", "see": ["DecompModel.h for model container classes", "DecompAlgo.h for algorithm that uses this app", "DecompParam.h for configuration parameters"], "return": "True, if x is feasible; otherwise, false.", "has_pass2": true}, "Dip/src/DecompCutPool.h": {"path": "layer-3/Dip/Dip/src/DecompCutPool.h", "filename": "DecompCutPool.h", "file": "DecompCutPool.h", "brief": "Pool of generated cuts (DecompCut) for cut management\n\nDecompCutPool manages cuts waiting to enter the master problem.\nInherits from std::vector<DecompWaitingRow> for storage.\n\n**Key Functions:**\n- reExpand(): Regenerate row coefficients when columns change\n- createRowReform(): Transform x-space cut to lambda-space\n- setViolations(): Calculate cut violations vs current solution\n\n**Cut Selection:**\n- is_greater_thanD comparator sorts by violation\n- Most violated cuts enter master first\n- Duplicate cuts filtered via hash\n\n**Reformulation:**\nCuts in x-space must be reformulated to lambda-space for\nDantzig-Wolfe master: a'x >= b becomes a'(sum_s s*lambda_s) >= b\ncreateRowReform() handles this transformation.\n\n**Validity Flag:**\n- m_rowsAreValid: Track if cuts need re-expansion\n- Set false when column set changes\n- reExpand() regenerates coefficients", "see": ["DecompCut.h for cut representation", "DecompWaitingRow.h for pool entry wrapper", "DecompAlgoCGL.h for CGL cut generators"], "has_pass2": false}, "Dip/src/DecompAlgoCGL.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoCGL.h", "filename": "DecompAlgoCGL.h", "file": "DecompAlgoCGL.h", "brief": "Interface to CGL (COIN-OR Cut Generation Library)\n\nDecompAlgoCGL wraps CGL cut generators for use in DIP algorithms.\nProvides automatic generation of standard MIP cuts.\n\n**Available CGL Generators:**\n- CglClique: Clique cuts from conflict graph\n- CglOddHole: Odd hole inequalities\n- CglFlowCover: Flow cover cuts\n- CglKnapsackCover: Knapsack cover cuts\n- CglMixedIntegerRounding2: MIR cuts\n- CglGomory: Gomory mixed-integer cuts\n\n**Usage:**\n- Created by DecompAlgo during initialization\n- Called during PHASE_CUT to generate cuts\n- Returns cuts via DecompCutOsi wrapper\n\n**Configuration:**\n- Generators enabled/disabled via DecompParam\n- Per-generator parameters can be set\n- Different strategies for PC vs C algorithms\n\n**Integration:**\nCGL generates cuts in x-space. For Price-and-Cut, these are\nreformulated to lambda-space before adding to master.", "see": ["DecompCutPool.h for cut storage", "DecompParam.h for CGL configuration", "CglCutGenerator (CGL) for base generator interface"], "has_pass2": false}, "Dip/src/DecompCut.h": {"path": "layer-3/Dip/Dip/src/DecompCut.h", "filename": "DecompCut.h", "file": "DecompCut.h", "brief": "Cut representation in original x-space\n\nDecompCut represents a cutting plane (valid inequality) that can be\nadded to strengthen the formulation. Cuts are defined in original\nx-space and expanded to the master problem.", "algorithm": "Duplicate Detection via Hash:\nAvoid adding same cut multiple times.\n\n  m_strHash = string encoding of (indices, coeffs, sense, rhs)\n  setStringHash(): Called after expandCutToRow()\n  Comparison: O(1) hash equality check", "math": "Why remove ineffective cuts:\n  - Slack cuts don't affect LP solution\n  - Large LPs slow down simplex iterations\n  - Memory overhead for storing coefficients\n  - Better to add fresh violated cuts", "complexity": "calcViolation: O(nnz(cut))\n  expandCutToRow: User-defined, typically O(cut_size)\n  setStringHash: O(nnz(cut))", "see": ["DecompCutPool.h for cut pool management", "DecompAlgoCGL.h for CGL integration"], "has_pass2": true}, "Dip/src/DecompConstraintSet.h": {"path": "layer-3/Dip/Dip/src/DecompConstraintSet.h", "filename": "DecompConstraintSet.h", "file": "DecompConstraintSet.h", "brief": "Storage for constraint matrix and bounds (A, b, l, u)\n\nDecompConstraintSet stores the full specification of a constraint set\nused in decomposition: matrix, row/column bounds, integrality markers.\n\n**Matrix Storage:**\n- M: CoinPackedMatrix (row-major or column-major)\n- rowLB, rowUB: Row bounds (rowLB <= Ax <= rowUB)\n- colLB, colUB: Variable bounds\n- rowSense, rowRhs: Alternative row format ('L', 'G', 'E', 'R')\n\n**Variable Information:**\n- integerVars: Indices of integer variables\n- integerMark: 'I' for integer, 'C' for continuous\n- colNames, rowNames: Human-readable names\n\n**Block Structure:**\n- activeColumns: Which columns appear in this block\n- activeColumnsS: Set version for O(1) lookup\n- masterOnlyCols: Columns that only appear in master\n\n**Sparse Representation:**\nFor blocks with few active columns:\n- m_isSparse: Enable sparse mode\n- m_origToSparse, m_sparseToOrig: Column mappings\nReduces memory for blocks with disjoint variable sets.", "see": ["DecompModel.h for wrapper with metadata", "DecompApp.h where constraint sets are defined"], "has_pass2": false}, "Dip/src/AlpsDecompTreeNode.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompTreeNode.h", "filename": "AlpsDecompTreeNode.h", "file": "AlpsDecompTreeNode.h", "brief": "ALPS tree node for DIP branch-and-bound\n\nAlpsDecompTreeNode represents a node in the B&B tree, deriving from\nAlpsTreeNode to integrate with ALPS's tree management.\n\n**Key Responsibilities:**\n- Store branching decisions (bound changes from parent)\n- Implement process() to solve node via DecompAlgo\n- Implement branch() to create child nodes\n- Check for new incumbents\n\n**Branching Storage:**\n- downBranchLB_, downBranchUB_: Bounds for down branch\n- upBranchLB_, upBranchUB_: Bounds for up branch\nStored as (var_index, bound_value) pairs.\n\n**Key Methods:**\n- process(): Main bounding - calls DecompAlgo::processNode()\n- branch(): Create children with new bounds\n- checkIncumbent(): Test if solution improves best known\n- createNewTreeNode(): Factory for child nodes\n\n**Node Processing Flow:**\n1. ALPS selects node from tree\n2. Calls process() which invokes DecompAlgo\n3. DecompAlgo returns bound and status\n4. If fractional, branch() creates children", "see": ["AlpsDecompModel.h for parent model", "DecompAlgo.h for node processing logic", "AlpsTreeNode (ALPS) for base class"], "has_pass2": false}, "Dip/src/AlpsDecompModel.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompModel.h", "filename": "AlpsDecompModel.h", "file": "AlpsDecompModel.h", "brief": "ALPS integration - model class bridging DIP and ALPS B&B\n\nAlpsDecompModel derives from AlpsModel to integrate DIP's decomposition\nalgorithms with ALPS's branch-and-bound framework.\n\n**Key Responsibilities:**\n- Hold pointer to active DecompAlgo\n- Create root node (AlpsDecompTreeNode)\n- Manage solution pool and incumbent\n- Handle ALPS callbacks and parameters\n\n**ALPS Hierarchy:**\nAlpsKnowledge -> AlpsModel -> AlpsDecompModel\n\n**Key Methods:**\n- createRoot(): Create initial tree node\n- setupSelf(): Initialize from parameters\n- registerKnowledge(): Register solution types\n\n**Data Flow:**\n1. User creates DecompApp and DecompAlgo\n2. AlpsDecompModel wraps the algorithm\n3. ALPS drives B&B, calling process() on nodes\n4. Nodes delegate to DecompAlgo for bounding", "see": ["AlpsDecompTreeNode.h for B&B tree nodes", "DecompAlgo.h for the algorithm being integrated", "AlpsModel (ALPS) for base class interface"], "has_pass2": false}, "Dip/src/DecompVar.h": {"path": "layer-3/Dip/Dip/src/DecompVar.h", "filename": "DecompVar.h", "file": "DecompVar.h", "brief": "Column generation variable (lambda) representation\n\nDecompVar represents a column in the Dantzig-Wolfe reformulation.\nEach lambda_s corresponds to an extreme point s of a subproblem\npolyhedron: conv{x : A'x >= b', x integer}.", "algorithm": "Column Effectiveness Counter:\nTrack column usefulness for pool management.\n\n  m_effCnt > 0: Column in basis (positive = good)\n  m_effCnt < 0: Column out of basis (negative = candidate for removal)\n  increaseEffCnt(): When column enters basis\n  decreaseEffCnt(): When column leaves basis\n\nLow effectiveness columns may be removed to control master size.", "math": "Let (π, μ_k) be duals of (linking rows, convexity row k):\n  r̄(s) = c's - π'(A''s) - μ_k\n       = (c - A''^T π)'s - μ_k\n\nPRICING ORACLE:\n  For block k: min_{s ∈ P_k} (c - A''^T π)'s\n  Add column if optimal value - μ_k < 0\n\nECONOMIC INTERPRETATION:\n  c's = original cost contribution\n  π'(A''s) = \"payment\" from linking constraint duals\n  μ_k = threshold (convexity dual, one per block)\n  Column profitable if cost < payment + threshold", "complexity": "fillDenseArr: O(nnz(s))\n  isDuplicate: O(vars.size()) hash comparisons\n  calcNorm: O(nnz(s))", "see": ["DecompVarPool.h for column pool management", "DecompAlgoPC.h for pricing/column generation"], "has_pass2": true}, "Dip/src/DecompWaitingCol.h": {"path": "layer-3/Dip/Dip/src/DecompWaitingCol.h", "filename": "DecompWaitingCol.h", "file": "DecompWaitingCol.h", "brief": "Wrapper pairing DecompVar with its master column coefficients\n\nDecompWaitingCol bundles a DecompVar (the x-space representation)\nwith its expanded column (A''s) for the master problem.\n\n**Data Members:**\n- m_var: The variable in x-space (extreme point s)\n- m_col: CoinPackedVector of (A''s) master coefficients\n\n**Purpose:**\nWhen pricing generates a new column s, we need both:\n1. The x-space representation for solution reconstruction\n2. The master coefficients for LP updates\nThis class keeps them paired.\n\n**Lifecycle:**\n- Created during pricing when subproblem returns solution\n- Stored in DecompVarPool\n- m_col regenerated via reExpand() when master changes\n- Eventually transferred to master LP", "see": ["DecompVar.h for the variable itself", "DecompVarPool.h for column pool management"], "has_pass2": false}, "Dip/src/DecompStats.h": {"path": "layer-3/Dip/Dip/src/DecompStats.h", "filename": "DecompStats.h", "file": "DecompStats.h", "brief": "Statistics tracking for DIP algorithm performance\n\nProvides classes for recording and reporting algorithm statistics.\n\n**DecompObjBound:**\nRecords bound history at checkpoints:\n- phase, cutPass, pricePass: Algorithm state\n- timeStamp: Wall clock from start\n- thisBound/thisBoundUB: Current node bounds\n- bestBound: Global lower bound\n- thisBoundIP: Best integer solution\n\n**DecompStats:**\nAggregate statistics for the algorithm:\n- objHistoryBound: Vector of DecompObjBound snapshots\n- Various timers for profiling subsystems\n- Counts of cuts/columns generated\n\n**Usage:**\n- DecompAlgo creates and updates stats during execution\n- Bound history enables convergence analysis\n- Timers help identify bottlenecks", "see": ["DecompAlgo.h where stats are collected", "UtilTimer.h for timer implementation"], "has_pass2": false}, "Dip/src/DecompWaitingRow.h": {"path": "layer-3/Dip/Dip/src/DecompWaitingRow.h", "filename": "DecompWaitingRow.h", "file": "DecompWaitingRow.h", "brief": "Wrapper pairing DecompCut with its row coefficients\n\nDecompWaitingRow bundles a DecompCut with both its x-space row\nand reformulated lambda-space row for the master problem.\n\n**Data Members:**\n- m_cut: The cut object with bounds and metadata\n- m_row: CoinPackedVector in original x-space\n- m_rowReform: CoinPackedVector in lambda-space for master\n\n**Reformulation:**\nOriginal cut: a'x >= b\nIn master: sum_s (a's)lambda_s >= b\nm_rowReform stores coefficients (a's) for each lambda variable.\n\n**Lifecycle:**\n- Created during cut generation\n- Stored in DecompCutPool\n- m_rowReform computed via createRowReform()\n- Eventually added to master LP", "see": ["DecompCut.h for the cut itself", "DecompCutPool.h for cut pool management"], "has_pass2": false}, "Dip/src/DecompParam.h": {"path": "layer-3/Dip/Dip/src/DecompParam.h", "filename": "DecompParam.h", "file": "DecompParam.h", "brief": "Comprehensive parameter class for DIP algorithm configuration\n\nDecompParam contains 100+ parameters controlling all aspects of DIP\ndecomposition algorithms. Parameters read from file via UtilParameters.\n\n**Logging Parameters:**\n- LogLevel: Verbosity (0=silent to 5=debug)\n- LogDumpModel: When to dump LP models (0=never, 2=always)\n- LogObjHistory: Print objective progress\n\n**Algorithm Limits:**\n- TotalCutItersLimit, TotalPriceItersLimit: Iteration caps\n- RoundCutItersLimit, RoundPriceItersLimit: Per-round limits\n- TimeLimit: Wall clock limit\n- NodeLimit: Max B&B nodes\n\n**Subproblem Control:**\n- SubProbTimeLimitExact: Time for exact subproblem solves\n- SubProbTimeLimitHeur: Time for heuristic solves\n- SubProbNumSolLimit: Max solutions per subproblem\n- SubProbParallelType: OpenMP scheduling strategy\n\n**Dual Stabilization:**\n- DualStab: Enable Wentges stabilization\n- DualStabAlpha: Smoothing parameter [0,1]\n\n**Branching:**\n- BranchEnforceInSubProb: Branch on x in subproblems\n- BranchEnforceInMaster: Branch on lambda in master\n\n**Strategy:**\n- PCStrategy: Balance pricing vs cutting\n- CutCglStrategy: Which CGL cuts to use", "see": ["DecompApp.h where m_param is stored", "UtilParameters for parameter file parsing"], "has_pass2": false}, "Dip/src/DecompAlgo.h": {"path": "layer-3/Dip/Dip/src/DecompAlgo.h", "filename": "DecompAlgo.h", "file": "DecompAlgo.h", "brief": "Base class for all DIP decomposition algorithms\n\nDecompAlgo is the algorithmic engine that orchestrates:\n- Master problem management (LP relaxation)\n- Subproblem solving (pricing/column generation)\n- Cut generation and management\n- Phase transitions and convergence\n\n**Key Data Members:**\n- m_masterSI: Master LP solver interface\n- m_app: Pointer to user's DecompApp\n- m_modelCore/m_modelRelax: Problem decomposition\n- m_vars/m_cuts: Generated columns and cuts\n- m_xhat: Current LP solution in original x-space\n\n**Algorithm Phases:**\n- PHASE_PRICE1: Feasibility with artificial variables\n- PHASE_PRICE2: Optimizing with generated columns\n- PHASE_CUT: Adding violated inequalities\n\n**Virtual Methods for Subclasses:**\n- createMasterProblem(): Build initial restricted master\n- processNode(): Main node processing loop\n- generateVars(): Column generation (pricing)\n- generateCuts(): Cut separation\n- getMasterDualSolution(): Dual values for pricing\n\n**Derived Classes:**\n- DecompAlgoPC: Price-and-Cut (Dantzig-Wolfe)\n- DecompAlgoC: Cutting plane only\n- DecompAlgoRC: Relax-and-Cut (Lagrangian)", "algorithm": "Price-and-Cut Hybrid (generateVars + generateCuts):\n  Interleave column generation and cut separation:\n  while (improving):\n    1. generateVars(): Solve pricing, add columns\n    2. generateCuts(): Separate cuts on current x̂\n    3. Update master LP, reoptimize\n  Cuts added to master affect dual space and pricing.", "math": "L(u) = u'b + min_x {(c - A'u)'x : x ∈ X}\n  Solve Lagrangian dual: max_u L(u) via subgradient or bundle methods.\n  - Subgradient: u^{t+1} = u^t + step * (b - Ax^t)\n  - L(u) provides lower bound; project x to get upper bound.", "ref": ["Dantzig, G.B. and Wolfe, P. (1960). \"Decomposition principle for\n       linear programs\". Operations Research 8(1):101-111.", "Barnhart, C. et al. (1998). \"Branch-and-price: Column generation\n       for solving huge integer programs\". Operations Research 46(3).", "Held, M. and Karp, R.M. (1971). \"The traveling-salesman problem\n       and minimum spanning trees: Part II\". Math Programming 1(1):6-25.", "Desaulniers, G. et al. (2005). \"Column Generation\". Springer."], "complexity": "Column generation: O(#iterations × pricing_cost)\n  Pricing cost depends on subproblem structure (often NP-hard but small)\n  Master LP: O(m × n × simplex_iterations) where n grows with columns", "see": ["DecompAlgoPC.h for Dantzig-Wolfe implementation", "DecompAlgoC.h for cutting plane method", "DecompAlgoRC.h for Lagrangian relaxation", "DecompApp.h for user model definition"], "has_pass2": true}, "Dip/src/DecompModel.h": {"path": "layer-3/Dip/Dip/src/DecompModel.h", "filename": "DecompModel.h", "file": "DecompModel.h", "brief": "Wrapper classes for constraint sets used in decomposition\n\nProvides model containers that wrap DecompConstraintSet with\nmetadata and optional solver interfaces.\n\n**DecompModel:**\nBasic wrapper for a constraint set (A'x >= b' or A''x >= b'')\n- m_model: Pointer to the constraint set\n- m_modelName: Human-readable name for debugging\n- m_blockId: Which block this belongs to (-1 for core)\n\n**DecompSubModel:**\nExtended model with OsiSolverInterface for subproblem solving\n- m_osi: Solver interface (Clp, Cpx, Grb, etc.)\n- solveAsMIP(): Solve subproblem as MIP\n- Used when user doesn't provide custom solveRelaxed()\n\n**Usage in Decomposition:**\n- Core model (A''): Linking constraints kept in master\n- Relax models (A'): Block constraints for subproblems\n- Each block b has m_modelRelax[b] in DecompApp", "see": ["DecompConstraintSet.h for underlying matrix storage", "DecompApp.h for model decomposition setup"], "has_pass2": false}, "Dip/src/AlpsDecompSolution.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompSolution.h", "filename": "AlpsDecompSolution.h", "file": "AlpsDecompSolution.h", "brief": "ALPS solution wrapper for DIP incumbent solutions\n\nAlpsDecompSolution derives from AlpsSolution to integrate DIP\nsolutions with ALPS's solution management and reporting.\n\n**Data Members:**\n- m_size: Number of variables\n- m_values: Solution vector\n- m_quality: Objective value\n- m_app: Pointer to DecompApp for printing\n\n**ALPS Integration:**\n- Stored in ALPS solution pool\n- Used for incumbent tracking\n- Supports encode/decode for parallelism\n\n**Lifecycle:**\n- Created when DIP finds integer-feasible solution\n- Passed to ALPS via registerKnowledge()\n- Best solution reported at termination", "see": ["DecompSolution.h for base solution class", "AlpsDecompModel.h for model integration", "AlpsSolution (ALPS) for base class"], "has_pass2": false}, "Dip/src/AlpsDecompNodeDesc.h": {"path": "layer-3/Dip/Dip/src/AlpsDecompNodeDesc.h", "filename": "AlpsDecompNodeDesc.h", "file": "AlpsDecompNodeDesc.h", "brief": "ALPS node descriptor storing branching bounds\n\nAlpsDecompNodeDesc derives from AlpsNodeDesc to store the\ndescription of a tree node for serialization and reconstruction.\n\n**Storage:**\nDIP doesn't use full differencing (delta encoding), so node\ndescriptions primarily store bound changes from branching:\n- Variable lower/upper bound modifications\n- Warm start basis (optional)\n\n**Key Methods:**\n- encode(): Serialize to AlpsEncoded for transmission\n- decode(): Reconstruct from encoded form\n\n**ALPS Integration:**\n- Each AlpsDecompTreeNode has an AlpsDecompNodeDesc\n- Used for checkpointing and parallel distribution\n- Compact representation for tree storage", "see": ["AlpsDecompTreeNode.h for the tree node using this", "AlpsNodeDesc (ALPS) for base class interface"], "has_pass2": false}, "Dip/src/DecompAlgoRC.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoRC.h", "filename": "DecompAlgoRC.h", "file": "DecompAlgoRC.h", "brief": "Relax-and-Cut algorithm (Lagrangian relaxation with cuts)\n\nDecompAlgoRC implements Lagrangian relaxation:\n- Dualize complicating constraints with multipliers u\n- Solve Lagrangian subproblem: min (c - u'A'')x s.t. A'x >= b'\n- Update multipliers via subgradient optimization\n- Add cuts to improve bounds", "algorithm": "Relax-and-Cut Enhancement:\nAdd cuts to tighten Lagrangian bound.\n\n  During subgradient iterations:\n    1. Solve Lagrangian subproblem → x_k\n    2. Check for violated cuts at x_k\n    3. Add cuts to subproblem (not dualized)\n    4. Continue until convergence or cut limit", "math": "Weak duality: L(u) ≤ z* for all u ≥ 0\nStrong duality (linear case): max_u L(u) = LP relaxation value\nIntegrality gap: z* - max_u L(u) (may be positive for IP)", "complexity": "Per iteration: O(subproblem) + O(subgradient update)\nConvergence: O(1/ε²) iterations for ε-optimal multipliers", "ref": ["Held & Karp (1970,1971). \"The Traveling Salesman Problem and\n  Minimum Spanning Trees\". Operations Research."], "see": ["DecompAlgo.h for base class", "DecompAlgoPC.h for LP-based alternative"], "has_pass2": true}, "Dip/src/Decomp.h": {"path": "layer-3/Dip/Dip/src/Decomp.h", "filename": "Decomp.h", "file": "Decomp.h", "brief": "Central header with enums, constants, and solver interfaces for DIP\n\nThis is the foundational include for DIP (Decomposition for Integer\nProgramming). It provides all enums, constants, and conditional solver\ninterface includes.", "algorithm": "Decomposition Methods for Integer Programming:\nExploit problem structure by partitioning constraints into\n\"easy\" subproblems and \"complicating\" linking constraints.\n\nDANTZIG-WOLFE DECOMPOSITION (Price-and-Cut):\nOriginal:  min c'x s.t. A·x ≥ b, D·x ≥ d, x ∈ Z\nMaster:    min Σₖ (c'sₖ)λₖ s.t. Σₖ (A·sₖ)λₖ ≥ b, Σλₖ = 1\nwhere sₖ are extreme points of P = conv{x: D·x ≥ d, x ∈ Z}\n\nPricing subproblem: min (c - π'A)·x over x ∈ P\nColumn sₖ has negative reduced cost if min_x < π₀\n\nLAGRANGIAN RELAXATION (Relax-and-Cut):\nRelax complicating constraints with multipliers μ:\n  L(μ) = min c'x + μ'(b - A·x) s.t. D·x ≥ d\n       = μ'b + min (c - A'μ)'x s.t. D·x ≥ d\nL(μ) is lower bound; maximize over μ (Lagrangian dual)\n\nBRANCH-PRICE-CUT:\nCombines: B&B tree search + column generation + cut generation\nAt each node: price columns, add cuts, then branch", "math": "Dantzig-Wolfe bound:\nz_DW ≥ z_LP (at least as tight as LP relaxation)\nEquality when linking constraints have block-angular structure\n\nLagrangian bound: z_LR = max_μ L(μ) ≥ z_LP\nz_DW = z_LR under mild conditions (no duality gap)", "complexity": "- Pricing: complexity of subproblem (e.g., knapsack = O(nW))\n- Master LP: O(iterations × columns × linking_rows)\n- Overall: pseudo-polynomial for structured problems", "ref": ["Vanderbeck & Wolsey (2010). \"Reformulation and Decomposition\n  of Integer Programs\". 50 Years of Integer Programming.\n\n**Algorithm Types (DecompAlgoType):**\n- CUT: Cutting plane method only\n- PRICE_AND_CUT: Dantzig-Wolfe column generation with cuts\n- RELAX_AND_CUT: Lagrangian relaxation with cuts\n- VOL_AND_CUT: Volume algorithm variant\n\n**Algorithm Phases (DecompPhase):**\n- PHASE_PRICE1: Restricted master with artificial columns\n- PHASE_PRICE2: Normal pricing after Phase 1 feasibility\n- PHASE_CUT: Cut generation phase\n- PHASE_DONE: Node processing complete\n\n**Solver Support:**\nConditionally includes OSI interfaces for:\n- Clp (DIP_HAS_CLP): Default open-source LP\n- CPLEX (DIP_HAS_CPX): IBM commercial solver\n- Gurobi (DIP_HAS_GRB): Commercial solver\n- Xpress (DIP_HAS_XPR): FICO commercial solver\n- CBC (DIP_HAS_CBC): COIN-OR MIP solver\n- SYMPHONY (DIP_HAS_SYMPHONY): Parallel MIP solver\n\n**Key Constants:**\n- DecompBigNum: Large value for infinity (1e21)\n- DecompEpsilon: Numerical tolerance (1e-6)\n- DecompZero: Near-zero threshold (1e-14)"], "see": ["DecompAlgo.h for base algorithm class", "DecompApp.h for user application interface", "DecompParam.h for algorithm parameters"], "has_pass2": true}, "Dip/src/DecompMemPool.h": {"path": "layer-3/Dip/Dip/src/DecompMemPool.h", "filename": "DecompMemPool.h", "file": "DecompMemPool.h", "brief": "Pre-allocated memory arrays for performance\n\nDecompMemPool provides reusable scratch arrays to avoid repeated\nallocations during the inner loops of the algorithm.\n\n**Pre-allocated Arrays:**\n- dblArrNCoreCols: Double array sized for core columns\n- dblArrNCoreRows: Double array sized for core rows\n\n**Usage:**\nRather than allocating temporary arrays in hot paths like\nreduced cost calculation or constraint evaluation, use these\npre-allocated buffers for better cache performance.\n\n**Memory Management:**\n- allocateMemory(): Size arrays based on problem dimensions\n- Destructor frees all allocated memory\n- Single allocation at algorithm start", "see": ["DecompAlgo.h where memory pool is used"], "has_pass2": false}, "Dip/src/DecompAlgoPC.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoPC.h", "filename": "DecompAlgoPC.h", "file": "DecompAlgoPC.h", "brief": "Price-and-Cut algorithm (Dantzig-Wolfe decomposition with cuts)\n\nDecompAlgoPC implements the most powerful DIP algorithm combining:\n- Column generation (pricing subproblems)\n- Cut generation (violated inequalities)\n- Branch-and-bound integration via ALPS", "algorithm": "Price-and-Cut (Column Generation with Cuts):\nSolves reformulated MIP by iterating between pricing and cutting.\n\nCOLUMN GENERATION LOOP:\n  while (has_negative_reduced_cost):\n    1. Solve restricted master LP → duals π\n    2. For each subproblem k:\n       Solve: min (c - A'π)·x s.t. x ∈ Pₖ\n       If optimal < convexity_dual: add column\n    3. If no columns added: optimal for current cut set\n\nCUT GENERATION:\n  After pricing converges:\n  1. Compute x = Σₖ sₖλₖ (convex combination)\n  2. Search for violated cuts: a'x > b\n  3. Add cuts to master, return to pricing\n\nBRANCHING:\n  When both pricing and cutting converge with fractional solution:\n  Branch on λ variables or original x variables\n\nDUAL STABILIZATION (Wentges smoothing):\n  Raw duals π_RM from master may oscillate wildly.\n  Stabilized: π_stab = α·π_RM + (1-α)·π_prev\n  Better subproblem objectives, faster convergence.", "math": "Reduced cost for column s from subproblem k:\n  r̄ₛ = c'sₖ - π'(A·sₖ) - μₖ\n  where μₖ is dual of convexity constraint for block k\n\nPricing oracle: min_{x ∈ Pₖ} (c - A'π)'x\nAdd column if optimal value < μₖ", "complexity": "- Per pricing iteration: k × subproblem_complexity\n- Per master solve: O(cols × rows) simplex\n- Total: difficult to bound, empirically fast for structured problems", "ref": ["Barnhart, Johnson, Nemhauser, Savelsbergh & Vance (1998).\n  \"Branch-and-Price: Column Generation for Solving Huge Integer Programs\".\n  Operations Research 46(3):316-329.\n\n**Dantzig-Wolfe Reformulation:**\nOriginal: min c'x s.t. A''x >= b'', A'x >= b', x integer\nReformulated: min sum_s (c's_s)lambda_s s.t. sum_s (A''s_s)lambda_s >= b''\nwhere s_s are extreme points of conv{x: A'x >= b', x integer}\n\n**Dual Stabilization:**\n- m_dual: Stabilized dual used for pricing\n- m_dualRM: Raw dual from restricted master\n- m_dualST: Smoothed dual (Wentges smoothing)\nPrevents oscillation and improves convergence using:\n  dual_stab = alpha * dual_RM + (1-alpha) * dual_prev\n\n**Key Overrides:**\n- createMasterProblem(): Build restricted master with convexity rows\n- getMasterDualSolution(): Return stabilized duals for pricing\n- phaseUpdate(): Manage PRICE1->PRICE2->CUT transitions\n\n**Phases:**\n- PHASE_PRICE1: Generate initial columns, drive out artificials\n- PHASE_PRICE2: Normal pricing until no negative reduced cost\n- PHASE_CUT: Add cuts, then return to pricing if cuts found"], "see": ["DecompAlgo.h for base class interface", "DecompVar.h for column representation", "DecompCut.h for cut representation"], "has_pass2": true}, "Dip/src/DecompSolution.h": {"path": "layer-3/Dip/Dip/src/DecompSolution.h", "filename": "DecompSolution.h", "file": "DecompSolution.h", "brief": "Solution storage for feasible/incumbent solutions\n\nDecompSolution stores primal solutions found during decomposition\n(from heuristics, subproblems, or node processing).\n\n**Key Data Members:**\n- m_size: Number of variables (columns)\n- m_values: Solution vector x*\n- m_quality: Objective value c'x* (for incumbent tracking)\n\n**Solution Sources:**\n- Subproblem solutions (during pricing)\n- User heuristics (DecompApp::APPheuristics)\n- Rounding/diving heuristics\n- Integer LP solutions\n\n**Output Methods:**\n- print(): Display solution with specified precision\n- printMIPLIB(): Output in MIPLIB solution format\n\n**Integration:**\nSolutions are passed to ALPS via AlpsDecompModel for\nincumbent tracking and solution reporting.", "see": ["DecompApp.h for heuristic callbacks", "AlpsDecompModel.h for ALPS integration"], "has_pass2": false}, "Dip/src/DecompSolverResult.h": {"path": "layer-3/Dip/Dip/src/DecompSolverResult.h", "filename": "DecompSolverResult.h", "file": "DecompSolverResult.h", "brief": "Container for subproblem/LP solver results\n\nDecompSolverResult captures all outputs from solving a subproblem\nor LP relaxation, including status, bounds, and solutions.\n\n**Status Information:**\n- m_solStatus: Solver-specific status code\n- m_isOptimal: True if proven optimal\n- m_isUnbounded: True if unbounded detected\n- m_isCutoff: True if cutoff by bound\n\n**Bounds:**\n- m_objLB: Lower bound on optimal value\n- m_objUB: Upper bound / incumbent value\n\n**Solutions:**\n- m_nSolutions: Number of solutions found\n- m_solution: Vector of solution vectors (for solution pools)\n\n**Usage:**\nReturned by subproblem solvers to DecompAlgo. Used for:\n- Generating new columns (m_solution contains extreme points)\n- Checking feasibility (m_isOptimal, m_solStatus)\n- Updating bounds (m_objLB, m_objUB)", "see": ["DecompSubModel.h for subproblem solving", "DecompAlgo.h for result processing"], "has_pass2": false}, "Dip/src/DecompVarPool.h": {"path": "layer-3/Dip/Dip/src/DecompVarPool.h", "filename": "DecompVarPool.h", "file": "DecompVarPool.h", "brief": "Pool of generated columns (DecompVar) for column generation\n\nDecompVarPool manages columns waiting to enter the master problem.\nInherits from std::vector<DecompWaitingCol> for storage.", "algorithm": "Column Re-expansion (reExpand):\nWhen master constraints change, column coefficients may be stale.\n\n  m_colsAreValid flag tracks validity\n  When cuts added: setColsAreValid(false)\n  Before pricing: if !colsAreValid, call reExpand()\n  reExpand(): Recompute As for each stored s", "math": "Why filter parallel columns:\n  Near-parallel columns provide marginal improvement\n  Adding both increases master LP size without benefit\n  Better to diversify column set for faster convergence", "complexity": "isDuplicate: O(pool_size) hash comparisons\n  isParallel: O(pool_size × column_nnz) for dot products\n  setReducedCosts: O(pool_size × num_duals)\n  reExpand: O(pool_size × nnz(A))", "see": ["DecompVar.h for column representation", "DecompWaitingCol.h for pool entry wrapper", "DecompAlgoPC.h for column generation driver"], "has_pass2": true}, "Dip/src/DecompAlgoC.h": {"path": "layer-3/Dip/Dip/src/DecompAlgoC.h", "filename": "DecompAlgoC.h", "file": "DecompAlgoC.h", "brief": "Cutting Plane Method algorithm (no column generation)\n\nDecompAlgoC implements classic cutting plane method:\n- Solve LP relaxation\n- Find violated cuts\n- Add cuts and resolve\n- Repeat until integer or no cuts found", "algorithm": "Cutting Plane Method (CPM):\nIteratively tighten LP relaxation with valid inequalities.\n\nALGORITHM:\n  1. Solve LP relaxation: min c'x s.t. Ax ≥ b\n  2. if x* is integer-feasible: STOP (optimal)\n  3. Search for violated cut: a'x ≥ β with a'x* < β\n  4. if cut found: add to LP, goto 1\n  5. else: BRANCH on fractional variable\n\nMASTER PROBLEM STRUCTURE:\n  Original variables x (not Dantzig-Wolfe lambdas)\n  Rows grow as cuts are added\n  Columns remain fixed", "math": "Convergence for polyhedra:\n  If conv(X) = {x : Ax ≥ b} (LP = IP), CPM finds optimum.\n  In general, CPM finds optimum over intersection of\n  LP relaxation with all generated cuts.\n\nWHEN TO USE:\n- LP relaxation is naturally strong\n- No obvious block structure for decomposition\n- Want simplicity over Dantzig-Wolfe", "complexity": "Per iteration: O(LP_solve) + O(cut_generation)\nIterations bounded by number of extreme points cut off.", "ref": ["Kelley (1960). \"The Cutting-Plane Method for Solving Convex Programs\"."], "see": ["DecompAlgo.h for base class", "DecompAlgoPC.h for full Price-and-Cut", "DecompAlgoCGL.h for CGL cut integration"], "has_pass2": true}, "Dip/src/dippy/DippyDecompAlgo.h": {"path": "layer-3/Dip/Dip/src/dippy/DippyDecompAlgo.h", "filename": "DippyDecompAlgo.h", "param": ["utilParam parameter class", "pProb a DipProblem python object"], "has_pass2": false}}}, "DisCO": {"name": "DisCO", "file_count": 26, "pass2_count": 6, "files": {"src/DcoHeuristic.hpp": {"path": "layer-3/DisCO/src/DcoHeuristic.hpp", "filename": "DcoHeuristic.hpp", "file": "DcoHeuristic.hpp", "brief": "Abstract base class for primal heuristics", "algorithm": "Primal Heuristic Framework for Mixed-Integer Conic Optimization", "math": "Heuristics find feasible solutions x̃ satisfying:\n      Ax̃ ≤ b, x̃_j ∈ Z for j ∈ I, x̃ ∈ K (conic constraints)\n      Good incumbents enable pruning: nodes with LB ≥ UB(x̃) are pruned.\n\nDcoHeuristic provides the interface for heuristics that search\nfor feasible solutions during branch-and-bound.\n\n**Heuristic Types (DcoHeurType):**\n- DcoHeurTypeRounding: Round fractional solution\n\n**Strategies (DcoHeurStrategy):**\n- None: Disabled\n- Root: Only at root node\n- Auto: System decides\n- Periodic: Every N nodes\n- BeforeRoot: Before first LP solve\n\n**Statistics (DcoHeurStats):**\n- numCalls_: Times heuristic invoked\n- numNoSolCalls_: Calls with no solution found\n- time_: CPU time consumed\n- numSolutions_: Solutions found\n\n**Pure Virtual:**\n- searchSolution(): Find and return feasible solution or NULL", "complexity": "Varies by heuristic: O(n) for rounding to O(solve) for sub-MIP", "see": ["DcoHeurRounding.hpp for simple rounding heuristic", "DcoTreeNode.hpp::callHeuristics() for invocation", "DcoModel.hpp::heuristics_ for storage"], "has_pass2": true}, "src/DcoModel.hpp": {"path": "layer-3/DisCO/src/DcoModel.hpp", "filename": "DcoModel.hpp", "file": "DcoModel.hpp", "brief": "Main model class for Mixed-Integer Conic Optimization\n\nDcoModel derives from BcpsModel and represents the master MISOCP problem.\nIt manages problem data, solver interfaces, and algorithm configuration.\n\n**Two Solving Modes:**\n- Direct conic: Uses OsiConicSolverInterface (CPLEX, Mosek)\n- OA (Outer Approximation): Uses OsiSolverInterface with linear cuts\n\n**Problem Structure:**\n- matrix_: Linear constraint matrix (CoinPackedMatrix)\n- coneStart_, coneMembers_, coneType_: Conic constraints\n- integerCols_: Indices of integer variables\n- relaxedCols_, relaxedRows_: Objects with relaxed integrality/cone\n\n**Algorithm Components:**\n- branchStrategy_: Variable selection (MaxInf, Pseudo, Strong, Reliability)\n- conGenerators_: Map of constraint generators (MILP + conic cuts)\n- heuristics_: Primal heuristics (rounding)\n\n**Key Virtual Methods:**\n- readInstance(): Load MPS or CBF files\n- setupSelf(): Initialize solver, cuts, heuristics\n- createRoot(): Create root DcoTreeNode\n- feasibleSolution(): Check integer and cone feasibility", "see": ["DcoTreeNode.hpp for tree node processing", "DcoConGenerator.hpp for cut generation interface", "DcoParams.hpp for configuration parameters"], "has_pass2": false}, "src/DcoNodeDesc.hpp": {"path": "layer-3/DisCO/src/DcoNodeDesc.hpp", "filename": "DcoNodeDesc.hpp", "file": "DcoNodeDesc.hpp", "brief": "Tree node description storing branching and warm-start data\n\nDcoNodeDesc stores the information that distinguishes a tree node\nfrom its parent, enabling node reconstruction and pseudocost updates.\n\n**Inheritance:** DcoNodeDesc -> BcpsNodeDesc -> AlpsNodeDesc\n\n**Branching Data (for pseudocost updates):**\n- branchedDir_: Up or down branch direction\n- branchedInd_: Index of variable branched on\n- branchedVal_: Value at which branching occurred\n\n**Warm Start:**\n- basis_: CoinWarmStartBasis for LP warm-starting\n\n**From BcpsNodeDesc (inherited):**\n- vars_: Variable bound modifications (BcpsObjectListMod)\n- cons_: Constraint modifications (BcpsObjectListMod)\n\n**Serialization:**\nencode()/decode() for parallel tree distribution.", "see": ["DcoTreeNode.hpp for the node using this descriptor", "DcoBranchObject.hpp for branching decisions", "CoinWarmStartBasis (CoinUtils) for LP basis"], "has_pass2": false}, "src/DcoConstraint.hpp": {"path": "layer-3/DisCO/src/DcoConstraint.hpp", "filename": "DcoConstraint.hpp", "file": "DcoConstraint.hpp", "brief": "Abstract base class for DisCO constraints\n\nDcoConstraint is the base class for all constraints in DisCO,\nsupporting both linear and conic constraint types.\n\n**Inheritance:** DcoConstraint -> BcpsConstraint -> BcpsObject -> AlpsKnowledge\n\n**Constraint Hierarchy:**\n- DcoConstraint (ABC)\n  - DcoLinearConstraint: Linear row with coefficients\n  - DcoConicConstraint: Lorentz or rotated Lorentz cone\n\n**Type Field (DcoConstraintType):**\n- Core: Original problem constraints\n- MILP cuts: Clique, Gomory, MIR, etc.\n- Conic cuts: IPM, OA approximations\n\n**Key Methods:**\n- createOsiRowCut(): Convert to OsiRowCut (returns NULL for conic)\n- constraintType(): Get/set the constraint source type", "see": ["DcoLinearConstraint.hpp for linear constraints", "DcoConicConstraint.hpp for conic constraints", "Dco.hpp for DcoConstraintType enum"], "has_pass2": false}, "src/DcoConicConstraint.hpp": {"path": "layer-3/DisCO/src/DcoConicConstraint.hpp", "filename": "DcoConicConstraint.hpp", "file": "DcoConicConstraint.hpp", "brief": "Second-order cone (Lorentz cone) constraint representation\n\nDcoConicConstraint represents conic constraints of two types:\n- Lorentz cone: ||x_1...x_{n-1}|| <= x_n\n- Rotated Lorentz cone: 2*x_1*x_2 >= ||x_3...x_n||^2\n\n**Data Members:**\n- coneType_: DcoLorentzCone or DcoRotatedLorentzCone\n- coneSize_: Number of variables in cone\n- members_: Variable indices forming the cone\n- supports_: Linear constraints approximating the cone (OA)\n- activeSupports_: Which supports are binding\n\n**OA (Outer Approximation):**\nWhen using linear solvers, conic constraints are approximated by\nlinear supporting hyperplanes. The supports_ array stores these\ncuts, and activeSupports_ tracks which are currently tight.\n\n**Feasibility:**\ninfeasibility() returns the cone violation at the current solution,\nused to decide if more OA cuts are needed.", "see": ["DcoConstraint.hpp for base class", "DcoLinearConstraint.hpp for linear supports", "Dco.hpp for DcoLorentzConeType enum"], "has_pass2": false}, "src/DcoVariable.hpp": {"path": "layer-3/DisCO/src/DcoVariable.hpp", "filename": "DcoVariable.hpp", "file": "DcoVariable.hpp", "brief": "Variable representation for MISOCP\n\nDcoVariable represents a decision variable in the optimization problem,\nstoring bounds, integrality type, and branching information.\n\n**Inheritance:** DcoVariable -> BcpsVariable -> BcpsObject -> AlpsKnowledge\n\n**BcpsObject Fields (inherited):**\n- lbHard_/ubHard_: Original bounds\n- lbSoft_/ubSoft_: Current bounds (may be tighter from branching)\n- intType_: Integer ('I'), binary ('B'), or continuous ('C')\n- objectIndex_: Position in model's variable array\n\n**Key Methods:**\n- infeasibility(): Returns integrality violation (for integer variables)\n- createBranchObject(): Create DcoBranchObject for branching on this\n- clone(): Deep copy for node differencing\n\n**Storage:**\nVariables are stored in BcpsModel::variables_ (inherited by DcoModel).\nInteger variable indices are tracked in DcoModel::integerCols_.", "see": ["DcoBranchObject.hpp for branching on variables", "DcoModel.hpp for variable storage", "BcpsObject (BCPS) for base class fields"], "has_pass2": false}, "src/DcoBranchStrategyRel.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyRel.hpp", "filename": "DcoBranchStrategyRel.hpp", "file": "DcoBranchStrategyRel.hpp", "brief": "DisCO reliability branching", "algorithm": "Reliability Branching", "math": "Hybrid strategy combining strong and pseudocost branching:\n      - If observations(j) < η_rel (reliability threshold): strong branch\n      - If observations(j) ≥ η_rel: use pseudocost estimate\n      Default η_rel = 4-8 observations before trusting pseudocosts.\n\nReliability branching combines strong branching initialization\nwith pseudocost-based selection after reliability threshold.\n\n**Rationale:** Strong branching is accurate but expensive. Pseudocosts\nare cheap but need calibration. Reliability branching uses strong\nbranching to initialize pseudocosts, then switches to cheap estimation.\n\n**Algorithm:**\n1. For unreliable variables (few observations), do strong branching\n2. Update pseudocost estimates from strong branching results\n3. Once reliable (η_rel observations), use pseudocost scoring", "complexity": "O(N·LP) early in tree, O(n) per node after warmup", "ref": ["Achterberg, T. et al. (2005). \"Branching rules revisited\".\n     Operations Research Letters 33(1):42-54."], "see": ["DcoBranchStrategyStrong.hpp for pure strong branching", "DcoBranchStrategyPseudo.hpp for pure pseudocost branching"], "has_pass2": true}, "src/DcoSolution.hpp": {"path": "layer-3/DisCO/src/DcoSolution.hpp", "filename": "DcoSolution.hpp", "file": "DcoSolution.hpp", "brief": "Incumbent solution for MISOCP\n\nDcoSolution stores a feasible solution found during branch-and-bound,\nincluding the variable values and objective quality.\n\n**Inheritance:** DcoSolution -> BcpsSolution -> AlpsSolution -> AlpsKnowledge\n\n**From BcpsSolution (inherited):**\n- size_: Number of variables\n- values_: Solution vector\n- quality_: Objective value\n\n**Selection Methods:**\n- selectNonzeros(): Return solution with only non-zero entries\n- selectFractional(): Return entries with fractional values\n\n**Solution Sources:**\n- DcoSolutionTypeBounding: From node relaxation\n- DcoSolutionTypeHeuristic: From primal heuristics\n- DcoSolutionTypeStrong: From strong branching", "see": ["DcoModel.hpp::feasibleSolution() for feasibility checking", "DcoHeuristic.hpp for primal heuristics", "BcpsSolution (BCPS) for base class"], "has_pass2": false}, "src/DcoLinearConGenerator.hpp": {"path": "layer-3/DisCO/src/DcoLinearConGenerator.hpp", "filename": "DcoLinearConGenerator.hpp", "file": "DcoLinearConGenerator.hpp", "brief": "Wrapper for CGL linear cut generators\n\nDcoLinearConGenerator wraps CglCutGenerator objects to integrate\nstandard MILP cutting planes into DisCO's branch-and-cut.\n\n**Available CGL Generators:**\n- CglClique: Clique cuts\n- CglOddHole: Odd hole cuts\n- CglFlowCover: Flow cover cuts\n- CglKnapsackCover: Knapsack cover cuts\n- CglMixedIntegerRounding: MIR cuts\n- CglGomory: Gomory cuts\n- CglProbing: Probing cuts\n- CglTwomir: Two-MIR cuts\n\n**generateConstraints() Flow:**\n1. Call CglCutGenerator::generateCuts()\n2. Convert OsiCuts to DcoLinearConstraint objects\n3. Add to BcpsConstraintPool\n4. Update statistics", "see": ["DcoConGenerator.hpp for base class", "DcoConicConGenerator.hpp for conic cuts", "CglCutGenerator (CGL) for interface"], "has_pass2": false}, "src/DcoPresolve.hpp": {"path": "layer-3/DisCO/src/DcoPresolve.hpp", "filename": "DcoPresolve.hpp", "file": "DcoPresolve.hpp", "brief": "Presolve/preprocessing for MISOCP problems\n\nDcoPresolve extends OsiPresolve to handle conic problems,\napplying reductions before branch-and-bound.\n\n**Supported Operations:**\n- Bound tightening (improve_bounds)\n- Redundant row/column removal (from OsiPresolve)\n- Problem scaling and reformulation\n\n**Two Modes (compile-time):**\n- OA mode (__OA__): Uses OsiSolverInterface\n- Direct mode: Uses OsiConicSolverInterface\n\n**Usage:**\n1. DcoPresolve(origModel) - Initialize with original model\n2. presolve() - Apply reductions, create presolvedModel_\n3. Solve presolved problem\n4. postsolve() - Restore solution to original space", "see": ["DcoModel.hpp::preprocess() for integration", "OsiPresolve (Osi) for base class", "DcoParams.hpp::presolve for enable/disable"], "has_pass2": false}, "src/DcoMessage.hpp": {"path": "layer-3/DisCO/src/DcoMessage.hpp", "filename": "DcoMessage.hpp", "file": "DcoMessage.hpp", "brief": "Message codes and handler for DisCO logging\n\nDcoMessage extends CoinMessages to provide structured logging\nwith configurable verbosity and debug levels.\n\n**Message Categories (DISCO_Message):**\n- Gap/Cutoff: DISCO_CUTOFF_INC, DISCO_GAP_YES/NO\n- Cut stats: DISCO_CUT_STATS_FINAL, DISCO_CUT_GENERATED\n- Node logging: DISCO_NODE_LOG, DISCO_NODE_BRANCH\n- Input: DISCO_READ_NOINTS, DISCO_READ_NOCONES, DISCO_READ_MPSERROR\n- Solver: DISCO_SOLVER_STATUS, DISCO_SOLVER_FAILED\n- Heuristics: DISCO_HEUR_SOL_FOUND, DISCO_HEUR_STATS_FINAL\n- Branching: DISCO_PSEUDO_REPORT, DISCO_STRONG_REPORT\n\n**Debug Levels (DISCO_Debug_Level):**\n- DISCO_DLOG_BRANCH: Branching decisions\n- DISCO_DLOG_CUT: Cut generation\n- DISCO_DLOG_PROCESS: Node processing\n- DISCO_DLOG_GRUMPY: Visualization output", "see": ["DcoModel.hpp::dcoMessageHandler_ for message handling", "CoinMessages (CoinUtils) for base class"], "has_pass2": false}, "src/DcoCbfIO.hpp": {"path": "layer-3/DisCO/src/DcoCbfIO.hpp", "filename": "DcoCbfIO.hpp", "file": "DcoCbfIO.hpp", "brief": "CBF (Conic Benchmark Format) file reader for DisCO\n\nDcoCbfIO reads and writes conic problems in the standard CBF format,\nwhich is a text-based format for conic optimization problems.\n\n**CONES Enum (CBF domain types):**\n- FREE_RANGE: Free variables (F)\n- POSITIVE_ORT: Nonnegative orthant (L+)\n- NEGATIVE_ORT: Nonpositive orthant (L-)\n- FIXPOINT_ZERO: Fixed to zero (L=)\n- QUAD_CONE: Lorentz/quadratic cone (Q): ||x_{2:n}|| <= x_1\n- RQUAD_CONE: Rotated quadratic cone (QR): 2*x_1*x_2 >= ||x_{3:n}||^2\n\n**File Structure (CBF format):**\n- VER: Version number\n- OBJSENSE: MIN or MAX\n- VAR: Number of variables and domains\n- INT: Integer variables\n- CON: Number of constraints and domains\n- OBJACOORD: Objective coefficients\n- ACOORD: Constraint matrix in coordinate format\n- BCOORD: Constraint bounds/fixed terms\n\n**Key Methods:**\n- readCbf(): Parse CBF file into internal structures\n- writeCbf(): Output problem in CBF format\n- getProblem(): Convert to standard form (colLB, colUB, rowLB, rowUB, matrix, cones)", "see": ["DcoModel.hpp::readInstance() for usage", "http://cblib.zib.de/ for CBF format specification"], "has_pass2": false}, "src/DcoLicense.hpp": {"path": "layer-3/DisCO/src/DcoLicense.hpp", "filename": "DcoLicense.hpp", "file": "DcoLicense.hpp", "brief": "DisCO license information\n\nEPL license text and copyright information.", "has_pass2": false}, "src/DcoBranchStrategyStrong.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyStrong.hpp", "filename": "DcoBranchStrategyStrong.hpp", "file": "DcoBranchStrategyStrong.hpp", "brief": "Strong branching variable selection strategy", "algorithm": "Strong Branching", "math": "For each candidate j, temporarily fix x_j to floor/ceil and solve:\n      Δ_j^- = obj(x_j ≤ ⌊x_j⌋) - obj_current  (down branch degradation)\n      Δ_j^+ = obj(x_j ≥ ⌈x_j⌉) - obj_current  (up branch degradation)\n      Score = Δ_j^- · Δ_j^+  (product rule favors balanced branches)\n\nStrong branching evaluates candidate variables by actually solving\nthe LP/conic relaxation for both branches before deciding.\n\n**Algorithm:**\n1. Select top N candidates (strongCandSize parameter)\n2. For each candidate, temporarily apply up/down bounds\n3. Solve relaxation and record objective change\n4. Score = product rule (down_change * up_change)\n5. Choose variable with best score\n\n**Scoring (updateScore):**\nUses product scoring: score = down_degradation * up_degradation\nThis favors variables where both branches improve the bound.\n\n**Trade-offs:**\n- Pro: Best branching decisions, smaller trees\n- Con: Expensive, many LP/conic solves per node", "complexity": "O(N · LP) per node, where N = strongCandSize candidates", "ref": ["Applegate, D. et al. (1995). \"Finding Cuts in the TSP\".\n     DIMACS Series in Discrete Mathematics 47:91-107."], "see": ["DcoBranchStrategyPseudo.hpp for pseudocost (cheaper)", "DcoBranchStrategyRel.hpp for reliability (hybrid)", "DcoParams.hpp::strongCandSize for candidate count"], "has_pass2": true}, "src/DcoTreeNode.hpp": {"path": "layer-3/DisCO/src/DcoTreeNode.hpp", "filename": "DcoTreeNode.hpp", "file": "DcoTreeNode.hpp", "brief": "Branch-and-bound tree node for MISOCP\n\nDcoTreeNode inherits BcpsTreeNode->AlpsTreeNode and implements\nthe core branch-and-cut loop for conic optimization.\n\n**Inheritance:** DcoTreeNode -> BcpsTreeNode -> AlpsTreeNode\n\n**Node Status (inherited from ALPS):**\n- Candidate: Fresh, unprocessed\n- Evaluated: Processed, may need more cuts\n- Pregnant: Ready to branch\n- Branched: Children created\n- Fathomed/Discarded: Pruned\n\n**Processing Flow:**\n1. process(): Main entry point called by ALPS\n2. installSubProblem(): Load bounds from node descriptor\n3. boundingLoop(): Iterate solve->cuts until done\n4. bound(): Solve conic/LP relaxation\n5. generateConstraints(): Add MILP and conic cuts\n6. branchConstrainOrPrice(): Decide next action\n7. branch(): Create child nodes\n\n**BcpStats Structure:**\nTracks cuts, bounds, and objective improvement per node.", "see": ["DcoNodeDesc.hpp for node description storage", "DcoModel.hpp for model context", "DcoBranchObject.hpp for branching decisions"], "has_pass2": false}, "src/DcoLinearConstraint.hpp": {"path": "layer-3/DisCO/src/DcoLinearConstraint.hpp", "filename": "DcoLinearConstraint.hpp", "file": "DcoLinearConstraint.hpp", "brief": "Linear constraint (row) representation\n\nDcoLinearConstraint stores a linear constraint in sparse form:\nlb <= sum(values[i] * x[indices[i]]) <= ub\n\n**Data Members:**\n- size_: Number of nonzero coefficients\n- indices_: Variable indices with nonzero coefficients\n- values_: Coefficient values\n- Bounds inherited from DcoConstraint/BcpsObject\n\n**Usage:**\n- Core constraints from the original problem\n- MILP cutting planes (Gomory, MIR, etc.)\n- OA supports approximating conic constraints\n\n**Conversion:**\n- createOsiRowCut(): Convert to OsiRowCut for solver\n\n**Feasibility:**\n- infeasibility(): Returns constraint violation", "see": ["DcoConstraint.hpp for base class", "DcoConicConstraint.hpp::supports_ uses these", "OsiRowCut (Osi) for solver representation"], "has_pass2": false}, "src/DcoConicConGenerator.hpp": {"path": "layer-3/DisCO/src/DcoConicConGenerator.hpp", "filename": "DcoConicConGenerator.hpp", "file": "DcoConicConGenerator.hpp", "brief": "Wrapper for conic outer approximation cut generators\n\nDcoConicConGenerator wraps CglConicCutGenerator to generate linear\ncuts that approximate conic constraints (Outer Approximation).\n\n**Conic Cut Types (DcoConicCutType):**\n- IPM: Interior point method based\n- IPMInt: Integer-aware IPM cuts\n- OA: Standard outer approximation\n- MIR: Conic MIR cuts\n- GD1: Gradient descent cuts\n\n**OA Algorithm:**\nFor a Lorentz cone ||x|| <= t, generate linear supports:\n- At current solution point x*, generate tangent hyperplane\n- a'x <= b where (a,b) defines supporting hyperplane\n\n**generateConstraints() Flow:**\n1. Call CglConicCutGenerator::generateCuts()\n2. Convert OsiConicCuts to DcoLinearConstraint objects\n3. Add to BcpsConstraintPool", "see": ["DcoConGenerator.hpp for base class", "DcoLinearConGenerator.hpp for MILP cuts", "DcoConicConstraint.hpp for conic constraint storage"], "has_pass2": false}, "src/DcoConGenerator.hpp": {"path": "layer-3/DisCO/src/DcoConGenerator.hpp", "filename": "DcoConGenerator.hpp", "file": "DcoConGenerator.hpp", "brief": "Abstract base class for constraint/cut generators\n\nDcoConGenerator provides the interface for generating cutting planes\n(constraints) during the branch-and-cut algorithm.\n\n**Generator Hierarchy:**\n- DcoConGenerator (ABC)\n  - DcoLinearConGenerator: Wraps CglCutGenerator (MILP cuts)\n  - DcoConicConGenerator: Wraps CglConicCutGenerator (OA cuts)\n\n**Statistics (DcoConGeneratorStats):**\n- numConsGenerated_: Total cuts generated\n- numConsUsed_: Cuts that were actually added\n- time_: CPU time consumed\n- numCalls_: Times generator was invoked\n- numNoConsCalls_: Calls producing no cuts\n\n**Control Parameters:**\n- strategy_: None, Root, Auto, Periodic\n- frequency_: How often to call (1 = every node)\n\n**Pure Virtual:**\n- generateConstraints(): Populate BcpsConstraintPool", "see": ["DcoLinearConGenerator.hpp for MILP cuts", "DcoConicConGenerator.hpp for conic OA cuts", "DcoModel.hpp::conGenerators_ for storage"], "has_pass2": false}, "src/DcoParams.hpp": {"path": "layer-3/DisCO/src/DcoParams.hpp", "filename": "DcoParams.hpp", "file": "DcoParams.hpp", "brief": "DisCO algorithm parameters and configuration\n\nDcoParams extends AlpsParameterSet to provide MISOCP-specific\nparameters for controlling the branch-and-cut algorithm.\n\n**Parameter Categories:**\n\n**Boolean (chrParams):**\n- cutRampUp, presolve, shareConstraints/Variables\n- sharePseudocostRampUp/Search\n\n**Integer (intParams):**\n- branchStrategy: 0=MaxInf, 1=Pseudo, 2=Reliability, 3=Strong, 4=Bilevel\n- Cut strategies: cutCliqueStrategy, cutGomoryStrategy, etc.\n- Cut frequencies: cutCliqueFreq, cutGomoryFreq, etc.\n- Conic cuts: cutIpmStrategy, cutOaStrategy\n- heurStrategy, heurRoundStrategy\n- lookAhead, pseudoReliability, strongCandSize\n\n**Double (dblParams):**\n- cutFactor, cutoff, objTol, integerTol, coneTol\n- optimalRelGap, optimalAbsGap\n- pseudoWeight, tailOff\n- OA parameters: cutOaBeta, cutOaSlack1/2\n\n**Serialization:**\n- pack()/unpack(): Encode for parallel distribution", "see": ["DcoModel.hpp for parameter usage", "AlpsParameterSet (ALPS) for base class"], "has_pass2": false}, "src/DcoHeurRounding.hpp": {"path": "layer-3/DisCO/src/DcoHeurRounding.hpp", "filename": "DcoHeurRounding.hpp", "file": "DcoHeurRounding.hpp", "brief": "Simple rounding heuristic for finding feasible solutions", "algorithm": "Simple Rounding Heuristic for MICO", "math": "Given LP relaxation x*, round integer variables:\n      x̃_j = round(x*_j) for all j ∈ I (integer set)\n      x̃_j = x*_j for j ∉ I (continuous variables)\n      Accept if x̃ is feasible: Ax̃ ≤ b, x̃ ∈ K (conic constraints)\n\nDcoHeurRounding implements a simple rounding heuristic based on\nAchterberg's dissertation to find integer feasible solutions.\n\n**Algorithm (searchSolution):**\n1. Take fractional LP/conic solution\n2. Round each integer variable to nearest integer\n3. Check feasibility (constraints and cones)\n4. Return solution if feasible, NULL otherwise\n\n**Bound Fixing (bound_fix):**\nUses constraint structure to determine safe rounding directions:\n- down_fix[i]: Variable i can be safely rounded down\n- up_fix[i]: Variable i can be safely rounded up\n\n**searchSolution2():**\nAlternative rounding strategy considering constraint types.", "complexity": "O(n + m) for rounding and feasibility check", "ref": ["Achterberg, T. (2007). \"Constraint Integer Programming\". PhD thesis, TU Berlin."], "see": ["DcoHeuristic.hpp for base class", "DcoModel.hpp::feasibleSolution() for feasibility checking", "DcoSolution.hpp for solution storage"], "has_pass2": true}, "src/DcoBranchStrategyPseudo.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyPseudo.hpp", "filename": "DcoBranchStrategyPseudo.hpp", "file": "DcoBranchStrategyPseudo.hpp", "brief": "Pseudocost branching variable selection", "algorithm": "Pseudocost Branching", "math": "Pseudocost φ_j estimates objective gain per unit fractionality:\n      φ_j^- = avg(Δ_j^- / f_j^-), φ_j^+ = avg(Δ_j^+ / f_j^+)\n      where f_j^- = x_j - ⌊x_j⌋, f_j^+ = ⌈x_j⌉ - x_j\n      Weighted score: s = (1-μ)·min(φ^-, φ^+) + μ·max(φ^-, φ^+), μ=1/6\n      Select j* = argmax_j s(φ_j^-, φ_j^+)\n\nPseudocost branching uses historical data about variable branching\nperformance to estimate which variable will improve bounds most.\n\n**Pseudocost Definitions (from Achterberg):**\n- f_j^+ = ceil(x_j) - x_j (fractional up)\n- f_j^- = x_j - floor(x_j) (fractional down)\n- phi_j^- = avg(Delta_j^- / f_j^-) over all down branches on j\n- phi_j^+ = avg(Delta_j^+ / f_j^+) over all up branches on j\n\n**Scoring Formula:**\nscore = (1-u)*min(phi^-, phi^+) + u*max(phi^-, phi^+)\nwhere u = 1/6 (score_factor_)\n\n**Statistics Tracked:**\n- down_num_/up_num_: Observation counts per variable\n- down_derivative_/up_derivative_: phi values", "complexity": "O(n) per node; O(n) storage for statistics", "ref": ["Achterberg, T. (2007). \"Constraint Integer Programming\".\n     PhD thesis, TU Berlin. Section 7.2."], "see": ["DcoBranchStrategyStrong.hpp for initial pseudocost estimation", "DcoBranchStrategyRel.hpp for reliability branching (hybrid)", "DcoParams.hpp::pseudoWeight for score factor"], "has_pass2": true}, "src/Dco.hpp": {"path": "layer-3/DisCO/src/Dco.hpp", "filename": "Dco.hpp", "file": "Dco.hpp", "brief": "Central header defining DisCO enums, types, and constants\n\nThis header defines all enumeration types used throughout DisCO\n(Discrete Conic Optimization), a Mixed-Integer Second-Order Cone\nProgramming (MISOCP) solver.\n\n**Constraint Types (DcoConstraintType):**\n- Core constraints from the original problem\n- MILP cuts: Clique, FlowCover, Gomory, Knapsack, MIR, OddHole, Probe, TwoMIR\n- Conic cuts: IPM, IPMint, OA (Outer Approximation), CMIR, GD1\n\n**Cone Types (DcoLorentzConeType):**\n- DcoLorentzCone: Standard ||x|| <= t\n- DcoRotatedLorentzCone: 2*x1*x2 >= ||x_rest||^2\n\n**Cut Strategies (DcoCutStrategy):**\n- None, Root only, Auto, Periodic\n\n**Branching Strategies (DcoBranchingStrategy):**\n- MaxInfeasibility, PseudoCost, Reliability, Strong, Bilevel", "see": ["DcoModel.hpp for the main model class", "DcoConstraint.hpp for constraint hierarchy", "DcoConicConstraint.hpp for conic constraint handling"], "has_pass2": false}, "src/DcoBranchStrategyMaxInf.hpp": {"path": "layer-3/DisCO/src/DcoBranchStrategyMaxInf.hpp", "filename": "DcoBranchStrategyMaxInf.hpp", "file": "DcoBranchStrategyMaxInf.hpp", "brief": "Maximum infeasibility branching strategy", "algorithm": "Maximum Infeasibility Branching (Most Fractional)", "math": "Variable selection: j* = argmax_j min(f_j, 1-f_j)\n      where f_j = x_j - ⌊x_j⌋ is fractional part.\n      Equivalent to selecting variable closest to 0.5.\n\nMaximum infeasibility branching selects the variable with the\nlargest integrality violation (closest to 0.5 fractional part).\n\n**Algorithm:**\nFor each fractional integer variable x_j with value v:\n- Compute fractionality = min(v - floor(v), ceil(v) - v)\n- Select variable with maximum fractionality\n\n**Branching Flow:**\n1. createCandBranchObjects(): Check all integer variables\n2. Create DcoBranchObject for each fractional variable\n3. betterBranchObject(): Compare by infeasibility score\n4. bestBranchObject_ set by BcpsBranchStrategy\n\n**Trade-offs:**\n- Pro: Simple, fast, no historical data needed\n- Con: Often poor branching decisions, larger trees", "complexity": "O(n) per node, no storage overhead", "ref": ["Land, A.H. & Doig, A.G. (1960). \"An Automatic Method of\n     Solving Discrete Programming Problems\". Econometrica 28(3):497-520."], "see": ["DcoBranchStrategyPseudo.hpp for pseudocost (smarter)", "DcoBranchStrategyStrong.hpp for strong (best but slow)", "DcoVariable.hpp::infeasibility() for violation calculation"], "has_pass2": true}, "src/DcoSubTree.hpp": {"path": "layer-3/DisCO/src/DcoSubTree.hpp", "filename": "DcoSubTree.hpp", "file": "DcoSubTree.hpp", "brief": "Subtree representation for parallel search\n\nDcoSubTree extends BcpsSubTree to represent a portion of the\nbranch-and-bound tree for parallel distribution.\n\n**Inheritance:** DcoSubTree -> BcpsSubTree -> AlpsSubTree\n\n**Usage in Parallel:**\n- Subtrees are work units distributed to workers\n- Contains subset of tree nodes for processing\n- Supports encode/decode for network transmission\n\nNote: Currently minimal implementation - most logic in base class.", "see": ["DcoTreeNode.hpp for tree node implementation", "BcpsSubTree (BCPS) for base class", "AlpsSubTree (ALPS) for parallel infrastructure"], "has_pass2": false}, "src/DcoConfig.hpp": {"path": "layer-3/DisCO/src/DcoConfig.hpp", "filename": "DcoConfig.hpp", "file": "DcoConfig.hpp", "brief": "Build configuration header for DisCO\n\nStandard COIN-OR configuration pattern that selects appropriate\nconfig files based on build system and context.\n\n**Configuration Selection:**\n- With autotools (HAVE_CONFIG_H defined):\n  - Library build (DISCO_BUILD): uses config.h\n  - Client code: uses config_dco.h\n- Without autotools (e.g., Visual Studio):\n  - Library build: uses config_default.h\n  - Client code: uses config_dco_default.h", "see": ["DcoModel.hpp for main solver interface"], "has_pass2": false}, "src/DcoBranchObject.hpp": {"path": "layer-3/DisCO/src/DcoBranchObject.hpp", "filename": "DcoBranchObject.hpp", "file": "DcoBranchObject.hpp", "brief": "Branching decision for integer variable dichotomy\n\nDcoBranchObject stores the information needed to create two child\nnodes by branching on a fractional integer variable.\n\n**Branching Dichotomy:**\nFor variable x_i with fractional value v:\n- Down branch: x_i <= floor(v) (ubDownBranch_)\n- Up branch: x_i >= ceil(v) (lbUpBranch_)\n\n**Inherited from BcpsBranchObject:**\n- objectIndex_: Variable index being branched\n- score_: Branching score (from strategy)\n- value_: Current fractional value\n- direction_: Which branch to explore next\n\n**Key Methods:**\n- numBranches(): Returns 2 (binary branching)\n- branch(): Apply bound change to solver, return new bound", "see": ["DcoVariable.hpp::createBranchObject()", "DcoTreeNode.hpp::branch() for child creation", "DcoBranchStrategyStrong.hpp for scoring"], "has_pass2": false}}}, "SYMPHONY": {"name": "SYMPHONY", "file_count": 43, "pass2_count": 8, "files": {"src/OsiSym/OsiSymSolverParameters.hpp": {"path": "layer-3/SYMPHONY/src/OsiSym/OsiSymSolverParameters.hpp", "filename": "OsiSymSolverParameters.hpp", "file": "OsiSymSolverParameters.hpp", "brief": "SYMPHONY solver parameter enums\n\nParameter type enumerations for OsiSymSolverInterface.\nMaps Osi generic parameters to SYMPHONY-specific settings.", "has_pass2": false}, "src/OsiSym/OsiSymConfig.hpp": {"path": "layer-3/SYMPHONY/src/OsiSym/OsiSymConfig.hpp", "filename": "OsiSymConfig.hpp", "file": "OsiSymConfig.hpp", "brief": "SYMPHONY build configuration\n\nBuild configuration macros from autotools/cmake for OsiSym.", "has_pass2": false}, "src/OsiSym/OsiSymSolverInterface.hpp": {"path": "layer-3/SYMPHONY/src/OsiSym/OsiSymSolverInterface.hpp", "filename": "OsiSymSolverInterface.hpp", "file": "OsiSymSolverInterface.hpp", "brief": "Osi interface for SYMPHONY MIP solver\n\nOsiSymSolverInterface wraps SYMPHONY (parallel MIP solver) with Osi\nabstraction. Enables SYMPHONY use with Cbc, Cgl, and other COIN-OR.\nSupports parallel branch-and-bound with warm start capability.", "has_pass2": false}, "src/OsiSym/SymWarmStart.hpp": {"path": "layer-3/SYMPHONY/src/OsiSym/SymWarmStart.hpp", "filename": "SymWarmStart.hpp", "file": "SymWarmStart.hpp", "brief": "SYMPHONY warm start information\n\nSymWarmStart: stores basis and tree information for restarting\nSYMPHONY MIP solver from previous state. Enables incremental solving.", "has_pass2": false}, "include/sym_prep.h": {"path": "layer-3/SYMPHONY/include/sym_prep.h", "filename": "sym_prep.h", "file": "sym_prep.h", "brief": "MIP preprocessing (presolve) for SYMPHONY\n\nPreprocessing reduces problem size before B&C by fixing variables,\nremoving redundant constraints, and tightening bounds.", "algorithm": "Implication Analysis (impl_*):\nDerive logical consequences of fixing variables.\n\n  If x_k = 0 implies x_j = 1 (from constraints):\n    Record in implication list\n    Use during probing and fixing", "math": "Row i is redundant if:\n  max Σ_j a_ij x_j ≤ b_i (row never binding)\n  Compute row activity bounds to verify", "complexity": "prep_basic iteration: O(nnz) for bound propagation\n  prep_delete_duplicate: O(m log m + n log n) with sorting\n  sr_solve: O(n log n) per row\n  Total: O(iterations × nnz), typically few iterations", "see": ["sym_prep_params.h for preprocessing parameters", "sym_types.h for ROWinfo, COLinfo structures"], "has_pass2": true}, "include/sym_cp_params.h": {"path": "layer-3/SYMPHONY/include/sym_cp_params.h", "filename": "sym_cp_params.h", "file": "sym_cp_params.h", "brief": "Cut pool process parameters\n\nParameters for managing the global cut pool storage.\n\n**cp_params structure:**\n- verbosity: Output level\n- warm_start: Load cuts from file at start\n- warm_start_file_name: Saved cut pool file\n- logging: Save cut pool during solve\n- log_file_name: Cut pool log file\n\n**Size management:**\n- block_size: Allocation granularity\n- max_size: Maximum memory for cuts\n- max_number_of_cuts: Hard limit on cut count\n\n**Quality control:**\n- cuts_to_check: How many cuts to check for violations\n- delete_which: Deletion strategy (quality/touches)\n- touches_until_deletion: Inactivity threshold\n- min_to_delete: Minimum cuts to remove at once\n- check_which: Which cuts to check (level/touches)", "see": ["sym_cp.h for cut pool process", "sym_constants.h for CHECK_* and DELETE_* values"], "has_pass2": false}, "include/sym_macros.h": {"path": "layer-3/SYMPHONY/include/sym_macros.h", "filename": "sym_macros.h", "file": "sym_macros.h", "brief": "Utility macros for SYMPHONY\n\nCommon macros for memory management, parameter parsing,\nPVM communication, and utility operations.\n\n**Random number generation:**\n- SRANDOM(seed): Seed generator (srand/srandom)\n- RANDOM(): Get random number (rand/random)\n\n**Memory allocation:**\n- REMALLOC(ptr, type, old, new, block): Realloc with free first\n- REALLOC(ptr, type, old, new, block): Standard realloc\n- FREE(p): Safe free with NULL check\n\n**PVM communication (parallel mode):**\n- READ_INT_DESC(): Read integer array descriptor\n- READ_CHAR_ARRAY_WITH_SIZE(): Read sized char array\n- READ_STR_LIST(): Read string list\n\n**Parameter file parsing:**\n- READ_INT_PAR(par): Parse integer parameter\n- READ_DBL_PAR(par): Parse double parameter\n- READ_STR_PAR(par): Parse string parameter\n- READ_STRINT_PAR(): Parse string-to-int mapping\n- READPAR_ERROR(x): Report parse error\n\n**Data copying:**\n- COPY_DBL_ARRAY_DESC(): Copy double array descriptor\n- COPY_ARRAY_DESC(): Copy integer array descriptor\n- COPY_STAT(): Copy status array\n\n**User function handling:**\n- CALL_USER_FUNCTION(f): Check USER_ERROR return\n- CALL_WRAPPER_FUNCTION(f): Check wrapper return\n\n**Standard utilities:**\n- PRINT(verb, thresh, args): Conditional print\n- MIN(a,b), MAX(a,b): Comparisons\n- isset(a,i), setbit(a,i): Bit array operations\n\n**OpenMP atomics:**\n- OPENMP_ATOMIC_WRITE: Thread-safe write\n- OPENMP_ATOMIC_UPDATE: Thread-safe update", "see": ["sym_proto.h for PROTO macro"], "has_pass2": false}, "include/sym_messages.h": {"path": "layer-3/SYMPHONY/include/sym_messages.h", "filename": "sym_messages.h", "file": "sym_messages.h", "brief": "PVM message tags for parallel SYMPHONY\n\nDefines message types for inter-process communication in parallel mode.\nUsed with PVM (Parallel Virtual Machine) for distributed solving.\n\n**Message numbering scheme:**\n- 1xx: General messages (lifecycle, bounds)\n- 2xx: Master process messages (data requests)\n- 3xx: Tree manager ↔ LP messages\n- 4xx: LP process outgoing messages\n- 5xx: Cut/solution pool messages\n- 6xx: Cut packing messages\n- 7xx: Column packing messages\n\n**Lifecycle messages (1xx):**\n- YOU_CAN_DIE (100): Permission to terminate\n- I_AM_DEAD (101): Termination acknowledgment\n- UPPER_BOUND (103): New incumbent found\n- WRITE_LOG_FILE (105): Checkpoint request\n\n**Data request messages (2xx):**\n- REQUEST_FOR_LP_DATA/LP_DATA: LP worker setup\n- REQUEST_FOR_CG_DATA/CG_DATA: Cut generator setup\n- REQUEST_FOR_CP_DATA/CP_DATA: Cut pool setup\n- TM_DATA (210): Tree manager startup\n\n**Tree manager ↔ LP (3xx):**\n- LP__NODE_DESCRIPTION (300): Node state to TM\n- LP__BRANCHING_INFO (301): Branching decision\n- LP__IS_FREE (302): Ready for new node\n- LP__ACTIVE_NODE_DATA (306): Node assignment\n- LP__DIVING_INFO (307): Dive/backtrack instruction\n\n**Solution messages (4xx):**\n- FEASIBLE_SOLUTION_NONZEROS (410): Sparse solution\n- LP_SOLUTION_NONZEROS (420): LP solution for cuts\n\n**Pool messages (5xx-6xx):**\n- POOL_YOU_ARE_USELESS (501): Pool shutdown\n- PACKED_CUT (600): Single cut\n- PACKED_CUTS_TO_CP (601): Cuts to pool\n- NO_MORE_CUTS (605): End of cut stream", "see": ["sym_proccomm.h for communication functions"], "has_pass2": false}, "include/sym_pack_array.h": {"path": "layer-3/SYMPHONY/include/sym_pack_array.h", "filename": "sym_pack_array.h", "file": "sym_pack_array.h", "brief": "Array serialization for PVM communication\n\nPack/unpack functions for SYMPHONY data structures in parallel mode.\n\n**Array packing:**\n- pack_array_desc(): Serialize array_desc (int list)\n- unpack_array_desc(): Deserialize array_desc\n- pack_double_array_desc(): Serialize with double stats\n- unpack_double_array_desc(): Deserialize double array\n\n**Basis packing:**\n- pack_basis(): Serialize basis_desc for warm start\n- unpack_basis(): Deserialize basis_desc", "see": ["sym_proccomm.h for communication functions", "sym_types.h for array_desc, basis_desc"], "has_pass2": false}, "include/sym_master_u.h": {"path": "layer-3/SYMPHONY/include/sym_master_u.h", "filename": "sym_master_u.h", "file": "sym_master_u.h", "brief": "User callbacks for master process\n\nDefines callbacks for customizing problem setup and solution handling.\nReturn USER_DEFAULT to use built-in behavior for standard MIP solving.\n\n**Initialization callbacks:**\n- user_usage(): Print custom command-line help\n- user_initialize(): Allocate user data structure\n- user_free_master(): Clean up user data\n- user_readparams(): Read custom parameters\n- user_io(): Custom problem input\n\n**Problem setup:**\n- user_initialize_root_node(): Define base problem\n  - Set base variables, base cuts, column gen strategy\n  - This is the main hook for problem definition\n- user_start_heurs(): Run initial heuristics for bounds\n- user_init_draw_graph(): Setup visualization\n\n**Solution handling:**\n- user_receive_feasible_solution(): Process new incumbent\n- user_display_solution(): Custom solution output\n- user_send_feas_sol(): Known feasible solution for warm start\n\n**Data distribution (parallel mode):**\n- user_send_lp_data(): Send data to LP workers\n- user_send_cg_data(): Send data to cut generators\n- user_send_cp_data(): Send data to cut pools\n\n**Warm start support:**\n- user_ws_update_cuts(): Update cuts for modified problem\n- user_process_own_messages(): Custom message handling", "see": ["sym_master.h for master process", "sym_lp_u.h for LP process callbacks"], "has_pass2": false}, "include/sym_master.h": {"path": "layer-3/SYMPHONY/include/sym_master.h", "filename": "sym_master.h", "file": "sym_master.h", "brief": "Master process for SYMPHONY's parallel branch-and-cut\n\nThe master process coordinates the overall solve, managing problem\ndata, solution bounds, and communication with worker processes.", "algorithm": "Master Process Coordination:\nCentral coordinator for distributed branch-and-cut execution.\n\nRESPONSIBILITIES:\n1. Problem management: Load, presolve, distribute problem data\n2. Bound tracking: Maintain global upper bound (incumbent)\n3. Termination detection: Monitor gap, time limits, node limits\n4. Solution collection: Gather feasible solutions from workers\n\nINITIALIZATION FLOW:\n1. sym_open_environment(): Allocate sym_environment\n2. sym_read_mps/lp/gmpl(): Parse problem file\n3. Presolve (if enabled): Reduce problem size\n4. initialize_root_node_u(): Set up root B&B node\n5. start_heurs_u(): Run primal heuristics for initial UB\n\nSOLVE ORCHESTRATION:\nMaster spawns tree manager(s) that:\n- Maintain B&B tree with candidate lists\n- Dispatch nodes to LP workers\n- Collect bounds and solutions\n- Coordinate cut pool access\n\nWARM START:\nFor re-solving with modified problem:\n- Save B&B tree structure with node descriptions\n- Store cut pool and incumbent\n- On restart: Prune invalid nodes, update bounds", "math": "Sensitivity analysis:\nget_lb_for_new_rhs(): Given RHS changes, compute valid lower bound\nby re-evaluating nodes with modified constraints without re-solving LP.\nUses dual information from solved nodes.", "complexity": "- Initialization: O(nnz) for problem setup\n- Bound tracking: O(1) per update\n- Warm start: O(tree_size) for tree traversal\n\n**sym_environment structure:**\n- mip: Current MIP description (possibly presolved)\n- orig_mip: Original problem before presolve\n- prep_mip: Presolved problem description\n- par: Solver parameters\n- ub/lb: Best upper/lower bounds\n- best_sol: Best feasible solution found\n- warm_start: Warm start data for re-solving\n- tm: Tree manager (when compiled in)\n- cp: Cut pools (when compiled in)\n\n**User callback functions (_u suffix):**\n- initialize_u(): User initialization\n- io_u(): Custom problem I/O\n- start_heurs_u(): Run initial heuristics\n- display_solution_u(): Custom solution display\n- initialize_root_node_u(): Set up root node\n\n**Warm start operations:**\n- create_copy_warm_start(): Clone warm start data\n- get_lb_for_new_rhs(): Sensitivity analysis\n- get_ub_for_new_rhs(): Upper bound for modified RHS", "see": ["symphony.h for the public API", "sym_tm.h for tree manager details"], "has_pass2": true}, "include/sym_dg.h": {"path": "layer-3/SYMPHONY/include/sym_dg.h", "filename": "sym_dg.h", "file": "sym_dg.h", "brief": "Draw Graph (DG) process for SYMPHONY visualization\n\nInteractive graphical display of solutions and search progress.\nCommunicates with Tcl/Tk-based GUI via pipes.\n\n**dg_prob structure:**\n- windows: Array of display windows\n- par: Visualization parameters\n- master: Master process tid\n\n**Window management:**\n- win_desc: Window display settings (size, fonts, scale)\n- dg_graph: Graph data (nodes, edges)\n- buf_fifo: Message buffer queue\n\n**dg_node:**\n- node_id, posx, posy: Identifier and position\n- radius, label, weight: Display properties\n- dash: Line pattern for node border\n\n**dg_edge:**\n- edge_id, tail, head: Identifier and endpoints\n- weight, dash: Display properties\n\n**Main functions:**\n- init_dgwin(): Create new window\n- display_graph_on_canvas(): Render graph\n- copy_window_structure(): Clone window\n- find_node(), find_edge(): Lookup elements\n- compress_graph(): Remove deleted elements\n\n**Communication:**\n- spprint(): Printf to pipe\n- start_child(): Launch GUI process\n- wait_for_you_can_die(): Graceful shutdown", "see": ["sym_dg_params.h for visualization parameters", "sym_dg_u.h for user callbacks"], "has_pass2": false}, "include/symphony_api.h": {"path": "layer-3/SYMPHONY/include/symphony_api.h", "filename": "symphony_api.h", "file": "symphony_api.h", "brief": "Internal API header (legacy, includes sym_master.h)\n\nThis header provides the internal API declarations using the\nPROTO() macro for K&R C compatibility. Modern code should use\nsymphony.h which provides the same functions with standard\nANSI C prototypes.\n\n**Included headers:**\n- sym_proto.h: PROTO() macro definition\n- sym_master.h: Master process data structures\n- sym_messages.h: Message type definitions\n\n@note Prefer using symphony.h for new code", "see": ["symphony.h for the modern public API"], "has_pass2": false}, "include/sym_proccomm.h": {"path": "layer-3/SYMPHONY/include/sym_proccomm.h", "filename": "sym_proccomm.h", "file": "sym_proccomm.h", "brief": "Process communication abstraction for parallel SYMPHONY\n\nWrapper functions for PVM (Parallel Virtual Machine) communication.\nProvides portable inter-process communication in distributed mode.\n\n**PVM integration:**\n- __PVM__ flag enables PVM3 library\n- DataInPlace = PvmDataRaw for efficient packing\n- PROCESS_OK = PvmOk for status checks\n\n**Process management:**\n- register_process(): Join PVM\n- spawn(): Start remote processes\n- pstat(): Check process status\n- kill_proc(): Terminate process\n- comm_exit(): Leave PVM\n\n**Message sending:**\n- init_send(): Initialize send buffer\n- send_char/int/dbl/float_array(): Pack arrays\n- send_str(): Pack string\n- send_msg(): Send to one recipient\n- msend_msg(): Multicast to multiple recipients\n\n**Message receiving:**\n- receive_msg(): Blocking receive\n- treceive_msg(): Timed receive\n- nreceive_msg(): Non-blocking receive\n- receive_char/int/dbl/float_array(): Unpack arrays\n- receive_str(): Unpack string\n\n**Buffer management:**\n- bufinfo(): Get buffer metadata\n- freebuf(): Release buffer\n- setsbuf(), setrbuf(): Set active buffers", "see": ["sym_messages.h for message tags", "sym_tm.h for tree manager using communication"], "has_pass2": false}, "include/sym_dg_params.h": {"path": "layer-3/SYMPHONY/include/sym_dg_params.h", "filename": "sym_dg_params.h", "file": "sym_dg_params.h", "brief": "Draw Graph parameters and message constants\n\nParameters for visualization and GUI communication constants.\n\n**dg_params structure:**\n- canvas_width/height: Drawing area size\n- viewable_width/height: Visible window size\n- disp_nodelabels/nodeweights/edgeweights: Show labels\n- node_radius: Default node size\n- scale_factor: Zoom level\n- *_font: Font specifications\n- interactive_mode: Enable user interaction\n- mouse_tracking: Track mouse position\n\n**Message protocols (CTOI_* = Client to Intermediary):**\n- CTOI_INITIALIZE_WINDOW: Create new window\n- CTOI_SET_GRAPH: Load graph data\n- CTOI_DRAW_GRAPH: Render current graph\n- CTOI_MODIFY_GRAPH: Update graph elements\n- CTOI_WAIT_FOR_CLICK_*: User interaction\n\n**Messages (ITOC_* = Intermediary to Client):**\n- ITOC_CLICK_HAPPENED: User clicked\n- ITOC_WINDOW_*: Window status responses\n\n**Modification types (MODIFY_*):**\n- ADD_NODES, DELETE_NODES: Node operations\n- ADD_EDGES, DELETE_EDGES: Edge operations\n- CHANGE_WEIGHTS_*: Update labels\n- CHANGE_DASH_*: Update line styles", "see": ["sym_dg.h for draw graph process"], "has_pass2": false}, "include/sym_cg_params.h": {"path": "layer-3/SYMPHONY/include/sym_cg_params.h", "filename": "sym_cg_params.h", "file": "sym_cg_params.h", "brief": "Cut generator process parameters\n\nMinimal parameter structure for the cut generator process.\nMost cut generation parameters are in sym_lp_params.h (cgl_params).\n\n**cg_params:**\n- verbosity: Output level for cut generation\n- do_findcuts: Enable/disable user cut generation", "see": ["sym_cg.h for cut generator process", "sym_lp_params.h for CGL cut parameters"], "has_pass2": false}, "include/sym_types.h": {"path": "layer-3/SYMPHONY/include/sym_types.h", "filename": "sym_types.h", "file": "sym_types.h", "brief": "Core data structures for SYMPHONY's branch-and-cut\n\nDefines the fundamental data structures used throughout SYMPHONY\nfor representing problems, solutions, tree nodes, and cuts.\n\n**Problem representation:**\n- MIPdesc: Complete MIP in CSC format (matbeg, matind, matval)\n- MIPinfo: Problem statistics (var types, row types, density)\n\n**Tree node structures:**\n- bc_node: B&C tree node with bounds, solution, children\n- node_desc: Node description (basis, cuts, variables)\n- branch_obj: Branching decision (variable, children, bounds)\n- branch_desc: Single branch description (sense, rhs)\n\n**Cut structures:**\n- cut_data: Cut coefficients and metadata\n- row_data: Cut with effectiveness tracking\n- waiting_row: Pending cut with violation info\n\n**Solution structures:**\n- lp_sol: LP solution (sparse xind/xval format)\n- sp_solution: Solution pool entry\n- warm_start_desc: Complete warm start state\n\n**Statistics:**\n- problem_stat: Tree size, depth, diving stats\n- lp_stat_desc: LP calls, cuts generated by type\n- node_times: Timing breakdown per node\n\n**Presolve support:**\n- COLinfo: Column statistics and implications\n- ROWinfo: Row bounds, types, redundancy\n- IMPvar/IMPlist: Variable implication lists", "see": ["sym_master.h for sym_environment", "sym_tm.h for tm_prob"], "has_pass2": false}, "include/sym_lp_params.h": {"path": "layer-3/SYMPHONY/include/sym_lp_params.h", "filename": "sym_lp_params.h", "file": "sym_lp_params.h", "brief": "LP solver process parameters for SYMPHONY\n\nParameters controlling LP relaxation solving, cut generation,\nbranching, and primal heuristics in each B&C node.\n\n**cgl_params (CGL cut generation):**\n- generate_cgl_*_cuts: Enable specific cut families\n- generate_cgl_*_cuts_freq: How often to generate (1=every node)\n- *_max_depth: Tree depth limit for each cut type\n- use_chain_strategy: Smart cut generation chain\n\n**lp_params structure sections:**\n\n**Matrix growth control:**\n- max_non_dual_feas_to_add_*: Limits on violated cuts added\n- mat_row/col_compress_*: When to compress matrix\n\n**Tailing off detection:**\n- tailoff_gap_backsteps/frac: Detect stalling LP progress\n- tailoff_obj_backsteps/frac: Objective improvement threshold\n\n**Branching control:**\n- strong_branching_cand_num_*: Candidates for strong branching\n- rel_br_threshold: Reliability branching parameters\n- use_sos_branching: Special ordered sets support\n\n**Primal heuristics:**\n- fp_*: Feasibility pump parameters\n- fr_*: Feasibility-based restricted search\n- rs_*: RINS (Relaxation Induced Neighborhood Search)\n- lb_*: Local branching\n- ds_*: Diving heuristics (fractional, guided, etc.)", "see": ["sym_lp.h for LP process using these parameters", "sym_cg.h for cut generation"], "has_pass2": false}, "include/sym_pack_cut.h": {"path": "layer-3/SYMPHONY/include/sym_pack_cut.h", "filename": "sym_pack_cut.h", "file": "sym_pack_cut.h", "brief": "Cut serialization for PVM communication\n\nPack/unpack functions for cut_data in parallel mode.\n\n**Cut packing:**\n- pack_cut(): Serialize cut_data for transmission\n- unpack_cut(): Deserialize received cut_data", "see": ["sym_proccomm.h for communication functions", "sym_types.h for cut_data structure", "sym_cp.h for cut pool using these functions"], "has_pass2": false}, "include/sym_proto.h": {"path": "layer-3/SYMPHONY/include/sym_proto.h", "filename": "sym_proto.h", "file": "sym_proto.h", "brief": "Function prototype and path length macros\n\nDefines the PROTO macro for ANSI C function prototypes\nand standard path/line length constants.\n\n**Length constants:**\n- MAX_FILE_NAME_LENGTH (255): File path buffer size\n- MACH_NAME_LENGTH (255): Machine name buffer size\n- MAX_LINE_LENGTH (255): Line buffer size\n\n**PROTO macro:**\n- PROTO(x) expands to x for ANSI C prototypes\n- Allows: `int foo PROTO((int a, int b));`\n- Legacy support for K&R C compilers (commented out)", "see": ["sym_types.h for data types"], "has_pass2": false}, "include/sym_constants.h": {"path": "layer-3/SYMPHONY/include/sym_constants.h", "filename": "sym_constants.h", "file": "sym_constants.h", "brief": "Core constants and status codes for SYMPHONY\n\nDefines all numeric constants used throughout SYMPHONY including\nerror codes, status values, algorithm modes, and return codes.\n\n**Error codes (process_chain):**\n- ERROR__NO_BRANCHING_CANDIDATE, ERROR__ILLEGAL_RETURN_CODE\n- ERROR__NUMERICAL_INSTABILITY, ERROR__COMM_ERROR\n\n**Problem types:**\n- ZERO_ONE_PROBLEM, INTEGER_PROBLEM, MIXED_INTEGER_PROBLEM\n\n**Node status (NODE_STATUS__*):**\n- CANDIDATE, BRANCHED_ON, HELD, ROOT, PRUNED\n- TIME_LIMIT, ITERATION_LIMIT, WARM_STARTED\n\n**LP solver status:**\n- LP_OPTIMAL, LP_D_INFEASIBLE, LP_D_UNBOUNDED\n- LP_D_ITLIM, LP_D_OBJLIM, LP_TIME_LIMIT\n\n**Basis status:**\n- VAR_AT_LB, VAR_BASIC, VAR_AT_UB, VAR_FREE, VAR_FIXED\n\n**Cut source:**\n- INTERNAL_CUT_POOL, EXTERNAL_CUT_POOL\n- INTERNAL_CUT_GEN, EXTERNAL_CUT_GEN\n\n**CGL generators (CGL_*_GENERATOR):**\n- PROBING, KNAPSACK, CLIQUE, GOMORY, TWOMIR, FLOWCOVER, MIR\n\n**Branching actions:**\n- DO_BRANCH, DO_NOT_BRANCH, DO_NOT_BRANCH__FATHOMED\n- PRUNE_THIS_CHILD, RETURN_THIS_CHILD, KEEP_THIS_CHILD\n\n**Variable status flags:**\n- NOT_FIXED, TEMP_FIXED_TO_LB, PERM_FIXED_TO_LB\n- BASE_VARIABLE, VARIABLE_BRANCHED_ON, NOT_REMOVABLE\n\n**Column generation strategies:**\n- FATHOM__DO_NOT_GENERATE_COLS__DISCARD\n- FATHOM__GENERATE_COLS__RESOLVE\n- BEFORE_BRANCH__GENERATE_COLS__RESOLVE\n\n**VBC visualization (VBC_*):**\n- INTERIOR_NODE, PRUNED, ACTIVE_NODE, CAND_NODE, FEAS_SOL_FOUND\n\n**Diving heuristics:**\n- VLENGTH_DIVING, GUIDED_DIVING, CROSSOVER_DIVING\n- EUC_DIVING, RANK_DIVING, FRAC_DIVING\n\n**Presolve return codes:**\n- PREP_UNMODIFIED, PREP_MODIFIED, PREP_INFEAS, PREP_SOLVED", "see": ["sym_types.h for data structures using these constants", "sym_lp.h for LP process using these codes"], "has_pass2": false}, "include/sym_lp.h": {"path": "layer-3/SYMPHONY/include/sym_lp.h", "filename": "sym_lp.h", "file": "sym_lp.h", "brief": "LP solver process for SYMPHONY's branch-and-cut\n\nThe LP process solves LP relaxations at each B&C node, manages\ncuts, and performs branching decisions.", "algorithm": "LP Process in Branch-and-Cut:\nCore solver process that handles LP relaxations and cut management\nat each node of the branch-and-bound tree.\n\nCUT LOOP (process_chain):\nThe heart of branch-and-cut is the cutting plane loop:\n  while (not_converged):\n    1. Solve LP relaxation (warm start from parent)\n    2. If LP infeasible → prune node (infeasibility)\n    3. If LP ≥ incumbent → prune node (bound)\n    4. If solution integer → update incumbent, prune\n    5. Generate cuts from CGL and user callbacks\n    6. Filter cuts by violation and orthogonality\n    7. Add best cuts to LP, re-solve\n    8. Check tail-off: if bound improvement < threshold, branch\n\nCUT MANAGEMENT:\n- waiting_rows: Cuts generated but not yet added\n- slack_cuts: Cuts removed due to zero slack (may re-add later)\n- Cut filtering: Avoid adding near-parallel cuts\n- Cut aging: Remove cuts inactive for several iterations\n\nSTRONG BRANCHING:\nEvaluate candidate variables by temporarily fixing and re-solving:\n  For each candidate variable xⱼ with fractional value f:\n    1. Create two subproblems: xⱼ ≤ ⌊f⌋ and xⱼ ≥ ⌈f⌉\n    2. Solve LP relaxations (limited iterations)\n    3. Record objective degradation: Δ⁻ = obj(down) - obj, Δ⁺ = obj(up) - obj\n    4. Update pseudo-costs from observed degradation\n  Select variable maximizing: score = (1-μ)·min(Δ⁻,Δ⁺) + μ·max(Δ⁻,Δ⁺)\n  Typically μ = 1/6 (emphasizes weaker branch)\n\nPSEUDO-COST BRANCHING:\nAfter enough observations, use historical data instead of solving:\n  Estimated Δ⁻ = pcost_down[j] · (f - ⌊f⌋)\n  Estimated Δ⁺ = pcost_up[j] · (⌈f⌉ - f)\nReliability: require k observations before trusting pseudo-cost", "math": "Cut effectiveness:\nFor cut a'x ≤ b with current solution x*:\n  Violation: v = a'x* - b (positive if violated)\n  Efficacy: v / ‖a‖₂ (normalized violation)\n  Orthogonality: cuts should be linearly independent\n\nStrong branching score:\n  score(j) = (1-μ)·min(Δ⁻ⱼ, Δ⁺ⱼ) + μ·max(Δ⁻ⱼ, Δ⁺ⱼ)", "complexity": "- Cut loop iteration: O(nnz · simplex_iters)\n- Strong branching: O(k · simplex_iters) for k candidates\n- Cut generation: O(m·n) for some generators, varies by type", "ref": ["Achterberg (2007). \"Constraint Integer Programming\". PhD thesis.\n  Chapters 5-6 on branching and cutting.", "Balas & Ceria & Cornuéjols (1996). \"Mixed 0-1 Programming by\n  Lift-and-Project in a Branch-and-Cut Framework\".\n\n**lp_prob structure:**\n- lp_data: Underlying LP solver interface (LPdata)\n- desc: Current node description\n- waiting_rows: Cuts ready to be added\n- slack_cuts: Cuts that became slack\n- pcost_down/up: Pseudo-costs for branching\n- bdesc: Branch descriptions along path from root\n\n**Main processing loop (process_chain):**\n1. Solve LP relaxation\n2. Check feasibility, update bounds\n3. Generate cuts (CGL cuts + user cuts)\n4. Add best cuts, resolve LP\n5. Check tail-off, decide to branch\n6. Select branching variable (strong branching)\n7. Create children, send to tree manager\n\n**Cut generation (generate_cgl_cuts_new):**\nIntegrates CGL cut generators: Gomory, knapsack, clique,\nprobing, MIR, flow cover, lift-and-project, etc.\n\n**Branching functions:**\n- select_branching_object(): Variable selection\n- strong_branch(): Evaluate branching by LP re-solve\n- branch(): Create child nodes\n\n**User callbacks (_u functions):**\n- is_feasible_u(): Check integer feasibility\n- select_candidates_u(): Custom branching candidates\n- generate_cuts_in_lp_u(): User-defined cuts"], "see": ["sym_tm.h for tree manager", "sym_cg.h for cut generator process", "sym_lp_solver.h for LP solver interface"], "has_pass2": true}, "include/sym_cp_u.h": {"path": "layer-3/SYMPHONY/include/sym_cp_u.h", "filename": "sym_cp_u.h", "file": "sym_cp_u.h", "brief": "User callbacks for cut pool process\n\nDefines callbacks for custom cut checking in the cut pool.\nCut pool stores cuts for reuse across B&C nodes.\n\n**User callbacks:**\n- user_receive_cp_data(): Receive problem-specific data\n- user_free_cp(): Clean up user data\n- user_receive_lp_solution_cp(): Custom solution handling\n\n**Cut checking protocol (three-phase):**\n1. user_prepare_to_check_cuts(): Initialize with LP solution\n2. user_check_cut(): Check single cut for violation\n   - Sets is_violated flag\n   - Optionally computes quality score\n3. user_finished_checking_cuts(): Cleanup after checking\n\n**Default behavior:**\nReturns USER_DEFAULT for built-in EXPLICIT_ROW handling.\nUser cuts need custom checking implementation.", "see": ["sym_cp.h for cut pool process", "sym_cg_u.h for cut generation callbacks"], "has_pass2": false}, "include/sym_master_params.h": {"path": "layer-3/SYMPHONY/include/sym_master_params.h", "filename": "sym_master_params.h", "file": "sym_master_params.h", "brief": "Master process parameters aggregating all component params\n\nTop-level parameter structure containing parameters for all\nSYMPHONY processes: master, TM, LP, CG, CP, DG, and preprocessing.\n\n**params structure contains:**\n- cp_par: Cut pool parameters\n- cg_par: Cut generator parameters\n- lp_par: LP solver parameters\n- tm_par: Tree manager parameters\n- dg_par: Draw graph parameters (visualization)\n- prep_par: Preprocessing parameters\n\n**Execution control:**\n- warm_start: Resume from saved state\n- verbosity: Global output level\n- random_seed: For reproducibility\n- do_branch_and_cut: Enable B&C algorithm\n- do_draw_graph: Enable visualization\n- use_permanent_cut_pools: Persistent cut storage\n\n**Input/output:**\n- infile: Problem file (MPS/LP/GMPL)\n- file_type: MPS_FORMAT, LP_FORMAT, GMPL_FORMAT\n- datafile: GMPL data file if needed\n- test_dir: Directory for test problems\n- obj_offset: Constant added to objective\n\n**Multi-criteria optimization:**\n- multi_criteria: Enable bi-objective mode\n- mc_search_order: Search strategy\n- mc_compare_solution_tolerance: Pareto comparison\n- mc_warm_start: Reuse solutions across objectives\n\n**Parallel configuration:**\n- tm_exe, dg_exe: Process executables\n- tm_machine, dg_machine: Machine assignments\n- pvm_trace: PVM debugging", "see": ["sym_master.h for master process", "symphony.h for API using these parameters"], "has_pass2": false}, "include/sym_timemeas.h": {"path": "layer-3/SYMPHONY/include/sym_timemeas.h", "filename": "sym_timemeas.h", "file": "sym_timemeas.h", "brief": "Time measurement utilities for SYMPHONY\n\nPortable timing functions and timeval manipulation macros.\nUses Windows time on MSVC, sys/time.h otherwise.\n\n**Timing functions:**\n- start_time(): Start timer\n- used_time(): CPU time since last call\n- wall_clock(): Wall-clock elapsed time\n\n**timeval manipulation macros:**\n- TVCLEAR(tv): Zero a timeval\n- TVISSET(tv): Check if timeval is non-zero\n- TVXLTY(x,y): Compare x < y\n- TVXADDY(z,x,y): z = x + y\n- TVXSUBY(z,x,y): z = x - y\n- TVTODBL(tv): Convert to double seconds\n- DBLTOTV(d,tv): Convert double to timeval\n\n**VBC output macros:**\n- PRINT_TIME(tm, f): Print HH:MM:SS:MS format\n- PRINT_TIME2(tm, f): Print decimal seconds", "see": ["sym_win32_time.h for Windows implementation"], "has_pass2": false}, "include/sym_cg.h": {"path": "layer-3/SYMPHONY/include/sym_cg.h", "filename": "sym_cg.h", "file": "sym_cg.h", "brief": "Cut Generator process for SYMPHONY\n\nThe Cut Generator (CG) receives LP solutions and generates\nviolated cutting planes. Can run as separate process or\ncompiled into LP process (SYM_COMPILE_IN_CG).", "algorithm": "Explicit vs User Cuts:\nTwo paradigms for cut representation.\n\n  EXPLICIT CUTS (create_explicit_cut):\n    Full coefficient vector stored\n    Direct addition to LP matrix\n    Memory: O(nonzeros in cut)\n\n  USER CUTS (cg_add_user_cut):\n    Compact representation (e.g., set for subtour)\n    Requires expansion callback\n    Memory-efficient for structured cuts", "complexity": "Separation: User-defined, typically O(n²) to O(n³)\n  Cut addition: O(nnz(cut)) per cut\n  Communication: O(cuts × avg_cut_size)", "see": ["sym_lp.h for LP process which calls CG", "sym_cp.h for cut pool storage", "sym_cg_u.h for user callback declarations"], "has_pass2": true}, "include/sym_tm_params.h": {"path": "layer-3/SYMPHONY/include/sym_tm_params.h", "filename": "sym_tm_params.h", "file": "sym_tm_params.h", "brief": "Tree manager parameters for SYMPHONY\n\nParameters controlling the branch-and-cut search tree exploration.\n\n**Process configuration:**\n- lp_exe, cg_exe, cp_exe: Worker executables\n- lp/cg/cp_mach_num: Number of machines per type\n- lp/cg/cp_machs: Machine name arrays\n- max_active_nodes: Parallelism limit\n- max_cp_num: Maximum cut pools\n\n**Node selection (node_selection_rule):**\n- LOWEST_LP_FIRST: Best-bound search\n- DEPTH_FIRST_SEARCH: Deep diving\n- BREADTH_FIRST_SEARCH: Level-by-level\n- BEST_FIRST_SEARCH: Estimate-based\n\n**Diving strategy:**\n- unconditional_dive_frac: Always dive this fraction\n- diving_strategy: How to choose dive vs backtrack\n- diving_k, diving_threshold: Diving parameters\n\n**Termination criteria:**\n- time_limit: Maximum solve time\n- gap_limit: Optimality tolerance\n- node_limit: Maximum nodes to explore\n- find_first_feasible: Stop at first solution\n\n**Logging and warm start:**\n- logging, logging_interval: Progress logging\n- warm_start: Resume from saved state\n- warm_start_node_limit: Nodes to load\n- tree_log_file_name, cut_log_file_name: State files\n- vbc_emulation: Visualization output\n\n**Column generation:**\n- colgen_strat[2]: Strategy for two phases\n- price_in_root: Column generation at root\n\n**Solution pool:**\n- max_sp_size: Maximum solutions to keep", "see": ["sym_tm.h for tree manager process", "sym_constants.h for selection rule values"], "has_pass2": false}, "include/sym_primal_heuristics.h": {"path": "layer-3/SYMPHONY/include/sym_primal_heuristics.h", "filename": "sym_primal_heuristics.h", "file": "sym_primal_heuristics.h", "brief": "Primal heuristics for finding feasible solutions\n\nCollection of heuristics to find feasible MIP solutions quickly.\nCalled during B&C to improve incumbent and provide bounds.", "algorithm": "RINS (Danna, Rothberg, Le Pape 2005):\nRelaxation Induced Neighborhood Search.\n\nrestricted_search() with fr_force_feasible():\n  Fix variables where LP = incumbent\n  Free variables where LP ≠ incumbent\n  Solve resulting smaller MIP", "math": "Local branching cut:\n  Σ_{i: x*_i=0} x_i + Σ_{i: x*_i=1} (1-x_i) ≤ k\n  Hamming distance from x* at most k\n\nAdd cut to MIP and solve restricted sub-problem.", "complexity": "Feasibility pump: O(iterations × LP_solve)\n  Diving: O(n × LP_solve) worst case\n  Local branching: O(sub-MIP solve)", "ref": ["Fischetti, M., Glover, F., & Lodi, A. (2005).\n  \"The feasibility pump\". Math Programming 104:91-104."], "see": ["sym_lp_params.h for heuristic parameters (fp_*, fr_*, rs_*, lb_*, ds_*)", "sym_lp.h for LP process calling heuristics"], "has_pass2": true}, "include/sym_prep_params.h": {"path": "layer-3/SYMPHONY/include/sym_prep_params.h", "filename": "sym_prep_params.h", "file": "sym_prep_params.h", "brief": "Preprocessing parameters for SYMPHONY\n\nParameters controlling MIP presolve operations.\n\n**prep_params structure:**\n- level: Preprocessing aggressiveness (0=off)\n- dive_level: Bound propagation depth\n- impl_dive_level: Implication chain depth\n- impl_limit: Max implications to explore\n- do_probe: Enable probing\n- verbosity: Output detail level\n- reduce_mip: Enable full problem reduction\n\n**Probing control:**\n- probe_verbosity: Probing output level\n- probe_level: Probing aggressiveness\n\n**Single-row relaxation:**\n- do_single_row_rlx: Enable SR bounds\n- single_row_rlx_ratio: Sparsity threshold\n- max_sr_cnt: Max rows to analyze\n\n**Aggregated row relaxation:**\n- do_aggregate_row_rlx: Combine rows for bounds\n- max_aggr_row_ratio: Density limit\n- max_aggr_row_cnt: Max aggregations\n\n**Resource limits:**\n- iteration_limit: Max preprocessing passes\n- time_limit: Preprocessing time budget\n- etol: Numerical tolerance\n\n**Output:**\n- display_stats: Show preprocessing summary\n- write_mps/write_lp: Save preprocessed problem", "see": ["sym_prep.h for preprocessing functions"], "has_pass2": false}, "include/symphony.h": {"path": "layer-3/SYMPHONY/include/symphony.h", "filename": "symphony.h", "file": "symphony.h", "brief": "Main public API for SYMPHONY MILP solver\n\nSYMPHONY is a parallel branch-cut-price framework for solving\nMixed Integer Linear Programs (MILPs). Supports both shared-memory\nand distributed-memory (MPI) parallelism.", "algorithm": "Branch-Cut-Price Framework:\nSYMPHONY implements a parallel branch-cut-price algorithm combining\nthree techniques for solving MILPs.\n\nBRANCH-AND-BOUND:\n- Divide problem by branching on integer variables\n- Maintain tree of subproblems with LP relaxation bounds\n- Prune nodes where LB ≥ best known UB (incumbent)\n\nCUTTING PLANES:\n- Strengthen LP relaxation at nodes via valid inequalities\n- Supports Gomory cuts, CGL cuts, and user-defined cuts\n- Cut pools allow reuse of cuts across subtrees\n\nCOLUMN GENERATION (PRICE):\n- Dynamically add variables with negative reduced cost\n- Enables huge implicit formulations (e.g., set covering)\n- User callback interface for pricing subproblems\n\nPARALLELISM:\n- Master-worker architecture for distributed B&B\n- Multiple tree managers can run concurrently\n- Load balancing via dynamic work stealing", "math": "Convergence guarantee:\nAt termination: lb ≤ z* ≤ ub, where:\n- lb = best lower bound (max LP bound over open nodes)\n- ub = objective of best feasible solution (incumbent)\n- Gap = (ub - lb) / |ub| → 0 proves optimality\n\nNode selection affects performance:\n- BEST_FIRST: Minimizes total nodes but uses more memory\n- DEPTH_FIRST: Low memory, finds feasible solutions fast\n- LOWEST_LP_FIRST: Good lower bound improvement", "complexity": "- Worst case: O(2^n) nodes for n binary variables\n- Practice: Strong cuts and branching reduce tree dramatically\n- LP solve: O(m²n) per node with basis warm-start", "ref": ["Ralphs & Ladányi (2001). \"SYMPHONY: A Framework for Branch and Cut\".\n\n**Core API workflow:**\n```c\nsym_environment *env = sym_open_environment();\nsym_read_mps(env, \"problem.mps\");  // or sym_explicit_load_problem()\nsym_solve(env);\nsym_get_col_solution(env, solution);\nsym_close_environment(env);\n```\n\n**Problem input methods:**\n- sym_read_mps(): Read MPS format\n- sym_read_lp(): Read LP format\n- sym_read_gmpl(): Read GMPL/AMPL format\n- sym_explicit_load_problem(): Load from arrays\n\n**Solve methods:**\n- sym_solve(): Standard solve\n- sym_warm_solve(): Warm start from previous solution\n- sym_mc_solve(): Multi-criteria (bicriteria) solve\n\n**Return codes:**\n- TM_OPTIMAL_SOLUTION_FOUND (227): Optimal found\n- TM_TIME_LIMIT_EXCEEDED (228): Time limit\n- TM_NODE_LIMIT_EXCEEDED (229): Node limit\n- TM_FEASIBLE_SOLUTION_FOUND (235): Feasible but not proven optimal\n\n**Key parameters (via sym_set_int_param):**\n- \"node_selection_rule\": LOWEST_LP_FIRST, DEPTH_FIRST_SEARCH, etc.\n- \"time_limit\": Maximum solve time in seconds\n- \"gap_limit\": Relative MIP gap tolerance"], "see": ["sym_master.h for internal master process structure", "sym_tm.h for tree manager internals"], "has_pass2": true}, "include/sym_cg_u.h": {"path": "layer-3/SYMPHONY/include/sym_cg_u.h", "filename": "sym_cg_u.h", "file": "sym_cg_u.h", "brief": "User callbacks for cut generator process\n\nDefines callbacks for custom cut generation in SYMPHONY.\nUsers implement problem-specific separation routines here.\n\n**Cut addition helpers (non-user):**\n- cg_add_explicit_cut(): Add cut with explicit coefficients\n- cg_add_user_cut(): Add packed user-defined cut\n- cg_send_cut(): Send cut to LP (internal)\n\n**User callbacks:**\n- user_receive_cg_data(): Receive problem-specific data\n- user_free_cg(): Clean up user data\n- user_find_cuts(): Main separation routine\n  - Called with LP solution (indices, values, objval)\n  - Populates cuts array via cg_add_* helpers\n- user_receive_lp_solution_cg(): Custom solution handling\n- user_check_validity_of_cut(): Debug validation (ifdef)\n\n**Example user_find_cuts implementation:**\n```c\nint user_find_cuts(void *user, int varnum, ...) {\n    // Check for violated inequalities\n    for (each inequality class) {\n        if (is_violated(values, ...)) {\n            cg_add_explicit_cut(nz, ind, val, rhs, 0.0,\n                                'L', TRUE, num_cuts, ...);\n        }\n    }\n    return USER_SUCCESS;\n}\n```", "see": ["sym_cg.h for cut generator process", "sym_lp_u.h for LP-side cut callbacks"], "has_pass2": false}, "include/sym_lp_solver.h": {"path": "layer-3/SYMPHONY/include/sym_lp_solver.h", "filename": "sym_lp_solver.h", "file": "sym_lp_solver.h", "brief": "LP solver abstraction layer for SYMPHONY\n\nProvides uniform interface to multiple LP solvers via OSI\n(Open Solver Interface) or native APIs.\n\n**Supported solvers (via compile flags):**\n- __OSI_CLP__: COIN-OR CLP (default, open source)\n- __OSI_CPLEX__: IBM CPLEX\n- __OSI_XPRESS__: FICO Xpress\n- __OSI_GLPK__: GNU Linear Programming Kit\n- __CPLEX__: Native CPLEX API\n- __OSL__: IBM OSL (legacy)\n\n**LPdata structure:**\n- si: OsiSolverInterface pointer (OSI mode)\n- lp: Native solver object (CPLEX/OSL mode)\n- n, m, nz: Columns, rows, nonzeros\n- x, dj, dualsol, slacks: Solution vectors\n- vars: Variable descriptors with bounds/status\n- mip: Original problem description\n- cgl: CGL cut generation parameters\n\n**Core LP operations:**\n- open/close_lp_solver(): Initialize/cleanup\n- load_lp_prob(): Load problem into solver\n- initial_lp_solve(): First solve from scratch\n- dual_simplex(): Resolve after modifications\n- solve_hotstart(): Quick re-solve with basis\n\n**Matrix modification:**\n- add_rows(), add_cols(): Extend problem\n- delete_rows(), delete_cols(): Remove elements\n- change_bounds(), change_rhs(): Modify constraints\n\n**Solution access:**\n- get_x(), get_dj_pi(), get_slacks(): Retrieve solutions\n- get_basis(), load_basis(): Basis manipulation\n\n**CGL cut generation:**\n- generate_cgl_cuts(): Call CGL generators\n- Supports: Gomory, MIR, Probing, Knapsack, Clique, etc.", "see": ["sym_lp.h for LP process using this interface", "CoinUtils for OSI documentation"], "has_pass2": false}, "include/SymConfig.h": {"path": "layer-3/SYMPHONY/include/SymConfig.h", "filename": "SymConfig.h", "file": "SymConfig.h", "brief": "Build configuration for SYMPHONY\n\nManages platform-specific configuration and symbol visibility.\n\n**Configuration modes:**\n- HAVE_CONFIG_H: Uses autoconf-generated config.h\n- Otherwise: Uses config_default.h / config_sym_default.h\n\n**Symbol visibility:**\n- SYMPHONYLIB_EXPORT: DLL export on Windows\n- __visibility__(\"default\"): GCC visibility for -fvisibility=hidden\n\n**Include hierarchy:**\n- SYMPHONYLIB_BUILD + HAVE_CONFIG_H → config.h\n- Client + HAVE_CONFIG_H → config_sym.h\n- SYMPHONYLIB_BUILD no autoconf → config_default.h\n- Client no autoconf → config_sym_default.h", "see": ["config_default.h for package dependencies", "config_sym_default.h for public API config"], "has_pass2": false}, "include/sym_tm.h": {"path": "layer-3/SYMPHONY/include/sym_tm.h", "filename": "sym_tm.h", "file": "sym_tm.h", "brief": "Tree Manager for SYMPHONY's parallel B&C\n\nThe Tree Manager (TM) maintains the branch-and-cut search tree,\ndispatches nodes to LP workers, and coordinates cut generation.", "algorithm": "Tree Management for Branch-and-Cut:\nMaintains B&B tree data structure and orchestrates parallel node processing.\n\nTREE DATA STRUCTURE:\n- Binary tree of bc_node structures linked by parent/child pointers\n- Each node stores: bounds, branching decision, LP basis, active cuts\n- Candidate lists maintain nodes awaiting processing\n\nNODE SELECTION STRATEGIES:\n- LOWEST_LP_FIRST (best-bound): Minimizes total nodes explored\n  Best for proving optimality; high memory usage\n- DEPTH_FIRST_SEARCH: Minimal memory, finds feasible solutions fast\n  May explore many suboptimal nodes\n- BEST_FIRST_SEARCH: Uses LP estimate + pseudo-cost prediction\n- DEPTH_FIRST_THEN_BEST_FIRST: Hybrid approach\n\nDIVING STRATEGY (shall_we_dive):\nAfter processing a node, decide:\n- Continue diving: Process a child immediately (depth-first locally)\n- Backtrack: Select best node from candidate list\nDiving maintains LP basis warmth but may miss better nodes.\n\nPRUNING (install_new_ub):\nWhen new incumbent found:\n1. Update global upper bound\n2. Scan candidate lists, prune nodes with lb ≥ ub\n3. Mark active nodes for potential pruning\n\nTWO-PHASE ALGORITHM:\nPhase 1: Explore with minimal cutting (fast but weak bounds)\nPhase 2: Add aggressive cuts to promising nodes\nBalances exploration speed with bound quality.", "math": "Node ordering:\nBest-bound: Select n* = argmin{lb(n) : n ∈ candidates}\nDepth-first: Select n* = argmax{depth(n) : n ∈ candidates}", "complexity": "- Insert node: O(log |candidates|) with heap\n- Select best: O(log |candidates|)\n- Pruning: O(|candidates|) per new incumbent\n\n**tm_prob structure:**\n- rootnode: Root of the B&C tree\n- active_nodes: Nodes currently being processed\n- samephase_cand: Nodes ready for processing in current phase\n- nextphase_cand: Nodes for next phase (two-phase algorithm)\n- lp/cg/cp: Process sets for LP solvers, cut generators, cut pools\n- pcost_down/pcost_up: Pseudo-costs for branching decisions\n- cuts: Global cut list\n\n**Node selection (del_best_node):**\n- LOWEST_LP_FIRST: Best-bound search\n- DEPTH_FIRST_SEARCH: Dive to find feasible solutions\n- BREADTH_FIRST_SEARCH: Level-by-level\n- BEST_FIRST_SEARCH: Estimate-based\n\n**Key operations:**\n- tm_initialize(): Set up tree from root description\n- solve(): Main tree search loop\n- generate_children(): Create child nodes after branching\n- shall_we_dive(): Decide diving vs backtracking\n- install_new_ub(): Update incumbent and prune\n\n**Two-phase algorithm:**\nPhase 1 explores with fewer cuts, Phase 2 adds more cuts\nto nodes marked for reconsideration.", "see": ["sym_master.h for master process", "sym_lp.h for LP solver process"], "has_pass2": true}, "include/sym_qsort.h": {"path": "layer-3/SYMPHONY/include/sym_qsort.h", "filename": "sym_qsort.h", "file": "sym_qsort.h", "brief": "Quicksort variants and utility functions\n\nSpecialized quicksort implementations for common data patterns.\n\n**Quicksort variants:**\n- qsort_i(): Sort integer array\n- qsort_id(): Sort int array, permute double array\n- qsort_ic(): Sort int array, permute char array\n- qsort_ii(): Sort int array, permute second int array\n- qsort_di(): Sort double array, permute int array\n\n**Utility functions:**\n- sym_gcd(): Greatest common divisor\n- d_gap(): Compute optimality gap percentage", "see": ["sym_lp.h for LP process using sort functions"], "has_pass2": false}, "include/sym_dg_u.h": {"path": "layer-3/SYMPHONY/include/sym_dg_u.h", "filename": "sym_dg_u.h", "file": "sym_dg_u.h", "brief": "User callbacks for Draw Graph visualization\n\nDefines callbacks for custom visualization behavior.\n\n**User callbacks:**\n- user_initialize_dg(): Global DG initialization\n- user_free_dg(): Global DG cleanup\n- user_dg_init_window(): Per-window setup\n- user_dg_free_window(): Per-window cleanup\n- user_dg_process_message(): Handle custom messages\n- user_interpret_text(): Process text input", "see": ["sym_dg.h for draw graph process", "sym_dg_params.h for message constants"], "has_pass2": false}, "include/sym_lp_u.h": {"path": "layer-3/SYMPHONY/include/sym_lp_u.h", "filename": "sym_lp_u.h", "file": "sym_lp_u.h", "brief": "User callbacks for LP solver process\n\nDefines callbacks that users implement to customize LP solving.\nReturn USER_DEFAULT to use built-in behavior.\n\n**Data transfer:**\n- user_receive_lp_data(): Receive problem-specific data\n- user_free_lp(): Clean up user data\n\n**Feasibility checking:**\n- user_is_feasible(): Custom integrality check\n- user_send_feasible_solution(): Pack solution for master\n- user_display_lp_solution(): Custom solution display\n\n**Branching callbacks:**\n- user_shall_we_branch(): Decide whether to branch\n- user_select_candidates(): Choose branching candidates\n- user_compare_candidates(): Rank two candidates\n- user_select_child(): Choose child node to explore\n\n**Cut handling:**\n- user_unpack_cuts(): Convert packed cuts to rows\n- user_generate_cuts_in_lp(): Generate cuts in LP process\n- user_same_cuts(): Compare two cuts for equality\n\n**Column generation:**\n- user_generate_column(): Create new variables dynamically\n- user_create_subproblem(): Build restricted LP\n\n**Built-in selection rules:**\n- branch_close_to_half(): Fractional variables near 0.5\n- branch_close_to_half_and_expensive(): Combine with objective\n- branch_close_to_one_and_cheap(): Variables near 1 with low cost", "see": ["sym_lp.h for LP process", "sym_cg_u.h for cut generator callbacks"], "has_pass2": false}, "include/config_default.h": {"path": "layer-3/SYMPHONY/include/config_default.h", "filename": "config_default.h", "file": "config_default.h", "brief": "Default build configuration for SYMPHONY library\n\nDefines package dependencies when not using autoconf.\nEdit this file to enable/disable optional solver backends.\n\n**Required packages (always enabled):**\n- COIN_HAS_CGL: CGL cut generators\n- COIN_HAS_CLP: CLP linear solver\n- COIN_HAS_COINUTILS: CoinUtils foundation\n- COIN_HAS_OSI: OSI solver interface\n\n**Optional packages (commented by default):**\n- COIN_HAS_CBC: Cbc MIP solver\n- COIN_HAS_VOL: Volume algorithm\n- COIN_HAS_CPX: CPLEX\n- COIN_HAS_GLPK: GLPK\n- COIN_HAS_MSK: Mosek\n- COIN_HAS_XPR: Xpress\n- COIN_DEBUG: Runtime sanity checks", "see": ["SymConfig.h for configuration selection"], "has_pass2": false}, "include/sym_cp.h": {"path": "layer-3/SYMPHONY/include/sym_cp.h", "filename": "sym_cp.h", "file": "sym_cp.h", "brief": "Cut Pool process for SYMPHONY\n\nThe Cut Pool (CP) stores generated cuts for potential reuse\nacross multiple B&C nodes. Manages cut quality and deduplication.", "algorithm": "Pool Size Management:\nPrevent memory exhaustion from unbounded cut storage.\n\n  delete_ineffective_cuts():\n    Remove cuts with quality below threshold\n    Called when pool exceeds size limit\n\n  delete_duplicate_cuts():\n    Hash-based comparison via cutcmp()\n    Keep one copy of identical cuts\n\n  order_cuts_by_quality():\n    High-quality cuts checked first\n    Improves separation efficiency", "math": "Quality factors:\n  - touches: Times cut was binding (higher = better)\n  - level: Tree depth where generated (root cuts more valuable)\n  - check_num: How often checked without being useful\n\n  Effective cuts: touches >> check_num\n  Ineffective cuts: touches << check_num", "complexity": "check_cut: O(nnz(cut)) per cut\n  delete_duplicates: O(n log n) with sorting\n  order_by_quality: O(n log n) sort", "see": ["sym_cg.h for cut generation", "sym_lp.h for cut addition to LP"], "has_pass2": true}, "include/decomp/sp_params.h": {"path": "layer-3/SYMPHONY/include/decomp/sp_params.h", "filename": "sp_params.h", "file": "sp_params.h", "brief": "Solution/column pool parameters for decomposition\n\nParameters controlling column storage and management.\n\n**sp_params structure:**\n- verbosity: Output level\n- etol: Numerical tolerance\n- block_size: Allocation granularity\n- max_size: Maximum pool memory\n- max_number_of_sols: Hard limit on columns\n- min_to_delete: Minimum batch deletion size\n- touches_until_deletion: Inactivity threshold\n- compress_num/ratio: When to compress pool\n\n**Check strategies (check_which):**\n- CHECK_ALL_COLS: Check everything\n- CHECK_COL_LEVEL: By tree depth\n- CHECK_COL_TOUCHES: By activity\n- CHECK_COL_LEVEL_AND_TOUCHES: Combined\n\n**Delete strategies (delete_which):**\n- DELETE_DUPLICATE_COLS: Only duplicates\n- DELETE_DUPLICATE_AND_INEFFECTIVE_COLS: Also inactive", "see": ["decomp.h for column generation using pool"], "has_pass2": false}, "include/decomp/decomp_lp.h": {"path": "layer-3/SYMPHONY/include/decomp/decomp_lp.h", "filename": "decomp_lp.h", "file": "decomp_lp.h", "brief": "LP interface for decomposition master problem\n\nLoad/unload functions for the restricted master LP.\n\n**Functions:**\n- load_decomp_lp(): Initialize master LP with initial columns\n- unload_decomp_lp(): Cleanup master LP", "see": ["decomp.h for column generation algorithm", "sym_lp_solver.h for LPdata structure"], "has_pass2": false}, "include/decomp/decomp.h": {"path": "layer-3/SYMPHONY/include/decomp/decomp.h", "filename": "decomp.h", "file": "decomp.h", "brief": "Decomposition-based column generation for SYMPHONY\n\nImplements column generation via Dantzig-Wolfe decomposition\nfor problems with special structure.\n\n**Core functions:**\n- decomp(): Main decomposition algorithm\n- create_initial_lp(): Build restricted master problem\n- generate_new_cols(): Solve pricing subproblem\n- generate_cuts(): Cut generation in master\n- add_dcmp_cols(): Add columns to master LP\n\n**Column management:**\n- receive_cols(): Receive columns from subproblems\n- get_cols_from_pool(): Retrieve from column pool\n- free_dcmp_col_set(): Cleanup column set\n\n**User callbacks:**\n- user_generate_new_cols(): Custom pricing routine\n- user_unpack_col(): Expand packed column\n- user_pack_col(): Compress column for storage\n- user_check_col(): Verify column validity\n- user_set_rhs(): Define subproblem RHS\n- user_send_to_sol_pool(): Store promising columns", "see": ["decomp_types.h for col_data, dcmp_col_set", "decomp_lp.h for LP interface", "sp_params.h for column pool parameters"], "has_pass2": false}, "include/decomp/decomp_types.h": {"path": "layer-3/SYMPHONY/include/decomp/decomp_types.h", "filename": "decomp_types.h", "file": "decomp_types.h", "brief": "Data structures for decomposition column generation\n\nTypes for representing columns in Dantzig-Wolfe decomposition.\n\n**col_data (packed column):**\n- size: Size of coef array\n- coef: Packed column data\n- level: Tree level where generated\n- touches: Inactivity counter for deletion\n\n**dcmp_col_set (column batch):**\n- lb, ub, obj: Column bounds and costs\n- matbeg, matind, matval: CSC format matrix\n- num_cols, nzcnt: Dimensions\n- bd_type, ubnd: Bound type info", "see": ["decomp.h for column generation functions"], "has_pass2": false}}}, "Smi": {"name": "Smi", "file_count": 9, "pass2_count": 0, "files": {"Smi/src/SmiSmpsIO.hpp": {"path": "layer-3/Smi/Smi/src/SmiSmpsIO.hpp", "filename": "SmiSmpsIO.hpp", "file": "SmiSmpsIO.hpp", "brief": "SMPS (Stochastic MPS) file format reader and writer\n\nSmiSmpsIO extends CoinMpsIO to read/write the industry-standard SMPS\nformat for stochastic programming problems.\n\n**SMPS File Format (3 files):**\n- {name}.core: Core LP in MPS format (deterministic base problem)\n- {name}.time: Stage assignments (PERIODS section maps rows/cols to stages)\n- {name}.stoch: Stochastic data (SCENARIOS or INDEP sections)\n\n**SmiSectionType Enum:**\n- SMI_TIME_SECTION: PERIODS in time file\n- SMI_SCENARIOS_SECTION: SCENARIOS in stoch file\n- SMI_INDEPENDENT_SECTION: INDEP in stoch file\n\n**SmiSmpsType Enum:**\n- SMI_SC_CARD: SC card (scenario definition)\n- SMI_BL_CARD: BL card (branch location)\n- SMI_SMPS_COMBINE_ADD/REPLACE: Data combination rules\n\n**SmiSmpsCardReader Class:**\nExtends CoinMpsCardReader for SMPS-specific parsing:\n- periodName(): Stage name from time file\n- scenarioNew/Anc(): Scenario branching info\n- getProb(): Scenario probability\n\n**Key Methods:**\n- readTimeFile(): Parse stage assignments\n- readStochFile(): Parse stochastic data\n- writeSmps(): Output SMPS files", "see": ["SmiScnModel.hpp::readSmps() for high-level interface", "SmiScnData.hpp for SmiCoreData created from core file", "CoinMpsIO (CoinUtils) for MPS parsing base"], "param": ["filename The filename.", "extension The file extension.", "strictFormat Whether a strict format should be used or not.", "filename The filename.", "extension The file extension.", "strictFormat Whether a strict format should be used or not.", "filename The filename.", "extension The file extension.", "strictFormat Whether a strict format should be used or not."], "has_pass2": false}, "Smi/src/SmiMessage.hpp": {"path": "layer-3/Smi/Smi/src/SmiMessage.hpp", "filename": "SmiMessage.hpp", "file": "SmiMessage.hpp", "brief": "Message handler and codes for Smi logging\n\nSmiMessage extends CoinMessages to provide structured logging\nfor the stochastic programming interface.\n\n**SMI_Message Enum:**\n- SMI_SCENARIO_FINISHED: Scenario processing complete\n- SMI_DUMMY_END: End marker\n\n**SmiMessage Class:**\nCoinMessages-based message handler:\n- Constructor sets up message text templates\n- Supports multiple languages (default us_en)", "see": ["SmiScnModel.hpp for message handler usage", "CoinMessages, CoinMessageHandler (CoinUtils) for base classes"], "has_pass2": false}, "Smi/src/SmiScenarioTree.hpp": {"path": "layer-3/Smi/Smi/src/SmiScenarioTree.hpp", "filename": "SmiScenarioTree.hpp", "file": "SmiScenarioTree.hpp", "brief": "Scenario tree data structure for stochastic programming\n\nSmiScenarioTree is a template class for storing and navigating scenario\ntrees. Each root-to-leaf path represents one complete scenario.\n\n**SmiTreeNode<T> Template:**\nGeneric tree node with navigation:\n- parent_, child_, sibling_: Tree structure pointers\n- depth_: Distance from root (root = 0)\n- scen_: Scenario index for leaf nodes\n- child_labels_: Map for label-based child lookup\n\n**SmiScenarioTree<T> Template:**\nTree container with:\n- root_: Root node pointer\n- leaf_: Vector of leaf node pointers (one per scenario)\n- node_data: All node data in tree traversal order\n- scen_data: Temporary buffer for scenario path extraction\n\n**Key Operations:**\n- addPathtoLeaf(): Add new scenario path from branch point\n- find(scenario, stage): Get node at specific (scenario, stage)\n- find(labels): Get node by following label sequence\n- getScenario(s): Get vector of data along scenario s path\n\n**Tree Structure:**\n```\n      root (stage 0)\n     /    \\\n  node    node (stage 1)\n  / \\      |\nleaf leaf leaf (scenarios 0,1,2)\n```", "see": ["SmiScnModel.hpp for scenario model using this tree", "SmiScnData.hpp for SmiNodeData stored in tree nodes"], "has_pass2": false}, "Smi/src/SmiDiscreteDistribution.hpp": {"path": "layer-3/Smi/Smi/src/SmiDiscreteDistribution.hpp", "filename": "SmiDiscreteDistribution.hpp", "file": "SmiDiscreteDistribution.hpp", "brief": "Discrete probability distributions for stochastic programming\n\nClasses for representing discrete random variables and their distributions,\nused to generate scenario trees from independent stochastic elements.\n\n**SmiDiscreteDistribution Class:**\nContainer for multiple discrete random variables:\n- smiDiscrete_: Vector of SmiDiscreteRV objects\n- core_: Link to core problem data\n- combineRule_: How to merge events with core\n\n**SmiDiscreteRV Class:**\nSingle discrete random variable at a specific stage:\n- events_: Vector of SmiDiscreteEvent outcomes\n- stg_: Stage index for this RV\n- prob_: Total probability across events\n- addEvent(): Add outcome with LP data and probability\n\n**SmiDiscreteEvent Class:**\nOne possible outcome (realization) of a random variable:\n- Inherits SmiLinearData: matrix, bounds, objective changes\n- prob_: Probability of this event\n\n**Usage Pattern:**\n```cpp\nSmiDiscreteDistribution dist(core, rule);\nSmiDiscreteRV* rv = new SmiDiscreteRV(stage);\nrv->addEvent(matrix, clo, cup, obj, rlo, rup, 0.5);  // 50% chance\nrv->addEvent(..., 0.5);  // 50% chance\ndist.addDiscreteRV(rv);\nsmiModel.processDiscreteDistributionIntoScenarios(&dist);\n```", "see": ["SmiScnModel.hpp::processDiscreteDistributionIntoScenarios()", "SmiLinearData.hpp for LP data container", "SmiCoreCombineRule.hpp for combination rules"], "has_pass2": false}, "Smi/src/SmiScnModel.hpp": {"path": "layer-3/Smi/Smi/src/SmiScnModel.hpp", "filename": "SmiScnModel.hpp", "file": "SmiScnModel.hpp", "brief": "Main scenario model class for stochastic linear/quadratic programming\n\nSmiScnModel is the central class for representing and solving scenario-based\nstochastic programs. It builds deterministic equivalent (DE) formulations\nfrom scenario trees.\n\n**Typical Usage:**\n```cpp\nSmiScnModel smi;\nsmi.readSmps(\"problem\");              // Read SMPS files\nsmi.setOsiSolverHandle(new OsiClpSolverInterface());\nOsiSolverInterface* osi = smi.loadOsiSolverData();  // Build DE\nosi->initialSolve();                  // Solve extensive form\n```\n\n**Key Methods:**\n- readSmps(): Parse SMPS files (core + time + stoch)\n- generateScenario(): Add scenario with probability and branching info\n- loadOsiSolverData(): Build deterministic equivalent LP\n- getColSolution(scenario): Extract solution for specific scenario\n\n**Solution Values:**\n- solveWS(): Wait-and-See solution (perfect information)\n- solveEV(): Expected Value solution (average scenario)\n- solveEEV(): Expected result of using EV solution\n\n**SmiScnNode Class:**\nRepresents a node in the scenario tree with:\n- Stage index, probability, parent link\n- Column/row offsets into DE matrix\n- Link to SmiNodeData for LP data", "see": ["SmiScenarioTree.hpp for tree data structure", "SmiScnData.hpp for core/node data classes", "SmiSmpsIO.hpp for SMPS file I/O", "SmiCoreCombineRule.hpp for replace/add rules"], "param": ["name The name for the model and the written files", "winFileExtensions optional; false by default, so extensions will be [core, time, stoch]. With this parameter set to true, it will be [cor, tim, sto].", "strictFormat optional, true by default. Set to false if SMPS files should be written in free format."], "return": "-1 in case of no existing SMI model, otherwise 0", "has_pass2": false}, "Smi/src/SmiQuadratic.hpp": {"path": "layer-3/Smi/Smi/src/SmiQuadratic.hpp", "filename": "SmiQuadratic.hpp", "file": "SmiQuadratic.hpp", "brief": "Quadratic objective data for stochastic quadratic programming\n\nClasses for storing quadratic objective terms in stochastic QP problems.\nThe quadratic objective is: (1/2) x' Q x + c' x\n\n**SmiQuadraticData Class:**\nBase class storing Q matrix in compressed sparse column format:\n- _n: Number of columns\n- _starts: Column start indices (size n+1)\n- _indx: Row indices of nonzeros\n- _els: Values of nonzeros\n- _coff: Column offset for stage-based indexing\n- _hasData: Flag for valid data\n\n**CSC Format:**\nFor column j, nonzeros are at positions [starts[j], starts[j+1]):\n- Row indices: indx[starts[j]] ... indx[starts[j+1]-1]\n- Values: els[starts[j]] ... els[starts[j+1]-1]\n\n**SmiQuadraticDataDC Class:**\n\"Data Container\" subclass that owns its memory:\n- Constructor allocates arrays\n- Destructor frees arrays", "see": ["SmiScnData.hpp::SmiNodeData::addQuadraticObjective()", "SmiScnModel.hpp::loadQuadraticSolverData()", "ClpModel (Clp) for QP solving"], "has_pass2": false}, "Smi/src/SmiCoreCombineRule.hpp": {"path": "layer-3/Smi/Smi/src/SmiCoreCombineRule.hpp", "filename": "SmiCoreCombineRule.hpp", "file": "SmiCoreCombineRule.hpp", "brief": "Rules for combining core and stochastic data in SMPS\n\nIn SMPS, stochastic data is a \"diff\" from the core problem. This file\ndefines how to apply that diff (combine core + stochastic → scenario).\n\n**SmiCoreCombineRule (Abstract Base):**\nVirtual interface for combination rules:\n- Process(d1, offset, cpv): Modify dense array d1 using sparse cpv\n- Process(cpv1, cpv2): Combine two sparse vectors\n\n**SmiCoreCombineReplace (Singleton):**\nDefault SMPS rule - stochastic values REPLACE core values:\n- scenario[i] = stochastic[i] if defined, else core[i]\n- Use case: Changing a coefficient value entirely\n\n**SmiCoreCombineAdd (Singleton):**\nAlternative rule - stochastic values ADD to core values:\n- scenario[i] = core[i] + stochastic[i]\n- Use case: Perturbations, additive uncertainty\n\n**Singleton Pattern:**\nBoth concrete rules use singleton pattern for efficiency:\n- SmiCoreCombineReplace::Instance()\n- SmiCoreCombineAdd::Instance()\n\n**Usage:**\nRules are passed to generateScenario() or readSmps() to control\nhow scenario data is constructed from core + stochastic inputs.", "see": ["SmiScnModel.hpp::generateScenario() for usage", "SmiSmpsIO.hpp::readStochFile() for SMPS parsing", "SmiScnData.hpp::SmiNodeData for node-level application"], "has_pass2": false}, "Smi/src/SmiScnData.hpp": {"path": "layer-3/Smi/Smi/src/SmiScnData.hpp", "filename": "SmiScnData.hpp", "file": "SmiScnData.hpp", "brief": "Core and node data classes for stochastic models\n\nContains SmiCoreData (deterministic base LP) and SmiNodeData (scenario\nmodifications) that together define stochastic linear programs.\n\n**SmiCoreData Class:**\nStores the deterministic \"core\" LP problem with stage structure:\n- nrow_, ncol_: Total rows/columns in core problem\n- nstag_: Number of stages (periods)\n- stageRowPtr_[t], stageColPtr_[t]: Row/col offsets for stage t\n- nRowInStage_[t], nColInStage_[t]: Counts per stage\n- nodes_[t]: Core SmiNodeData for each stage\n- Dense bound arrays: cdrlo_, cdrup_, cdclo_, cdcup_, cdobj_\n\n**SmiNodeData Class:**\nStores LP modifications for a tree node (scenario/stage):\n- Sparse delta data: matrix rows, bounds, objective\n- combineRule_: How to merge with core (replace/add)\n- Stage membership and core linkage\n- Dense row map for efficient access\n\n**SMPS Data Flow:**\n1. Core MPS → SmiCoreData (base LP with stage assignments)\n2. Stoch file → SmiNodeData (per-scenario modifications)\n3. Combine with rule → Full scenario LP data\n\n**Type Aliases:**\n- SmiCoreIndex, SmiScenarioIndex, SmiStageIndex: int aliases", "see": ["SmiCoreCombineRule.hpp for combination rules", "SmiScnModel.hpp for scenario model using these classes", "SmiSmpsIO.hpp for SMPS file parsing"], "has_pass2": false}, "Smi/src/SmiLinearData.hpp": {"path": "layer-3/Smi/Smi/src/SmiLinearData.hpp", "filename": "SmiLinearData.hpp", "file": "SmiLinearData.hpp", "brief": "Container for linear programming data (matrix + bounds + objective)\n\nSmiLinearData bundles all LP data needed to define a linear subproblem\nor modification in stochastic programming.\n\n**Data Members:**\n- matrix_: Constraint matrix (CoinPackedMatrix)\n- dclo_, dcup_: Column (variable) bounds\n- drlo_, drup_: Row (constraint) bounds\n- dobj_: Objective coefficients\n\n**Constructors:**\n- Default: Empty data\n- Copy: From another SmiLinearData\n- From components: matrix + 5 sparse vectors\n- From OSI: Extract from OsiSolverInterface\n\n**Accessors:**\n- getMatrix(), getColLower(), etc.: Const references\n- getMutableMatrix(), etc.: Non-const for modification\n\n**Usage:**\nUsed as base class for SmiDiscreteEvent and as a data container\nfor passing LP modifications around the Smi system.", "see": ["SmiDiscreteDistribution.hpp::SmiDiscreteEvent for subclass", "SmiScnData.hpp::SmiNodeData for related node storage", "CoinPackedVector, CoinPackedMatrix (CoinUtils) for sparse data"], "has_pass2": false}}}, "oBB": {"name": "oBB", "file_count": 5, "pass2_count": 0, "files": {"nlopt/nlopt.hpp": {"path": "layer-3/oBB/nlopt/nlopt.hpp", "filename": "nlopt.hpp", "file": "nlopt.hpp", "brief": "C++ wrapper around NLopt C API - bundled with oBB\n\nProvides an object-oriented C++ interface to NLopt with RAII,\nexception handling, and std::vector support.\n\n**nlopt::opt Class:**\nMain optimizer class wrapping nlopt_opt:\n- Constructor: opt(algorithm, n) creates n-dimensional optimizer\n- set_min/max_objective(): Set objective (func or vfunc)\n- set_lower/upper_bounds(): Variable bounds\n- add_inequality/equality_constraint(): Add constraints\n- set_ftol_rel/abs(), set_xtol_rel/abs(): Tolerances\n- optimize(x, opt_f): Run optimization, returns nlopt::result\n\n**Function Types:**\n- nlopt::func: C-style callback (same as nlopt_func)\n- nlopt::vfunc: C++ style with std::vector<double> args\n\n**Exception Types:**\n- nlopt::roundoff_limited: Roundoff prevented progress\n- nlopt::forced_stop: Optimization was force-stopped\n- std::bad_alloc, std::invalid_argument: Standard exceptions\n\n**Usage Example:**\n```cpp\nnlopt::opt opt(nlopt::LD_SLSQP, 2);\nopt.set_min_objective(myFunc, NULL);\nopt.set_lower_bounds(lb);\nstd::vector<double> x = {1.0, 1.0};\ndouble minf;\nnlopt::result res = opt.optimize(x, minf);\n```", "see": ["nlopt.h for C API", "oBB for global optimization using this for local steps"], "has_pass2": false}, "slsqp/slsqp.h": {"path": "layer-3/oBB/slsqp/slsqp.h", "filename": "slsqp.h", "file": "slsqp.h", "brief": "SLSQP (Sequential Least Squares Programming) algorithm - bundled with oBB\n\nSLSQP is a gradient-based algorithm for constrained nonlinear optimization\nusing sequential quadratic programming with BFGS updates.\n\n**Algorithm (Kraft, 1988):**\n1. Solve QP subproblem: min (1/2)d'Bd + g'd s.t. linearized constraints\n2. Line search along direction d\n3. Update Hessian approximation B using BFGS\n4. Repeat until convergence\n\n**nlopt_slsqp() Parameters:**\n- n: Number of variables\n- f, f_data: Objective function (with gradient)\n- m, fc: Inequality constraints (m constraints, c(x) <= 0)\n- p, h: Equality constraints (p constraints, h(x) = 0)\n- lb, ub: Variable bounds\n- x: Initial point (modified in place)\n- minf: Output - optimal objective value\n- stop: Stopping criteria (nlopt_stopping struct)\n\n**Characteristics:**\n- Local, derivative-based optimization\n- Handles nonlinear equality and inequality constraints\n- Efficient for smooth, medium-scale problems\n- Used by oBB for local refinement in global optimization", "see": ["nlopt.h for nlopt_constraint structure", "nlopt-util.h for nlopt_stopping structure", "oBB for global optimization framework"], "has_pass2": false}, "nlopt/nlopt.h": {"path": "layer-3/oBB/nlopt/nlopt.h", "filename": "nlopt.h", "file": "nlopt.h", "brief": "NLopt (Nonlinear Optimization) C API - bundled with oBB\n\nNLopt is a library for nonlinear optimization providing a common\ninterface to many algorithms. This is a subset bundled with oBB.\n\n**nlopt_algorithm Enum (in this subset):**\n- NLOPT_LD_SLSQP: Sequential Least Squares Programming (local, derivative)\n\n**nlopt_result Enum:**\n- NLOPT_SUCCESS: Generic success\n- NLOPT_STOPVAL_REACHED, FTOL_REACHED, XTOL_REACHED: Convergence\n- NLOPT_MAXEVAL_REACHED, MAXTIME_REACHED: Limits hit\n- NLOPT_FAILURE, INVALID_ARGS, OUT_OF_MEMORY: Errors\n\n**Object-Oriented API:**\n- nlopt_create(algorithm, n): Create optimizer for n dimensions\n- nlopt_set_min/max_objective(): Set objective function\n- nlopt_set_lower/upper_bounds(): Set variable bounds\n- nlopt_add_inequality/equality_constraint(): Add constraints\n- nlopt_set_ftol_rel/abs(), xtol_rel/abs(): Set tolerances\n- nlopt_optimize(opt, x, &f): Run optimization\n- nlopt_destroy(opt): Clean up\n\n**Function Types:**\n- nlopt_func: double f(n, x, grad, data) - objective/constraint\n- nlopt_mfunc: void f(m, result, n, x, grad, data) - vector constraint", "see": ["nlopt.hpp for C++ wrapper", "slsqp.h for SLSQP implementation", "https://nlopt.readthedocs.io/ for full NLopt documentation"], "has_pass2": false}, "nlopt/nlopt-util.h": {"path": "layer-3/oBB/nlopt/nlopt-util.h", "filename": "nlopt-util.h", "file": "nlopt-util.h", "brief": "NLopt utility functions and stopping criteria - bundled with oBB\n\nInternal utilities for NLopt algorithms including floating-point\nhelpers, timing, stopping criteria, and constraint handling.\n\n**Floating-Point Helpers:**\n- nlopt_isinf(), nlopt_isfinite(), nlopt_istiny(), nlopt_isnan()\n\n**Timing:**\n- nlopt_seconds(): Current time in seconds\n- nlopt_time_seed(): Seed based on current time\n\n**nlopt_stopping Struct:**\nStopping criteria state:\n- n: Problem dimension\n- minf_max: Stop if f <= minf_max\n- ftol_rel, ftol_abs: Function tolerance (relative/absolute)\n- xtol_rel, xtol_abs: Variable tolerance\n- nevals, maxeval: Evaluation count/limit\n- maxtime, start: Time limit and start time\n- force_stop: External stop signal\n\n**Stopping Tests:**\n- nlopt_stop_f(): Check function value convergence\n- nlopt_stop_x(), nlopt_stop_dx(): Check variable convergence\n- nlopt_stop_evals(), nlopt_stop_time(): Check limits\n\n**nlopt_constraint Struct:**\nConstraint representation (f <= 0 or f = 0):\n- m: Constraint dimension\n- f/mf: Scalar or vector constraint function\n- tol: Feasibility tolerance", "see": ["nlopt.h for public API", "nlopt-internal.h for optimizer structure"], "has_pass2": false}, "nlopt/nlopt-internal.h": {"path": "layer-3/oBB/nlopt/nlopt-internal.h", "filename": "nlopt-internal.h", "file": "nlopt-internal.h", "brief": "NLopt internal optimizer structure - bundled with oBB\n\nInternal header defining the nlopt_opt_s structure (opaque nlopt_opt).\nNot part of public API - used by algorithm implementations.\n\n**struct nlopt_opt_s (nlopt_opt):**\nComplete optimizer state:\n- algorithm, n: Algorithm type and dimension (immutable)\n- f, f_data: Objective function and user data\n- pre: Optional preconditioner\n- maximize: Nonzero if maximizing\n- lb, ub: Variable bounds (length n)\n- m, fc: Inequality constraints (m constraints)\n- p, h: Equality constraints (p constraints)\n- Stopping criteria: stopval, ftol_rel, ftol_abs, xtol_rel, xtol_abs, maxeval, maxtime\n- force_stop: External stop flag\n- local_opt: Nested local optimizer (for hybrid algorithms)\n- dx: Initial step sizes for derivative-free algorithms\n- work: Algorithm-specific workspace\n- errmsg: Error message buffer\n\n**Global Defaults (deprecated):**\n- nlopt_local_search_alg_deriv/nonderiv\n- nlopt_local_search_maxeval\n- nlopt_stochastic_population", "see": ["nlopt.h for public API", "nlopt-util.h for stopping criteria helpers"], "has_pass2": false}}}}}, "layer-4": {"name": "layer-4", "library_count": 8, "libraries": {"Creme": {"name": "Creme", "file_count": 11, "pass2_count": 0, "files": {"Creme/src/Base/sparse.h": {"path": "layer-4/Creme/Creme/src/Base/sparse.h", "filename": "sparse.h", "file": "Base/sparse.h", "brief": "Sparse LP data structures for Creme solver\n\nCore data structure (sparseLP) for randomized LP solving.\nStores constraint matrix in sparse row format with bounds.", "see": ["linopt.h for one-opt local search", "rtr.h for randomized rounding"], "has_pass2": false}, "Creme/src/Base/chooseblock.h": {"path": "layer-4/Creme/Creme/src/Base/chooseblock.h", "filename": "chooseblock.h", "file": "Base/chooseblock.h", "brief": "Constraint block selection for Creme RTR algorithm\n\nchoose_block(): Select a block of constraints for randomized\nrounding based on violation and satisfaction status.", "see": ["rtr.h for randomized rounding algorithm", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/rtr.h": {"path": "layer-4/Creme/Creme/src/Base/rtr.h", "filename": "rtr.h", "file": "Base/rtr.h", "brief": "Randomized rounding algorithm for Creme LP solver\n\nImplements rtr() to find maximal feasible subsystems (MFS) using\nrandomized iterative rounding. USE_RTR/USE_LOCSRCH mode selection.", "see": ["locsrch.h for local search alternative", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/linopt.h": {"path": "layer-4/Creme/Creme/src/Base/linopt.h", "filename": "linopt.h", "file": "Base/linopt.h", "brief": "One-variable local optimization for Creme\n\nImplements one_opt() for single-variable neighborhood search.", "see": ["sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/move.h": {"path": "layer-4/Creme/Creme/src/Base/move.h", "filename": "move.h", "file": "Base/move.h", "brief": "Variable move operation for Creme local search\n\nmove(): Perform a single variable update in local search,\nadjusting primal values and updating constraint satisfaction.", "see": ["locsrch.h for local search algorithm", "linopt.h for one-opt improvement"], "has_pass2": false}, "Creme/src/Base/lpio.h": {"path": "layer-4/Creme/Creme/src/Base/lpio.h", "filename": "lpio.h", "file": "Base/lpio.h", "brief": "LP file I/O functions for Creme solver\n\nread_problem(): Parse LP problem from file into sparseLP structure.\nprintLP(): Display LP problem contents.\nclearLP(): Deallocate LP memory.", "see": ["sparse.h for sparseLP data structure"], "has_pass2": false}, "Creme/src/Base/misc.h": {"path": "layer-4/Creme/Creme/src/Base/misc.h", "filename": "misc.h", "file": "Base/misc.h", "brief": "Miscellaneous utilities for Creme solver\n\nMemory allocation helpers (reallocate_double/int), bzip2 file I/O,\nmatrix transpose creation, and vector norm computation.", "see": ["lpio.h for LP file reading", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/isfeas.h": {"path": "layer-4/Creme/Creme/src/Base/isfeas.h", "filename": "isfeas.h", "file": "Base/isfeas.h", "brief": "Feasibility checking for Creme LP solutions\n\nisFeas(): Verify if a point satisfies LP constraints.\nReturns number of satisfied constraints and updates satisfaction flags.", "see": ["locsrch.h for local search that uses feasibility checks", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/locsrch.h": {"path": "layer-4/Creme/Creme/src/Base/locsrch.h", "filename": "locsrch.h", "file": "Base/locsrch.h", "brief": "Local search algorithm for Creme LP solver\n\nImplements locsrch() with dvar (delta variable) and frontier structures.", "see": ["rtr.h for randomized rounding alternative", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/init.h": {"path": "layer-4/Creme/Creme/src/Base/init.h", "filename": "init.h", "file": "Base/init.h", "brief": "Solution initialization for Creme randomized LP solver\n\ninit_x(): Initialize primal variable values.\ninit_sat(): Initialize constraint satisfaction tracking (b-Ax, sat flags).\ncalc_lhs(): Compute left-hand side of constraints (with SIMD pragmas).", "see": ["rtr.h for randomized rounding algorithm", "sparse.h for LP data structure"], "has_pass2": false}, "Creme/src/Base/cmdline.h": {"path": "layer-4/Creme/Creme/src/Base/cmdline.h", "filename": "cmdline.h", "file": "Base/cmdline.h", "brief": "Command-line argument parsing for Creme solver\n\ntpar struct: Defines command-line options (short/long flags, types, defaults).\nreadargs(): Parse argc/argv against option definitions.\nprint_help(): Display usage information.", "see": ["sparse.h for LP data structure"], "has_pass2": false}}}, "GAMSlinks": {"name": "GAMSlinks", "file_count": 19, "pass2_count": 0, "files": {"src/scip/GamsScip.hpp": {"path": "layer-4/GAMSlinks/src/scip/GamsScip.hpp", "filename": "GamsScip.hpp", "file": "scip/GamsScip.hpp", "brief": "GAMS interface to SCIP solver\n\nConnects GAMS to SCIP for constraint integer programming.\nSupports LP, MIP, MINLP, and general constraint optimization.", "see": ["reader_gmo.h for GAMS model reader plugin", "event_solvetrace.h for solve tracing"], "has_pass2": false}, "src/bonmin/GamsBonmin.hpp": {"path": "layer-4/GAMSlinks/src/bonmin/GamsBonmin.hpp", "filename": "GamsBonmin.hpp", "file": "bonmin/GamsBonmin.hpp", "brief": "GAMS interface to Bonmin MINLP solver\n\nConnects GAMS to COIN-OR Bonmin for mixed-integer nonlinear programming.", "see": ["GamsMINLP.hpp for NLP subproblem representation"], "has_pass2": false}, "src/bonmin/GamsMINLP.hpp": {"path": "layer-4/GAMSlinks/src/bonmin/GamsMINLP.hpp", "filename": "GamsMINLP.hpp", "file": "bonmin/GamsMINLP.hpp", "brief": "Bonmin TMINLP implementation for GAMS models\n\nGamsMINLP: Implements Bonmin::TMINLP using GMO for MINLP evaluation.\nHandles variable types, SOS constraints, and branching priorities.", "see": ["GamsBonmin.hpp for Bonmin driver", "GamsCouenne.hpp for Couenne MINLP solver"], "has_pass2": false}, "src/osi/GamsOsiHelper.hpp": {"path": "layer-4/GAMSlinks/src/osi/GamsOsiHelper.hpp", "filename": "GamsOsiHelper.hpp", "file": "osi/GamsOsiHelper.hpp", "brief": "GAMS-OSI data transfer utilities\n\nHelper functions for loading LP problems from GMO to OSI,\nstoring solutions from OSI to GMO, and writing problem files.", "see": ["GamsOsi.hpp for OSI-based solver driver"], "return": "true on success, false on failure", "has_pass2": false}, "src/osi/GamsMessageHandler.hpp": {"path": "layer-4/GAMSlinks/src/osi/GamsMessageHandler.hpp", "filename": "GamsMessageHandler.hpp", "file": "osi/GamsMessageHandler.hpp", "brief": "CoinMessageHandler implementation for GAMS output\n\nRoutes COIN-OR messages through GAMS environment (gev) logging.\nThread-safe via shared mutex for concurrent solver access.", "see": ["GamsOsi.hpp for OSI solver integration", "GamsOsiHelper.hpp for problem loading utilities"], "has_pass2": false}, "src/osi/GamsOsi.hpp": {"path": "layer-4/GAMSlinks/src/osi/GamsOsi.hpp", "filename": "GamsOsi.hpp", "file": "osi/GamsOsi.hpp", "brief": "GAMS interface to OSI-compatible solvers\n\nConnects GAMS to commercial solvers (CPLEX, Gurobi, Mosek, Xpress)\nvia the COIN-OR Open Solver Interface.", "see": ["GamsOsiHelper.hpp, GamsMessageHandler.hpp"], "has_pass2": false}, "src/ipopt/GamsNLP.hpp": {"path": "layer-4/GAMSlinks/src/ipopt/GamsNLP.hpp", "filename": "GamsNLP.hpp", "file": "ipopt/GamsNLP.hpp", "brief": "Ipopt TNLP implementation for GAMS models\n\nGamsNLP: Implements Ipopt::TNLP using GMO (GAMS Modeling Object)\nfor objective/constraint evaluation, Jacobian sparsity, Hessian.", "see": ["GamsIpopt.hpp for solver driver", "GamsJournal.hpp for output handling"], "has_pass2": false}, "src/ipopt/GamsIpopt.hpp": {"path": "layer-4/GAMSlinks/src/ipopt/GamsIpopt.hpp", "filename": "GamsIpopt.hpp", "file": "ipopt/GamsIpopt.hpp", "brief": "GAMS interface to Ipopt NLP solver\n\nConnects GAMS modeling environment to COIN-OR Ipopt.\n\n**GamsIpopt Class:**\n- ipopt: SmartPtr to IpoptApplication\n- nlp: GamsNLP problem representation\n- Warmstart support for sequential solves", "see": ["GamsNLP.hpp for NLP problem representation", "GamsJournal.hpp for message handling"], "has_pass2": false}, "src/ipopt/GamsJournal.hpp": {"path": "layer-4/GAMSlinks/src/ipopt/GamsJournal.hpp", "filename": "GamsJournal.hpp", "file": "ipopt/GamsJournal.hpp", "brief": "Ipopt Journal implementation for GAMS output\n\nRoutes Ipopt logging through GAMS environment (gev) routines.\nSupports configurable status file output levels.", "see": ["GamsIpopt.hpp for main GAMS-Ipopt interface", "GamsNLP.hpp for NLP problem formulation"], "has_pass2": false}, "src/couenne/GamsCouenne.hpp": {"path": "layer-4/GAMSlinks/src/couenne/GamsCouenne.hpp", "filename": "GamsCouenne.hpp", "file": "couenne/GamsCouenne.hpp", "brief": "GAMS interface to Couenne global optimizer\n\nConnects GAMS to COIN-OR Couenne for global optimization of\nnon-convex MINLP problems via spatial branch-and-bound.", "has_pass2": false}, "src/utils/GamsOptionsSpecWriter.hpp": {"path": "layer-4/GAMSlinks/src/utils/GamsOptionsSpecWriter.hpp", "filename": "GamsOptionsSpecWriter.hpp", "file": "GamsOptionsSpecWriter.hpp", "author": "Stefan Vigerske", "has_pass2": false}, "src/soplex/GamsSoPlex.hpp": {"path": "layer-4/GAMSlinks/src/soplex/GamsSoPlex.hpp", "filename": "GamsSoPlex.hpp", "file": "soplex/GamsSoPlex.hpp", "brief": "GAMS interface to SoPlex LP solver\n\nConnects GAMS modeling environment to the ZIB SoPlex\nsimplex-based LP solver.", "has_pass2": false}, "src/cbc/GamsCbcHeurSolveTrace.hpp": {"path": "layer-4/GAMSlinks/src/cbc/GamsCbcHeurSolveTrace.hpp", "filename": "GamsCbcHeurSolveTrace.hpp", "file": "cbc/GamsCbcHeurSolveTrace.hpp", "brief": "CBC heuristic for GAMS solve trace reporting\n\nCbcHeuristic implementation that reports bound progress\nto GAMS solve trace data structure for performance analysis.", "see": ["GamsCbc.hpp for main GAMS-CBC interface"], "has_pass2": false}, "src/cbc/GamsCbc.hpp": {"path": "layer-4/GAMSlinks/src/cbc/GamsCbc.hpp", "filename": "GamsCbc.hpp", "file": "cbc/GamsCbc.hpp", "brief": "GAMS interface to CBC MIP solver\n\nConnects GAMS modeling environment to COIN-OR CBC.\n\n**GamsCbc Class:**\n- gmo/gev: GAMS modeling object and environment handles\n- setupProblem()/setupParameters(): Configure CBC from GAMS\n- Solve trace support for performance analysis", "see": ["GamsCbcHeurSolveTrace.hpp for MIP heuristic callback"], "has_pass2": false}, "src/scip/reader_gmo.h": {"path": "layer-4/GAMSlinks/src/scip/reader_gmo.h", "filename": "reader_gmo.h", "file": "reader_gmo.h", "brief": "GMO file reader", "author": "Stefan Vigerske", "has_pass2": false}, "src/scip/event_solvetrace.h": {"path": "layer-4/GAMSlinks/src/scip/event_solvetrace.h", "filename": "event_solvetrace.h", "file": "event_solvetrace.h", "brief": "event handler to write GAMS solve trace file", "author": "Stefan Vigerske", "has_pass2": false}, "src/amplsolver/convert_nl.h": {"path": "layer-4/GAMSlinks/src/amplsolver/convert_nl.h", "filename": "convert_nl.h", "return": "buf", "has_pass2": false}, "src/utils/GamsLicensing.h": {"path": "layer-4/GAMSlinks/src/utils/GamsLicensing.h", "filename": "GamsLicensing.h", "return": "True if a GAMS/CPLEX license is available, false otherwise", "has_pass2": false}, "src/utils/GamsSolveTrace.h": {"path": "layer-4/GAMSlinks/src/utils/GamsSolveTrace.h", "filename": "GamsSolveTrace.h", "return": "0, if successful; nonzero, if failure", "has_pass2": false}}}, "Gravity": {"name": "Gravity", "file_count": 27, "pass2_count": 6, "files": {"thirdparty/Ipopt/include/coin/AmplTNLP.hpp": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/AmplTNLP.hpp", "filename": "AmplTNLP.hpp", "return": "a pointer to a char* with the name of the stub", "has_pass2": false}, "thirdparty/Ipopt/include/coin/IpIpoptApplication.hpp": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/IpIpoptApplication.hpp", "filename": "IpIpoptApplication.hpp", "return": "Returns whether non-ipopt exceptions were rethrown before.", "has_pass2": false}, "include/gravity/param.h": {"path": "layer-4/Gravity/include/gravity/param.h", "filename": "param.h", "file": "gravity/param.h", "brief": "Parameters (data constants) indexed over sets\n\nParameters hold fixed data values used in optimization models.\n\n**param_ Base Class:**\n- _name: Parameter name\n- _id: Unique identifier\n- _indices: Index set this parameter is defined over\n- _intype: Internal storage type (double_, integer_, etc.)\n\n**Complex Number Support:**\n- _is_conjugate: Complex conjugate flag\n- _is_sqrmag: Magnitude squared\n- _is_angle: Phase angle\n- _real, _imag: Real/imaginary part pointers\n- _mag, _ang: Magnitude/angle pointers\n\n**Ipopt Integration:**\n- _l_dual, _u_dual: Dual values for bound constraints\n- _off: On/off flags per instance\n\n**param<type> Template Class:**\n- _val: Vector of values\n- _range: Min/max bounds\n- Indexed access via operator()\n\n**Usage:**\n```cpp\nparam<> cost(\"cost\");\ncost.in(Nodes);                    // Index over Nodes\ncost.set_val(node_costs);          // Set values\ncost(\"node1\");                     // Access by key\n```", "see": ["gravity/var.h for variables (params with bounds)", "gravity/constant.h for scalar constants"], "has_pass2": false}, "include/gravity/types.h": {"path": "layer-4/Gravity/include/gravity/types.h", "filename": "types.h", "file": "gravity/types.h", "brief": "Core type definitions and index set infrastructure for Gravity\n\nFoundational enums and classes for the Gravity modeling language.\n\n**Convexity Tracking:**\n- Convexity: linear_, convex_, concave_, undet_\n- Sign: neg_, non_pos_, zero_, non_neg_, pos_, unknown_\n- Enables automatic convexity detection in expressions\n\n**Function Classification:**\n- FType: const_, lin_, quad_, pol_, nlin_ (constraint complexity)\n- MType: lin_m, quad_m, pol_m, nlin_m (model type)\n- OperatorType: All mathematical operations (+, -, *, /, ^, sin, cos, etc.)\n\n**Index Set System (indices class):**\n- Defines sets over which variables/constraints are declared\n- Supports: ordered pairs, time expansion, matrix indexing\n- Operations: union, combine, exclude, filter\n- Used for: \"for all i in Nodes, j in Arcs\" style modeling\n\n**Space Types:**\n- R: Real numbers (R^n)\n- R_p: Positive reals\n- C: Complex numbers", "see": ["gravity/var.h for variable declarations over indices", "gravity/constraint.h for constraints over index sets", "gravity/model.h for combining all components"], "return": "index set of added indices.", "has_pass2": false}, "include/gravity/GurobiProgram.h": {"path": "layer-4/Gravity/include/gravity/GurobiProgram.h", "filename": "GurobiProgram.h", "file": "gravity/GurobiProgram.h", "brief": "Gurobi solver interface for LP/MIP/QP problems\n\nAdapts Gravity models to the Gurobi C++ API.\n\n**GurobiProgram Class:**\n- grb_env: Gurobi environment\n- grb_mod: Gurobi model\n- _grb_vars: Mapping from Gravity vars to GRBVar\n- _model: Pointer to Gravity Model\n\n**Model Building:**\n- prepare_model(): Initial conversion from Gravity\n- fill_in_grb_vmap(): Create GRBVar for each variable\n- create_grb_constraints(): Add constraints (linear/quadratic)\n- set_grb_objective(): Set objective function\n\n**Solving:**\n- solve(relax, mipgap): Optimize with optional LP relaxation\n- relax_model(): Remove integrality constraints\n- update_solution(): Copy solution back to Gravity model\n\n**Supported Problem Types:**\n- LP: Linear programming\n- MIP: Mixed-integer programming\n- QP: Quadratic programming (convex)\n- MIQP: Mixed-integer QP", "see": ["gravity/solver.h for unified solver dispatch", "gravity/model.h for Gravity model definition"], "has_pass2": false}, "include/gravity/poly.h": {"path": "layer-4/Gravity/include/gravity/poly.h", "filename": "poly.h", "file": "gravity/poly.h", "brief": "Polynomial term representations (linear, quadratic, polynomial)\n\nTerm classes for building polynomial expressions.\n\n**lterm (Linear Term):**\n- _coef: Coefficient (constant or param)\n- _p: Variable/parameter pointer\n- _sign: True if positive, false if negative\n- Represents: ±coef × p\n\n**qterm (Quadratic Term):**\n- _coef: Coefficient\n- _p1, _p2: Two variable/parameter pointers\n- _sign: Sign flag\n- Represents: ±coef × p1 × p2\n\n**pterm (Polynomial Term):**\n- _coef: Coefficient\n- _l: List of (param, power) pairs\n- Represents: coef × ∏(pᵢ^kᵢ)\n\n**Usage in Functions:**\n```cpp\nfunc.add_lterm(coef, x);      // coef*x\nfunc.add_qterm(coef, x, y);   // coef*x*y\n```", "see": ["gravity/func.h for functions using these terms", "gravity/expr.h for expression tree nodes"], "has_pass2": false}, "include/gravity/utils.h": {"path": "layer-4/Gravity/include/gravity/utils.h", "filename": "utils.h", "file": "gravity/utils.h", "brief": "Utility functions for timing, printing, and sign operations\n\nGeneral utilities used throughout the Gravity library.\n\n**Timing:**\n- get_wall_time(): Wall clock time in seconds\n- get_cpu_time(): CPU time in seconds\n\n**Printing:**\n- clean_print(): Format value with sign for display\n- RESET macro: Terminal color reset code\n\n**Parallelism:**\n- bounds(parts, mem): Split memory/indices into chunks\n  for parallel processing\n\n**Sign Arithmetic:**\n- reverse(Sign): Negate sign\n- sign_add(s1, s2): Result sign of sum\n- sign_product(s1, s2): Result sign of product\n\n**Time Indexing:**\n- time(p1, p2): Create time-indexed set [p1, p2]\n- time(idx1, ...): Variadic time index construction\n\n**Complex Number Support:**\n- Comparison operators for Cpx type", "see": ["gravity/types.h for Sign enum"], "has_pass2": false}, "include/gravity/constraint.h": {"path": "layer-4/Gravity/include/gravity/constraint.h", "filename": "constraint.h", "file": "gravity/constraint.h", "brief": "Constraint classes with type, duals, and lazy evaluation\n\nConstraints are functions with a constraint type (<=, >=, ==) and bounds.", "algorithm": "Dual Recovery:\nStore Lagrange multipliers from KKT conditions.", "math": "KKT conditions at optimal x*:\n    ∇f(x*) + Σ_i λ_i ∇g_i(x*) = 0\n    λ_i ≥ 0 for g_i(x) ≤ 0 (inequality)\n    λ_i unrestricted for h_i(x) = 0 (equality)\n\n  _dual[i] stores λ for constraint instance i\n  Retrieved via finalize_solution() callback\n\n**Constraint_ Base Class:**\n- _id: Unique constraint identifier\n- _ctype: ConstraintType (leq, geq, eq)\n- _dual: Lagrange multipliers at KKT point\n- _jac_cstr_idx: First index in Jacobian nonzeros\n\n**Lazy Constraint Support:**\n- _all_lazy: Flag if all instances are lazy\n- _lazy: Per-instance lazy flags\n- _violated: Tracks which instances are violated\n- Used for cutting plane methods\n\n**On/Off Constraints:**\n- _onCoef, _offCoef: Coefficient vectors\n- For indicator constraints or big-M reformulations\n\n**Constraint<type> Template:**\n- Inherits from Constraint_ and func<type>\n- Combines function expression with constraint metadata\n\n**Creating Constraints:**\n```cpp\nConstraint<> c(\"flow_balance\", eq);\n// Or via operator overloading:\nauto c = (sum(x) <= 10);  // Creates leq constraint\nauto c = (y == 5);         // Creates eq constraint\n```", "complexity": "Constraint evaluation: O(nnz in expression)\n  Violation check: O(1) after evaluation\n  Lazy separation: O(nb_lazy × eval_cost)", "see": ["gravity/func.h for expression functionality", "gravity/model.h for adding constraints to models"], "return": "True if line search successfully solved\n     The function assumes that the current value stored in vars is the outer point.\n     Interior and outer point classification depends on constraint type (\\geq 0 or \\leq 0) as input by ctype", "has_pass2": true}, "include/gravity/Auxiliary.h": {"path": "layer-4/Gravity/include/gravity/Auxiliary.h", "filename": "Auxiliary.h", "file": "gravity/Auxiliary.h", "brief": "Base class for auxiliary network objects (generators, loads)\n\nProvides a base class for objects attached to network nodes.\n\n**aux Class:**\n- _active: Whether this auxiliary is active in the model\n- _name: Identifier string\n- _phases: Set of electrical phases (for power systems)\n\n**Phase Support:**\n- set_phases(str): Parse phase string (e.g., \"1,2,3\")\n- has_phase(ph): Check if auxiliary has given phase\n\n**Usage in Power Systems:**\n- Generators attached to buses\n- Loads at network nodes\n- Capacitors, transformers, etc.", "see": ["gravity/Net.h for network container", "gravity/Node.h for bus/node representation"], "has_pass2": false}, "include/gravity/expr.h": {"path": "layer-4/Gravity/include/gravity/expr.h", "filename": "expr.h", "file": "gravity/expr.h", "brief": "Expression tree nodes for unary and binary operations\n\nExpressions are the building blocks for functions and constraints.", "algorithm": "Sign Analysis:\nTrack sign of expressions for bound tightening.\n\n  SIGN TYPES: pos_ (≥0), neg_ (≤0), zero_, unknown_\n\n  RULES:\n    x² → pos_ (always non-negative)\n    exp(x) → pos_\n    x*y: pos_*pos_ → pos_, neg_*neg_ → pos_\n         pos_*neg_ → neg_, otherwise unknown_\n\n**expr<type> Base Class:**\n- _coef: Coefficient multiplying expression\n- _all_convexity: Convexity type (linear, convex, concave, undet)\n- _all_sign: Sign type (pos, neg, zero, unknown)\n- _range: (min, max) value bounds\n- _to_str: String representation\n\n**uexpr<type> (Unary Expression):**\n- _otype: Operator type (cos_, sin_, exp_, log_, sqrt_, abs_, etc.)\n- _son: Child expression/constant\n- Used for: sin(x), cos(x), exp(x), log(x), sqrt(x), abs(x)\n\n**bexpr<type> (Binary Expression):**\n- _otype: Operator type (plus_, minus_, product_, div_, power_)\n- _lson, _rson: Left and right children\n- Used for: x+y, x-y, x*y, x/y, x^n\n\n**Expression Tree:**\n- Trees are built via operator overloading\n- Convexity propagates through operations\n- Sign analysis for bound tightening", "math": "Composition rules:\n    f(g(x)) convex if: f convex increasing, g convex\n                    or: f convex decreasing, g concave\n    f + g convex if: both f, g convex\n    α·f convex if: f convex, α > 0\n\n  PROPAGATION:\n    - Leaf: linear (unless nonlinear param)\n    - Unary: lookup table for each operator\n    - Binary: combine based on operator rules\n    - Result: linear_, convex_, concave_, or undet_", "complexity": "Tree construction: O(1) per operator\n  Evaluation: O(nodes) with memoization\n  Convexity analysis: O(nodes) single pass", "see": ["gravity/func.h for complete function class", "gravity/types.h for OperatorType enum"], "has_pass2": true}, "include/gravity/Net.h": {"path": "layer-4/Gravity/include/gravity/Net.h", "filename": "Net.h", "file": "gravity/Net.h", "brief": "Graph/network data structure for power systems and network optimization\n\nRepresents graphs with nodes, arcs, and cycles for network-based models.\n\n**Net Class:**\n- nodes: Vector of Node pointers\n- arcs: All arcs (existing + potential)\n- _exist_arcs: Only existing arcs\n- conting_arcs: Contingency arcs (for N-1 analysis)\n\n**Indexing Structures:**\n- nodeID: Map from node name to Node*\n- arcID: Map from (src_name, dest_name) to Arc*\n- arcMap: Map from arc name to Arc*\n\n**Node Pairs (for SDP relaxations):**\n- _node_pairs: Bus pairs for standard formulation\n- _node_pairs_chord: Bus pairs in chordal completion\n\n**Cycle Basis (for Kirchhoff constraints):**\n- cycle_basis: Vector of Path* forming basis\n- horton_net: Subnetwork for Horton algorithm\n\n**Tree Decomposition:**\n- _bags: Cliques from chordal extension\n- Used for SDP rank-1 constraints\n\n**Graph Algorithms:**\n- clone(), clone_undirected(): Copy graphs\n- Cycle detection, shortest paths\n- Chordal completion", "see": ["gravity/Node.h for node data", "gravity/Arc.h for arc/edge data", "gravity/Path.h for paths/cycles"], "return": "the id of the node removed", "has_pass2": false}, "include/gravity/IpoptProgram.h": {"path": "layer-4/Gravity/include/gravity/IpoptProgram.h", "filename": "IpoptProgram.h", "file": "gravity/IpoptProgram.h", "brief": "Ipopt solver interface implementing TNLP callbacks\n\nAdapts Gravity models to Ipopt's TNLP (Templated NLP) interface.\n\n**IpoptProgram<type> Class:**\n- Inherits from Ipopt::TNLP and Program<type>\n- _model: Pointer to Gravity Model\n\n**Required TNLP Callbacks:**\n- get_nlp_info(): Return problem dimensions (n, m, nnz_jac, nnz_hess)\n- get_bounds_info(): Variable and constraint bounds\n- get_starting_point(): Initial x, z_L, z_U, lambda\n- eval_f(): Objective function value\n- eval_grad_f(): Objective gradient\n- eval_g(): Constraint values\n- eval_jac_g(): Jacobian values and structure\n- eval_h(): Hessian of Lagrangian\n- finalize_solution(): Copy solution back to model\n\n**Sparsity Pattern:**\n- First call to eval_jac_g/eval_h: return structure (iRow, jCol)\n- Subsequent calls: return values only\n- Gravity tracks via _first_call_jac, _first_call_hess\n\n**Solution Recovery:**\n- finalize_solution() copies x values to model variables\n- Retrieves dual values (lambda) for constraints\n- Retrieves bound multipliers (z_L, z_U)", "algorithm": "Interior Point Method via Ipopt", "see": ["gravity/solver.h for unified solver interface", "layer-2/Ipopt/ for Ipopt implementation details"], "has_pass2": true}, "include/gravity/rapidcsv.h": {"path": "layer-4/Gravity/include/gravity/rapidcsv.h", "filename": "rapidcsv.h", "brief": "Datastructure holding parameters controlling how invalid numbers (including\n           empty strings) should be handled.", "param": ["pHasDefaultConverter  specifies if conversion of non-numerical strings shall be\n                               converted to a default numerical value, instead of causing\n                               an exception to be thrown (default).", "pDefaultFloat         floating-point default value to represent invalid numbers.", "pDefaultInteger       integer default value to represent invalid numbers.", "pConverterParams      specifies how conversion of non-numerical values to\n                               numerical datatype shall be handled.", "pVal                  numerical value", "pStr                  output string", "pVal                  numerical value", "pStr                  output string", "pVal                  string", "pStr                  string", "pVal                  string", "pStr                  string", "pColumnNameIdx        specifies the zero-based row index of the column labels, setting\n                               it to -1 prevents column lookup by label name, and gives access\n                               to all rows as document data.", "pRowNameIdx           specifies the zero-based column index of the row labels, setting\n                               it to -1 prevents row lookup by label name, and gives access\n                               to all columns as document data.", "pSeparator            specifies the column separator (default ',').", "pTrim                 specifies whether to trim leading and trailing spaces from\n                               cells read.", "pHasCR                specifies whether a new document (i.e. not an existing document read)\n                               should use CR/LF instead of only LF (default is to use standard\n                               behavior of underlying platforms - CR/LF for Win, and LF for others).", "pPath                 specifies the path of an existing CSV-file to populate the Document\n                               data with.", "pLabelParams          specifies which row and column should be treated as labels.", "pSeparatorParams      specifies which field and row separators should be used.", "pConverterParams      specifies how invalid numbers (including empty strings) should be\n                               handled.", "pStream               specifies an input stream to read CSV data from.", "pLabelParams          specifies which row and column should be treated as labels.", "pSeparatorParams      specifies which field and row separators should be used.", "pConverterParams      specifies how invalid numbers (including empty strings) should be\n                               handled.", "pDocument             specifies the Document instance to copy.", "pPath                 specifies the path of an existing CSV-file to populate the Document\n                               data with.", "pPath                 optionally specifies the path where the CSV-file will be created\n                               (if not specified, the original path provided when creating or\n                               loading the Document data will be used).", "pStream               specifies an output stream to write the data to.", "pColumnIdx            zero-based column index.", "pColumnName           column label name.", "pColumnIdx            zero-based column index.", "pColumn               vector of column data.", "pColumnName           column label name.", "pColumn               vector of column data.", "pColumnIdx            zero-based column index.", "pColumnName           column label name.", "pRowIdx               zero-based row index.", "pRowName              row label name.", "pRowIdx               zero-based row index.", "pRow                  vector of row data.", "pRowName              row label name.", "pRow                  vector of row data.", "pRowIdx               zero-based row index.", "pRowName              row label name.", "pColumnIdx            zero-based column index.", "pRowIdx               zero-based row index.", "pColumnName           column label name.", "pRowName              row label name.", "pColumnName           column label name.", "pRowIdx               zero-based row index.", "pColumnIdx            zero-based column index.", "pRowName              row label name.", "pRowIdx               zero-based row index.", "pColumnIdx            zero-based column index.", "pCell                 cell data.", "pColumnName           column label name.", "pRowName              row label name.", "pCell                 cell data.", "pColumnIdx            zero-based column index.", "pColumnIdx            zero-based column index.", "pColumnName           column name.", "pRowIdx               zero-based column index.", "pRowIdx               zero-based row index.", "pRowName              row name."], "has_pass2": false}, "include/gravity/func.h": {"path": "layer-4/Gravity/include/gravity/func.h", "filename": "func.h", "file": "gravity/func.h", "brief": "Expression functions with automatic differentiation and convexity tracking\n\nThe func class represents mathematical expressions with symbolic analysis.", "algorithm": "Convexity Analysis (DCP Rules):\nDisciplined Convex Programming rules propagate convexity through expressions.\n\nATOMIC CONVEXITIES:\n  linear: ax + b                     → linear\n  convex: x², |x|, exp(x), -log(x)   → convex\n  concave: log(x), sqrt(x), -x²      → concave\n\nCOMPOSITION RULES:\n  convex(linear)   → convex         exp(ax+b) is convex\n  concave(linear)  → concave        log(ax+b) is concave (a>0)\n  convex(convex)   → convex         exp(x²) is convex\n  convex(concave)  → unknown        exp(log(x)) needs analysis\n\nSIGN PROPAGATION (for multiplication):\n  pos × convex   → convex\n  neg × convex   → concave\n  pos × concave  → concave\n  neg × concave  → convex\n\nQUADRATIC ANALYSIS:\n  Σᵢⱼ aᵢⱼxᵢxⱼ is convex iff matrix A is positive semidefinite\n  For single variable: ax² convex iff a ≥ 0", "math": "For multivariate f(x), convexity requires ∇²f(x) ≽ 0 (PSD Hessian).\n  Gravity uses composition rules to determine convexity without computing Hessian.\n\n**func_ Base Class:**\n- _ftype: Function type (const_, lin_, quad_, pol_, nlin_)\n- _return_type: Numeric type (double_, integer_, binary_, complex_)\n- _all_convexity: Convexity of all instances (linear_, convex_, concave_, undet_)\n- _all_sign: Sign of all instances (pos_, neg_, zero_, unknown_)\n\n**Expression Components:**\n- _cst: Constant term\n- _lterms: Linear terms (map<string, lterm>)\n- _qterms: Quadratic terms (map<string, qterm>)\n- _pterms: Polynomial terms (map<string, pterm>)\n- _vars: Variables appearing in expression\n- _params: Parameters appearing in expression\n\n**Derivative Information:**\n- Symbolic differentiation via get_stored_derivative()\n- _hess_link: Sparsity pattern of Hessian\n- _nnz_j, _nnz_h: Jacobian/Hessian nonzeros\n\n**Operator Overloading:**\n- Arithmetic: +, -, *, /, ^\n- Transcendental: sin, cos, exp, log, sqrt\n- Comparison: <, <=, ==, >=, > (for constraints)\n\n**Convexity Rules:**\n- sum(linear) → linear\n- x² → convex, -x² → concave\n- exp(linear) → convex, log(linear) → concave\n- Composition rules applied automatically", "complexity": "- Derivative computation: O(expression_size) per variable\n- Sparsity detection: O(vars × expression_depth)\n- Convexity analysis: O(expression_size)", "ref": ["Grant, Boyd & Ye (2006). \"Disciplined Convex Programming\".\n  Global Optimization: From Theory to Implementation, pp. 155-210."], "see": ["gravity/expr.h for expression tree nodes", "gravity/constraint.h for constraints built from functions"], "return": "a vector of monomials of degree d using the variables in the current function", "has_pass2": true}, "include/gravity/constant.h": {"path": "layer-4/Gravity/include/gravity/constant.h", "filename": "constant.h", "file": "gravity/constant.h", "brief": "Scalar and vector constants with type tracking\n\nBase constant class and typed constant templates for numeric values.\n\n**constant_ Base Class:**\n- _type: CType enum (binary_c, integer_c, double_c, complex_c, etc.)\n- _is_transposed: Transpose flag for matrix operations\n- _is_vector: True if vector/matrix\n- _dim[2]: Dimensions (rows, cols)\n- _polar: Complex polar representation flag\n\n**CType Classification:**\n- Scalar types: binary_c, short_c, integer_c, float_c, double_c, long_c\n- Special types: par_c (param), var_c (variable), func_c (function)\n- Expression types: uexp_c (unary), bexp_c (binary)\n\n**constant<type> Template:**\n- Holds single typed value\n- Implicit conversion from numeric types\n- String conversion with precision\n\n**Helper Functions:**\n- to_string_with_precision(): Format numbers\n- unit<type>(): Multiplicative identity (1)\n- zero<type>(): Additive identity (0)\n\n**Complex Support:**\n- Cpx typedef for complex<double>\n- Polar/rectangular representation", "see": ["gravity/param.h for indexed parameters", "gravity/var.h for optimization variables"], "return": "a string with the specified precision.", "has_pass2": false}, "include/gravity/CplexProgram.h": {"path": "layer-4/Gravity/include/gravity/CplexProgram.h", "filename": "CplexProgram.h", "file": "gravity/CplexProgram.h", "brief": "IBM CPLEX solver interface for LP/MIP/QP problems\n\nAdapts Gravity models to the IBM ILOG CPLEX C++ API.\n\n**CplexProgram Class:**\n- _cplex_env: IloEnv (CPLEX environment)\n- _cplex_model: IloModel (CPLEX model)\n- _cplex_vars: Mapping from Gravity vars to IloNumVarArray\n\n**Model Building:**\n- prepare_model(): Initial conversion from Gravity\n- fill_in_cplex_vars(): Create IloNumVar for each variable\n- create_cplex_constraints(): Add constraints\n- set_cplex_objective(): Set objective function\n\n**Solving:**\n- solve(relax, mipgap): Optimize with optional LP relaxation\n- warm_start(): Initialize from current solution\n- relax_model(): Remove integrality constraints\n\n**Callback Support:**\n- _cplex_contextmask: Context for callback location\n- create_callback(): Set up user callbacks", "see": ["gravity/solver.h for unified solver dispatch", "gravity/model.h for Gravity model definition"], "has_pass2": false}, "include/gravity/Arc.h": {"path": "layer-4/Gravity/include/gravity/Arc.h", "filename": "Arc.h", "file": "gravity/Arc.h", "brief": "Directed arc/edge for network optimization models\n\nRepresents an arc (edge) connecting two nodes in a graph.\n\n**Arc Class:**\n- _id: Numeric identifier\n- _name: String name\n- _src, _dest: Source and destination Node pointers\n- _weight: Arc weight/cost\n- _len: Arc length\n\n**Power System Attributes:**\n- _is_transformer: True if arc represents a transformer\n- _phases: Set of phases (for 3-phase systems)\n- _parallel: True if parallel arc exists\n\n**Planning Attributes:**\n- _active: Whether arc is active in model\n- _expansion: True if potential expansion arc\n- _imaginary: Imaginary arc for algorithm use\n\n**Graph Algorithm Support:**\n- in_cycle: True if arc is in a cycle\n- horton_path: Pointer to Horton path through this arc\n- _intersection: Common neighbors of src and dest\n- _intersection_clique: For clique tree algorithms\n\n**Methods:**\n- neighbour(Node* n): Get other endpoint\n- clone(): Deep copy arc", "see": ["gravity/Node.h for node data", "gravity/Path.h for paths/cycles", "gravity/Net.h for graph container"], "has_pass2": false}, "include/gravity/MosekProgram.h": {"path": "layer-4/Gravity/include/gravity/MosekProgram.h", "filename": "MosekProgram.h", "file": "gravity/MosekProgram.h", "brief": "Mosek Fusion API interface for conic optimization\n\nAdapts Gravity models to Mosek's Fusion API for conic programming.\n\n**MosekProgram Class:**\n- _mosek_model: Mosek Fusion Model\n- _mosek_vars: Vector of Mosek Variable::t handles\n\n**Conic Capabilities:**\n- Second-order cone (SOCP)\n- Semidefinite programming (SDP)\n- Exponential cone\n\n**Expression Building:**\n- form_Fx(qterms): Build quadratic matrix form F*x\n- create_lin_expr(lterms, cst): Linear expression construction\n\n**Model Building:**\n- fill_in_mosek_vars(): Create Mosek variables\n- create_mosek_constraints(): Add conic constraints\n- set_mosek_objective(): Set objective\n\n@note Mosek Fusion is designed specifically for conic optimization,\n      making it ideal for SOCP and SDP problems in power systems.", "see": ["gravity/solver.h for unified solver dispatch"], "has_pass2": false}, "include/gravity/solver.h": {"path": "layer-4/Gravity/include/gravity/solver.h", "filename": "solver.h", "file": "gravity/solver.h", "brief": "Unified solver interface for multiple optimization backends\n\nDispatches Gravity models to various solver implementations.", "algorithm": "Warm Starting:\nReuse previous solution for faster convergence.\n\n  For NLP (Ipopt):\n    - Primal: x_init = previous x*\n    - Dual: lambda_init = previous lambda*\n    - Bound multipliers: z_L, z_U from previous solve\n\n  For LP/MIP:\n    - Basis information (if available)\n    - MIP start from previous incumbent\n\n**Supported Solvers (via SolverType enum):**\n- ipopt: Interior point NLP (nonlinear)\n- gurobi: Commercial LP/QP/MIP\n- cplex: Commercial LP/QP/MIP\n- bonmin: Open-source MINLP (NLP-based B&B)\n- mosek: Commercial conic/SDP\n- clp: COIN-OR simplex LP\n- highs: Open-source LP/MIP\n- sdpa: SDP solver\n\n**solver<type> Template Class:**\n- _model: Pointer to Gravity Model\n- _prog: Solver-specific program adapter\n- _stype: Which solver to use\n- _tol: Solver tolerance (default 1e-6)\n\n**Solver Options:**\n- set_option(string, string): String options\n- set_option(string, int): Integer options\n- set_option(string, double): Numeric options\n\n**Usage:**\n```cpp\nModel<> m;\n// ... build model ...\nsolver<> s(m, ipopt);\ns.run();\n```\n\n**Conditional Compilation:**\n- USE_IPOPT, USE_GUROBI, USE_CPLEX, etc.\n- SolverNotAvailable() throws if solver not compiled in", "see": ["gravity/model.h for model building", "gravity/IpoptProgram.h for Ipopt adapter", "gravity/GurobiProgram.h for Gurobi adapter"], "has_pass2": true}, "include/gravity/var.h": {"path": "layer-4/Gravity/include/gravity/var.h", "filename": "var.h", "file": "gravity/var.h", "brief": "Decision variables with bounds for optimization models\n\nVariables are parameters with lower and upper bounds that the solver optimizes.\n\n**var<type> Template Class:**\n- Inherits from param<type> (shares indexing, values storage)\n- Adds _lb, _ub as func<type> (bounds can be expressions)\n- Supports: double, int, bool (binary), complex<double>\n\n**Bound Types:**\n- Unbounded: var(\"x\") - uses numeric_limits\n- Non-negative: var(\"x\", non_neg_)\n- Non-positive: var(\"x\", non_pos_)\n- Bounded: var(\"x\", lb, ub)\n- Indexed bounds: var(\"x\", lb_param, ub_param) over indices\n\n**Lifted Variables:**\n- _lift flag for McCormick/RLT linearization\n- _original_vars: pointers to variables being lifted\n- _lift_lb, _lift_ub: whether lifted var needs bounding functions\n\n**Mosek/SDP Support:**\n- _in_q_cone: variable in quadratic cone\n- _psd: positive semidefinite matrix variable\n\n**Usage Example:**\n```cpp\nvar<> x(\"x\", 0, 1);           // x in [0,1]\nvar<int> y(\"y\", 0, 10);       // integer y in [0,10]\nvar<bool> z(\"z\");             // binary z\nx.in(Nodes);                   // x indexed over Nodes\n```", "see": ["gravity/param.h for base parameter class", "gravity/model.h for adding variables to models"], "return": "index set of added indices.", "has_pass2": false}, "include/gravity/model.h": {"path": "layer-4/Gravity/include/gravity/model.h", "filename": "model.h", "file": "gravity/model.h", "brief": "Main optimization model class combining variables, constraints, objective\n\nThe Model class is the central container for building optimization problems.", "algorithm": "Automatic Model Classification:\nProblem type determined by analyzing expressions:\n\n  MType = lin_m   iff all constraints/objective are linear\n  MType = quad_m  iff max degree = 2 (quadratic)\n  MType = pol_m   iff polynomial with degree > 2\n  MType = nlin_m  iff contains transcendental functions\n\nSolver selection based on MType:\n  lin_m  -> CLP, Gurobi LP, CPLEX LP, HiGHS\n  quad_m -> Gurobi QP, CPLEX QP, MOSEK\n  nlin_m -> Ipopt, Bonmin (with integers)\n\n**Model<type> Template Class:**\n- type: Numeric type (default double)\n- _vars: Map of all variables\n- _cons: Map of all constraints\n- _obj: Objective function\n- _objt: Objective type (minimize/maximize)\n\n**Building a Model:**\n```cpp\nModel<> m(\"MyModel\");\nvar<> x(\"x\", 0, 10);              // Variable x in [0,10]\nm.add(x.in(Nodes));                // Add indexed variable\nm.add(sum(x) <= 100);              // Add constraint\nm.min(sum(cost*x));                // Set objective\n```\n\n**Constraint Handling:**\n- Indexed constraints: one func<> generates multiple rows\n- Lazy constraints via _lazy vector\n- Parallel constraint evaluation (compute_constrs)\n- Parallel Jacobian computation (compute_jac)\n\n**Derivative Computation:**\n- Automatic symbolic differentiation\n- Jacobian sparsity pattern (_jac_cstr_idx)\n- Hessian sparsity pattern (_hess_link)\n\n**Model Types (MType):**\n- lin_m: Linear (LP/MIP)\n- quad_m: Quadratic (QP/MIQP)\n- pol_m: Polynomial\n- nlin_m: General nonlinear (NLP/MINLP)\n\n**I/O Support:**\n- MPS format (via CoinUtils)\n- NL format (AMPL, via mp library)", "complexity": "- Constraint evaluation: O(nnz_constraints / num_threads)\n- Jacobian evaluation: O(nnz_jacobian / num_threads)\n- Hessian evaluation: O(nnz_hessian / num_threads)", "see": ["gravity/solver.h for solving models", "gravity/constraint.h for constraint types"], "return": "the linearized constraint\n         @note This function will add constraints linking the lifted variables to the original ones, if a variable's partition is greater than 1, it will also add the disjunctive constraints corresponding to the partitionning of the variables.", "has_pass2": true}, "include/gravity/Path.h": {"path": "layer-4/Gravity/include/gravity/Path.h", "filename": "Path.h", "file": "gravity/Path.h", "brief": "Graph path/cycle representation for network algorithms\n\nRepresents a sequence of nodes forming a path or cycle in a network.\n\n**Path Class:**\n- nodes: List of Node pointers in sequence\n\n**Methods:**\n- source_dest(n1, n2): Check if path connects n1 to n2\n- length(): Number of edges in path\n- cycle(): True if path is a closed cycle\n- clone(): Deep copy path\n- to_str(): String representation\n\n**Usage in Gravity:**\n- cycle_basis in Net: Fundamental cycles for Kirchhoff constraints\n- horton_path in Arc: Horton algorithm shortest paths\n- Power flow loop constraints", "see": ["gravity/Net.h for graph container", "gravity/Arc.h for arc/edge data"], "has_pass2": false}, "include/gravity/Node.h": {"path": "layer-4/Gravity/include/gravity/Node.h", "filename": "Node.h", "file": "gravity/Node.h", "brief": "Graph node/vertex for network optimization models\n\nRepresents a node in network graphs (power systems, transportation, etc.).\n\n**Node Class:**\n- _name: Unique string identifier\n- _id: Numeric ID within container\n- _type_name: \"Nodes\" (for indexing)\n- _active: Whether node is active in model\n\n**Connectivity:**\n- branches: Vector of incident Arc pointers\n- degree(): Number of incident arcs\n- addArc(), removeArc(): Modify adjacency\n\n**Graph Algorithms:**\n- explored: BFS/DFS traversal flag\n- cycle: True if node is in a cycle\n- predecessor: Parent in BFS tree\n- distance: Distance from source in BFS\n- fill_in: Edges needed to make neighbors a clique\n\n**Power System Extensions:**\n- _phases: Set of phases (for 3-phase systems)", "see": ["gravity/Arc.h for edge/arc data", "gravity/Net.h for graph container"], "has_pass2": false}, "include/gravity/HiGHSProgram.h": {"path": "layer-4/Gravity/include/gravity/HiGHSProgram.h", "filename": "HiGHSProgram.h", "file": "gravity/HiGHSProgram.h", "brief": "HiGHS solver interface for LP/MIP problems\n\nAdapts Gravity models to the HiGHS open-source LP/MIP solver.\n\n**HiGHSProgram Class:**\n- Highs_mod: HighsModel (HiGHS model container)\n- Highs_inst: Highs solver instance\n\n**Model Building:**\n- prepare_model(): Initial conversion from Gravity\n- fill_in_var_map(): Create variable mappings\n- create_constraints(): Add linear constraints\n- set_objective(): Set linear objective\n\n**Solving:**\n- solve(relax, mipgap): Optimize with optional LP relaxation\n- update_solution(): Copy solution back to Gravity model\n- relax_model(): Remove integrality constraints\n\n**Supported Problem Types:**\n- LP: Linear programming\n- MIP: Mixed-integer programming\n\n@note HiGHS is an open-source solver that excels at large-scale LP\n      and MIP problems with state-of-the-art performance.", "see": ["gravity/solver.h for unified solver dispatch"], "has_pass2": false}, "thirdparty/mp/presolve.h": {"path": "layer-4/Gravity/thirdparty/mp/presolve.h", "filename": "presolve.h", "file": "presolve.h\n  Implementation of value presolver", "has_pass2": false}, "thirdparty/Ipopt/include/coin/PardisoLoader.h": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/PardisoLoader.h", "filename": "PardisoLoader.h", "param": ["libname The name under which the Pardiso lib can be found, or NULL to use a default name (libpardiso.SHAREDLIBEXT).", "msgbuf A buffer where we can store a failure message. Assumed to be NOT NULL!", "msglen Length of the message buffer."], "return": "Zero on success, nonzero on failure.", "has_pass2": false}, "thirdparty/Ipopt/include/coin/HSLLoader.h": {"path": "layer-4/Gravity/thirdparty/Ipopt/include/coin/HSLLoader.h", "filename": "HSLLoader.h", "see": ["LSL_isMA27available", "LSL_isMA28available", "LSL_isMA57available", "LSL_isMA77available", "LSL_isMA86available", "LSL_isMA97available", "LSL_isMC19available"], "param": ["libname The name under which the HSL lib can be found, or NULL to use a default name (libhsl.SHAREDLIBEXT).", "msgbuf A buffer where we can store a failure message. Assumed to be NOT NULL!", "msglen Length of the message buffer."], "return": "Zero on success, nonzero on failure.", "has_pass2": false}}}, "HiGHS": {"name": "HiGHS", "file_count": 167, "pass2_count": 30, "files": {"extern/catch.hpp": {"path": "layer-4/HiGHS/extern/catch.hpp", "filename": "catch.hpp", "file": "catch.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "app/CLI11.hpp": {"path": "layer-4/HiGHS/app/CLI11.hpp", "filename": "CLI11.hpp", "file": "CLI11.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "check/matrix_multiplication.hpp": {"path": "layer-4/HiGHS/check/matrix_multiplication.hpp", "filename": "matrix_multiplication.hpp", "file": "matrix_multiplication.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/devexpricing.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/devexpricing.hpp", "filename": "devexpricing.hpp", "file": "devexpricing.hpp", "brief": "HiGHS Devex pricing for QP active set method\n\nDevex pricing strategy: approximate steepest edge using\nreference framework with periodic weight updates.", "algorithm": "Devex (Approximate Steepest Edge) for QP:\n  Maintains approximate edge weights for efficient pricing:\n  1. Initialize weights w_i = 1 for all constraints\n  2. Select constraint s = argmax_i { λ_i² / w_i } with λ_i > threshold\n  3. After active set change, update: w_s' = actual weight, w_i' = max(w_i, update²)\n  4. Periodically reset weights to 1 (reference framework restart)", "math": "Devex weight update (Harris, 1973):\n  Entering constraint gets exact weight: w_s' = ||∇_s h(x)||²\n  Other active: w_i' = max(w_i, (γ_i/γ_s)² · w_s')\n  where γ is the pivot element contribution.\n  Approximate weights converge to exact weights over iterations.", "complexity": "O(|active_set|) per pricing operation.\n  Weight update: O(|active_set|) per iteration (vs O(m²) for exact).\n  Typically 80-90% as effective as steepest edge at lower cost.", "ref": ["Harris (1973). \"Pivot selection methods of the Devex LP code\".\n     Mathematical Programming 5:1-28."], "see": ["Pricing for the base pricing interface", "SteepestEdgePricing for exact weights (fewer iterations but more expensive)"], "has_pass2": true}, "highs/qpsolver/pricing.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/pricing.hpp", "filename": "pricing.hpp", "file": "pricing.hpp", "brief": "HiGHS QP pricing base class\n\nAbstract pricing strategy interface for QP active set method.\nSelects entering variable for basis change.", "has_pass2": false}, "highs/qpsolver/reducedcosts.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/reducedcosts.hpp", "filename": "reducedcosts.hpp", "file": "reducedcosts.hpp", "brief": "HiGHS QP reduced costs\n\nReduced cost computation for QP active set algorithm.\nManages gradient projection onto feasible directions.", "has_pass2": false}, "highs/qpsolver/factor.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/factor.hpp", "filename": "factor.hpp", "file": "factor.hpp", "brief": "HiGHS QP reduced Hessian Cholesky factorization\n\nBasis factorization for QP solver. Cholesky decomposition\nof reduced Hessian Z'QZ for efficient direction computation.", "algorithm": "Cholesky Factorization for Reduced Hessian:\n  Maintains L L' = Z'QZ for null-space QP direction computation:\n  1. Factor reduced Hessian: Z'QZ = LL' (dense Cholesky)\n  2. Solve for direction: LL'p_Z = -Z'g → p_Z\n  3. Update L when active set changes (rank-1 updates)\n  4. Detect negative eigenvalues (non-convex warning)", "math": "Cholesky update formulas:\n  Adding constraint: Delete row/col from L, shift remaining\n  Dropping constraint: Rank-1 update L'L' = L'L + vv' via Givens rotations\n  Solve: Forward solve Ly = b, backward solve L'x = y", "complexity": "Factor: O(k³) for k = n - |active_set| (null-space dimension)\n  Solve: O(k²)\n  Update: O(k²) per constraint change", "ref": ["Nocedal & Wright, \"Numerical Optimization\", Ch. 16.5 (active set)"], "see": ["Basis for active set tracking", "HFactor for constraint matrix factorization"], "has_pass2": true}, "highs/qpsolver/a_quass.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/a_quass.hpp", "filename": "a_quass.hpp", "file": "a_quass.hpp", "brief": "HiGHS QUASS QP algorithm\n\nQUASS (QUadratic Active Set Solver) main algorithm.\nPrimal active set method for convex QP.", "algorithm": "QUASS (Quadratic Active Set Solver):\n  Solves convex QP: min ½x'Qx + c'x s.t. Ax ≤ b using active set method:\n  1. Start with feasible point and initial active set (working set)\n  2. Solve equality-constrained QP on current active set for direction p\n  3. If p = 0: compute Lagrange multipliers λ\n     - If all λ correct sign: optimal\n     - Else: drop constraint with most negative λ, goto 2\n  4. If p ≠ 0: ratio test to find step length α\n     - If α = ∞: unbounded\n     - Else: x ← x + α·p, add blocking constraint to active set, goto 2", "math": "KKT system for active set A:\n  [Q  A'] [p ]   [-g]\n  [A  0 ] [λ ] = [0 ]\n  where g = Qx + c is gradient. Solve via basis factorization.", "complexity": "O(m·n²) per iteration for dense problems.\n  Iteration count varies widely; finite for convex QP.\n  Warm start from LP basis dramatically reduces iterations.", "ref": ["Goldfarb & Idnani (1983). \"A numerically stable dual method for solving\n     strictly convex quadratic programs\". Math. Programming 27:1-33."], "see": ["Settings for algorithm parameters", "Statistics for performance metrics"], "has_pass2": true}, "highs/qpsolver/feasibility_highs.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/feasibility_highs.hpp", "filename": "feasibility_highs.hpp", "file": "feasibility_highs.hpp", "brief": "HiGHS QP feasibility restoration\n\nFeasibility restoration using HiGHS LP solver.\nHandles infeasible iterates during QP solving.", "has_pass2": false}, "highs/qpsolver/devexharrispricing.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/devexharrispricing.hpp", "filename": "devexharrispricing.hpp", "file": "devexharrispricing.hpp", "brief": "HiGHS Devex-Harris hybrid pricing\n\nCombined Devex and Harris ratio test for QP pricing.\nBalances accuracy and numerical stability.", "has_pass2": false}, "highs/qpsolver/matrix.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/matrix.hpp", "filename": "matrix.hpp", "file": "matrix.hpp", "brief": "HiGHS QP matrix operations\n\nSparse matrix utilities for QP solver. Row/column access,\nproducts, and basis matrix maintenance.", "has_pass2": false}, "highs/qpsolver/steepestedgepricing.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/steepestedgepricing.hpp", "filename": "steepestedgepricing.hpp", "file": "steepestedgepricing.hpp", "brief": "HiGHS steepest edge pricing for QP active set method\n\nSteepest edge pricing for QP active set. Exact edge weights\nfor optimal variable selection (more expensive than Devex).", "algorithm": "Steepest Edge Pricing for QP Active Set:\n  Selects constraint to drop/activate based on normalized multipliers:\n  1. Maintain exact weights: w_i = ||∇_i h(x)||² for each active constraint\n  2. Select constraint s = argmax_i { λ_i² / w_i } with λ_i > threshold\n  3. Only consider constraints that can be dropped (sign-correct multipliers)\n  4. Update weights when active set changes using basis factor information", "math": "Steepest edge selection criterion:\n  For active constraint i with multiplier λ_i:\n  score_i = λ_i² / w_i where w_i = ||gradient||² in reduced space\n  This normalizes multipliers by constraint \"difficulty\" to move.", "complexity": "O(|active_set|) per pricing operation.\n  Weight maintenance: O(m²) per active set update for exact weights.\n  More expensive than Devex but typically fewer iterations.", "ref": ["Harris (1973). \"Pivot selection methods of the Devex LP code\".\n     Mathematical Programming 5:1-28."], "see": ["Pricing for the base pricing interface", "DevexPricing for approximate steepest edge (faster per iteration)"], "has_pass2": true}, "highs/qpsolver/eventhandler.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/eventhandler.hpp", "filename": "eventhandler.hpp", "file": "eventhandler.hpp", "brief": "HiGHS QP event handler\n\nCallback interface for QP solver events. Iteration callbacks,\nlogging, and early termination hooks.", "has_pass2": false}, "highs/qpsolver/basis.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/basis.hpp", "filename": "basis.hpp", "file": "basis.hpp", "brief": "HiGHS QP basis management for active set method\n\nWorking set (active set) management for QP solver.\nTracks which constraints are currently active.", "algorithm": "QP Working Set (Active Set) Management:\n  Maintains active constraint set W and provides efficient operations:\n  1. Track constraint status: inactive, active at lower, active at upper\n  2. Null-space operations: Z'p products for reduced Hessian\n  3. Range-space operations: Y'b products for constraint restoration\n  4. Basis updates when constraints added/removed (HFactor integration)", "math": "Null-space basis decomposition:\n  For active constraints A_W x = b_W, decompose:\n  A_W = [Y | Z] where Y spans range(A_W'), Z spans null(A_W)\n  Direction p = Zp_Z + Yp_Y with p_Z from reduced problem\n  Z'QZ is the reduced Hessian for null-space direction computation.", "complexity": "Active set update: O(m²) for basis refactorization.\n  Z'v product: O(m·k) where k = |active set|.\n  Uses HFactor for efficient LU updates.", "see": ["HFactor for the underlying LU factorization", "CholeskyFactor for reduced Hessian factorization"], "has_pass2": true}, "highs/qpsolver/reducedgradient.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/reducedgradient.hpp", "filename": "reducedgradient.hpp", "file": "reducedgradient.hpp", "brief": "HiGHS QP reduced gradient", "algorithm": "Reduced Gradient for QP Active Set", "math": "For active constraints Ax_A = b, compute gradient in null space:\n      ∇_Z f(x) = Z'∇f(x) where Z spans null(A) for active constraints.\n      When ∇_Z f(x) = 0, KKT conditions satisfied for current active set.\n      Non-zero reduced gradient → direction of descent exists.\n\nReduced gradient computation for active set method.\nProjects objective gradient onto null space of active constraints.\n\n**Key operations:**\n- recompute(): Full Z'∇f computation via basis\n- reduce(): Update when constraint added to active set\n- expand(): Update when constraint dropped from active set\n- update(): Scale after step (minor) or invalidate (major)", "complexity": "O(n × |inactive|) for full computation.\nIncremental updates: O(|inactive|) per active set change.", "has_pass2": true}, "highs/qpsolver/perturbation.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/perturbation.hpp", "filename": "perturbation.hpp", "file": "perturbation.hpp", "brief": "HiGHS QP perturbation\n\nPerturbation for handling degeneracy in QP.\nAdds small shifts to ensure unique optima.", "has_pass2": false}, "highs/qpsolver/feasibility_bounded.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/feasibility_bounded.hpp", "filename": "feasibility_bounded.hpp", "file": "feasibility_bounded.hpp", "brief": "HiGHS QP bounded feasibility\n\nBounded feasibility subroutine for QP.\nHandles box constraints during active set updates.", "has_pass2": false}, "highs/qpsolver/qpvector.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/qpvector.hpp", "filename": "qpvector.hpp", "file": "qpvector.hpp", "brief": "HiGHS QP vector utilities\n\nDense vector operations for QP solver.\nBasic linear algebra with QP-specific optimizations.", "has_pass2": false}, "highs/qpsolver/settings.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/settings.hpp", "filename": "settings.hpp", "file": "settings.hpp", "brief": "HiGHS QP solver settings\n\nParameter settings for QUASS QP solver.\nTolerances, iteration limits, and algorithm options.", "has_pass2": false}, "highs/qpsolver/snippets.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/snippets.hpp", "filename": "snippets.hpp", "file": "snippets.hpp", "brief": "HiGHS QP code snippets\n\nUtility code snippets for QP solver implementation.\nCommon patterns and helper macros.", "has_pass2": false}, "highs/qpsolver/crashsolution.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/crashsolution.hpp", "filename": "crashsolution.hpp", "file": "crashsolution.hpp", "brief": "HiGHS QP crash solution\n\nInitial solution heuristic for QP solver.\nGenerates starting point for active set method.", "has_pass2": false}, "highs/qpsolver/runtime.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/runtime.hpp", "filename": "runtime.hpp", "file": "runtime.hpp", "brief": "HiGHS QP runtime data\n\nRuntime state for QUASS algorithm.\nStores iteration counts, timers, and status.", "has_pass2": false}, "highs/qpsolver/gradient.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/gradient.hpp", "filename": "gradient.hpp", "file": "gradient.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/qpconst.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/qpconst.hpp", "filename": "qpconst.hpp", "file": "qpconst.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/a_asm.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/a_asm.hpp", "filename": "a_asm.hpp", "file": "a_asm.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/quass.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/quass.hpp", "filename": "quass.hpp", "file": "quass.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/statistics.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/statistics.hpp", "filename": "statistics.hpp", "file": "statistics.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/dantzigpricing.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/dantzigpricing.hpp", "filename": "dantzigpricing.hpp", "file": "dantzigpricing.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/scaling.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/scaling.hpp", "filename": "scaling.hpp", "file": "scaling.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/qpsolver/instance.hpp": {"path": "layer-4/HiGHS/highs/qpsolver/instance.hpp", "filename": "instance.hpp", "file": "instance.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "extern/zstr/zstr.hpp": {"path": "layer-4/HiGHS/extern/zstr/zstr.hpp", "filename": "zstr.hpp", "file": "zstr.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "extern/zstr/strict_fstream.hpp": {"path": "layer-4/HiGHS/extern/zstr/strict_fstream.hpp", "filename": "strict_fstream.hpp", "file": "strict_fstream.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "extern/filereaderlp/builder.hpp": {"path": "layer-4/HiGHS/extern/filereaderlp/builder.hpp", "filename": "builder.hpp", "file": "builder.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "extern/filereaderlp/reader.hpp": {"path": "layer-4/HiGHS/extern/filereaderlp/reader.hpp", "filename": "reader.hpp", "file": "reader.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "extern/filereaderlp/model.hpp": {"path": "layer-4/HiGHS/extern/filereaderlp/model.hpp", "filename": "model.hpp", "file": "model.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "extern/filereaderlp/def.hpp": {"path": "layer-4/HiGHS/extern/filereaderlp/def.hpp", "filename": "def.hpp", "file": "def.hpp", "brief": "HiGHS QP solver component\n\nPart of HiGHS quadratic programming active set solver.", "has_pass2": false}, "highs/Highs.h": {"path": "layer-4/HiGHS/highs/Highs.h", "filename": "Highs.h", "file": "Highs.h", "brief": "Main HiGHS solver class - high-performance LP/MIP/QP optimization\n\nHiGHS (High-performance Interior point and Gradient descent Solvers) is\nan open-source solver for LP, MIP, and convex QP problems.\n\n**Highs Class (Main API):**\nPrimary interface for model input, solving, and solution retrieval:\n- passModel(): Load LP/QP/MIP from HighsModel, HighsLp, or raw arrays\n- run(): Solve the incumbent model\n- getSolution(), getBasis(): Retrieve solution and basis\n- getModelStatus(): Check optimization result\n\n**Solving Capabilities:**\n- LP: Dual/primal simplex (HEkk) or interior point (IPX)\n- MIP: Branch-and-cut with presolve, cuts, and heuristics\n- QP: Convex quadratic programming via interior point or active set\n\n**Model Modification:**\n- addCol/addRow, deleteCols/deleteRows: Incremental model building\n- changeColBounds, changeRowBounds, changeColCost: Hot-start friendly\n- changeCoeff: Modify individual matrix coefficients\n\n**Basis Operations:**\n- getBasisInverseRow/Col: Access B^{-1} for advanced use\n- getBasisSolve/getBasisTransposeSolve: Solve B*x=b or B'*x=b\n- getReducedRow/Col: Compute B^{-1}*A columns\n\n**Options and Info:**\n- setOptionValue/getOptionValue: Configure solver behavior\n- getInfo: Retrieve solve statistics (iterations, time, etc.)", "algorithm": "MIP Solving:\n  - Presolve: probing, clique detection, coefficient strengthening\n  - Cuts: Gomory, MIR, clique, zero-half, knapsack cover\n  - Branching: reliability branching with pseudocost initialization\n  - Heuristics: RINS, local search, rounding, feasibility pump", "complexity": "LP: O(m·n·iterations) simplex, O(n³) per IPM iteration\n  MIP: exponential worst-case, highly structure-dependent", "ref": ["Huangfu, Q. and Hall, J.A.J. (2018). \"Parallelizing the dual revised\n  simplex method\". Math. Prog. Computation 10:119-142."], "see": ["lp_data/HighsLp.h for LP data structure", "lp_data/HighsOptions.h for configuration options", "simplex/HEkk.h for simplex implementation", "ipm/ipx/lp_solver.h for interior point solver", "mip/HighsMipSolver.h for MIP solver"], "has_pass2": true}, "check/Avgas.h": {"path": "layer-4/HiGHS/check/Avgas.h", "filename": "Avgas.h", "file": "simplex/Avgas.h", "brief": "Utilities for tests with AVGAS", "has_pass2": false}, "check/SpecialLps.h": {"path": "layer-4/HiGHS/check/SpecialLps.h", "filename": "SpecialLps.h", "file": "simplex/SpeciaLps.h", "brief": "Utilities for tests with special LPs", "has_pass2": false}, "highs/interfaces/highs_c_api.h": {"path": "layer-4/HiGHS/highs/interfaces/highs_c_api.h", "filename": "highs_c_api.h", "file": "interfaces/highs_c_api.h", "brief": "Pure C interface to HiGHS solver\n\nProvides C-compatible API for LP/MIP/QP solving without C++ dependencies.\n\n**Quick-Start Functions:**\n- Highs_lpCall(): Solve LP in one call with solution returned\n- Highs_mipCall(): Solve MIP in one call\n- Highs_qpCall(): Solve QP in one call\n\n**Full API (via Highs pointer):**\n- Highs_create()/Highs_destroy(): Instance management\n- Highs_passLp()/Highs_passMip(): Pass model data\n- Highs_run(): Solve current model\n- Highs_getSolution()/Highs_getBasis(): Retrieve results\n- Highs_setOption*(): Configure solver options\n- Highs_getInfo*(): Query solver statistics\n\n**Status Constants:**\n- kHighsStatus{Ok,Warning,Error}: Return codes\n- kHighsModelStatus*: Optimal, Infeasible, Unbounded, etc.\n- kHighsBasisStatus*: Lower, Basic, Upper, Zero, Nonbasic\n\n**Type Constants:**\n- kHighsVarType*: Continuous, Integer, SemiContinuous, etc.\n- kHighsOptionType*: Bool, Int, Double, String\n- kHighsMatrixFormat*: Colwise, Rowwise", "see": ["Highs.h for C++ API", "lp_data/HConst.h for C++ enum definitions"], "param": ["num_col   The number of columns.", "num_row   The number of rows.", "num_nz    The number of nonzeros in the constraint matrix.", "a_format  The format of the constraint matrix as a\n                 `kHighsMatrixFormat` constant.", "sense     The optimization sense as a `kHighsObjSense` constant.", "offset    The objective constant.", "col_cost  An array of length [num_col] with the column costs.", "col_lower An array of length [num_col] with the column lower bounds.", "col_upper An array of length [num_col] with the column upper bounds.", "row_lower An array of length [num_row] with the row lower bounds.", "row_upper An array of length [num_row] with the row upper bounds.", "a_start   The constraint matrix is provided to HiGHS in compressed\n                 sparse column form (if `a_format` is\n                 `kHighsMatrixFormatColwise`, otherwise compressed sparse row\n                 form). The sparse matrix consists of three arrays,\n                 `a_start`, `a_index`, and `a_value`. `a_start` is an array\n                 of length [num_col] containing the starting index of each\n                 column in `a_index`. If `a_format` is\n                 `kHighsMatrixFormatRowwise` the array is of length [num_row]\n                 corresponding to each row.", "a_index   An array of length [num_nz] with indices of matrix entries.", "a_value   An array of length [num_nz] with values of matrix entries.", "col_value      An array of length [num_col], to be filled with the\n                      primal column solution.", "col_dual       An array of length [num_col], to be filled with the\n                      dual column solution.", "row_value      An array of length [num_row], to be filled with the\n                      primal row solution.", "row_dual       An array of length [num_row], to be filled with the\n                      dual row solution.", "col_basis_status  An array of length [num_col], to be filled with the\n                         basis status of the columns in the form of a\n                         `kHighsBasisStatus` constant.", "row_basis_status  An array of length [num_row], to be filled with the\n                         basis status of the rows in the form of a\n                         `kHighsBasisStatus` constant.", "model_status      The location in which to place the termination\n                         status of the model after the solve in the form of a\n                         `kHighsModelStatus` constant.", "integrality   An array of length [num_col], containing a\n                     `kHighsVarType` constant for each column.", "q_num_nz  The number of nonzeros in the Hessian matrix.", "q_format  The format of the Hessian matrix in the form of a\n                 `kHighsHessianStatus` constant. If q_num_nz > 0, this must\n                 be `kHighsHessianFormatTriangular`.", "q_start   The Hessian matrix is provided to HiGHS as the lower\n                 triangular component in compressed sparse column form\n                 (or, equivalently, as the upper triangular component\n                 in compressed sparse row form). The sparse matrix consists\n                 of three arrays, `q_start`, `q_index`, and `q_value`.\n                 `q_start` is an array of length [num_col].", "q_index   An array of length [q_num_nz] with indices of matrix\n                 entries.", "q_value   An array of length [q_num_nz] with values of matrix entries.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "filename  The filename to read.", "highs     A pointer to the Highs instance.", "filename  The filename to write.", "highs     A pointer to the Highs instance.", "filename  The filename to write.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs       A pointer to the Highs instance.", "col_value   An array of length [num_col] with the column solution\n                   values.", "col_dual    An array of length [num_col] with the column dual\n                   values, or a null pointer if not known.", "row_dual    An array of length [num_row] with the row dual values,\n                   or a null pointer if not known.", "highs     A pointer to the Highs instance.", "filename  The name of the file to write the results to.", "highs     A pointer to the Highs instance.", "filename  The name of the file to write the results to.", "highs       A pointer to the Highs instance.", "num_col     The number of columns.", "num_row     The number of rows.", "num_nz      The number of elements in the constraint matrix.", "q_num_nz    The number of elements in the Hessian matrix.", "a_format    The format of the constraint matrix to use in the form of\n                   a `kHighsMatrixFormat` constant.", "q_format    The format of the Hessian matrix to use in the form of a\n                   `kHighsHessianFormat` constant.", "sense       The optimization sense in the form of a `kHighsObjSense`\n                   constant.", "offset      The constant term in the objective function.", "col_cost    An array of length [num_col] with the objective\n                   coefficients.", "col_lower   An array of length [num_col] with the lower column bounds.", "col_upper   An array of length [num_col] with the upper column bounds.", "row_lower   An array of length [num_row] with the upper row bounds.", "row_upper   An array of length [num_row] with the upper row bounds.", "a_start     The constraint matrix is provided to HiGHS in compressed\n                   sparse column form (if `a_format` is\n                   `kHighsMatrixFormatColwise`, otherwise compressed sparse\n                   row form). The sparse matrix consists of three arrays,\n                   `a_start`, `a_index`, and `a_value`. `a_start` is an array\n                   of length [num_col] containing the starting index of each\n                   column in `a_index`. If `a_format` is\n                   `kHighsMatrixFormatRowwise` the array is of length\n                   [num_row] corresponding to each row.", "a_index     An array of length [num_nz] with indices of matrix\n                   entries.", "a_value     An array of length [num_nz] with values of matrix entries.", "q_start     The Hessian matrix is provided to HiGHS as the lower\n                   triangular component in compressed sparse column form\n                   (or, equivalently, as the upper triangular component\n                   in compressed sparse row form). The sparse matrix consists\n                   of three arrays, `q_start`, `q_index`, and `q_value`.\n                   `q_start` is an array of length [num_col]. If the model\n                   is linear, pass NULL.", "q_index     An array of length [q_num_nz] with indices of matrix\n                   entries. If the model is linear, pass NULL.", "q_value     An array of length [q_num_nz] with values of matrix\n                    entries. If the model is linear, pass NULL.", "integrality An array of length [num_col] containing a `kHighsVarType`\n                   constant for each column.", "highs     A pointer to the Highs instance.", "dim       The dimension of the Hessian matrix. Should be [num_col].", "num_nz    The number of non-zero elements in the Hessian matrix.", "format    The format of the Hessian matrix as a `kHighsHessianFormat`\n                 constant. This must be `kHighsHessianFormatTriangular`.", "start     The Hessian matrix is provided to HiGHS as the lower\n                 triangular component in compressed sparse column form\n                 (or, equivalently, as the upper triangular component\n                 in compressed sparse row form), using `q_start`, `q_index`,\n                 and `q_value`.The Hessian matrix is provided to HiGHS as the\n                 lower triangular component in compressed sparse column form.\n                 The sparse matrix consists of three arrays, `start`,\n                 `index`, and `value`. `start` is an array of length\n                 [num_col] containing the starting index of each column in\n                 `index`.", "index     An array of length [num_nz] with indices of matrix entries.", "value     An array of length [num_nz] with values of matrix entries.", "highs         A pointer to the Highs instance.", "weight        A pointer to the weights of the linear objective, with\n                     its positive/negative sign determining whether it is\n                     minimized or maximized during lexicographic optimization", "offset        A pointer to the objective offsets", "coefficients  A pointer to the objective coefficients", "abs_tolerance A pointer to the absolute tolerances used when\n                     constructing objective constraints during lexicographic\n                     optimization", "rel_tolerance A pointer to the relative tolerances used when\n                     constructing objective constraints during lexicographic\n                     optimization", "priority      A pointer to the priorities of the objectives during\n                     lexicographic optimization", "highs         A pointer to the Highs instance.", "weight        The weight of the linear objective, with its\n                     positive/negative sign determining whether it is\n                     minimized or maximized during lexicographic\n                     optimization", "offset        The objective offset", "coefficients  A pointer to the objective coefficients", "abs_tolerance The absolute tolerance used when constructing an\n                     objective constraint during lexicographic optimization", "rel_tolerance The relative tolerance used when constructing an\n                     objective constraint during lexicographic optimization", "priority      The priority of this objective during lexicographic\n                     optimization", "highs A pointer to the Highs instance.", "highs A pointer to the Highs instance.", "row   The row for which the name is supplied.", "name  The name of the row.", "highs A pointer to the Highs instance.", "col   The column for which the name is supplied.", "name  The name of the column.", "highs A pointer to the Highs instance.", "name  The name of the model.", "highs     A pointer to the Highs instance.", "filename  The filename from which to read the option values.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The new value of the option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The location in which the current value of the option should\n                 be placed.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The location in which the current value of the option should\n                 be placed.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     The location in which the current value of the option should\n                 be placed.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "value     A pointer to allocated memory (of at least\n                 `kMaximumStringLength`) to store the current value of the\n                 option.", "highs     A pointer to the Highs instance.", "option    The name of the option.", "type      A HighsInt in which the corresponding `kHighsOptionType`\n                 constant should be placed.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "filename  The filename to write the options to.", "highs     A pointer to the Highs instance.", "filename  The filename to write the options to.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "index     The index of the option.", "name      The name of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "default_value A pointer to the default value of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "min_value     A pointer to the minimum value of the option.", "max_value     A pointer to the maximum value of the option.", "default_value A pointer to the default value of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "min_value     A pointer to the minimum value of the option.", "max_value     A pointer to the maximum value of the option.", "default_value A pointer to the default value of the option.", "highs         A pointer to the Highs instance.", "current_value A pointer to the current value of the option.", "default_value A pointer to the default value of the option.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "value     A reference to an integer that the result will be stored in.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "value     A reference to a double that the result will be stored in.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "value     A reference to an int64 that the result will be stored in.", "highs     A pointer to the Highs instance.", "info      The name of the info item.", "type      A HighsInt in which the corresponding `kHighsOptionType`\n                 constant is stored.", "highs      A pointer to the Highs instance.", "col_value  An array of length [num_col], to be filled with primal\n                  column values.", "col_dual   An array of length [num_col], to be filled with dual column\n                  values.", "row_value  An array of length [num_row], to be filled with primal row\n                  values.", "row_dual   An array of length [num_row], to be filled with dual row\n                  values.", "highs       A pointer to the Highs instance.", "col_status  An array of length [num_col], to be filled with the column\n                   basis statuses in the form of a `kHighsBasisStatus`\n                   constant.", "row_status  An array of length [num_row], to be filled with the row\n                   basis statuses in the form of a `kHighsBasisStatus`\n                   constant.", "highs     A pointer to the Highs instance.", "highs             A pointer to the Highs instance.", "has_dual_ray      A pointer to a HighsInt to store 1 if a dual ray\n                         currently exists.", "dual_ray_value    An array of length [num_row] filled with the\n                         unbounded ray.", "highs                                   A pointer to the Highs\n                                               instance.", "has_dual_unboundedness_direction        A pointer to a HighsInt to\n                                               store 1 if the dual\n                                               unboundedness direction\n                                               exists.", "dual_unboundedness_direction_value      An array of length [num_col]\n                                               filled with the unboundedness\n                                               direction.", "highs             A pointer to the Highs instance.", "has_primal_ray    A pointer to a HighsInt to store 1 if the primal ray\n                         exists.", "primal_ray_value  An array of length [num_col] filled with the\n                         unbounded ray.", "highs     A pointer to the Highs instance.", "highs             A pointer to the Highs instance.", "basic_variables   An array of size [num_rows], filled with the indices\n                         of the basic variables.", "highs         A pointer to the Highs instance.", "row           The index of the row to compute.", "row_vector    An array of length [num_row] in which to store the\n                     values of the non-zero elements.", "row_num_nz    The number of non-zeros in the row.", "row_index     An array of length [num_row] in which to store the\n                     indices of the non-zero elements.", "highs         A pointer to the Highs instance.", "col           The index of the column to compute.", "col_vector    An array of length [num_row] in which to store the\n                     values of the non-zero elements.", "col_num_nz    The number of non-zeros in the column.", "col_index     An array of length [num_row] in which to store the\n                     indices of the non-zero elements.", "highs             A pointer to the Highs instance.", "rhs               The right-hand side vector ``b``.", "solution_vector   An array of length [num_row] in which to store the\n                         values of the non-zero elements.", "solution_num_nz   The number of non-zeros in the solution.", "solution_index    An array of length [num_row] in which to store the\n                         indices of the non-zero elements.", "highs             A pointer to the Highs instance.", "rhs               The right-hand side vector ``b``", "solution_vector   An array of length [num_row] in which to store the\n                         values of the non-zero elements.", "solution_num_nz   The number of non-zeros in the solution.", "solution_index    An array of length [num_row] in which to store the\n                         indices of the non-zero elements.", "highs         A pointer to the Highs instance.", "row           The index of the row to compute.", "row_vector    An array of length [num_col] in which to store the\n                     values of the non-zero elements.", "row_num_nz    The number of non-zeros in the row.", "row_index     An array of length [num_col] in which to store the\n                     indices of the non-zero elements.", "highs         A pointer to the Highs instance.", "col           The index of the column to compute.", "col_vector    An array of length [num_row] in which to store the\n                      values of the non-zero elements.", "col_num_nz    The number of non-zeros in the column.", "col_index     An array of length [num_row] in which to store the\n                      indices of the non-zero elements.", "highs       A pointer to the Highs instance.", "col_status  an array of length [num_col] with the column basis status\n                   in the form of `kHighsBasisStatus` constants", "row_status  an array of length [num_row] with the row basis status\n                   in the form of `kHighsBasisStatus` constants", "highs     A pointer to the Highs instance.", "highs       A pointer to the Highs instance.", "col_value   An array of length [num_col] with the column solution\n                   values.", "row_value   An array of length [num_row] with the row solution\n                   values.", "col_dual    An array of length [num_col] with the column dual values.", "row_dual    An array of length [num_row] with the row dual values.", "highs       A pointer to the Highs instance.", "num_entries Number of variables in the set", "index       Indices of variables in the set", "value       Values of variables in the set", "highs              A pointer to the Highs instance.", "user_callback      A pointer to the user callback", "user_callback_data A pointer to the user callback data", "highs         A pointer to the Highs instance.", "callback_type The type of callback to be started", "highs         A pointer to the Highs instance.", "callback_type The type of callback to be stopped", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs         A pointer to the Highs instance.", "cost          The objective coefficient of the column.", "lower         The lower bound of the column.", "upper         The upper bound of the column.", "num_new_nz    The number of non-zeros in the column.", "index         An array of size [num_new_nz] with the row indices.", "value         An array of size [num_new_nz] with row values.", "highs         A pointer to the Highs instance.", "num_new_col   The number of new columns to add.", "costs         An array of size [num_new_col] with objective\n                     coefficients.", "lower         An array of size [num_new_col] with lower bounds.", "upper         An array of size [num_new_col] with upper bounds.", "num_new_nz    The number of new nonzeros in the constraint matrix.", "starts        The constraint coefficients are given as a matrix in\n                     compressed sparse column form by the arrays `starts`,\n                     `index`, and `value`. `starts` is an array of size\n                     [num_new_cols] with the start index of each row in\n                     indices and values.", "index         An array of size [num_new_nz] with row indices.", "value         An array of size [num_new_nz] with row values.", "highs         A pointer to the Highs instance.", "lower         The lower bound of the column.", "upper         The upper bound of the column.", "highs         A pointer to the Highs instance.", "num_new_var   The number of new variables to add.", "lower         An array of size [num_new_var] with lower bounds.", "upper         An array of size [num_new_var] with upper bounds.", "highs         A pointer to the Highs instance.", "lower         The lower bound of the row.", "upper         The upper bound of the row.", "num_new_nz    The number of non-zeros in the row", "index         An array of size [num_new_nz] with column indices.", "value         An array of size [num_new_nz] with column values.", "highs         A pointer to the Highs instance.", "num_new_row   The number of new rows to add", "lower         An array of size [num_new_row] with the lower bounds of\n                     the rows.", "upper         An array of size [num_new_row] with the upper bounds of\n                     the rows.", "num_new_nz    The number of non-zeros in the rows.", "starts        The constraint coefficients are given as a matrix in\n                     compressed sparse row form by the arrays `starts`,\n                     `index`, and `value`. `starts` is an array of size\n                     [num_new_rows] with the start index of each row in\n                     indices and values.", "index         An array of size [num_new_nz] with column indices.", "value         An array of size [num_new_nz] with column values.", "highs         A pointer to the Highs instance.", "highs         A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "sense     The new optimization sense in the form of a `kHighsObjSense`\n                 constant.", "highs     A pointer to the Highs instance.", "offset    The new objective offset.", "highs         A pointer to the Highs instance.", "col           The column index to change.", "integrality   The new integrality of the column in the form of a\n                     `kHighsVarType` constant.", "highs         A pointer to the Highs instance.", "from_col      The index of the first column whose integrality changes.", "to_col        The index of the last column whose integrality\n                     changes.", "integrality   An array of length [to_col - from_col + 1] with the new\n                     integralities of the columns in the form of\n                     `kHighsVarType` constants.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to change.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to change.", "integrality       An array of length [num_set_entries] with the new\n                         integralities of the columns in the form of\n                         `kHighsVarType` constants.", "highs         A pointer to the Highs instance.", "mask          An array of length [num_col] with 1 if the column\n                     integrality should be changed and 0 otherwise.", "integrality   An array of length [num_col] with the new\n                     integralities of the columns in the form of\n                     `kHighsVarType` constants.", "highs         A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "col       The index of the column fo change.", "cost      The new objective coefficient.", "highs     A pointer to the Highs instance.", "from_col  The index of the first column whose cost changes.", "to_col    The index of the last column whose cost changes.", "cost      An array of length [to_col - from_col + 1] with the new\n                 objective coefficients.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to change.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to change.", "cost              An array of length [num_set_entries] with the new\n                         costs of the columns.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_col] with 1 if the column\n                 cost should be changed and 0 otherwise.", "cost      An array of length [num_col] with the new costs.", "highs     A pointer to the Highs instance.", "col       The index of the column whose bounds are to change.", "lower     The new lower bound.", "upper     The new upper bound.", "highs     A pointer to the Highs instance.", "from_col  The index of the first column whose bound changes.", "to_col    The index of the last column whose bound changes.", "lower     An array of length [to_col - from_col + 1] with the new\n                 lower bounds.", "upper     An array of length [to_col - from_col + 1] with the new\n                 upper bounds.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to change.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to change.", "lower             An array of length [num_set_entries] with the new\n                         lower bounds.", "upper             An array of length [num_set_entries] with the new\n                         upper bounds.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_col] with 1 if the column\n                 bounds should be changed and 0 otherwise.", "lower     An array of length [num_col] with the new lower bounds.", "upper     An array of length [num_col] with the new upper bounds.", "highs     A pointer to the Highs instance.", "row       The index of the row whose bounds are to change.", "lower     The new lower bound.", "upper     The new upper bound.", "highs     A pointer to the Highs instance.", "from_row  The index of the first row whose bound changes.", "to_row    The index of the last row whose bound changes.", "lower     An array of length [to_row - from_row + 1] with the new\n                 lower bounds.", "upper     An array of length [to_row - from_row + 1] with the new\n                 upper bounds.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of rows to change.", "set               An array of size [num_set_entries] with the indices\n                         of the rows to change.", "lower             An array of length [num_set_entries] with the new\n                         lower bounds.", "upper             An array of length [num_set_entries] with the new\n                         upper bounds.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_row] with 1 if the row\n                 bounds should be changed and 0 otherwise.", "lower     An array of length [num_row] with the new lower bounds.", "upper     An array of length [num_row] with the new upper bounds.", "highs     A pointer to the Highs instance.", "row       The index of the row to change.", "col       The index of the column to change.", "value     The new constraint coefficient.", "highs     A pointer to the Highs instance.", "sense     The location in which the current objective sense should be\n                 placed. The sense is a `kHighsObjSense` constant.", "highs     A pointer to the Highs instance.", "offset    The location in which the current objective offset should be\n                 placed.", "highs         A pointer to the Highs instance.", "from_col      The first column for which to query data for.", "to_col        The last column (inclusive) for which to query data for.", "num_col       A HighsInt populated with the number of columns got from\n                     the model (this should equal `to_col - from_col + 1`).", "costs         An array of size [to_col - from_col + 1] for the column\n                     cost coefficients.", "lower         An array of size [to_col - from_col + 1] for the column\n                     lower bounds.", "upper         An array of size [to_col - from_col + 1] for the column\n                     upper bounds.", "num_nz        A HighsInt to be populated with the number of non-zero\n                     elements in the constraint matrix.", "matrix_start  An array of size [to_col - from_col + 1] with the start\n                     indices of each column in `matrix_index` and\n                     `matrix_value`.", "matrix_index  An array of size [num_nz] with the row indices of each\n                     element in the constraint matrix.", "matrix_value  An array of size [num_nz] with the non-zero elements of\n                     the constraint matrix.", "num_set_indices   The number of indices in `set`.", "set               An array of size [num_set_entries] with the column\n                         indices to get.", "mask  An array of length [num_col] containing a `1` to get the column\n             and `0` otherwise.", "highs         A pointer to the Highs instance.", "from_row      The first row for which to query data for.", "to_row        The last row (inclusive) for which to query data for.", "num_row       A HighsInt to be populated with the number of rows got\n                     from the model.", "lower         An array of size [to_row - from_row + 1] for the row\n                     lower bounds.", "upper         An array of size [to_row - from_row + 1] for the row\n                     upper bounds.", "num_nz        A HighsInt to be populated with the number of non-zero\n                     elements in the constraint matrix.", "matrix_start  An array of size [to_row - from_row + 1] with the start\n                     indices of each row in `matrix_index` and\n                     `matrix_value`.", "matrix_index  An array of size [num_nz] with the column indices of\n                     each element in the constraint matrix.", "matrix_value  An array of size [num_nz] with the non-zero elements of\n                     the constraint matrix.", "num_set_indices   The number of indices in `set`.", "set               An array of size [num_set_entries] containing the\n                         row indices to get.", "mask  An array of length [num_row] containing a `1` to get the row and\n             `0` otherwise.", "row   The index of the row to query.", "name  A pointer in which to store the name of the row. This must have\n             length `kHighsMaximumStringLength`.", "name A pointer of the name of the row to query.", "row  A pointer in which to store the index of the row", "col   The index of the column to query.", "name  A pointer in which to store the name of the column. This must\n             have length `kHighsMaximumStringLength`.", "name A pointer of the name of the column to query.", "col  A pointer in which to store the index of the column", "col          The index of the column to query.", "integrality  A HighsInt in which the integrality of the column should\n                    be placed. The integer is one of the `kHighsVarTypeXXX`\n                    constants.", "highs     A pointer to the Highs instance.", "from_col  The index of the first column to delete.", "to_col    The index of the last column to delete.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of columns to delete.", "set               An array of size [num_set_entries] with the indices\n                         of the columns to delete.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_col] with 1 if the column\n                 should be deleted and 0 otherwise.", "highs     A pointer to the Highs instance.", "from_row  The index of the first row to delete.", "to_row    The index of the last row to delete.", "highs             A pointer to the Highs instance.", "num_set_entries   The number of rows to delete.", "set               An array of size [num_set_entries] with the indices\n                         of the rows to delete.", "highs     A pointer to the Highs instance.", "mask      An array of length [num_row] with `1` if the row should be\n                 deleted and `0` otherwise. The new index of any column not\n                 deleted is stored in place of the value `0`.", "highs     A pointer to the Highs instance.", "col       The index of the column to scale.", "scaleval  The value by which to scale the column. If `scaleval < 0`,\n                 the variable bounds flipped.", "highs     A pointer to the Highs instance.", "row       The index of the row to scale.", "scaleval  The value by which to scale the row. If `scaleval < 0`, the\n                 row bounds are flipped.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs     A pointer to the Highs instance.", "highs      A pointer to the Highs instance.", "num_col    The number of variables.", "num_row    The number of rows.", "col_value  An array of length [num_col] with optimal primal solution\n                  for each column.", "col_dual   An array of length [num_col] with optimal dual solution for\n                  each column. May be `NULL`, in which case no dual solution\n                  is passed.", "row_dual   An array of length [num_row] with optimal dual solution for\n                  each row. . May be `NULL`, in which case no dual solution\n                  is passed.", "highs                  A pointer to the Highs instance.", "col_cost_up_value      The upper range of the cost value", "col_cost_up_objective  The objective at the upper cost range", "col_cost_up_in_var     The variable entering the basis at the upper\n                              cost range", "col_cost_up_ou_var     The variable leaving the basis at the upper\n                              cost range", "col_cost_dn_value      The lower range of the cost value", "col_cost_dn_objective  The objective at the lower cost range", "col_cost_dn_in_var     The variable entering the basis at the lower\n                              cost range", "col_cost_dn_ou_var     The variable leaving the basis at the lower\n                              cost range", "col_bound_up_value     The upper range of the column bound value", "col_bound_up_objective The objective at the upper column bound range", "col_bound_up_in_var    The variable entering the basis at the upper\n                              column bound range", "col_bound_up_ou_var    The variable leaving the basis at the upper\n                              column bound range", "col_bound_dn_value     The lower range of the column bound value", "col_bound_dn_objective The objective at the lower column bound range", "col_bound_dn_in_var    The variable entering the basis at the lower\n                              column bound range", "col_bound_dn_ou_var    The variable leaving the basis at the lower\n                              column bound range", "row_bound_up_value     The upper range of the row bound value", "row_bound_up_objective The objective at the upper row bound range", "row_bound_up_in_var    The variable entering the basis at the upper\n                              row bound range", "row_bound_up_ou_var    The variable leaving the basis at the upper row\n                              bound range", "row_bound_dn_value     The lower range of the row bound value", "row_bound_dn_objective The objective at the lower row bound range", "row_bound_dn_in_var    The variable entering the basis at the lower\n                              row bound range", "row_bound_dn_ou_var    The variable leaving the basis at the lower row\n                              bound range", "highs                             A pointer to the Highs instance.", "const double global_lower_penalty The penalty for violating lower\nbounds on variables", "const double global_upper_penalty The penalty for violating upper\nbounds on variables", "const double global_rhs_penalty   The penalty for violating constraint\nRHS values", "const double* local_lower_penalty The penalties for violating specific\nlower bounds on variables", "const double* local_upper_penalty The penalties for violating specific\nupper bounds on variables", "const double* local_rhs_penalty   The penalties for violating specific\nconstraint RHS values", "highs                      A pointer to the Highs instance.", "const HighsInt iis_num_col Number of columns in the IIS.", "const HighsInt iis_num_row Number of rows in the IIS.", "const HighsInt* col_index  An array of length [iis_num_col], to be\n                                  filled with the indices of original\n                                  variables in the IIS.", "const HighsInt* row_index  An array of length [iis_num_col], to be\n                                  filled with the indices of original\n                                  constraints in the IIS.", "const HighsInt* col_bound  An array of length [iis_num_col], to be\n                                  filled with the bound status of variables\n                                  in the IIS.", "const HighsInt* row_bound  An array of length [iis_num_col], to be\n                                  filled with the bound status of constraints\n                                  in the IIS.", "const HighsInt* col_status An array of length [num_col], to be\n                                  filled with the IIS status of all original\n                                  variables.", "const HighsInt* row_status n array of length [num_col], to be\n                                  filled with the IIS status of all original\n                                  constraints.", "blocking   If the `blocking` parameter has a nonzero value, then this\n                  function will not return until all memory is freed, which\n                  might be desirable when debugging heap memory, but it\n                  requires the calling thread to wait for all scheduler\n                  threads to wake-up which is usually not necessary.", "data_out      A pointer to the HighsCallbackDataOut instance.", "item_name     The name of the item.", "data_in     A pointer to the callback input data instance.", "num_entries Number of variables in the set", "value       An array of length [num_entries <= num_col] with\n                   column solution values.", "data_in     A pointer to the callback input data instance.", "num_entries Number of variables in the set", "index       Indices of variables in the set", "value       Values of variables in the set"], "has_pass2": false}, "highs/pdlp/CupdlpWrapper.h": {"path": "layer-4/HiGHS/highs/pdlp/CupdlpWrapper.h", "filename": "CupdlpWrapper.h", "file": "pdlp/CupdlpWrapper.h", "brief": "Wrapper for CUPDLP first-order LP solver\n\nIntegrates CUPDLP (CUDA/CPU Primal-Dual LP) solver into HiGHS.\nCUPDLP uses first-order methods (PDHG) for LP solving without pivoting.\n\n**Main Functions:**\n- solveLpCupdlp(): Solve LP using CUPDLP algorithm\n- formulateLP_highs(): Convert HighsLp to CUPDLP format (CSC, rhs, bounds)\n- getCupdlpLogLevel(): Map HiGHS verbosity to CUPDLP log level\n\n**Problem Setup:**\n- problem_create()/problem_alloc(): Allocate CUPDLP problem structure\n- data_alloc(): Allocate matrix data (supports GPU if CUPDLP_GPU defined)\n\n**Memory Macros:**\n- cupdlp_init_*: Allocation helpers for int, double, work, problem, data\n- cupdlp_copy_vec: CPU vector copy (GPU uses CUDA memcpy)\n\n**GPU Support:**\nCompile with CUPDLP_GPU for CUDA acceleration via cuPDLP backend.", "see": ["pdlp/cupdlp/cupdlp.h for CUPDLP algorithm implementation", "ipm/IpxWrapper.h for alternative interior point method"], "has_pass2": false}, "highs/lp_data/HighsLpUtils.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsLpUtils.h", "filename": "HighsLpUtils.h", "file": "lp_data/HighsLpUtils.h", "brief": "Utility functions for LP manipulation, I/O, and validation\n\nCollection of free functions operating on HighsLp and related structures.\n\n**Basis I/O:**\n- writeBasisFile(): Export basis to file\n- readBasisFile()/readBasisStream(): Import basis from file/stream\n\n**Index/Name Lookup:**\n- getIndexFromName(): Resolve variable/constraint name to index\n\n**LP Assessment:**\n- assessLp(): Validate LP data consistency\n- lpDimensionsOk(): Check matrix dimensions\n- assessCosts(): Validate objective coefficients\n- assessBounds(): Validate variable/constraint bounds\n- applyScalingToLp()/applyScalingToLpCol/Row(): Apply scaling factors\n\n**LP Modification:**\n- appendColsToLp(), appendRowsToLp(): Extend LP\n- transformIntoEqualityProblem(): Convert to standard form\n\n**Solution Utilities:**\n- getLpCosts(), analyseObjective(): Cost analysis\n- writeSolution(): Export solution to file", "see": ["lp_data/HighsLp.h for LP data structure", "lp_data/HighsSolution.h for solution structure", "lp_data/HighsModelUtils.h for model-level utilities"], "has_pass2": false}, "highs/lp_data/HighsIis.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsIis.h", "filename": "HighsIis.h", "file": "lp_data/HighsIis.h", "brief": "Irreducible Infeasible Set (IIS) computation for infeasible LPs\n\nIdentifies minimal subset of constraints that cannot be satisfied.\n\n**IIS Definition:**\n- Minimal infeasible subsystem: removing any constraint makes it feasible\n- Helps diagnose why an LP is infeasible\n- Also identifies which variable bounds participate\n\n**IisBoundStatus:**\n- kDropped: Bound removed from IIS\n- kNull: Not yet classified\n- kFree: Variable is free (not in IIS)\n- kLower/kUpper/kBoxed: Which bound(s) are in IIS\n\n**Algorithm:**\n- compute(): Main IIS computation using simplex iterations\n- trivial(): Check for obviously infeasible (single row/col)\n- rowValueBounds(): Check row activity vs bounds\n\n**Strategy:**\n- kIisStrategyMin: Minimize IIS size (more iterations)\n- Iteratively removes constraints until minimal set remains\n\n**Output:**\n- col_index_/row_index_: Variables/constraints in IIS\n- col_bound_/row_bound_: Which bounds contribute\n- info_: Simplex stats per iteration", "see": ["Highs.h for getIis() API"], "has_pass2": false}, "highs/lp_data/HighsAnalysis.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsAnalysis.h", "filename": "HighsAnalysis.h", "file": "lp_data/HighsAnalysis.h", "brief": "Timer clock aggregation for performance analysis\n\nGroups named timer clocks for measuring algorithm phases.\n\n**HighsTimerClock:**\n- timer_pointer_: Reference to parent HighsTimer\n- clock_[]: Vector of clock indices for this group\n\n**Usage:**\n- Simplex phases (pricing, ratio test, update)\n- IPM iterations\n- MIP operations (cuts, branching, heuristics)", "see": ["util/HighsTimer.h for clock management"], "has_pass2": false}, "highs/lp_data/HighsLp.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsLp.h", "filename": "HighsLp.h", "file": "lp_data/HighsLp.h", "brief": "Linear programming model data structure\n\n**HighsLp Class:**\nCore LP representation: min/max c'x s.t. row_lower <= Ax <= row_upper,\ncol_lower <= x <= col_upper\n\n**Data Members:**\n- num_col_, num_row_: Problem dimensions\n- col_cost_: Objective coefficients (c)\n- col_lower_, col_upper_: Variable bounds\n- row_lower_, row_upper_: Constraint bounds\n- a_matrix_: Constraint matrix A (HighsSparseMatrix, CSC or CSR)\n- sense_: Minimize (1) or Maximize (-1)\n- offset_: Constant objective offset\n- integrality_: Variable types (continuous, integer, semi-continuous, etc.)\n\n**Naming:**\n- col_names_, row_names_: Optional variable/constraint names\n- col_hash_, row_hash_: Name lookup hash tables\n\n**Scaling:**\n- scale_: Row/column scaling factors\n- is_scaled_: Whether scaling has been applied", "see": ["model/HighsModel.h for LP + Hessian (QP)", "HStruct.h for supporting types (ObjSense, HighsScale, etc.)", "util/HighsSparseMatrix.h for matrix storage"], "has_pass2": false}, "highs/lp_data/HighsRanging.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsRanging.h", "filename": "HighsRanging.h", "file": "lp_data/HighsRanging.h", "brief": "Sensitivity analysis (ranging) for LP optimal solutions\n\nComputes allowable ranges for objective coefficients and bounds.\n\n**HighsRangingRecord:**\n- value_[]: New coefficient/bound value at range limit\n- objective_[]: Objective value at range limit\n- in_var_[]: Variable entering basis at limit\n- ou_var_[]: Variable leaving basis at limit\n\n**HighsRanging Structure:**\n- col_cost_up/dn: Objective coefficient increase/decrease\n- col_bound_up/dn: Variable bound increase/decrease\n- row_bound_up/dn: Constraint RHS increase/decrease\n\n**Sensitivity Analysis:**\n- For costs: Range where current basis remains optimal\n- For bounds: Range where current basis remains primal feasible\n- Shadow prices from dual variables", "see": ["lp_data/HighsLpSolverObject.h for solver state"], "has_pass2": false}, "highs/lp_data/HighsSolve.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsSolve.h", "filename": "HighsSolve.h", "file": "lp_data/HighsSolve.h", "brief": "Top-level LP solve dispatch and special case handling\n\nEntry points for solving LPs with solver selection and preprocessing.\n\n**Main Solve Functions:**\n- solveLp(): Primary entry point dispatching to simplex/IPM\n- solveUnconstrainedLp(): Handle LPs with no constraints\n\n**Solver Selection:**\n- useIpm(): Check if IPM should be used based on options\n- useHipo(): Check if parallel HIPO IPM is appropriate\n  - Considers problem size, structure, and options\n\n**Preprocessing:**\n- assessExcessiveObjectiveBoundScaling(): Detect numerical issues\n  - Warns if objective/bound ratio is extreme\n  - Populates user_scale_data for remediation\n\n**Dispatch Logic:**\n1. Check for special cases (unconstrained, trivially infeasible)\n2. Select solver (simplex vs IPM vs HIPO)\n3. Call appropriate solver with prepared solver object", "see": ["lp_data/HighsLpSolverObject.h for solver state container", "simplex/HEkk.h for simplex implementation", "ipm/IpxWrapper.h for IPM implementation"], "has_pass2": false}, "highs/lp_data/HighsModelUtils.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsModelUtils.h", "filename": "HighsModelUtils.h", "file": "lp_data/HighsModelUtils.h", "brief": "Utility functions for model analysis and solution output\n\nFree functions for analyzing models and writing solution files.\n\n**Model Analysis:**\n- analyseModelBounds(): Report statistics on variable/constraint bounds\n- hasNamesWithSpaces(): Check for problematic whitespace in names\n\n**Solution Output:**\n- writeModelBoundSolution(): Write variable/constraint solution to file\n- writeModelObjective(): Write objective function evaluation\n- writeLpObjective(): Write LP objective evaluation\n- writeObjectiveValue(): Write scalar objective value\n- writePrimalSolution()/writeDualSolution(): Export primal/dual values\n- writeSolutionFile(): Write complete solution to file\n\n**Model Status:**\n- utilModelStatusToString(): Convert HighsModelStatus to string\n- utilBasisStatusToString(): Convert HighsBasisStatus to string", "see": ["lp_data/HighsLpUtils.h for LP-specific utilities", "model/HighsModel.h for model data structure", "lp_data/HighsSolution.h for solution structure"], "has_pass2": false}, "highs/lp_data/HStruct.h": {"path": "layer-4/HiGHS/highs/lp_data/HStruct.h", "filename": "HStruct.h", "file": "lp_data/HStruct.h", "brief": "Core data structures for HiGHS solver\n\nDefines fundamental structs used throughout HiGHS.\n\n**Solution/Basis Structs:**\n- HighsSolution: Primal values (col_value, row_value) and duals (col_dual, row_dual)\n- HighsBasis: Column/row basis status (Lower, Basic, Upper, Zero, Nonbasic)\n- HighsObjectiveSolution: Objective value with column values (for MIP solutions)\n\n**Scaling Structs:**\n- HighsScale: Row/column scaling factors (strategy, col[], row[], cost)\n- HighsUserScaleData: User-specified objective/bound scaling parameters\n\n**Model Modification Structs:**\n- HighsLpMods: Tracks modifications for semi-variables and infinite costs\n- HighsLinearObjective: Multi-objective support (weight, offset, coefficients, priority)\n\n**Utility Structs:**\n- HighsFiles: File paths for reading/writing solutions, bases, models\n- HighsNameHash: Hash table for name-to-index lookup\n- RefactorInfo/HotStart: Basis refactorization data (deprecated HotStart)\n\n**Logging/Statistics:**\n- HighsPresolveRuleLog/HighsPresolveLog: Presolve rule application counts\n- HighsSimplexStats: Iteration count, invert frequency, density statistics\n- HighsIllConditioning: Records for ill-conditioned basis detection\n- HighsSubSolverCallTime: Sub-solver timing breakdown", "see": ["HighsLp.h for LP model structure", "HConst.h for HighsBasisStatus, HighsVarType enums"], "has_pass2": false}, "highs/lp_data/HConst.h": {"path": "layer-4/HiGHS/highs/lp_data/HConst.h", "filename": "HConst.h", "file": "lp_data/HConst.h", "brief": "Constants, enums, and type definitions for HiGHS\n\nCentral definitions for HiGHS types and enumerations.\n\n**Numeric Constants:**\n- kHighsInf: Infinity value (std::numeric_limits<double>::infinity())\n- kHighsIInf: Integer infinity (max HighsInt)\n- kHighsTiny, kHighsMacheps, kHighsZero: Numerical tolerances\n\n**Core Enums:**\n- HighsModelStatus: Optimization result (kOptimal, kInfeasible, kUnbounded, etc.)\n- HighsVarType: Variable types (kContinuous, kInteger, kSemiContinuous, etc.)\n- HighsBasisStatus: Basis state (kLower, kBasic, kUpper, kZero, kNonbasic)\n- ObjSense: Minimize (+1) or Maximize (-1)\n\n**Format Enums:**\n- MatrixFormat: kColwise (CSC), kRowwise (CSR), kRowwisePartitioned\n- HessianFormat: kTriangular, kSquare\n\n**Status Enums:**\n- HighsPresolveStatus: Presolve outcomes\n- SolutionStatus: kNone, kInfeasible, kFeasible\n- BasisValidity: kInvalid, kValid\n\n**Callback Types:**\n- HighsCallbackType: Logging, interrupt, MIP solution events\n\n**Presolve Rules:**\n- PresolveRuleType: EmptyRow, SingletonRow, FixedCol, ForcingRow, etc.", "see": ["HighsStatus.h for function return codes", "HighsOptions.h for option types"], "has_pass2": false}, "highs/lp_data/HighsDebug.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsDebug.h", "filename": "HighsDebug.h", "file": "lp_data/HighsDebug.h", "has_pass2": false}, "highs/lp_data/HighsSolutionDebug.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsSolutionDebug.h", "filename": "HighsSolutionDebug.h", "file": "lp_data/HighsSolutionDebug.h", "has_pass2": false}, "highs/lp_data/HighsSolution.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsSolution.h", "filename": "HighsSolution.h", "file": "lp_data/HighsSolution.h", "brief": "Solution validation and KKT condition checking utilities\n\nProvides functions to validate solutions and compute optimality metrics.\n\n**KKT Failure Analysis:**\n- getKktFailures(): Compute primal/dual infeasibilities for LP/QP\n- getLpKktFailures(): LP-specific KKT checking\n- getVariableKktFailures(): Per-variable bound/dual violations\n\n**Error Tracking (HighsError, HighsPrimalDualErrors):**\n- Absolute/relative primal infeasibility\n- Absolute/relative dual infeasibility\n- Nonzero basic duals, off-bound nonbasics\n- Glpsol-compatible residual output\n\n**Solution Utilities:**\n- computeObjectiveValue(): Evaluate c'x\n- computeDualObjectiveValue(): Dual objective for LP/QP\n- getComplementarityViolations(): x_i * s_i deviations\n- refineBasis(): Improve basis from solution values\n\n**IPX Integration:**\n- ipxSolutionToHighsSolution(): Convert IPX (interior point) solution\n- ipxBasicSolutionToHighsBasicSolution(): Convert IPX basis + crossover result", "see": ["HStruct.h for HighsSolution, HighsBasis structs", "HighsInfo.h for solver info output"], "has_pass2": false}, "highs/lp_data/HighsInfo.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsInfo.h", "filename": "HighsInfo.h", "file": "lp_data/HighsInfo.h", "brief": "Solver output information and statistics\n\nProvides typed info records for solver statistics accessible via API.\n\n**InfoRecord Hierarchy:**\n- InfoRecord: Base class (type, name, description, advanced)\n- InfoRecordInt64: 64-bit integer info (mip_node_count)\n- InfoRecordInt: Integer info (iteration counts, status codes)\n- InfoRecordDouble: Double info (objective, infeasibilities)\n\n**HighsInfoStruct/HighsInfo:**\n- Iteration counts: simplex, ipm, crossover, pdlp, qp\n- Solution status: primal_solution_status, dual_solution_status\n- Objective: objective_function_value\n- MIP statistics: mip_node_count, mip_dual_bound, mip_gap\n- Infeasibility metrics: num/max/sum primal/dual infeasibilities\n- Residuals: primal/dual residual errors (absolute and relative)\n- Complementarity: violation counts and max violation\n\n**Functions:**\n- getInfoIndex(): Look up info by name\n- getLocalInfoValue(): Retrieve int/int64/double values\n- writeInfoToFile()/reportInfo(): Output info records", "see": ["HConst.h for HighsInfoType enum", "HighsOptions.h for solver configuration"], "has_pass2": false}, "highs/lp_data/HighsCallbackStruct.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsCallbackStruct.h", "filename": "HighsCallbackStruct.h", "file": "lp_data/HighsCallbackStruct.h", "has_pass2": false}, "highs/lp_data/HighsLpSolverObject.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsLpSolverObject.h", "filename": "HighsLpSolverObject.h", "file": "lp_data/HighsLpSolverObject.h", "brief": "Aggregation of all objects needed for LP solving\n\n**HighsLpSolverObject:**\nBundles references to all components needed to solve an LP:\n- lp_: The LP model data (HighsLp)\n- basis_: Current simplex basis (HighsBasis)\n- solution_: Primal/dual solution vectors (HighsSolution)\n- highs_info_: Solver statistics and output (HighsInfo)\n- ekk_instance_: Edinburgh simplex kernel (HEkk)\n- callback_: User callback handler (HighsCallback)\n- options_: Solver options (HighsOptions)\n- timer_: Performance timer (HighsTimer)\n- sub_solver_call_time_: Timing breakdown for sub-solvers\n- model_status_: Current model status (kNotset, kOptimal, etc.)\n\nUsed to pass solver context between functions without long parameter lists.", "see": ["simplex/HEkk.h for simplex implementation", "lp_data/HighsLp.h for LP data structure"], "has_pass2": false}, "highs/lp_data/HighsOptions.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsOptions.h", "filename": "HighsOptions.h", "file": "lp_data/HighsOptions.h", "brief": "Solver configuration options system\n\n**OptionRecord Hierarchy:**\nBase class with derived types for each option type:\n- OptionRecordBool: Boolean options\n- OptionRecordInt: Integer options with bounds\n- OptionRecordDouble: Double options with bounds\n- OptionRecordString: String options\n\n**HighsOptions Class:**\nContainer holding all solver options:\n- records: Vector of OptionRecord pointers\n- Options by category: solving, tolerances, output, limits, etc.\n\n**Key Option Categories:**\n- Solver selection: solver (\"simplex\", \"ipm\", \"choose\"), run_crossover\n- Tolerances: primal_feasibility_tolerance, dual_feasibility_tolerance\n- Limits: time_limit, iteration_limit, solution_limit\n- Output: output_flag, log_to_console, log_file\n- Presolve: presolve (\"on\", \"off\", \"choose\")\n- Simplex: simplex_strategy, simplex_scale_strategy\n- IPM: ipm_iteration_limit\n- MIP: mip_max_nodes, mip_abs_gap, mip_rel_gap", "see": ["Highs.h for setOptionValue/getOptionValue API", "HConst.h for option type enums"], "has_pass2": false}, "highs/lp_data/HighsCallback.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsCallback.h", "filename": "HighsCallback.h", "file": "lp_data/HighsCallback.h", "brief": "Callback mechanism for solver events and user interaction\n\nEnables user code to receive solver events and inject solutions.\n\n**HighsCallbackOutput:**\nData provided to callback during solver execution:\n- running_time, iteration counts (simplex, ipm, pdlp)\n- objective_function_value\n- MIP data: node_count, primal/dual bounds, gap, incumbent solution\n- Cut pool data for callback-based cuts\n\n**HighsCallbackInput:**\nData provided by user callback:\n- user_interrupt: Signal early termination\n- user_solution: Provide heuristic solution to MIP\n- setSolution(): Set solution (dense or sparse)\n- repairSolution(): Fix partial solution to feasibility\n\n**HighsCallback:**\nMain callback container:\n- user_callback: std::function for C++/Python\n- c_callback: C function pointer\n- active[]: Bitmask of enabled callback types\n- callbackAction(): Trigger callback and process response\n\n**Callback Types (from HighsCallbackStruct.h):**\nLogging, SimplexInterrupt, IpmInterrupt, MipSolution, MipLogging, etc.", "see": ["interfaces/highs_c_api.h for C callback constants", "lp_data/HighsCallbackStruct.h for callback type enum"], "has_pass2": false}, "highs/lp_data/HighsStatus.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsStatus.h", "filename": "HighsStatus.h", "file": "lp_data/HighsStatus.h", "brief": "Return status enum for HiGHS API calls\n\n**HighsStatus enum:**\n- kError (-1): Operation failed\n- kOk (0): Operation succeeded\n- kWarning (1): Operation succeeded with warnings\n\n**Functions:**\n- highsStatusToString(): Convert status to string\n- interpretCallStatus(): Combine call and return status with logging\n- worseStatus(): Return max(status0, status1) for status propagation", "see": ["HConst.h for HighsModelStatus (solver termination status)"], "has_pass2": false}, "highs/lp_data/HighsInfoDebug.h": {"path": "layer-4/HiGHS/highs/lp_data/HighsInfoDebug.h", "filename": "HighsInfoDebug.h", "file": "lp_data/HighsInfoDebug.h", "has_pass2": false}, "highs/simplex/HSimplexNla.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplexNla.h", "filename": "HSimplexNla.h", "file": "simplex/HSimplexNla.h", "brief": "Numerical linear algebra interface for simplex\n\nWraps HFactor with scaling, product-form updates, and iterate storage.\n\n**ProductFormUpdate:**\nStores basis updates as explicit η-vectors when HFactor update fails:\n- pivot_index/value[]: Pivot positions and values\n- start/index/value[]: Eta-file sparse storage\n- btran/ftran(): Apply product form to vectors\n- Allows continued solve after HFactor becomes unstable\n\n**SimplexIterate:**\nCheckpoint storage for basis state:\n- basis_: Basis column indices and status\n- invert_: Factorization representation\n- dual_edge_weight_[]: DSE weights for restoration\n\n**Scaling Operations:**\n- btranInScaledSpace/ftranInScaledSpace(): Solve in scaled coordinates\n- applyBasisMatrixColScale/RowScale(): Transform vectors\n- variableScaleFactor(): Individual column/row scales\n\n**Key Methods:**\n- invert(): Refactorize basis via HFactor\n- btran(): B^(-T) * rhs (row pricing)\n- ftran(): B^(-1) * rhs (column evaluation)\n- update(): Basis update with pivot\n- putInvert/getInvert(): Save/restore iterate state\n\n**Debug Support:**\n- debugCheckInvert(): Verify factorization accuracy\n- debugInvertResidualError(): Measure solution error", "see": ["util/HFactor.h for LU factorization", "simplex/HEkk.h for simplex kernel using HSimplexNla"], "has_pass2": false}, "highs/simplex/SimplexConst.h": {"path": "layer-4/HiGHS/highs/simplex/SimplexConst.h", "filename": "SimplexConst.h", "file": "simplex/SimplexConst.h", "brief": "Constants and enums for HiGHS simplex solvers\n\n**SimplexStrategy Enum:**\n- kSimplexStrategyDual: Serial dual simplex\n- kSimplexStrategyDualTasks: SIP parallel dual\n- kSimplexStrategyDualMulti: PAMI parallel dual\n- kSimplexStrategyPrimal: Primal simplex\n\n**SimplexSolvePhase Enum:**\n- kSolvePhase1: Finding feasible basis\n- kSolvePhase2: Optimizing objective\n- kSolvePhaseOptimalCleanup: Removing perturbations\n\n**EdgeWeightMode Enum:**\n- kDantzig: Original pricing\n- kDevex: Approximate steepest edge\n- kSteepestEdge: Exact steepest edge\n\n**RebuildReason Enum:**\nReasons to reinvert basis matrix (INVERT):\n- kRebuildReasonUpdateLimitReached: Too many eta factors\n- kRebuildReasonSyntheticClockSaysInvert: Time-based trigger\n- kRebuildReasonPossiblyOptimal: Check optimality\n\n**Nonbasic Status Constants:**\n- kNonbasicMoveUp/Dn/Ze: Variable movement directions\n- kNonbasicFlagTrue/False: Basic/nonbasic indicator", "see": ["simplex/HEkk.h for simplex implementation", "lp_data/HConst.h for general constants"], "has_pass2": false}, "highs/simplex/HEkk.h": {"path": "layer-4/HiGHS/highs/simplex/HEkk.h", "filename": "HEkk.h", "file": "simplex/HEkk.h", "brief": "Edinburgh simplex kernel - high-performance LP solver core\n\nHEkk (Edinburgh Kernel) is the main simplex implementation in HiGHS,\nsupporting both dual and primal simplex methods.\n\n**HEkk Class:**\nCentral simplex solver managing LP data, basis, and solve state:\n- solve(): Run simplex algorithm (auto-selects dual/primal)\n- setBasis(): Initialize from HighsBasis\n- getSolution(): Extract primal/dual solution\n\n**Key Components:**\n- lp_: The LP being solved (may be scaled/dualized copy)\n- basis_: SimplexBasis with basic variable indices and status\n- simplex_nla_: Numeric linear algebra (factorization)\n- dual_edge_weight_: Steepest edge or Devex weights\n\n**Simplex Operations:**\n- btran/ftran: Backward/forward transformation with basis\n- pivotColumnFtran: Compute pivot column for ratio test\n- unitBtran: Compute row of B^{-1}\n\n**Transformations:**\n- dualize/undualize: Convert LP to/from dual form\n- permute/unpermute: Reorder LP for efficiency\n\n**Parallelism:**\n- chooseSimplexStrategyThreads(): Configure parallel strategy", "algorithm": "Hyper-sparse Computation:\n  Exploits sparsity in FTRAN/BTRAN when < 10% of elements nonzero.\n  Uses specialized scatter/gather for cache efficiency.", "complexity": "Per-iteration: O(nnz(B^{-1}·v)) for FTRAN/BTRAN\n  Hyper-sparse: O(nnz(result)) when exploiting sparsity\n  Parallelization: independent FTRAN/BTRAN across columns", "ref": ["Huangfu, Q. and Hall, J.A.J. (2018). \"Parallelizing the dual revised\n  simplex method\". Math. Prog. Computation 10:119-142.", "Hall, J.A.J. and McKinnon, K.I.M. (2005). \"Hyper-sparsity in the\n  revised simplex method and how to exploit it\". Comp. Opt. Appl. 32:259-283."], "see": ["HEkkDual.h for dual simplex implementation", "HEkkPrimal.h for primal simplex implementation", "HSimplexNla.h for basis factorization", "SimplexStruct.h for SimplexBasis, HighsSimplexStatus"], "has_pass2": true}, "highs/simplex/HSimplexReport.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplexReport.h", "filename": "HSimplexReport.h", "file": "simplex/HSimplexReport.h", "brief": "Simplex iteration progress reporting\n\nFunctions for logging simplex solve progress.\n\n**reportSimplexPhaseIterations():**\nReports iteration counts and phase transitions:\n- iteration_count: Total simplex iterations\n- info: HighsSimplexInfo with infeasibility counts\n- initialise: Reset counters for new phase\n\n**Output Includes:**\n- Phase 1/2 transition points\n- Iteration milestones\n- Infeasibility reduction progress", "see": ["simplex/HEkk.h for simplex solver using this", "simplex/HighsSimplexAnalysis.h for detailed analysis"], "has_pass2": false}, "highs/simplex/HEkkPrimal.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkPrimal.h", "filename": "HEkkPrimal.h", "file": "simplex/HEkkPrimal.h", "brief": "Phase 2 primal simplex solver for HiGHS", "algorithm": "Primal Simplex with Hyper-Sparse CHUZC\n\nImplements primal simplex with column selection (CHUZC) and row\nselection (CHUZR). Primarily used for Phase 1 when dual is infeasible.\n\n**Key Operations:**\n- chuzc()/chooseColumn(): Select entering variable (violating bound)\n- chooseRow(): Ratio test to find leaving variable\n- considerBoundSwap(): Handle bound-to-bound moves\n- updateDual(): Maintain reduced costs after pivot\n\n**Hyper-Sparse CHUZC:**\n- hyper_chuzc_candidate[]: Top candidates for entering\n- hyperChooseColumn*(): Efficient candidate tracking\n\n**Edge Weights:**\n- edge_weight_[]: Pricing weights (Devex or Steepest Edge)\n- devex_index_[]: Reference set for Devex framework\n- updatePrimalSteepestEdgeWeights(): Exact weight updates\n\n**Phase 1:**\n- phase1ChooseRow(): Modified ratio test for feasibility\n- phase1ComputeDual(): Dual values for infeasibility measure", "see": ["simplex/HEkk.h for main simplex class", "simplex/HEkkDual.h for dual simplex"], "has_pass2": true}, "highs/simplex/HEkkDualRHS.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkDualRHS.h", "filename": "HEkkDualRHS.h", "file": "simplex/HEkkDualRHS.h", "brief": "Dual simplex optimality test for HiGHS", "has_pass2": false}, "highs/simplex/HSimplex.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplex.h", "filename": "HSimplex.h", "file": "simplex/HSimplex.h", "brief": "Simplex utility functions\n\nStandalone functions for simplex basis and scaling operations.\n\n**Basis Extension:**\n- appendNonbasicColsToBasis(): Add new columns as nonbasic\n- appendBasicRowsToBasis(): Add new rows as basic (slack variables)\nWorks with both HighsBasis and SimplexBasis.\n\n**Solution Status:**\n- getUnscaledInfeasibilities(): Compute infeasibilities in original space\n- setSolutionStatus(): Set primal/dual solution status in HighsInfo\n\n**Scaling:**\n- scaleSimplexCost(): Scale objective coefficients\n- unscaleSimplexCost(): Reverse objective scaling\n\n**Validation:**\n- isBasisRightSize(): Check SimplexBasis dimensions match LP", "see": ["simplex/HEkk.h for main simplex solver class", "simplex/SimplexStruct.h for SimplexBasis definition"], "has_pass2": false}, "highs/simplex/SimplexTimer.h": {"path": "layer-4/HiGHS/highs/simplex/SimplexTimer.h", "filename": "SimplexTimer.h", "file": "simplex/SimplexTimer.h", "brief": "Timer clock indices for profiling simplex operations\n\nDefines clock identifiers for fine-grained simplex timing.\n\n**iClockSimplex Enum:**\nClock indices for HighsTimer used by simplex:\n\n**Top-Level Clocks:**\n- SimplexTotalClock: Total simplex time\n- SimplexDualPhase1/2Clock: Dual simplex phases\n- SimplexPrimalPhase1/2Clock: Primal simplex phases\n\n**Iteration Clocks:**\n- IterateClock: Per-iteration timing\n- IterateChuzrClock: CHUZR (row selection)\n- IterateChuzcClock: CHUZC (column selection)\n- IterateFtranClock: FTRAN operations\n- IterateDualClock: Dual value updates\n- IteratePrimalClock: Primal value updates\n\n**Infrastructure Clocks:**\n- InvertClock: Basis factorization\n- BasisConditionClock: Condition estimation\n- DseIzClock: DSE weight initialization", "see": ["util/HighsTimer.h for timer implementation", "simplex/HighsSimplexAnalysis.h for timing analysis"], "has_pass2": false}, "highs/simplex/HSimplexDebug.h": {"path": "layer-4/HiGHS/highs/simplex/HSimplexDebug.h", "filename": "HSimplexDebug.h", "file": "simplex/HSimplexDebug.h", "brief": "Debug utilities for simplex algorithm validation\n\nFunctions to detect numerical issues and algorithm failures.\n\n**CHUZC Failure Diagnosis:**\n- debugDualChuzcFailNorms(): Compute norms for failure analysis\n- debugDualChuzcFailQuad0/1(): Check quadratic pricing failures\n- debugDualChuzcFailHeap(): Check heap-based pricing failures\n\n**Basis Validation:**\n- debugNonbasicFlagConsistent(): Verify nonbasicFlag matches basis\n\n**Usage:**\nCalled when simplex encounters unexpected conditions:\n- Ratio test returns no candidate\n- Basis becomes numerically unstable\n- Iteration cycling detected\n\n**Return Type:**\nHighsDebugStatus indicates severity:\n- kOk: No issues detected\n- kWarning: Potential numerical concern\n- kError: Algorithmic failure", "see": ["simplex/HEkkDual.h for dual simplex CHUZC", "simplex/HEkkPrimal.h for primal simplex CHUZR"], "has_pass2": false}, "highs/simplex/HEkkDual.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkDual.h", "filename": "HEkkDual.h", "file": "simplex/HEkkDual.h", "brief": "Dual simplex solver for HiGHS\n\nImplements dual simplex algorithm with CHUZR (row selection), PRICE\n(pivot row computation), CHUZC (column selection), and basis update.\n\n**Parallelization Strategies:**\n- Plain: Serial dual simplex (kSimplexStrategyDualPlain)\n- SIP: Suboptimization with Independent Parallelism (Tasks)\n- PAMI: Parallel Minor Iterations (Multi)\n\n**Key Phases:**\n- Phase 1: Minimize sum of infeasibilities to find feasible basis\n- Phase 2: Optimize objective maintaining dual feasibility\n\n**Edge Weight Modes:**\n- Dantzig: Simple pricing\n- Devex: Approximate steepest edge\n- Steepest Edge: Exact steepest edge with DSE vector updates\n\n**PAMI Data Structures:**\n- MChoice: Multiple row candidates from CHUZR\n- MFinish: Minor iteration data for parallel updates\n- slice_*: Partitioned matrix for parallel PRICE", "algorithm": "PAMI - Parallel Minor Iterations (iterateMulti):\n  Exploit parallelism by batching multiple pivots:\n  1. majorChooseRow: Select k candidate leaving rows in parallel\n  2. majorChooseRowBtran: Parallel BTRAN for all k row_ep vectors\n  3. For each minor iteration i = 1..k:\n     a. minorChooseRow: Pick best from remaining candidates\n     b. chooseColumnSlice: Parallel PRICE across matrix slices\n     c. minorUpdate: Update data structures locally\n  4. majorUpdate: Apply all k updates to LU factors at once", "ref": ["Maros, I. (2003). \"Computational Techniques of the Simplex Method\".\n       Springer, Chapters 9-10.", "Forrest, J.J. and Goldfarb, D. (1992). \"Steepest-edge simplex\n       algorithms for linear programming\". Math. Programming 57:341-374.", "Harris, P.M.J. (1973). \"Pivot selection methods of the Devex\n       LP code\". Math. Programming Study 4:30-57.", "Hall, J.A.J. and McKinnon, K.I.M. (2005). \"Hyper-sparsity in the\n       revised simplex method and how to exploit it\". CMS 2(1):21-40."], "math": "Edge weight γ_i = ||B^{-1} e_i||² = ||τ_i||²\n  After basis change with pivot row r and column s:\n  - Compute τ_s = B^{-1} e_s (FTRAN of unit vector)\n  - Update weights: γ_i' = γ_i - 2(τ_r · a_q)α_i/α_r + γ_r(α_i/α_r)²\n  Cost: one extra FTRAN per iteration, but ~40% fewer iterations.", "complexity": "Serial iteration: O(nnz) for BTRAN/FTRAN, O(m) for ratio test\n  PAMI: O(k × nnz/p + k² × m) for k pivots on p processors\n  Total iterations: typically O(m) to O(2m) for dual simplex", "see": ["simplex/HEkk.h for main simplex class", "simplex/HEkkDualRow.h for pivot row handling", "simplex/HEkkDualRHS.h for RHS management"], "has_pass2": true}, "highs/simplex/HighsSimplexAnalysis.h": {"path": "layer-4/HiGHS/highs/simplex/HighsSimplexAnalysis.h", "filename": "HighsSimplexAnalysis.h", "file": "simplex/HighsSimplexAnalysis.h", "brief": "Analyse simplex iterations, both for run-time control and data\ngathering", "has_pass2": false}, "highs/simplex/SimplexStruct.h": {"path": "layer-4/HiGHS/highs/simplex/SimplexStruct.h", "filename": "SimplexStruct.h", "file": "simplex/SimplexStruct.h", "brief": "Core data structures for HiGHS simplex solvers\n\n**SimplexBasis:**\nLow-level basis representation:\n- basicIndex_[row]: Variable index of basic variable in each row\n- nonbasicFlag_[var]: 0=basic, 1=nonbasic\n- nonbasicMove_[var]: Direction to move (-1=lower, +1=upper, 0=fixed/free)\n- hash: Basis fingerprint for debugging\n\n**HighsSimplexStatus:**\nSimplex solver state flags:\n- has_basis, has_ar_matrix, has_nla, has_invert\n- has_dual_steepest_edge_weights\n- is_dualized, is_permuted (problem transformations)\n\n**HighsSimplexInfo:**\nWorking data for simplex iterations:\n- workCost_/workDual_/workShift_: Objective and dual values\n- workLower_/workUpper_/workRange_/workValue_: Nonbasic variable bounds\n- baseLower_/baseUpper_/baseValue_: Basic variable data\n- Perturbation flags: costs_perturbed, bounds_perturbed\n- Infeasibility counts: num_primal/dual_infeasibilities\n- Backtracking data for singular basis recovery\n- DSE (Dual Steepest Edge) control parameters\n\n**HighsSimplexBadBasisChangeRecord:**\nTracks tabooed pivot operations to avoid cycling.\n\n**HighsRayRecord:**\nStores primal/dual ray for unbounded/infeasible detection.", "see": ["simplex/HEkk.h for simplex solver using these structures", "simplex/SimplexConst.h for simplex constants"], "has_pass2": false}, "highs/simplex/HEkkDualRow.h": {"path": "layer-4/HiGHS/highs/simplex/HEkkDualRow.h", "filename": "HEkkDualRow.h", "file": "simplex/HEkkDualRow.h", "brief": "Dual simplex ratio test for HiGHS", "has_pass2": false}, "highs/parallel/HighsParallel.h": {"path": "layer-4/HiGHS/highs/parallel/HighsParallel.h", "filename": "HighsParallel.h", "file": "parallel/HighsParallel.h", "brief": "High-level parallel execution API with spawn/sync pattern\n\nTask-based parallelism modeled on Intel TBB / Cilk Plus semantics.\n\n**Initialization:**\n- initialize_scheduler(): Set up thread pool\n  - Default: hardware_concurrency/2 threads\n  - HIGHS_NO_DEFAULT_THREADS: Force single-threaded\n\n**Spawn/Sync Pattern:**\n- spawn(f): Push task to local deque, may be stolen\n- sync(): Wait for most recent spawn to complete\n- Child stealing: spawned tasks run depth-first or stolen\n\n**TaskGroup:**\n- RAII wrapper for spawn/sync blocks\n- taskWait(): Wait for all spawned tasks\n- cancel(): Mark pending tasks as cancelled\n- Destructor ensures cleanup\n\n**Parallel Loops:**\n- for_each(start, end, f, grainSize): Recursive binary splitting\n  - Splits until range <= grainSize\n  - Spawns right half, executes left half\n  - Task parallelism with load balancing\n\n**Thread Info:**\n- num_threads(): Total worker count\n- thread_num(): Current worker ID", "see": ["parallel/HighsTaskExecutor.h for executor implementation", "parallel/HighsSplitDeque.h for work-stealing deque"], "has_pass2": false}, "highs/parallel/HighsTask.h": {"path": "layer-4/HiGHS/highs/parallel/HighsTask.h", "filename": "HighsTask.h", "file": "parallel/HighsTask.h", "brief": "Fixed-size task for work-stealing scheduler\n\nInline callable storage with atomic state for task synchronization.\n\n**Memory Layout:**\n- taskData[]: 64-sizeof(Metadata) bytes for callable storage\n- metadata.stealer: Atomic for stealer pointer + status flags\n- Total: kMaxTaskSize = 64 bytes (cache line sized)\n\n**Status Flags (packed in stealer pointer):**\n- kFinishedFlag: Task execution completed\n- kCancelFlag: Task marked for cancellation\n- Pointer bits: Which worker stole this task\n\n**Callable Storage:**\n- Type-erased via virtual CallableBase interface\n- Placement new into taskData buffer\n- Requires trivially destructible callables\n\n**State Transitions:**\n- setTaskData(): Initialize with callable, stealer=0\n- run() by owner: Execute if not cancelled\n- run(stealer): Execute and mark stealer, return owner to notify\n- markAsFinished(): Set finished flag, return waiting owner\n- cancel(): Set cancel flag atomically\n\n**Interrupt Handling:**\n- HighsTask::Interrupt exception for task cancellation\n- Propagates up through task tree on cancel", "see": ["parallel/HighsSplitDeque.h for task queuing", "parallel/HighsTaskExecutor.h for execution"], "has_pass2": false}, "highs/parallel/HighsSplitDeque.h": {"path": "layer-4/HiGHS/highs/parallel/HighsSplitDeque.h", "filename": "HighsSplitDeque.h", "file": "parallel/HighsSplitDeque.h", "brief": "Chase-Lev work-stealing deque with split point optimization\n\nLock-free concurrent deque enabling efficient task-parallel execution.\n\n**Chase-Lev Deque:**\n- Owner pushes/pops from head (LIFO for locality)\n- Thieves steal from tail (FIFO for load balancing)\n- Single-word CAS for conflict resolution\n\n**Split Point Optimization:**\n- tail/split packed in 64-bit atomic (ts)\n- Split divides deque: [tail, split) available for stealing\n- Owner controls split growth via growShared()/shrinkShared()\n- Reduces contention when deque not fully shared\n\n**Memory Layout (Cache-Aligned):**\n- OwnerData (64B): head, split copy, workers, RNG\n- splitRequest (64B): Flag for thieves requesting more work\n- StealerData (64B): semaphore, injectedTask, ts atomic\n- WorkerBunkData (64B): nextSleeper for sleep stack\n- taskArray: 8192 task slots\n\n**WorkerBunk:**\n- Global sleep/wake coordination across workers\n- Lock-free sleeper stack with ABA-safe CAS\n- publishWork(): Wake sleepers when work available\n\n**Pop Status:**\n- kEmpty: No tasks\n- kStolen: Task was stolen, need sync\n- kWork: Task available for execution\n- kOverflown: Queue full, task executed inline", "see": ["parallel/HighsTaskExecutor.h for executor using this deque", "parallel/HighsTask.h for task representation"], "has_pass2": false}, "highs/parallel/HighsCacheAlign.h": {"path": "layer-4/HiGHS/highs/parallel/HighsCacheAlign.h", "filename": "HighsCacheAlign.h", "file": "parallel/HighsCacheAlign.h", "brief": "Cache-line aligned memory allocation for parallel performance\n\nPrevents false sharing by ensuring separate objects reside on different cache lines.\n\n**cache_aligned Struct:**\nStatic utilities for aligned allocation:\n- alignment() → 64 bytes (typical cache line size)\n- alloc(size): Allocate with 64-byte alignment\n- free(ptr): Deallocate aligned memory\n\n**Implementation:**\n- Over-allocates by alignment bytes\n- Stores original pointer before aligned address\n- Retrieves original pointer for deallocation\n\n**Smart Pointer Support:**\n- Deleter<T>: Custom deleter calling destructor + free\n- unique_ptr<T>: Alias with custom deleter\n- make_unique<T>(): Factory function\n- make_unique_array<T>(n): Array allocation\n\n**Usage:**\nWrap thread-local data in cache_aligned::unique_ptr to prevent\nperformance degradation from false sharing in parallel algorithms.", "see": ["parallel/HighsCombinable.h for thread-local storage", "parallel/HighsSplitDeque.h for aligned worker data"], "has_pass2": false}, "highs/parallel/HighsSpinMutex.h": {"path": "layer-4/HiGHS/highs/parallel/HighsSpinMutex.h", "filename": "HighsSpinMutex.h", "file": "parallel/HighsSpinMutex.h", "brief": "Lightweight spin lock for short critical sections\n\nBusy-waits instead of blocking, ideal for short-held locks.\n\n**HighsSpinMutex Class:**\nSimple test-and-set spin lock:\n- flag: Atomic bool (false = unlocked)\n- try_lock(): Non-blocking lock attempt\n- lock(): Spin until acquired\n- unlock(): Release with release semantics\n\n**Spinning Strategy:**\n- yieldProcessor(): CPU hint during spin wait\n  - x86: _mm_pause() (reduces power, prevents pipeline stalls)\n  - Others: std::this_thread::yield()\n\n**TTAS Pattern:**\nTest-and-Test-and-Set for reduced bus traffic:\n1. Exchange to try acquiring\n2. If failed, spin on load (cache-local)\n3. Only retry exchange when flag appears free\n\n**When to Use:**\nPrefer over std::mutex when critical sections are very short\nand contention is low. Avoids syscall overhead.", "see": ["parallel/HighsTaskExecutor.h for task queue locking"], "has_pass2": false}, "highs/parallel/HighsCombinable.h": {"path": "layer-4/HiGHS/highs/parallel/HighsCombinable.h", "filename": "HighsCombinable.h", "file": "parallel/HighsCombinable.h", "brief": "Thread-local storage with reduction for parallel algorithms\n\nPer-thread copies that are lazily initialized and can be combined.\n\n**Design (similar to TBB combinable):**\n- One T instance per worker thread\n- Cache-line aligned to prevent false sharing\n- Lazy initialization on first access\n\n**Construction:**\n- Default: T() for each thread's copy\n- Custom: Callable returning T for each thread\n\n**Access:**\n- local(): Get/create this thread's copy\n- Thread ID from HighsTaskExecutor::getThisWorkerDeque()\n\n**Reduction:**\n- combine_each(f): Apply f to each initialized copy\n- combine(f): Reduce all copies with binary operation f\n  - Returns combined result by move\n\n**Usage Pattern:**\n```cpp\nHighsCombinable<int> sum;\nparallel::for_each([&](int i, int j) {\n  sum.local() += compute(i, j);\n});\nint total = sum.combine(std::plus<int>());\n```", "see": ["parallel/HighsTaskExecutor.h for worker identification", "parallel/HighsParallel.h for parallel algorithms"], "has_pass2": false}, "highs/parallel/HighsTaskExecutor.h": {"path": "layer-4/HiGHS/highs/parallel/HighsTaskExecutor.h", "filename": "HighsTaskExecutor.h", "file": "parallel/HighsTaskExecutor.h", "brief": "Work-stealing thread pool for task parallelism\n\nManages worker threads and coordinates task execution via work stealing.\n\n**Thread Pool:**\n- Main thread (worker 0) + N-1 spawned workers\n- Each worker has own HighsSplitDeque for local tasks\n- Thread-local storage for current worker's deque\n\n**Work Stealing Loop:**\n- random_steal_loop(): Try stealing from random victims\n  - Exponential backoff with microsecond timing\n  - Falls back to global sync after timeout\n- Workers sleep when no work available (WorkerBunk)\n\n**Stolen Task Sync:**\n- sync_stolen_task(): Wait for task stolen by another worker\n  - Leapfrog stealing: steal from the stealer\n  - Spin wait with exponential backoff\n  - Sleep with notification when timeout exceeded\n\n**Lifecycle:**\n- initialize(): Create executor singleton\n- shutdown(): Stop all workers, join or detach threads\n- ExecutorHandle: RAII cleanup on thread exit", "see": ["parallel/HighsSplitDeque.h for deque implementation", "parallel/HighsParallel.h for high-level API"], "has_pass2": false}, "highs/parallel/HighsRaceTimer.h": {"path": "layer-4/HiGHS/highs/parallel/HighsRaceTimer.h", "filename": "HighsRaceTimer.h", "file": "parallel/HighsRaceTimer.h", "brief": "Lock-free timer for parallel algorithm racing\n\nAllows multiple threads to race, with early termination when limit reached.\n\n**HighsRaceTimer<T> Class:**\nAtomic limit value that can only decrease:\n- Constructor: Initialize limit to max value (no limit)\n- decreaseLimit(newLimit): Atomically reduce limit (CAS loop)\n- limitReached(currentTime): Check if time exceeds limit\n\n**Memory Ordering:**\nUses relaxed ordering since:\n- Only one direction (decrease) of updates\n- Eventual consistency sufficient for early termination\n- No happens-before relationships required\n\n**Usage:**\nMultiple solvers race; first to find solution decreases limit.\nSlower solvers check limitReached() and abort early.", "see": ["mip/HighsSearch.h for parallel node processing"], "has_pass2": false}, "highs/model/HighsHessianUtils.h": {"path": "layer-4/HiGHS/highs/model/HighsHessianUtils.h", "filename": "HighsHessianUtils.h", "file": "model/HighsHessianUtils.h", "brief": "Utility functions for Hessian matrix manipulation\n\nFree functions for validating, transforming, and operating on Hessian matrices.\n\n**Validation:**\n- assessHessian(): Comprehensive Hessian validation\n- assessHessianDimensions(): Check dimension consistency\n- okHessianDiagonal(): Verify positive diagonal for convexity\n\n**Transformation:**\n- normaliseHessian(): Standardize Hessian format\n- extractTriangularHessian(): Convert to lower-triangular storage\n- triangularToSquareHessian(): Expand to full symmetric matrix\n- completeHessianDiagonal(): Add missing diagonal entries\n- completeHessian(): Extend to full variable dimension\n\n**Scaling:**\n- userScaleHessian(): Apply/remove user-provided scaling factors\n\n**Reporting:**\n- reportHessian(): Print Hessian structure and values", "see": ["model/HighsHessian.h for Hessian data structure", "model/HighsModel.h for combined LP+QP model"], "has_pass2": false}, "highs/model/HighsModel.h": {"path": "layer-4/HiGHS/highs/model/HighsModel.h", "filename": "HighsModel.h", "file": "model/HighsModel.h", "brief": "Combined LP/QP model container\n\n**HighsModel Class:**\nCombines HighsLp (linear constraints/objective) with HighsHessian (quadratic terms).\n\n**Data Members:**\n- lp_: Linear program data (constraints, bounds, linear objective)\n- hessian_: Quadratic objective term Q for QP: min 0.5*x'Qx + c'x\n\n**Model Type Detection:**\n- isQp(): Returns true if hessian_.dim_ != 0\n- isMip(): Delegates to lp_.isMip() (checks integrality constraints)\n- isEmpty(): True if num_col_ == 0 and num_row_ == 0\n\n**Objective Evaluation:**\n- objectiveValue(): Compute c'x + 0.5*x'Qx\n- objectiveGradient(): Compute c + Qx", "see": ["HighsLp.h for LP data structure", "HighsHessian.h for Hessian matrix"], "has_pass2": false}, "highs/model/HighsHessian.h": {"path": "layer-4/HiGHS/highs/model/HighsHessian.h", "filename": "HighsHessian.h", "file": "model/HighsHessian.h", "brief": "Sparse Hessian matrix for QP objective\n\n**HighsHessian Class:**\nStores quadratic objective term Q for QP: min 0.5*x'Qx + c'x\n\n**Sparse Storage (CSC-like):**\n- dim_: Number of variables (Q is dim_ x dim_)\n- format_: kTriangular (lower triangle) or kSquare (full matrix)\n- start_[]: Column start indices\n- index_[]: Row indices\n- value_[]: Non-zero values\n\n**Operations:**\n- product(): Compute Qx (Hessian-vector product)\n- objectiveValue(): Compute 0.5*x'Qx\n- objectiveCDoubleValue(): High-precision objective using HighsCDouble\n- deleteCols(): Remove columns/rows for presolve", "see": ["HighsModel.h for combined LP+Hessian model", "HConst.h for HessianFormat enum"], "has_pass2": false}, "highs/ipm/IpxWrapper.h": {"path": "layer-4/HiGHS/highs/ipm/IpxWrapper.h", "filename": "IpxWrapper.h", "file": "ipm/IpxWrapper.h", "brief": "Wrapper for IPX interior point solver\n\nProvides interface between HiGHS and the IPX interior point solver.\n\n**Main Functions:**\n- solveLpIpx(): Solve LP using IPX with optional crossover to basis\n- fillInIpxData(): Convert HighsLp to IPX's input format\n- getHighsNonVertexSolution(): Extract non-vertex (interior) solution\n\n**Status Reporting:**\n- reportIpxSolveStatus(): Convert IPX status to HighsStatus\n- reportIpxIpmCrossoverStatus(): Report IPM/crossover termination\n- ipxStatusError(): Handle IPX error conditions\n- reportIpmNoProgress(): Log stalled IPM iterations\n\n**HIPO Support (optional):**\nParallel interior point variant (compile-time flag HIPO):\n- solveLpHipo(): High-performance IPM solver\n- getHipoNonVertexSolution(): Extract HIPO solution", "see": ["ipm/ipx/lp_solver.h for IPX solver class", "ipm/IpxSolution.h for IPX solution struct", "lp_data/HighsSolution.h for solution conversion utilities"], "has_pass2": false}, "highs/ipm/IpxSolution.h": {"path": "layer-4/HiGHS/highs/ipm/IpxSolution.h", "filename": "IpxSolution.h", "file": "ipm/IpxSolution.h", "brief": "IPX interior point solution container\n\nStores primal/dual solution and basis status from IPX solver.\n\n**IpxSolution Struct:**\nSolution data in IPX's native format for transfer to HiGHS:\n- num_col, num_row: Problem dimensions\n- ipx_col_value[], ipx_row_value[]: Primal solution (x, slacks)\n- ipx_col_dual[], ipx_row_dual[]: Dual solution (reduced costs, duals)\n- ipx_col_status[], ipx_row_status[]: Basis status for crossover\n\n**Status Values:**\nIPX uses its own status encoding (see ipx_status.h):\n- Basic, AtLower, AtUpper, Free for variables\n- Converted to HighsBasisStatus in IpxWrapper\n\n**Usage:**\n1. IPX populates after solve\n2. IpxWrapper extracts to HighsSolution/HighsBasis\n3. Crossover uses basis status for simplex warm start", "see": ["ipm/IpxWrapper.h for solution extraction", "lp_data/HighsSolution.h for HiGHS solution format"], "has_pass2": false}, "highs/mip/HighsPrimalHeuristics.h": {"path": "layer-4/HiGHS/highs/mip/HighsPrimalHeuristics.h", "filename": "HighsPrimalHeuristics.h", "file": "mip/HighsPrimalHeuristics.h", "brief": "Primal heuristics for finding MIP feasible solutions\n\nCollection of primal heuristics to discover incumbent solutions.", "algorithm": "ZI-Round (Zero-One Rounding):\nShifts variables one at a time to improve integrality.\n\n  For each fractional integer variable x_j:\n    Test shifting up: x_j -> ceil(x_j)\n    Test shifting down: x_j -> floor(x_j)\n    Choose shift that maintains feasibility\n    Update constraint slacks incrementally", "ref": ["Danna, Rothberg & Le Pape (2005). \"Exploring Relaxation Induced\n  Neighborhoods to Improve MIP Solutions\". Math. Programming 102(1):71-90.", "Berthold (2014). \"RENS: The Optimal Rounding\". Math. Programming Comp.\n\n**Sub-MIP Heuristics:**\n- solveSubMip(): Solve restricted MIP with fixed/bounded variables\n- RENS(): Relaxation Enforced Neighborhood Search (fix non-LP-integer vars)\n- RINS(): Relaxation Induced Neighborhood Search (fix incumbent-matching vars)\n\n**Rounding Heuristics:**\n- feasibilityPump(): Iterate between LP and MIP rounding until feasible\n- centralRounding(): Round from analytic center\n- randomizedRounding(): Probabilistic rounding based on fractionality\n- ziRound(): Berthold's ZI-Round shifting\n- tryRoundedPoint(): Simple rounding with constraint repair\n- linesearchRounding(): Round along line between two points\n\n**Other Methods:**\n- rootReducedCost(): Fix variables using reduced costs at root\n- shifting(): Variable shifting to repair constraint violations\n\n**Adaptive Targeting:**\n- determineTargetFixingRate(): Adjust fixing rate based on success history\n- successObservations/infeasObservations: Track heuristic effectiveness"], "complexity": "- RINS/RENS: O(sub-MIP size) - typically much smaller than original\n- Feasibility pump: O(iterations * LP_solve)\n- ZI-Round: O(n * m) for n variables, m constraints", "see": ["mip/HighsMipSolverData.h for heuristic controller", "mip/HighsLpRelaxation.h for LP solution access"], "has_pass2": true}, "highs/mip/HighsRedcostFixing.h": {"path": "layer-4/HiGHS/highs/mip/HighsRedcostFixing.h", "filename": "HighsRedcostFixing.h", "file": "mip/HighsRedcostFixing.h", "brief": "Reduced cost fixing and lurking bounds for MIP\n\nUses LP reduced costs to fix variable bounds based on cutoff.\n\n**Reduced Cost Fixing:**\nIf fixing variable j to its bound would increase objective beyond cutoff,\nthe opposite bound becomes valid. For minimization:\n- If reduced_cost[j] > 0 and x[j] = lb: fixing to ub proves lb valid\n- If reduced_cost[j] < 0 and x[j] = ub: fixing to lb proves ub valid\n\n**Lurking Bounds:**\nBounds that become valid at specific objective values:\n- lurkingColUpper[col]: (objective_threshold, bound_value) pairs\n- lurkingColLower[col]: Maps threshold to tighter bound\n- getLurkingBounds(): Extract bounds valid at current cutoff\n\n**Propagation Methods:**\n- propagateRootRedcost(): Apply fixings at root using stored costs\n- propagateRedCost(): Apply fixings at any node using LP reduced costs\n- addRootRedcost(): Store root LP reduced costs for later use\n\n**Integration:**\n- Called when incumbent improves (new cutoff enables more fixings)\n- Provides global domain tightening from LP dual information", "see": ["mip/HighsDomain.h for bound propagation", "mip/HighsLpRelaxation.h for LP solution access"], "has_pass2": false}, "highs/mip/HighsDebugSol.h": {"path": "layer-4/HiGHS/highs/mip/HighsDebugSol.h", "filename": "HighsDebugSol.h", "file": "mip/HighsDebugSol.h", "brief": "Debug solution tracking for MIP solver validation\n\nConditionally compiled (HIGHS_DEBUGSOL) facility to verify MIP solver\ncorrectness against a known optimal solution.\n\n**When HIGHS_DEBUGSOL Defined:**\n- debugSolution[]: Known optimal solution values\n- debugSolObjective: Optimal objective value\n- conflictingBounds: Tracks domain changes inconsistent with debug solution\n\n**Validation Methods:**\n- checkCut(): Verify cut doesn't exclude debug solution\n- checkRow(): Verify row bounds contain debug solution\n- checkClique(): Verify clique is satisfied by debug solution\n- checkVub/Vlb(): Verify variable bounds contain debug solution\n- nodePruned(): Alert if node containing debug solution is pruned\n\n**Domain Tracking:**\n- registerDomain(): Track a HighsDomain instance\n- boundChangeAdded/Removed(): Monitor bound changes\n- resetDomain(): Clear domain tracking\n\n**Conflict Analysis:**\n- checkConflictReasonFrontier(): Verify conflict derivation\n- checkConflictReconvergenceFrontier(): Verify reconvergence\n\n**When HIGHS_DEBUGSOL Not Defined:**\nAll methods become empty stubs with zero overhead.", "see": ["mip/HighsDomain.h for domain propagation", "mip/HighsConflictPool.h for conflict constraints"], "has_pass2": false}, "highs/mip/HighsSearch.h": {"path": "layer-4/HiGHS/highs/mip/HighsSearch.h", "filename": "HighsSearch.h", "file": "mip/HighsSearch.h", "brief": "Branch-and-bound tree search for MIP solver\n\nImplements depth-first search with backtracking and node evaluation.", "algorithm": "Child Selection Direction:\nDetermines which child to explore first at each branch.\n\n  kRootSol:  Direction toward LP relaxation solution\n  kBestCost: Direction with lower pseudocost estimate\n  kObj:      Direction that doesn't worsen objective\n  kHybrid:   Weighted combination of inference and cost\n\n**Node Management:**\n- nodestack[]: Stack of NodeData for current branch\n- NodeData: {lower_bound, estimate, branchingdecision, nodeBasis, etc.}\n- localdom: HighsDomain for current node's variable bounds\n\n**NodeResult Enum:**\n- kBoundExceeding: Node bound exceeds cutoff\n- kDomainInfeasible: Domain propagation found infeasibility\n- kLpInfeasible: LP relaxation infeasible\n- kBranched: Node branched on integer variable\n- kSubOptimal: Node proven suboptimal\n\n**Child Selection Rules:**\n- kUp/kDown: Always branch up/down first\n- kRootSol: Prefer direction toward root solution\n- kBestCost/kWorstCost: Use pseudocost estimates\n- kHybridInferenceCost: Combine inference and cost scores\n\n**Key Operations:**\n- dive(): Descend tree solving LP at each node\n- evaluateNode(): Solve LP relaxation, check bounds\n- branch(): Select variable and create child nodes\n- selectBranchingCandidate(): Strong branching + pseudocosts\n- backtrack(): Return to parent node\n- backtrackPlunge(): Intelligent backtracking with node queue\n\n**Heuristic Support:**\n- setRINSNeighbourhood(): Fix variables from incumbent\n- setRENSNeighbourhood(): Round LP solution", "complexity": "- Node evaluation: O(LP_solve) = O(m^2 * n) worst case\n- Strong branching: O(candidates * LP_iterations)\n- Pseudocost lookup: O(1)", "see": ["mip/HighsNodeQueue.h for best-first node storage", "mip/HighsPseudocost.h for branching scores"], "has_pass2": true}, "highs/mip/HighsTableauSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsTableauSeparator.h", "filename": "HighsTableauSeparator.h", "file": "mip/HighsTableauSeparator.h", "brief": "Gomory mixed-integer cuts from LP tableau\n\nGenerates cuts by applying MIR procedure to simplex tableau rows.\n\n**Gomory Cut Generation:**\nFor each fractional basic integer variable:\n1. Extract tableau row: x_B[i] = f_0 - sum(a_j * x_N[j])\n2. Apply mixed-integer rounding (MIR)\n3. Produce cut: sum(floor(a_j) * x_j) ≤ floor(f_0) (simplified)\n\n**Separation Flow:**\n1. Get fractional integer variables from LP relaxation\n2. For each candidate, extract tableau row via BTRAN\n3. Transform using HighsTransformedLp (bound substitution)\n4. Apply MIR strengthening\n5. Add valid cuts to cut pool\n\n**Implementation:**\n- numTries: Counter for separation attempts (controls effort)\n- Uses kTableauSepaString identifier for statistics\n- Inherits run() timing from HighsSeparator base class", "see": ["mip/HighsSeparator.h for base class interface", "mip/HighsTransformedLp.h for bound substitution", "mip/HighsSeparation.h for separator orchestration"], "has_pass2": false}, "highs/mip/HighsGFkSolve.h": {"path": "layer-4/HiGHS/highs/mip/HighsGFkSolve.h", "filename": "HighsGFkSolve.h", "file": "mip/HighsGFkSolve.h", "brief": "Linear system solver over finite field GF(k)\n\nSolves congruence systems for mod-k cut generation.\n\n**HighsGFk<k> Template:**\nCompile-time multiplicative inverse via Fermat's little theorem:\n- inverse(a) = a^(k-2) mod k (when k prime)\n- powk(): Recursive repeated squaring for a^k\n- Specializations for k=2,3 (trivial inverses)\n\n**HighsGFkSolve Class:**\nSparse LU factorization in GF(k):\n\n**Storage:**\n- Triplet format: Arow[], Acol[], Avalue[] (mod k)\n- Column-wise linked list: colhead, Anext, Aprev\n- Row-wise splay tree: rowroot, ARleft, ARright\n\n**Factorization:**\n- Markowitz-style pivot selection (min row × col size)\n- factorColPerm/factorRowPerm: Pivot order recording\n- colBasisStatus/rowUsed: Track basic columns and used rows\n\n**Solution:**\n- solve<k>(): Enumerate basic solutions\n- SolutionEntry: (index, weight) for solution vector\n- Reports multiple solutions via basis swapping\n\n**Usage:**\n1. fromCSC<k>(): Load constraint matrix modulo k\n2. setRhs<k>(): Set right-hand side (k-1 for mod-k MIR)\n3. solve<k>(): Find row weights yielding maximally violated cuts", "see": ["mip/HighsModkSeparator.h for mod-k cut generation using this solver"], "has_pass2": false}, "highs/mip/HighsCutPool.h": {"path": "layer-4/HiGHS/highs/mip/HighsCutPool.h", "filename": "HighsCutPool.h", "file": "mip/HighsCutPool.h", "brief": "Cutting plane storage and separation for MIP solver\n\n**HighsCutSet:**\nContainer for cuts to add to LP relaxation:\n- cutindices: Indices into cut pool\n- ARstart_/ARindex_/ARvalue_: CSR storage for cut coefficients\n- lower_/upper_: Cut bounds (typically -inf, rhs)\n\n**HighsCutPool:**\nManages all generated cutting planes:\n- matrix_: HighsDynamicRowMatrix storing all cuts\n- rhs_[]: Right-hand sides\n- ages_[]: Cut age (incremented when not binding, reset when used)\n- hashToCutMap: Duplicate detection via row hashing\n\n**Cut Lifecycle:**\n- addCut(): Add new cut with duplicate detection and clique extraction\n- separate(): Select violated cuts for LP (scoring + density limits)\n- performAging(): Increment ages, remove old cuts (age > agelim_)\n- lpCutRemoved(): Called when cut leaves LP\n\n**Cut Propagation:**\n- propagationDomains: Registered domains for cut-based bound tightening\n- propRows: Active propagation rows", "see": ["mip/HighsDynamicRowMatrix.h for cut storage", "mip/HighsDomain.h for domain propagation"], "has_pass2": false}, "highs/mip/HighsMipSolver.h": {"path": "layer-4/HiGHS/highs/mip/HighsMipSolver.h", "filename": "HighsMipSolver.h", "file": "mip/HighsMipSolver.h", "brief": "Branch-and-cut MIP solver", "algorithm": "Branch-and-Cut for Mixed-Integer Programming:\nCombines branch-and-bound with cutting planes for MIP.\n\nOVERALL FLOW:\n  1. PRESOLVE: Reduce problem size and tighten formulation\n  2. ROOT NODE: Solve LP relaxation, add cuts, find initial solutions\n  3. BRANCH-AND-BOUND: Explore tree until proven optimal\n  4. POSTSOLVE: Restore original solution space\n\nROOT NODE PROCESSING:\n  while (progress):\n    Solve LP relaxation\n    Add violated cuts (Gomory, MIR, clique, cover)\n    Run primal heuristics (diving, RINS, RENS)\n    Update bounds\n\nBRANCH-AND-BOUND TREE:\n  Initialize: dual_bound = root_LP_obj, primal_bound = +inf\n  while (open_nodes and gap > tolerance):\n    Select node (best-first, depth-first, hybrid)\n    Solve LP relaxation\n    if (LP_infeasible or LP_obj >= primal_bound):\n      Prune node\n    elif (solution is integer):\n      Update primal_bound if better\n      Prune node\n    else:\n      Add cuts if violated\n      Branch on fractional integer variable", "math": "Optimality gap: gap = (primal_bound - dual_bound) / |primal_bound|\n  Termination when gap <= mip_rel_gap tolerance (default 0.01%)", "complexity": "- Worst case: exponential in number of integer variables\n- Practice: often polynomial due to pruning and cuts\n- Root processing: O(cuts * LP_iterations)", "ref": ["Achterberg, Bixby, Gu, Rothberg & Weninger (2020).\n  \"Presolve Reductions in Mixed Integer Programming\". INFORMS J. Computing.\n\n**HighsMipSolver Class:**\nMain MIP solver using branch-and-cut with LP relaxations.\n\n**Key State:**\n- model_/orig_model_: Problem LP with integrality constraints\n- solution_: Best incumbent solution found\n- dual_bound_/primal_bound_: Bounds on optimal value\n- gap_: Optimality gap (primal_bound - dual_bound) / primal_bound\n- node_count_: Number of B&B nodes explored\n\n**Sub-MIP Support:**\n- submip/submip_level: For recursive sub-MIP solves (e.g., in heuristics)\n- rootbasis: Starting basis from parent MIP\n- pscostinit: Warm-start pseudocosts\n- clqtableinit/implicinit: Warm-start conflict graph structures\n\n**HighsTerminator:**\nParallel termination coordination for concurrent MIP instances.\n\n**Main Entry:**\n- run(): Execute MIP solve (presolve, root LP, branch-and-cut)\n- runMipPresolve(): Standalone presolve for warm-start scenarios"], "see": ["mip/HighsMipSolverData.h for internal B&B data", "mip/HighsCutPool.h for cutting plane management", "mip/HighsCliqueTable.h for clique detection"], "has_pass2": true}, "highs/mip/HighsDynamicRowMatrix.h": {"path": "layer-4/HiGHS/highs/mip/HighsDynamicRowMatrix.h", "filename": "HighsDynamicRowMatrix.h", "file": "mip/HighsDynamicRowMatrix.h", "brief": "Dynamic row matrix with efficient column-sign iteration\n\nSparse row-wise matrix supporting efficient row add/remove with\nseparate iteration over positive and negative column entries.\n\n**Row Storage:**\n- ARrange_[row]: (start, end) range in ARindex_/ARvalue_\n- ARindex_[]/ARvalue_[]: Column indices and values\n- ARrowindex_[]: Row index for each nonzero position\n\n**Column Iteration by Sign:**\nSeparate doubly-linked lists for positive and negative entries:\n- AheadPos_[col], AnextPos_[], AprevPos_[]: Positive entry list\n- AheadNeg_[col], AnextNeg_[], AprevNeg_[]: Negative entry list\n\n**Template Iterators:**\n- forEachPositiveColumnEntry(): Iterate positive entries in column\n- forEachNegativeColumnEntry(): Iterate negative entries in column\n- Useful for implication detection (sign determines bound type)\n\n**Dynamic Operations:**\n- addRow(): Insert row, reusing deleted space from freespaces_ set\n- removeRow(): Remove row, add index to deletedrows_ for reuse\n- unlinkColumns(): Disconnect row from column lists", "see": ["mip/HighsDomain.h for domain propagation using sign iteration", "mip/HighsCutPool.h for similar dynamic cut storage"], "has_pass2": false}, "highs/mip/HighsTransformedLp.h": {"path": "layer-4/HiGHS/highs/mip/HighsTransformedLp.h", "filename": "HighsTransformedLp.h", "file": "mip/HighsTransformedLp.h", "brief": "LP bound transformations for cutting plane separation\n\nTransforms LP rows into single-row relaxations suitable for cut generation\nby substituting bounds and handling complementation.\n\n**Bound Substitution Types (BoundType enum):**\n- kSimpleLb/kSimpleUb: Direct variable bounds (x ≥ lb, x ≤ ub)\n- kVariableLb/kVariableUb: VLB/VUB from implications (x ≥ a*y + b)\n\n**Bound Selection:**\n- bestVlb/bestVub[]: Tightest variable bound for each column\n- simpleLbDist/simpleUbDist[]: Distance of LP value from simple bounds\n- lbDist/ubDist[]: Distance considering all bound types\n- boundDist[]: Minimum distance (used for coefficient strengthening)\n\n**Transform Operations:**\n- transform(): Convert row to standard form for cut generation\n  - Substitutes bounds to get non-negative variables\n  - Tracks which bound type used per variable\n  - Returns integralPositive flag for MIR applicability\n- untransform(): Reverse transformation to original space\n\n**Cut Generation Flow:**\n1. Aggregate LP rows (via HighsLpAggregator)\n2. Transform to standard form (substitute bounds)\n3. Apply cut procedure (Gomory, MIR, etc.)\n4. Untransform back to original variables", "see": ["mip/HighsLpAggregator.h for row aggregation", "mip/HighsImplications.h for VUB/VLB structures", "mip/HighsSeparation.h for cut orchestration"], "has_pass2": false}, "highs/mip/MipTimer.h": {"path": "layer-4/HiGHS/highs/mip/MipTimer.h", "filename": "MipTimer.h", "file": "mip/MipTimer.h", "brief": "Indices of mip iClocks", "has_pass2": false}, "highs/mip/HighsMipSolverData.h": {"path": "layer-4/HiGHS/highs/mip/HighsMipSolverData.h", "filename": "HighsMipSolverData.h", "file": "mip/HighsMipSolverData.h", "brief": "Central data container for MIP solver state\n\nAggregates all MIP solver components and statistics.\n\n**MipSolutionSource enum:**\nTracks origin of solutions for statistics:\n- B: Branching, C: CentralRounding, F: FeasibilityPump\n- H: Heuristic, J: FeasibilityJump, L: SubMip\n- R: RandomizedRounding, S: SolveLp, X: UserSolution, Z: ZiRound\n\n**HighsPrimaDualIntegral:**\nTracks primal-dual gap integral for solution quality metric.\n\n**HighsMipSolverData Components:**\n\n*Core Structures:*\n- cutpool: Global cut storage\n- conflictPool: Learned conflict constraints\n- domain: Global domain with bounds and propagation\n- lp: LP relaxation solver\n- nodequeue: B&B tree node priority queue\n\n*Inference:*\n- cliquetable: Binary variable cliques\n- implications: Variable implications (VUB/VLB)\n- pseudocost: Branching score history\n- redcostfixing: Reduced cost-based bound fixing\n\n*Heuristics:*\n- heuristics: Primal heuristic controller\n- objectiveFunction: Objective analysis for probing\n\n*Symmetry:*\n- symmetries/globalOrbits: Symmetry detection/handling\n- SymmetryDetectionData: Async symmetry computation\n\n*Statistics:*\n- num_nodes/num_leaves: Tree exploration counts\n- total_lp_iterations: LP iteration counters by type\n- lower_bound/upper_bound: Best bounds\n- incumbent: Best solution vector\n\n**Key Methods:**\n- init/runSetup(): Initialize solver state\n- evaluateRootNode(): Root node processing\n- trySolution/addIncumbent(): Solution handling\n- performRestart(): Restart MIP search", "see": ["mip/HighsMipSolver.h for solver interface", "mip/HighsSearch.h for tree search"], "has_pass2": false}, "highs/mip/HighsObjectiveFunction.h": {"path": "layer-4/HiGHS/highs/mip/HighsObjectiveFunction.h", "filename": "HighsObjectiveFunction.h", "file": "mip/HighsObjectiveFunction.h", "brief": "Objective function analysis for MIP solving\n\nAnalyzes objective structure to enable specialized bound propagation.\n\n**Objective Structure:**\n- objectiveNonzeros[]: Columns with nonzero objective (binaries first)\n- objectiveVals[]: Packed objective coefficients\n- numBinary/numIntegral: Variable counts by type\n\n**Integrality Detection:**\n- objIntScale: Scale factor making all integer coefficients integral\n- isIntegral(): True if objective is integer for integer solutions\n- checkIntegrality(): Verify/update integrality with epsilon\n- Enables objective cutoff rounding\n\n**Clique Partitioning:**\nPartitions binary objective variables into cliques:\n- cliquePartitionStart[]: Start indices per partition\n- colToPartition[]: Maps column to its partition\n- At most one variable per clique can be 1\n- Enables tighter objective bounds from clique constraints\n\n**Usage:**\n- Objective bound computation during propagation\n- Detecting integral objectives for cutoff strengthening\n- Clique-based objective probing", "see": ["mip/HighsCliqueTable.h for clique detection", "mip/HighsDomain.h for bound propagation using objective"], "has_pass2": false}, "highs/mip/HighsModkSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsModkSeparator.h", "filename": "HighsModkSeparator.h", "file": "mip/HighsModkSeparator.h", "brief": "Class for separating maximally violated mod-k MIR cuts", "algorithm": "Mod-k MIR Cuts via Congruence System\n\nContrary to mod-k CG cuts as described in the literature, continuous\nvariables are allowed to appear in the rows used for separation. In case an\nLP row is already an integral row it is included into the congruence system\nin the same way as for mod-k CG cuts. Should the LP row contain continuous\nvariables that have a non-zero solution value after bound substitution, then\nit is discarded, as it can not participate in a maximally violated mod-K MIR\ncut.\n\nIf a row contains continuous variables that sit at zero after bound\nsubstitution, then those rows are included in the congruence system, as the\npresence of such variables does not reduce the cuts violation when applying\nthe MIR procedure. In order to handle their presence the row must simply be\nscaled, such that all integer variables that have a non-zero solution value\nafter bound substitution, as well as the right hand side value, attain an\nintegral value. If we succeed in finding such a scale that is not too large,\nthe resulting row might get a non-zero weight in the solution of the\ncongruence system. The aggregated row therefore can contain continuous\nvariables. These variables, however, all sit at zero in the current LP\nsolution. Using the weights from the solution of the congruence system all\ninteger variables with non-zero solution value will attain a coefficient that\nis divisible by k, and the integral right hand side value will have a\nremainder of k - 1 when dividing by k. All other variables do not contribute\nto the activity of the cut in this LP solution, hence applying the MIR\nprocedure will yield a cut that is violated by (k-1)/k. However, we prefer to\ngenerate inequalities with superadditive lifting from the aggregated row\nwhenever all integer variables are bounded.", "has_pass2": true}, "highs/mip/HighsPseudocost.h": {"path": "layer-4/HiGHS/highs/mip/HighsPseudocost.h", "filename": "HighsPseudocost.h", "file": "mip/HighsPseudocost.h", "brief": "Pseudocost branching for MIP variable selection\n\nEstimates objective change from branching based on historical observations.\n\n**Score Components:**\n- pseudocostup/down[col]: Average unit objective change per direction\n- inferencesup/down[col]: Average domain reductions (bound tightenings)\n- ncutoffsup/down[col]: Count of subtree cutoffs per direction\n- conflictscoreup/down[col]: Conflict analysis contribution\n\n**Reliability:**\n- nsamplesup/down[col]: Number of observations per direction\n- minreliable: Threshold for reliable pseudocost estimate\n- isReliable(): True when min(up_samples, down_samples) >= minreliable\n- Unreliable estimates blend with global average (cost_total)\n\n**Scoring (getScore):**\nCombines multiple signals into branching score:\n- costScore: Pseudocost product (up * down) / avg^2\n- inferenceScore: Domain reduction product\n- cutoffScore: Historical cutoff frequency\n- conflictScore: Conflict analysis contribution\n- degeneracyFactor: Adjusts weighting for degenerate LPs\n\n**Key Methods:**\n- addObservation(): Record objective change from branching\n- addInferenceObservation(): Record domain reductions\n- getPseudocostUp/Down(): Get estimated change for given fraction", "see": ["mip/HighsSearch.h for branching variable selection", "mip/HighsConflictPool.h for conflict scoring"], "has_pass2": false}, "highs/mip/HighsMipAnalysis.h": {"path": "layer-4/HiGHS/highs/mip/HighsMipAnalysis.h", "filename": "HighsMipAnalysis.h", "file": "mip/HighsMipAnalysis.h", "brief": "Analyse MIP iterations, both for run-time control and data\ngathering", "has_pass2": false}, "highs/mip/HighsCutGeneration.h": {"path": "layer-4/HiGHS/highs/mip/HighsCutGeneration.h", "filename": "HighsCutGeneration.h", "file": "mip/HighsCutGeneration.h", "brief": "Class that generates cuts from single row relaxations", "algorithm": "Mixed-Integer Rounding (MIR) Cut Generation\n\nGenerates violated cutting planes from single-row relaxations using\nMIR and lifted cover inequalities.\n\n**Cut Generation Pipeline:**\n1. Complement variables to ensure non-negative solution values\n2. Substitute bounds to create single-row relaxation\n3. Try CMIR (complemented MIR) with varying scale factors\n4. If integer support, try lifted knapsack cover cuts\n\n**Key Algorithms:**\n- cmirCutGenerationHeuristic(): Scale row for best MIR violation\n- separateLiftedKnapsackCover(): Knapsack cover lifting (Gu-Nemhauser-Savelsbergh)\n- separateLiftedMixedBinaryCover(): Mixed-binary cover cuts\n- separateLiftedMixedIntegerCover(): General integer cover lifting\n\n**Parameters:**\n- minEfficacy: Minimum cut violation required\n- feastol: Feasibility tolerance\n- initialScale: Starting scale factor for MIR procedure", "see": ["mip/HighsSeparation.h for separation orchestration", "mip/HighsTransformedLp.h for bound substitution"], "has_pass2": true}, "highs/mip/HighsDomain.h": {"path": "layer-4/HiGHS/highs/mip/HighsDomain.h", "filename": "HighsDomain.h", "file": "mip/HighsDomain.h", "brief": "Bound tracking, propagation, and conflict analysis for MIP\n\n**HighsDomain Class:**\nManages variable bounds during MIP branch-and-bound with:\n- col_lower_[], col_upper_[]: Current variable bounds\n- domchgstack_[]: Stack of bound changes (for backtracking)\n- domchgreason_[]: Reason for each bound change (branching, row, clique)\n\n**Bound Propagation:**\n- propagate(): Deduce implied bounds from constraints\n- activitymin_[]/activitymax_[]: Row activity bounds for propagation\n- markPropagate(): Queue row for propagation\n- ObjectivePropagation: Deduce bounds from objective cutoff\n\n**Reason Tracking:**\nEach bound change records its cause:\n- kBranching: B&B branching decision\n- kModelRowUpper/Lower: Implied by constraint\n- kCliqueTable: Clique conflict\n- kObjective: Objective bound\n- cut(pool,index): Cut from cut pool\n\n**Conflict Analysis (ConflictSet):**\nLearns from infeasibility:\n- conflictAnalysis(): Build conflict clause from infeasibility proof\n- reasonSideFrontier: Domain changes causing infeasibility\n- Generates learned conflicts for HighsConflictPool\n\n**Cut Pool Integration (CutpoolPropagation):**\n- Tracks activity of cuts for cut-based propagation\n- Updates when bounds change\n\n**Backtracking:**\n- backtrack(): Undo bound changes\n- backtrackToGlobal(): Reset to root node bounds\n- branchPos_[]: Positions of branching decisions in stack", "algorithm": "Objective Propagation:\n  Given incumbent z* and objective c'x:\n  For each variable, derive bound from c'x < z* constraint.\n  Particularly effective when objective has large coefficients.", "complexity": "propagate(): O(nnz × propagation_rounds) amortized\n  conflictAnalysis(): O(conflict_clause_length × reason_chain_depth)\n  backtrack(): O(number_of_bound_changes_to_undo)", "ref": ["Achterberg, T. (2007). \"Conflict analysis in mixed integer\n  programming\". Discrete Optimization 4:4-20."], "see": ["mip/HighsDomainChange.h for bound change struct", "mip/HighsConflictPool.h for learned conflicts"], "has_pass2": true}, "highs/mip/HighsSeparation.h": {"path": "layer-4/HiGHS/highs/mip/HighsSeparation.h", "filename": "HighsSeparation.h", "file": "mip/HighsSeparation.h", "brief": "Cut generation orchestration for MIP solver", "algorithm": "Cut Separation Manager (Rounds-Based)\n\nCoordinates multiple separators to generate cutting planes.\n\n**Separation Management:**\n- separators[]: Collection of HighsSeparator instances\n- cutset: HighsCutSet accumulating generated cuts\n- implBoundClock, cliqueClock: Timing for specific separator types\n\n**Key Methods:**\n- separationRound(): Run all separators once\n- separate(): Multiple rounds until no more cuts found\n- setLpRelaxation(): Bind to LP for solution access\n\n**Separator Types (via HighsSeparator subclasses):**\n- Gomory cuts from optimal basis\n- MIR (mixed-integer rounding)\n- Clique cuts from conflict graph\n- Implied bound cuts from VUB/VLB\n- Flow cover cuts", "see": ["mip/HighsSeparator.h for separator base class", "mip/HighsCutPool.h for cut storage", "mip/HighsLpRelaxation.h for LP interface"], "has_pass2": true}, "highs/mip/HighsLpAggregator.h": {"path": "layer-4/HiGHS/highs/mip/HighsLpAggregator.h", "filename": "HighsLpAggregator.h", "file": "mip/HighsLpAggregator.h", "brief": "Row aggregation for cut generation\n\nCombines LP rows with weights to create mixed-integer Gomory source rows.\n\n**Core Operations:**\n- addRow(): Add weighted LP row to current aggregation\n- getCurrentAggregation(): Extract combined row as sparse vector\n- clear(): Reset for next aggregation\n\n**Aggregation Format:**\nResult includes slack variables making it an equation with RHS = 0:\n  sum(a_ij * x_j) + s_i = b_i  →  aggregated as equality\n\n**Implementation:**\n- Uses HighsSparseVectorSum for efficient sparse combination\n- Tracks nonzero indices incrementally\n- negate parameter for sign flip (row sense conversion)\n\n**Cut Generation Context:**\n1. Start from LP tableau row (basis row)\n2. Aggregate additional rows to eliminate non-integer variables\n3. Pass to HighsTransformedLp for bound substitution\n4. Generate Gomory mixed-integer cuts", "see": ["mip/HighsTransformedLp.h for bound transformation", "mip/HighsSeparation.h for cut orchestration", "util/HighsSparseVectorSum.h for sparse vector arithmetic"], "has_pass2": false}, "highs/mip/HighsSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsSeparator.h", "filename": "HighsSeparator.h", "file": "mip/HighsSeparator.h", "brief": "Abstract base class for cut separators", "algorithm": "Cut Separator Interface\n\nDefines interface for generating cutting planes from LP relaxation.\n\n**Separator Types (string constants):**\n- kImplboundSepaString: Implied bound cuts from VUB/VLB\n- kCliqueSepaString: Clique cuts from conflict graph\n- kTableauSepaString: Tableau-based cuts (Gomory)\n- kPathAggrSepaString: Path aggregation cuts\n- kModKSepaString: Mod-k cuts\n\n**Abstract Interface:**\n- separateLpSolution(): Generate cuts from current LP solution\n  - lpRelaxation: Access to LP solution and basis\n  - lpAggregator: Row aggregation helper\n  - transLp: Transformed LP with bound substitutions\n  - cutpool: Destination for generated cuts\n\n**Statistics:**\n- numCutsFound: Total cuts generated\n- numCalls: Times separator was invoked\n- clockIndex: Timer index for profiling\n\n**Usage:**\n- run(): Wrapper that times and counts separateLpSolution()\n- Subclasses implement specific cut types", "see": ["mip/HighsSeparation.h for separator orchestration", "mip/HighsCutPool.h for cut storage"], "has_pass2": true}, "highs/mip/HighsPathSeparator.h": {"path": "layer-4/HiGHS/highs/mip/HighsPathSeparator.h", "filename": "HighsPathSeparator.h", "file": "mip/HighsPathSeparator.h", "brief": "Path aggregation cuts from network structure", "algorithm": "Network Path Aggregation Cuts\n\nHeuristically identifies network paths and generates flow-based cuts.\n\n**Network Path Detection:**\n1. Find rows with network structure (±1 coefficients)\n2. Aggregate rows to form source-sink paths\n3. Resulting aggregation gives flow conservation cuts\n\n**Separation Heuristic:**\n- Start from fractional integer variables\n- Follow flow through network-like constraints\n- Randomized selection (HighsRandom) for diverse cuts\n\n**Cut Generation:**\n- Aggregated row passed to MIR procedure\n- Path structure often yields strong cuts\n- Effective on transportation/assignment problems", "see": ["mip/HighsSeparator.h for base class interface", "mip/HighsModkSeparator.h for related mod-k aggregation", "util/HighsRandom.h for randomization"], "has_pass2": true}, "highs/mip/HighsCliqueTable.h": {"path": "layer-4/HiGHS/highs/mip/HighsCliqueTable.h", "filename": "HighsCliqueTable.h", "file": "mip/HighsCliqueTable.h", "brief": "Clique detection and storage for MIP solver", "algorithm": "Conflict Graph with Bron-Kerbosch Clique Separation\n\nMaintains cliques (sets of binary variables where at most one can be 1).\n\n**CliqueVar:**\nRepresents a literal (variable=0 or variable=1):\n- col: Variable index\n- val: 0 or 1 (whether literal is x or (1-x))\n- weight(sol): Contribution to clique violation (val ? sol : 1-sol)\n\n**Clique:**\nA set of CliqueVars where sum <= 1 (or ==1 for equality cliques).\n- start/end: Range in cliqueentries[] array\n- equality: True for \"exactly one\" (SOS1) constraints\n- origin: Row index if derived from constraint\n\n**Key Operations:**\n- addClique(): Store clique with subsumption checking\n- haveCommonClique(): Test if two literals conflict (fast conflict detection)\n- separateCliques(): Find violated clique inequalities (Bron-Kerbosch)\n- extractCliques(): Derive cliques from constraints\n- runCliqueMerging(): Extend cliques using transitivity\n\n**Substitution Support:**\nWhen x1 + x2 = 1 is detected, substitute x2 = 1 - x1:\n- colsubstituted[], substitutions[]: Track substitutions\n- resolveSubstitution(): Apply substitutions to variables\n\n**Conflict Graph:**\n- invertedHashList[]: Maps literal→cliques containing it\n- sizeTwoCliques: Fast lookup for pairwise conflicts\n- addImplications(): Add implied fixings when literal is set", "see": ["mip/HighsCutPool.h for cut management", "mip/HighsDomain.h for domain propagation"], "has_pass2": true}, "highs/mip/HighsConflictPool.h": {"path": "layer-4/HiGHS/highs/mip/HighsConflictPool.h", "filename": "HighsConflictPool.h", "file": "mip/HighsConflictPool.h", "brief": "Storage for learned conflict constraints\n\nManages nogood constraints derived from infeasible subproblems.\n\n**Conflict Storage:**\n- conflictEntries_[]: Domain changes forming each conflict\n- conflictRanges_[]: {start, end} pairs indexing entries\n- Each conflict: set of HighsDomainChange that together are infeasible\n\n**Aging System:**\n- ages_[]: Tracks how long since conflict was active\n- ageDistribution_[]: Count of conflicts at each age\n- performAging(): Increment ages, remove old conflicts\n- resetAge(): Reset age when conflict becomes active\n- agelim_: Maximum age before deletion\n\n**Memory Management:**\n- freeSpaces_: Ordered set of reusable entry ranges\n- deletedConflicts_: Reusable conflict indices\n- softlimit_: Target maximum conflicts\n\n**Key Methods:**\n- addConflictCut(): Store conflict from infeasibility analysis\n- addReconvergenceCut(): Store conflict with reconvergence point\n- removeConflict(): Delete conflict and free space\n\n**Propagation Integration:**\n- propagationDomains[]: Domains using conflicts for propagation\n- modification_[]: Version counter for conflict updates", "see": ["mip/HighsDomain.h for conflict derivation", "mip/HighsSearch.h for conflict generation during search"], "has_pass2": false}, "highs/mip/HighsImplications.h": {"path": "layer-4/HiGHS/highs/mip/HighsImplications.h", "filename": "HighsImplications.h", "file": "mip/HighsImplications.h", "brief": "Implication graphs and variable bound relationships for MIP", "algorithm": "Probing and Variable Bound Propagation\n\n**HighsImplications Class:**\nTracks logical implications and variable bounds derived from binary variables.\n\n**Implications Storage:**\n- implications[2*col + val]: Bound changes implied when col=val\n- computeImplications(): Derives implications via probing\n- getImplications(): Returns cached or computes implications\n\n**Variable Upper/Lower Bounds (VUB/VLB):**\nLinear bounds of form: x_j <= coef * y + constant (VUB)\n- vubs[col], vlbs[col]: Hash trees of variable bounds\n- VarBound: {coef, constant} with minValue()/maxValue()\n- getBestVub/Vlb(): Find tightest VUB/VLB given LP solution\n\n**Key Operations:**\n- runProbing(): Test fixing binary variable and propagate\n- separateImpliedBounds(): Generate cuts from VUB/VLB\n- addVUB/addVLB(): Store new variable bounds\n- columnTransformed(): Update VBDs after variable scaling\n\n**Substitutions:**\n- substitutions[], colsubstituted[]: Track variable substitutions\n- storeLiftingOpportunity: Callback for cut strengthening", "see": ["mip/HighsDomain.h for bound propagation", "mip/HighsCliqueTable.h for clique-based implications"], "has_pass2": true}, "highs/mip/HighsLpRelaxation.h": {"path": "layer-4/HiGHS/highs/mip/HighsLpRelaxation.h", "filename": "HighsLpRelaxation.h", "file": "mip/HighsLpRelaxation.h", "brief": "LP relaxation management for MIP branch-and-bound\n\nManages the LP relaxation at each B&B node with cut handling.\n\n**Status enum:**\n- kOptimal: LP solved to optimality\n- kInfeasible: LP is infeasible (node can be pruned)\n- kUnscaledDualFeasible: Dual feasible (valid bound)\n- kUnscaledPrimalFeasible: Primal feasible (valid solution)\n- kUnbounded/kError: Problem states\n\n**LpRow Tracking:**\n- origin: kModel (original constraint) or kCutPool (added cut)\n- index: Row index in model or cut pool\n- age: Iterations since cut was binding (for aging/removal)\n\n**Key Operations:**\n- run(): Solve LP relaxation\n- addCuts(): Add cuts from HighsCutSet\n- flushDomain(): Apply bound changes from HighsDomain\n- performAging(): Age cuts, remove old non-binding cuts\n- removeObsoleteRows(): Delete aged-out cuts\n\n**Solution Access:**\n- getObjective(): LP objective value\n- getSolution(): Primal/dual values\n- getFractionalIntegers(): Integer vars with fractional values\n- integerFeasible(): True if LP solution is MIP-feasible\n\n**Dual Proofs:**\n- computeDualProof(): Derive valid inequality from LP dual\n- storeDualInfProof(): Store proof when LP is infeasible\n- getDualProof(): Retrieve stored proof for conflict analysis\n\n**Playground (nested class):**\nRAII wrapper for temporary LP modifications (e.g., strong branching).", "see": ["mip/HighsMipSolver.h for MIP solver using this", "mip/HighsCutPool.h for cut management"], "has_pass2": false}, "highs/mip/HighsNodeQueue.h": {"path": "layer-4/HiGHS/highs/mip/HighsNodeQueue.h", "filename": "HighsNodeQueue.h", "file": "mip/HighsNodeQueue.h", "brief": "Priority queue for branch-and-bound nodes", "algorithm": "Best-First Search Node Management with Red-Black Trees\n\nManages open nodes for best-first search with efficient memory allocation.\n\n**OpenNode Structure:**\n- domchgstack: Vector of bound changes from root to node\n- branchings: Indices of branching decisions\n- lower_bound, estimate: Node bounds and estimates\n- depth: Node depth in tree\n- lowerLinks, hybridEstimLinks: Red-black tree links for ordering\n\n**Custom Allocator:**\n- AllocatorState: Chunk-based allocation with freelist\n- NodesetAllocator: STL-compatible allocator for NodeSet\n- Reduces allocation overhead for many small nodes\n\n**Node Indexing:**\n- colLowerNodes/colUpperNodes: Nodes indexed by bound changes\n- Enables efficient global bound propagation\n- numNodesUp/Down(col, val): Count nodes with given bound\n\n**Selection Strategies:**\n- popBestBoundNode(): Best-first search by lower_bound\n- popBestNode(): Hybrid using estimate\n- performBounding(): Prune nodes exceeding upper limit\n\n**Key Operations:**\n- emplaceNode(): Add new node with bound changes\n- pruneInfeasibleNodes(): Remove nodes with tightened global bounds\n- checkGlobalBounds(): Update tree weight from bound changes", "see": ["mip/HighsSearch.h for depth-first search", "mip/HighsDomainChange.h for bound change structure"], "has_pass2": true}, "highs/util/stringutil.h": {"path": "layer-4/HiGHS/highs/util/stringutil.h", "filename": "stringutil.h", "file": "util/stringutil.h", "brief": "String manipulation utilities for file parsing\n\nHelper functions for trimming, case conversion, and tokenization.\n\n**Case Conversion:**\n- tolower(): Convert string to lowercase in-place\n- toupper(): Convert string to uppercase in-place\n\n**Trimming:**\n- ltrim(): Remove leading whitespace\n- rtrim(): Remove trailing whitespace\n- trim(): Remove leading and trailing whitespace\n- default_non_chars: Standard whitespace characters\n\n**Predicates:**\n- is_empty(): Check if string contains only whitespace\n- is_end(): Check if position is at end of content\n\n**Tokenization:**\n- first_word(): Extract first word from position\n- first_word_end(): Find end position of first word\n\n**Usage:**\nUsed by MPS/LP parsers for section and field extraction.", "see": ["io/HMpsFF.h for MPS parsing", "io/FilereaderLp.h for LP parsing"], "has_pass2": false}, "highs/util/HighsUtils.h": {"path": "layer-4/HiGHS/highs/util/HighsUtils.h", "filename": "HighsUtils.h", "file": "util/HighsUtils.h", "brief": "Index collections, sparse transpose, and statistical utilities\n\nMiscellaneous utility types and functions used across HiGHS.\n\n**HighsIndexCollection:**\nFlexible index specification for API operations:\n- interval: Range [from_, to_]\n- set: Explicit index array\n- mask: Boolean array\n\n**Sparse Matrix Operations:**\n- highsSparseTranspose(): CSC to CSR conversion\n\n**Value Distribution:**\n- HighsValueDistribution: Histogram for analyzing value ranges\n- HighsScatterData: Scatter plot data for analysis\n\n**Index Collection Status:**\n- kIndexCollectionCreateOk: Valid collection\n- kIndexCollectionCreateIllegalInterval/Set/Mask: Error codes", "see": ["lp_data/HighsLp.h for LP index operations", "model/HighsModel.h for model modifications using indices"], "has_pass2": false}, "highs/util/HSet.h": {"path": "layer-4/HiGHS/highs/util/HSet.h", "filename": "HSet.h", "file": "util/HSet.h", "brief": "O(1) integer set with fast membership test and removal\n\nUnordered set of distinct non-negative integers with O(1) operations.\n\n**Data Structure:**\n- entry_[]: Dense array of current entries\n- pointer_[]: Sparse array mapping entry → position in entry_[]\n- count_: Number of elements\n\n**Operations (all O(1)):**\n- add(entry): Append to entry_[], record position in pointer_[]\n- remove(entry): Swap with last, update swapped entry's pointer\n- in(entry): Check pointer_[] for valid position\n\n**Design:**\n- Similar to \"swap and pop\" deletion pattern\n- pointer_[i] = no_pointer (-1) means i not in set\n- Trades O(max_entry) space for O(1) operations\n\n**Usage:**\n- Track active row/column indices in simplex\n- Maintain sets of candidates during pricing", "see": ["simplex/HEkkDual.h for dual pricing candidate sets"], "has_pass2": false}, "highs/util/HighsInt.h": {"path": "layer-4/HiGHS/highs/util/HighsInt.h", "filename": "HighsInt.h", "file": "HighsInt.h", "brief": "The definition for the integer type to use", "has_pass2": false}, "highs/util/HighsHashTree.h": {"path": "layer-4/HiGHS/highs/util/HighsHashTree.h", "filename": "HighsHashTree.h", "file": "util/HighsHashTree.h", "brief": "Adaptive hash-array mapped trie (HAMT) container\n\nCache-efficient hash map using trie structure with occupation bitmaps.\n\n**Node Types:**\n- InnerLeaf<SizeClass>: Leaf with 6-54 entries (4 size classes)\n- BranchNode: Internal node with 64-way branching via hash chunks\n- ListLeaf: Collision list at maximum depth (rare)\n\n**Hash Structure:**\n- 6 bits per level → 64-way branching (kBranchFactor)\n- Up to 9 depth levels (54 bits of 64-bit hash)\n- Occupation bitmap tracks which children exist\n\n**InnerLeaf Features:**\n- Linear probing within leaf for hash collisions\n- 16-bit hash prefix for fast filtering\n- Size class promotion/demotion on insert/erase\n\n**Burst Behavior:**\n- Leaf bursts to branch when exceeding kLeafBurstThreshold (54)\n- Branch merges back to leaf when children fit in threshold\n\n**Key Methods:**\n- insert/insert_or_get(): Add entry, return value pointer\n- find(): Lookup by key\n- erase(): Remove entry\n- find_common(): Find intersection with another tree\n- for_each(): Iterate all entries\n\n**Usage:**\nUsed for lifting opportunities in presolve (HPresolve.liftingOpportunities)", "see": ["util/HighsHash.h for hash functions", "presolve/HPresolve.h for usage in presolve"], "has_pass2": false}, "highs/util/HighsSparseMatrix.h": {"path": "layer-4/HiGHS/highs/util/HighsSparseMatrix.h", "filename": "HighsSparseMatrix.h", "file": "util/HighsSparseMatrix.h", "brief": "Sparse matrix in CSC or CSR format\n\n**HighsSparseMatrix Class:**\nStores constraint matrix A for LP/QP in compressed format.\n\n**Storage Format (MatrixFormat):**\n- kColwise (CSC): start_[col], index_[]/value_[] are row indices and values\n- kRowwise (CSR): start_[row], index_[]/value_[] are column indices and values\n- p_end_[]: Optional end pointers for partitioned/incremental updates\n\n**Format Conversion:**\n- ensureColwise()/ensureRowwise(): Convert to desired format\n- createColwise()/createRowwise(): Create from existing matrix\n- setFormat(): Set target format\n\n**Operations:**\n- product()/productTranspose(): Ax, A'x multiplication\n- productQuad(): Quad-precision multiplication\n- alphaProductPlusY(): y += alpha*Ax\n\n**Simplex PRICE Operations:**\n- priceByColumn()/priceByRow(): Compute pi'A\n- priceByRowWithSwitch(): Switch strategy based on density\n- createRowwisePartitioned(): Partition for hyper-sparse PRICE\n\n**Modification:**\n- addCols()/addRows(): Extend matrix\n- deleteCols()/deleteRows(): Remove columns/rows\n- scaleCol()/scaleRow(): Apply scaling", "see": ["lp_data/HighsLp.h for HighsLp.a_matrix_ usage", "HConst.h for MatrixFormat enum"], "has_pass2": false}, "highs/util/HighsSparseVectorSum.h": {"path": "layer-4/HiGHS/highs/util/HighsSparseVectorSum.h", "filename": "HighsSparseVectorSum.h", "file": "util/HighsSparseVectorSum.h", "brief": "Accumulator for sparse vector linear combinations\n\nEfficient accumulation of multiple sparse vectors with quad precision.\n\n**Hybrid Storage:**\n- values[]: Full-length HighsCDouble array for accumulation\n- nonzeroinds[]: Packed indices of nonzero entries\n- O(nnz) iteration with O(1) random access\n\n**Key Operations:**\n- add(): Accumulate value at index, track new nonzeros\n  - Uses DBL_MIN sentinel to distinguish zero from never-set\n- getNonzeros(): Return accumulated nonzero indices\n- getValue(): Extract accumulated value at index\n- clear(): Reset (O(nnz) or O(n) based on density)\n\n**Template Methods:**\n- partition(): Partition nonzeros by predicate\n- cleanup(): Remove entries satisfying isZero predicate\n\n**Usage:**\nRow aggregation for cut generation (HighsLpAggregator uses this)", "see": ["mip/HighsLpAggregator.h for row aggregation", "util/HighsCDouble.h for quad precision arithmetic"], "has_pass2": false}, "highs/util/FactorTimer.h": {"path": "layer-4/HiGHS/highs/util/FactorTimer.h", "filename": "FactorTimer.h", "file": "util/FactorTimer.h", "brief": "Indices of factor iClocks", "has_pass2": false}, "highs/util/HighsRbTree.h": {"path": "layer-4/HiGHS/highs/util/HighsRbTree.h", "filename": "HighsRbTree.h", "file": "util/HighsRbTree.h", "brief": "Array-based Red-Black tree with CRTP interface\n\nSelf-balancing BST using indices instead of pointers for cache efficiency.\n\n**CRTP Pattern:**\n- RbTreeTraits<Impl>: Define KeyType and LinkType\n- Impl::getRbTreeLinks(): Return reference to RbTreeLinks\n- Impl::getKey(): Return comparable key value\n\n**RbTreeLinks Structure:**\n- child[2]: Left/right child indices\n- parentAndColor: Parent packed with color bit (saves storage)\n- Works with both integer and pointer LinkTypes\n\n**Tree Operations:**\n- link(): Insert node, rebalance via red-black rotations\n- unlink(): Remove node with transplant and fixup\n- find(): Search returning (node, exact_match) pair\n- first()/last(): Minimum/maximum in subtree\n- successor()/predecessor(): In-order traversal\n\n**CacheMinRbTree Variant:**\n- Maintains cached pointer to minimum element\n- O(1) access to first() for priority queue usage\n\n**Usage in HiGHS:**\n- HighsNodeQueue: Priority queue for B&B nodes by bound\n- Efficient ordered containers with O(log n) operations", "see": ["mip/HighsNodeQueue.h for node selection", "util/HighsSplay.h for alternative tree structure"], "has_pass2": false}, "highs/util/HighsMatrixPic.h": {"path": "layer-4/HiGHS/highs/util/HighsMatrixPic.h", "filename": "HighsMatrixPic.h", "file": "util/HighsMatrixPic.h", "brief": "Class-independent utilities for HiGHS", "has_pass2": false}, "highs/util/HighsLinearSumBounds.h": {"path": "layer-4/HiGHS/highs/util/HighsLinearSumBounds.h", "filename": "HighsLinearSumBounds.h", "file": "util/HighsLinearSumBounds.h", "brief": "Row activity bounds for bound tightening (FBBT)\n\nIncrementally maintains min/max activity bounds for linear constraints.\n\n**Activity Computation:**\nFor row sum = Σ(a_i * x_i):\n- sumLower: Minimum activity using a_i>0 with lb_i, a_i<0 with ub_i\n- sumUpper: Maximum activity using a_i>0 with ub_i, a_i<0 with lb_i\n- numInfSumLower/Upper: Count of infinite contributions\n\n**Original vs Implied Bounds:**\n- sumLowerOrig/sumUpperOrig: Using original variable bounds only\n- sumLower/sumUpper: Using tightest of original and implied bounds\n- implVarLower/implVarUpper: Implied bounds from other constraints\n- Source tracking: implVarLowerSource[] for circular dependency detection\n\n**Incremental Updates:**\n- add(): Add variable contribution to sum\n- remove(): Remove variable contribution from sum\n- updatedVarLower/Upper(): Update after bound tightening\n- updatedImplVarLower/Upper(): Update after implied bound discovery\n\n**Residual Computation:**\n- getResidualSumLower/Upper(): Activity excluding specific variable\n  - Used for FBBT: derive x_i bounds from row ≤ rhs - residual\n  - Optional boundVar parameter for conditional bounds\n\n**Quad Precision:**\nUses HighsCDouble for activity sums to avoid numerical issues.", "see": ["presolve/HPresolve.h for presolve bound tightening", "mip/HighsDomain.h for MIP propagation", "util/HighsCDouble.h for compensated double arithmetic"], "has_pass2": false}, "highs/util/HVectorBase.h": {"path": "layer-4/HiGHS/highs/util/HVectorBase.h", "filename": "HVectorBase.h", "file": "util/HVectorBase.h", "brief": "Template sparse vector for simplex operations\n\nHybrid-storage vector supporting both sparse and dense access patterns.\n\n**Storage Model:**\n- array[]: Full-length value storage (dense access)\n- index[]: Packed nonzero indices (sparse iteration)\n- count: Number of nonzeros\n- Enables O(nnz) operations while allowing O(1) element access\n\n**Core Operations:**\n- setup(): Initialize to dimension\n- clear(): Reset all values and count\n- saxpy(): y += alpha * x (sparse BLAS-1)\n- pack(): Compress to packIndex/packValue arrays\n- reIndex(): Rebuild index[] from array[] (after cancellation)\n- norm2(): Squared 2-norm computation\n\n**Packing System:**\n- packFlag: Enable/disable packing\n- packIndex/packValue: Compressed representation\n- tight(): Zero small values exceeding kHighsTiny\n\n**PAMI Support:**\n- next: Link pointer for parallel minor iteration chains\n- cwork/iwork: Scratch buffers for UPDATE operations\n- synthetic_tick: Profiling counter for operation costs", "see": ["simplex/HEkkDual.h for PAMI parallelization", "util/HFactor.h for BTRAN/FTRAN using HVector"], "has_pass2": false}, "highs/util/HighsTimer.h": {"path": "layer-4/HiGHS/highs/util/HighsTimer.h", "filename": "HighsTimer.h", "file": "util/HighsTimer.h", "brief": "Profiling facility for HiGHS computational components\n\n**HighsTimer Class:**\nWall-clock timing with named clocks for performance profiling.\n\n**Clock Management:**\n- clock_def(): Define a named clock, returns clock index\n- start()/stop(): Start/stop a clock (asserts proper pairing)\n- read(): Get elapsed time (handles running clocks)\n- running(): Check if clock is running\n- numCall(): Get number of times clock was stopped\n\n**Built-in Clocks:**\n- Clock 0: \"Run HiGHS\" (total time)\n- presolve_clock, solve_clock, postsolve_clock: Phase timings\n\n**Reporting:**\n- report(): Print timing breakdown for clock list (grep-friendly format)\n- reportOnTolerance(): Only report clocks above percentage threshold\n- writeAllClocks(): Dump all clock times\n\n**Implementation:**\nUses std::chrono::high_resolution_clock. Clock start stores negative\nwall time; positive indicates stopped. This enables running/stopped detection.", "see": ["simplex/SimplexTimer.h for simplex-specific clocks", "mip/HighsMipAnalysis.h for MIP timing"], "has_pass2": false}, "highs/util/HighsRandom.h": {"path": "layer-4/HiGHS/highs/util/HighsRandom.h", "filename": "HighsRandom.h", "file": "util/HighsRandom.h", "brief": "Deterministic random number generator for HiGHS\n\nXorshift-based PRNG with strongly universal hash output functions.\n\n**State Management:**\n- 64-bit state with xorshift advance\n- initialise(): Seed with mixing to avoid weak states\n- Deterministic for reproducible optimization\n\n**Output Methods:**\n- integer(): Random in [0, 2^31-1] or [0, sup) or [min, sup)\n- fraction(): Open interval (0, 1)\n- closedFraction(): Closed interval [0, 1]\n- real(a, b): Uniform in [a, b]\n- bit(): Random boolean\n- shuffle(): Fisher-Yates shuffle\n\n**Uniform Range Algorithm:**\nUses rejection sampling with hash function outputs:\n- Multiple hash outputs per state (up to 32)\n- Minimizes state advances for bounded integers\n\n**Hash-Based Mixing:**\nApplies HighsHashHelpers::pair_hash to state for high-quality output\nin both high and low bits (unlike raw xorshift).", "see": ["util/HighsHash.h for hash functions", "mip/HighsPathSeparator.h for randomized cut selection"], "has_pass2": false}, "highs/util/HighsHash.h": {"path": "layer-4/HiGHS/highs/util/HighsHash.h", "filename": "HighsHash.h", "file": "util/HighsHash.h", "brief": "Hash functions and hash table for HiGHS\n\nProvides high-quality hashing with Mersenne prime arithmetic.\n\n**HighsHashHelpers:**\n- hash<T>(): Fast hash for trivially copyable types (up to 64 bytes)\n- vector_hash(): Hash for vectors using pair_hash with random constants\n- sparse_combine(): Order-independent hash for sparse vectors\n- multiply_modM61(): Multiply mod Mersenne prime 2^61-1\n- double_hash_code(): Hash for doubles handling epsilon equality\n\n**Sparse Hashing (for symmetry detection):**\nEvaluates polynomial over finite field GF(2^61-1):\n- sparse_combine(hash, index, value): Add term to polynomial\n- sparse_inverse_combine(): Remove term (for partition refinement)\n- Enables order-independent hashing of sparse vectors\n\n**HighsHashTable<K,V>:**\nRobin Hood hashing with linear probing:\n- 7-bit hash stored in metadata for fast comparison\n- Automatic grow/shrink at 7/8 and 1/4 load factors\n- Steals positions from entries closer to ideal slot\n\n**HighsHashTableEntry<K,V>:**\n- Specialized for void value (set behavior)\n- key(), value() accessors\n- forward() for functional-style iteration", "see": ["util/HighsHashTree.h for ordered hash structures", "mip/HighsCliqueTable.h for hash-based clique storage"], "has_pass2": false}, "highs/util/HFactorDebug.h": {"path": "layer-4/HiGHS/highs/util/HFactorDebug.h", "filename": "HFactorDebug.h", "file": "util/HFactorDebug.h", "has_pass2": false}, "highs/util/HighsSplay.h": {"path": "layer-4/HiGHS/highs/util/HighsSplay.h", "filename": "HighsSplay.h", "file": "util/HighsSplay.h", "brief": "Splay tree operations for array-based binary search trees\n\nTop-down splay tree using indices instead of pointers for cache efficiency.\n\n**Splay Operation:**\n- highs_splay(): Move accessed node to root via rotations\n- Amortized O(log n) operations through self-adjusting\n- Zig-zig and zig-zag rotations for balanced restructuring\n\n**Tree Operations:**\n- highs_splay_link(): Insert new node, splay to root\n- highs_splay_unlink(): Remove node, restructure tree\n\n**Lambda Interface:**\nTemplate parameters for flexible storage:\n- GetLeft: lambda(index) → reference to left child index\n- GetRight: lambda(index) → reference to right child index\n- GetKey: lambda(index) → comparable key value\n\n**Usage in HiGHS:**\n- HPresolve: Row-wise nonzero storage (ARleft, ARright, rowroot)\n- HighsGFkSolve: Sparse storage for GF(k) systems\n- Efficient O(1) access to recently accessed elements", "see": ["presolve/HPresolve.h for row iteration via splay tree", "mip/HighsGFkSolve.h for GF(k) sparse storage"], "has_pass2": false}, "highs/util/HighsCDouble.h": {"path": "layer-4/HiGHS/highs/util/HighsCDouble.h", "filename": "HighsCDouble.h", "file": "util/HighsCDouble.h", "brief": "Compensated double for quad precision arithmetic\n\nImplements quad precision using error-free transformations (Knuth/Rump).\n\n**Representation:**\n- hi: Primary double value\n- lo: Compensation term (error from previous operations)\n- Value = hi + lo with |lo| << |hi|\n\n**Error-Free Transformations:**\n- two_sum(x,y,a,b): Exact a+b=x+y with x=double(a+b), 6 FLOPs\n- two_product(x,y,a,b): Exact a*b=x+y with x=double(a*b), 17 FLOPs\n- split(x,y,a): Split 53-bit mantissa into two 26-bit parts\n\n**Supported Operations:**\n- +, -, *, / with double and HighsCDouble operands\n- Comparison operators (using double(hi+lo))\n- abs(), sqrt(), floor(), ceil(), round(), ldexp()\n- renormalize(): Restore |lo| << |hi| invariant\n\n**Usage:**\nCritical for accurate summation in LP (objective, constraint activity)\nand avoiding catastrophic cancellation in bound computations.", "see": ["util/HVector.h for HVectorQuad using HighsCDouble", "mip/HighsNodeQueue.h for tree weight tracking"], "has_pass2": false}, "highs/util/HighsDataStack.h": {"path": "layer-4/HiGHS/highs/util/HighsDataStack.h", "filename": "HighsDataStack.h", "file": "util/HighsDataStack.h", "brief": "Type-erased byte stack for presolve reduction storage\n\nLIFO storage for heterogeneous data with compile-time type safety.\n\n**Design:**\n- Stores arbitrary trivially-copyable types as raw bytes\n- push<T>(): Append value to stack\n- pop<T>(): Remove and return value from top\n- Vectors stored with trailing size for self-describing pops\n\n**Presolve Integration:**\n- HighsPostsolveStack uses this for reduction storage\n- Each presolve reduction pushes undo data\n- Postsolve pops in reverse order to restore solution\n- Compact binary format (no per-element overhead)\n\n**Memory Layout:**\n- Scalar: |--value bytes--|\n- Vector: |--element bytes--|--size_t count--|\n- Position tracks pop location; data.size() for push\n\n**Type Safety:**\n- IS_TRIVIALLY_COPYABLE enforces memcpy-safe types\n- Compatible with GCC < 5 via __has_trivial_copy", "see": ["presolve/HighsPostsolveStack.h for reduction undo storage", "presolve/HPresolve.h for presolve reductions"], "has_pass2": false}, "highs/util/HighsSort.h": {"path": "layer-4/HiGHS/highs/util/HighsSort.h", "filename": "HighsSort.h", "file": "util/HighsSort.h", "brief": "Heap-based sorting routines for HiGHS\n\nIn-place heapsort implementations for indices and value-index pairs.\n\n**Algorithm:**\n- Build max-heap in O(n) via buildMaxheap()\n- Extract elements in O(n log n) via maxHeapsort()\n- Result: increasing order (max-heap yields ascending sort)\n\n**Functions:**\n- maxheapsort(): Combined build + sort\n- buildMaxheap() + maxHeapsort(): Separate phases\n- maxHeapify(): Sift-down operation\n\n**Overloads:**\n- (HighsInt*): Sort indices only\n- (double*, HighsInt*): Sort values with corresponding indices\n- (HighsInt*, HighsInt*): Sort HighsInt values with indices\n\n**Decreasing Heap Utilities:**\n- addToDecreasingHeap(): Insert into bounded min-heap\n- sortDecreasingHeap(): Extract in decreasing order\n\n**Validation:**\n- increasingSetOk(): Check sorted order and bounds\n- sortSetData(): Sort indices and reorder associated data arrays\n\n@note Arrays use 1-based indexing: heap[1..n]", "see": ["simplex/HEkkDual.h for CHUZC ratio test using heaps"], "has_pass2": false}, "highs/util/HighsMatrixUtils.h": {"path": "layer-4/HiGHS/highs/util/HighsMatrixUtils.h", "filename": "HighsMatrixUtils.h", "file": "util/HighsMatrixUtils.h", "brief": "Class-independent utilities for HiGHS", "has_pass2": false}, "highs/util/HighsDisjointSets.h": {"path": "layer-4/HiGHS/highs/util/HighsDisjointSets.h", "filename": "HighsDisjointSets.h", "file": "util/HighsDisjointSets.h", "brief": "Union-Find data structure for disjoint sets\n\nEfficient partition management with path compression and union-by-size.\n\n**Template Parameter:**\n- kMinimalRepresentative: If true, smaller index becomes representative\n  (useful for symmetry detection where order matters)\n\n**Data Structure:**\n- sets[]: Parent pointers (self-loop = representative)\n- sizes[]: Set sizes for union-by-size\n- linkCompressionStack: Workspace for path compression\n\n**Operations:**\n- getSet(): Find representative with path compression\n  - Amortized O(α(n)) via iterative compression\n- merge(): Union two sets\n  - Union-by-size keeps tree balanced\n  - Or union-by-min-index if kMinimalRepresentative\n- getSetSize(): Size of set containing item\n\n**Usage in HiGHS:**\n- Symmetry detection: orbit computation (HighsSymmetry)\n- Clique merging (HighsCliqueTable)\n- Connected component identification", "see": ["presolve/HighsSymmetry.h for orbit computation", "mip/HighsCliqueTable.h for clique management"], "has_pass2": false}, "highs/util/HVector.h": {"path": "layer-4/HiGHS/highs/util/HVector.h", "filename": "HVector.h", "file": "util/HVector.h", "brief": "Sparse vector types for HiGHS simplex operations\n\nType aliases for HVectorBase with different precision.\n\n**Types:**\n- HVector: HVectorBase<double> - Standard precision sparse vector\n- HVectorQuad: HVectorBase<HighsCDouble> - Quad precision for accuracy\n- HVector_ptr, HVectorQuad_ptr: Pointer types for array storage\n\n**HVectorBase Features (from HVectorBase.h):**\n- Sparse representation: index[] + array[] with count\n- Dense operations: clear(), setup(), collectDense()\n- SAXPY: Add scalar multiple of another vector\n- Packed format for BLAS-style operations", "see": ["util/HVectorBase.h for implementation details", "util/HighsCDouble.h for quad precision type"], "has_pass2": false}, "highs/util/HighsComponent.h": {"path": "layer-4/HiGHS/highs/util/HighsComponent.h", "filename": "HighsComponent.h", "file": "HighsComponent.h", "brief": "The HiGHS class", "has_pass2": false}, "highs/util/HFactorConst.h": {"path": "layer-4/HiGHS/highs/util/HFactorConst.h", "filename": "HFactorConst.h", "file": "util/HFactorConst.h", "brief": "Constants for basis matrix factorization, update and solves for HiGHS", "has_pass2": false}, "highs/util/HighsMatrixSlice.h": {"path": "layer-4/HiGHS/highs/util/HighsMatrixSlice.h", "filename": "HighsMatrixSlice.h", "file": "util/HighsMatrixSlice.h", "brief": "Polymorphic iterators for sparse matrix storage formats\n\nSTL-compatible iterators over matrix rows/columns regardless of storage.\n\n**Storage Format Specializations:**\n- HighsEmptySlice: Zero nonzeros (null iterators)\n- HighsCompressedSlice: CSC/CSR format (index[], value[], len)\n- HighsIndexedSlice: Sparse index + dense values (hybrid storage)\n- HighsTripletListSlice: Linked list via next[] array\n- HighsTripletTreeSliceInOrder: BST traversal (sorted iteration)\n- HighsTripletTreeSlicePreOrder: BST pre-order (fast iteration)\n- HighsTripletPositionSlice: Position array indirection\n\n**Iterator Interface:**\n- HighsSliceNonzero: (index, value) pair via index()/value()\n- Forward iterator with begin()/end()\n- Compatible with range-based for loops\n\n**Usage Pattern:**\n```cpp\nfor (auto nz : HighsCompressedSlice(idx, val, len)) {\n  process(nz.index(), nz.value());\n}\n```\n\n**Usage in HiGHS:**\n- HPresolve: Unified iteration over row/column storage\n- Generic algorithms independent of underlying format\n- Splay tree and linked list iteration in presolve", "see": ["presolve/HPresolve.h for row-wise splay tree iteration", "util/HighsSparseMatrix.h for CSC matrix storage"], "has_pass2": false}, "highs/util/HFactor.h": {"path": "layer-4/HiGHS/highs/util/HFactor.h", "filename": "HFactor.h", "file": "util/HFactor.h", "brief": "Basis matrix factorization, update and solves for HiGHS", "return": "0 if successful, otherwise rank_deficiency>0", "has_pass2": false}, "highs/util/HighsIntegers.h": {"path": "layer-4/HiGHS/highs/util/HighsIntegers.h", "filename": "HighsIntegers.h", "file": "util/HighsIntegers.h", "brief": "Integer arithmetic utilities for cut generation\n\nNumber-theoretic functions critical for MIP cut generation.\n\n**Basic Operations:**\n- mod(): Proper modulo (always non-negative result)\n- gcd(): Euclidean algorithm for GCD\n- nearestInteger(): Round to nearest int64\n- isIntegral(): Test if double is within eps of integer\n\n**Modular Arithmetic:**\n- modularInverse(): Extended Euclidean algorithm for a^{-1} mod m\n  - Used in mod-k cut generation (GF(k) arithmetic)\n\n**Rational Approximation:**\n- denominator(): Continued fraction algorithm to find\n  smallest denominator d such that |x - p/d| < eps\n  - Used for detecting integer structure in coefficients\n\n**Integral Scaling:**\n- integralScale(): Find multiplier to make all values integral\n  - Combines continued fractions with GCD reduction\n  - Used for strengthening MIR/GMI cuts\n  - Returns 0 if no small denominator exists\n\n**Usage in HiGHS:**\n- HighsGFkSolve: Modular inverse for GF(k) systems\n- HighsTableauSeparator: Finding integral structure in cuts\n- Cut coefficient strengthening", "see": ["mip/HighsGFkSolve.h for GF(k) cut generation", "mip/HighsTableauSeparator.h for Gomory cuts"], "has_pass2": false}, "highs/util/HighsMemoryAllocation.h": {"path": "layer-4/HiGHS/highs/util/HighsMemoryAllocation.h", "filename": "HighsMemoryAllocation.h", "file": "HighsMemoryAllocation.h", "brief": "Utilities for memory allocation that return true if successful", "has_pass2": false}, "highs/io/HMpsFF.h": {"path": "layer-4/HiGHS/highs/io/HMpsFF.h", "filename": "HMpsFF.h", "file": "io/HMpsFF.h", "brief": "Free-format MPS file parser with full section support\n\nComprehensive MPS reader supporting LP, QP, MIP, and extensions.\n\n**MPS Sections Parsed:**\n- NAME, OBJSENSE: Problem metadata and optimization direction\n- ROWS: Constraint types (N=free, L/G/E=bounds)\n- COLUMNS: Variable coefficients, integrality markers\n- RHS: Right-hand sides for constraints\n- RANGES: Two-sided constraint bounds\n- BOUNDS: Variable bounds (LO/UP/FX/FR/MI/PL/BV/LI/UI)\n\n**Quadratic Extensions:**\n- QSECTION, QMATRIX, QUADOBJ: Objective Hessian\n- QCMATRIX: Quadratic constraints (per-row)\n\n**Additional Sections:**\n- SOS: Special ordered sets (Type 1, Type 2)\n- CSECTION: Conic constraints (MOSEK format)\n- INDICATORS: Indicator constraints (ignored)\n\n**Parsing Features:**\n- Triplet matrix accumulation, then CSC conversion\n- Duplicate row/column name detection\n- Timeout support for large files\n- Fixed-format detection and fallback\n\n**Data Flow:**\n- loadProblem() → parse() → fillMatrix()/fillHessian()\n- Populates HighsModel with LP/QP/MIP data", "see": ["io/FilereaderMps.h for Filereader wrapper", "io/HMPSIO.h for fixed-format legacy parser"], "has_pass2": false}, "highs/io/Filereader.h": {"path": "layer-4/HiGHS/highs/io/Filereader.h", "filename": "Filereader.h", "file": "io/Filereader.h", "brief": "Abstract file reader/writer interface for optimization models\n\nPolymorphic I/O with format detection from file extension.\n\n**Supported Formats:**\n- MPS: Mathematical Programming System (fixed/free format)\n- LP: CPLEX LP format (human-readable)\n- EMS: HiGHS extended model format\n\n**Factory Pattern:**\n- getFilereader(): Returns appropriate reader for extension\n  - \".mps\", \".mps.gz\" → FilereaderMps\n  - \".lp\" → FilereaderLp\n  - \".ems\" → FilereaderEms\n\n**Return Codes:**\n- kOk: Success\n- kFileNotFound: File doesn't exist\n- kParserError: Malformed input\n- kNotImplemented: Format not supported for operation\n- kTimeout: Read exceeded time limit", "see": ["io/FilereaderMps.h for MPS format", "io/FilereaderLp.h for LP format", "io/HMpsFF.h for free-format MPS parser"], "has_pass2": false}, "highs/io/FilereaderMps.h": {"path": "layer-4/HiGHS/highs/io/FilereaderMps.h", "filename": "FilereaderMps.h", "file": "io/FilereaderMps.h", "brief": "MPS format file reader/writer wrapper\n\nImplements Filereader interface for MPS format (industry standard).\n\n**FilereaderMps Class:**\nThin wrapper delegating to HMpsFF (free-format) or HMPSIO (fixed-format):\n- readModelFromFile(): Detect format, parse MPS into HighsModel\n- writeModelToFile(): Export HighsModel to MPS format\n\n**MPS Format Support:**\n- Standard sections: ROWS, COLUMNS, RHS, BOUNDS, RANGES\n- Extensions: OBJSENSE, QSECTION, SOS, integer markers\n- Both free-format and fixed-format variants\n\n**Read Flow:**\n1. Attempts free-format parse (HMpsFF)\n2. Falls back to fixed-format (HMPSIO) if needed\n3. Handles QP and MIP extensions", "see": ["io/HMpsFF.h for free-format parser implementation", "io/HMPSIO.h for fixed-format parser implementation", "io/Filereader.h for base class interface"], "has_pass2": false}, "highs/io/HMPSIO.h": {"path": "layer-4/HiGHS/highs/io/HMPSIO.h", "filename": "HMPSIO.h", "file": "io/HMPSIO.h", "brief": "Fixed-format MPS file parser (legacy format)\n\nParses classic fixed-column MPS format from IBM's MP/360.\n\n**Fixed-Format Layout:**\nColumns are at fixed character positions:\n- field_1_start (1): Indicator field (row type, bound type)\n- field_2_start (4): Name field (8 chars)\n- field_3_start (14): Name field 2 (8 chars)\n- field_4_start (24): Value field (12 chars)\n- field_5_start (39): Name field 3 (8 chars)\n- field_6_start (49): Value field 2 (12 chars)\n\n**Row Type Constants:**\n- MPS_ROW_TY_N: Free row (objective)\n- MPS_ROW_TY_E: Equality constraint\n- MPS_ROW_TY_L: Less-than-or-equal\n- MPS_ROW_TY_G: Greater-than-or-equal\n\n**readMps():**\nMain parsing function extracting:\n- Constraint matrix (Astart, Aindex, Avalue)\n- Bounds and costs\n- Integrality constraints\n- Quadratic terms (Qstart, Qindex, Qvalue)", "see": ["io/HMpsFF.h for free-format parser (preferred)", "io/FilereaderMps.h for unified MPS interface"], "has_pass2": false}, "highs/io/LoadOptions.h": {"path": "layer-4/HiGHS/highs/io/LoadOptions.h", "filename": "LoadOptions.h", "file": "io/LoadOptions.h", "brief": "Load solver options from file\n\nParses options files into HighsOptions structure.\n\n**HighsLoadOptionsStatus:**\n- kError: Parse failure or invalid option\n- kOk: Options loaded successfully\n- kEmpty: File exists but contains no options\n\n**loadOptionsFromFile():**\nReads option file with key=value pairs:\n- Recognizes all HighsOptions fields\n- Validates option names and value types\n- Logs warnings for unknown options\n\n**File Format:**\nSimple text format with one option per line:\n```\npresolve = on\ntime_limit = 3600.0\nmip_rel_gap = 0.01\n```", "see": ["lp_data/HighsOptions.h for option definitions", "Highs::readOptions() for user-facing API"], "has_pass2": false}, "highs/io/FilereaderEms.h": {"path": "layer-4/HiGHS/highs/io/FilereaderEms.h", "filename": "FilereaderEms.h", "file": "io/FilereaderEms.h", "brief": "EMS format file reader/writer (internal HiGHS format)\n\nImplements Filereader interface for EMS format.\n\n**EMS Format:**\nHiGHS internal text format for model serialization:\n- Compact representation of LP/MIP data\n- Preserves all model metadata\n- Used for debugging and testing\n\n**FilereaderEms Class:**\n- readModelFromFile(): Parse EMS file into HighsModel\n- writeModelToFile(): Export HighsModel to EMS format\n\n**Compared to MPS/LP:**\n- Less standardized but more compact\n- Primarily for internal use\n- Full round-trip fidelity with HighsModel", "see": ["io/Filereader.h for base class interface", "io/FilereaderMps.h for standard MPS format"], "has_pass2": false}, "highs/io/FilereaderLp.h": {"path": "layer-4/HiGHS/highs/io/FilereaderLp.h", "filename": "FilereaderLp.h", "file": "io/FilereaderLp.h", "brief": "CPLEX LP format file reader/writer\n\nImplements Filereader interface for LP format (human-readable).\n\n**LP Format:**\nCPLEX-style format with sections:\n- Minimize/Maximize: Objective function\n- Subject To: Linear constraints\n- Bounds: Variable bounds\n- General/Binary: Integer/binary declarations\n- End: File terminator\n\n**FilereaderLp Class:**\n- readModelFromFile(): Parse LP file into HighsModel\n- writeModelToFile(): Export HighsModel to LP format\n\n**Writing Utilities:**\n- writeToFile(): Buffered output with line-length management\n- writeToFileValue(): Format numeric coefficients\n- writeToFileVar(): Output variable names (quotes if needed)\n- writeToFileMatrixRow(): Output constraint row\n\n**Constants:**\n- LP_MAX_LINE_LENGTH (560): Maximum line length\n- LP_MAX_NAME_LENGTH (255): Maximum identifier length", "see": ["io/Filereader.h for base class interface", "io/HMpsFF.h for MPS format alternative"], "has_pass2": false}, "highs/io/HighsIO.h": {"path": "layer-4/HiGHS/highs/io/HighsIO.h", "filename": "HighsIO.h", "file": "io/HighsIO.h", "brief": "Logging and output utilities for HiGHS\n\nProvides structured logging with log levels and callbacks.\n\n**Log Levels (LogDevLevel):**\n- kHighsLogDevLevelNone: No output\n- kHighsLogDevLevelInfo: Standard progress info\n- kHighsLogDevLevelDetailed: More detailed info\n- kHighsLogDevLevelVerbose: Maximum detail\n\n**Log Types (HighsLogType):**\nTagged output: WARNING, ERROR, etc. (see HighsLogTypeTag[])\n\n**HighsLogOptions:**\nConfiguration for logging destination and callbacks:\n- log_stream: FILE* for output (nullptr = stdout)\n- output_flag/log_to_console: Enable/disable flags\n- user_log_callback: Custom logging callback\n- user_callback: Full callback with HighsCallbackOutput\n\n**Logging Functions:**\n- highsLogUser(): Single-line user messages with type tag\n- highsLogDev(): Development logging (respects log_dev_level)\n- highsLogHeader(): Print version and copyright\n- highsReportDevInfo(): Development info (accepts null log_options)\n\n**File Types (HighsFileType):**\nkMinimal, kFull, kMps, kLp, kMd for output format selection.", "see": ["lp_data/HighsCallback.h for callback structures"], "has_pass2": false}, "highs/presolve/ICrashUtil.h": {"path": "layer-4/HiGHS/highs/presolve/ICrashUtil.h", "filename": "ICrashUtil.h", "file": "presolve/ICrashUtil.h", "brief": "Low-level utilities for ICrash algorithms", "author": "Julian Hall, Ivet Galabova, Qi Huangfu and Michael Feldmeier\n\nHelper functions for iterative crash procedures.\n\n**Problem Transformation:**\n- convertToMinimization(): Negate objective for maximization\n- isEqualityProblem(): Check if all constraints are equations\n\n**Linear Algebra:**\n- vectorProduct(): Compute dot product of two vectors\n- muptiplyByTranspose(): Compute A^T * v\n\n**Initialization:**\n- initialize(): Set up initial solution and dual multipliers\n\n**Component-Wise Minimization:**\n- minimizeComponentQP(): Solve 1D quadratic subproblem\n- minimizeComponentIca(): Solve ICA subproblem for single variable\n\n**Diagnostics:**\n- printMinorIterationDetails(): Log iteration progress", "see": ["presolve/ICrash.h for main crash interface", "presolve/ICrashX.h for crossover support"], "has_pass2": false}, "highs/presolve/HPresolve.h": {"path": "layer-4/HiGHS/highs/presolve/HPresolve.h", "filename": "HPresolve.h", "file": "presolve/HPresolve.h", "brief": "LP/MIP presolve engine\n\n**HPresolve Class:**\nReduces problem size and tightens bounds before solving.\n\n**Matrix Storage:**\nTriplet format with linked list (column) and splay tree (row) for fast access:\n- Avalue[], Arow[], Acol[]: Non-zero storage\n- colhead[], Anext[], Aprev[]: Column-wise linked list\n- rowroot[], ARleft[], ARright[]: Row-wise splay tree\n- rowsize[], colsize[]: Current row/column lengths\n\n**Bound Tracking:**\n- implColLower[]/implColUpper[]: Implied variable bounds\n- rowDualLower[]/rowDualUpper[]: Dual bounds\n- impliedRowBounds, impliedDualRowBounds: Row activity bounds\n\n**Presolve Techniques (Result enum):**\n- singletonRow()/singletonCol(): Remove singleton rows/columns\n- emptyCol(): Remove columns with no constraints\n- doubletonEq(): Eliminate doubleton equalities\n- dominatedColumns(): Remove dominated variables\n- aggregator(): Aggregate rows/columns\n- runProbing(): Probing for integer variables\n- sparsify(): Reduce matrix density\n- detectParallelRowsAndCols(): Remove parallel constraints/variables", "algorithm": "Sparsification (sparsify):\n  Combine rows to create zeros in dense columns:\n  For equation row i with coefficient a_ij in column j:\n  - For each row k with coefficient a_kj:\n    - Add scaled row i to row k: row_k += (−a_kj/a_ij) * row_i\n    - If fill-in acceptable, apply permanently", "math": "Compute implied lower bound on x_k:\n    x_k >= (L - sum_{j≠k}(a_j * u_j if a_j>0 else a_j * l_j)) / a_k\n  If tighter than current l_k, update and propagate.\n  Dual analog: implied dual bounds from reduced cost constraints.", "ref": ["Savelsbergh, M.W.P. (1994). \"Preprocessing and probing techniques\n       for mixed integer programming problems\". ORSA J. Computing 6(4).", "Achterberg, T. et al. (2020). \"Presolve reductions in mixed\n       integer programming\". INFORMS J. Computing 32(2)."], "complexity": "Matrix access: O(1) amortized via splay trees (row) and linked lists (col)\n  Singleton elimination: O(nnz) per pass\n  Probing: O(#binaries * propagation_depth)\n  Full presolve: typically O(nnz * #passes), bounded by reductionLimit", "see": ["presolve/HighsPostsolveStack.h for solution recovery", "presolve/HPresolveAnalysis.h for presolve statistics"], "has_pass2": true}, "highs/presolve/ICrash.h": {"path": "layer-4/HiGHS/highs/presolve/ICrash.h", "filename": "ICrash.h", "file": "presolve/ICrash.h", "brief": "Iterative crash procedure for LP starting points", "author": "Julian Hall, Ivet Galabova, Qi Huangfu and Michael Feldmeier\n\nFinds feasible starting points by solving penalized/regularized subproblems.\n\n**ICrashStrategy Enum:**\n- kPenalty: Penalty method for constraint violations\n- kAdmm: Alternating Direction Method of Multipliers\n- kICA: Iterative Constraint Activation\n- kUpdatePenalty/kUpdateAdmm: Variants with parameter updates\n\n**ICrashIterationDetails:**\nPer-iteration statistics for analysis:\n- num, weight, lambda_norm_2: Iteration metadata\n- lp_objective, quadratic_objective: Objective values\n- residual_norm_2: Constraint violation\n\n**ICrashInfo:**\nOverall crash procedure results:\n- num_iterations, total_time: Execution stats\n- final_lp_objective, final_quadratic_objective: Terminal objectives\n- x_values[]: Final solution vector\n\n**ICrashOptions:**\nConfiguration for crash procedure:\n- dualize: Work with dual LP\n- Strategy selection and parameters", "see": ["presolve/ICrashUtil.h for helper functions", "presolve/ICrashX.h for extended crash variants"], "has_pass2": false}, "highs/presolve/HPresolveAnalysis.h": {"path": "layer-4/HiGHS/highs/presolve/HPresolveAnalysis.h", "filename": "HPresolveAnalysis.h", "file": "presolve/HPresolveAnalysis.h", "brief": "Presolve statistics and rule control\n\nTracks presolve reductions and controls which rules are enabled.\n\n**HPresolveAnalysis Class:**\n- model, options: References to problem being presolved\n- allow_rule_[]: Per-rule enable flags (from options->presolve_rule_off)\n- numDeletedRows, numDeletedCols: Reduction counts\n- presolve_log_: Detailed reduction history\n\n**Rule Control:**\n- setup(): Initialize rule flags from options\n- allow_rule_[]: Boolean array indexed by PresolveRuleType\n- Disabled rules are skipped during presolve passes\n\n**Logging:**\n- allow_logging_/logging_on_: Control verbosity\n- startPresolveRuleLog()/stopPresolveRuleLog(): Bracket rule execution\n- analysePresolveRuleLog(): Summarize effectiveness of rules\n- presolveReductionTypeToString(): Convert rule type to name\n\n**Statistics:**\n- original_num_col_/row_: Problem size before presolve\n- num_deleted_rows0_/cols0_: Running deletion counts", "see": ["presolve/HPresolve.h for presolve engine using this", "lp_data/HConst.h for PresolveRuleType enum"], "has_pass2": false}, "highs/presolve/HighsPostsolveStack.h": {"path": "layer-4/HiGHS/highs/presolve/HighsPostsolveStack.h", "filename": "HighsPostsolveStack.h", "file": "presolve/HighsPostsolveStack.h", "brief": "Postsolve stack to recover original-space solutions\n\nRecords all presolve transformations to enable solution recovery.\n\n**HighsPostsolveStack Class:**\nStack of reduction operations that can be undone in reverse order.\n\n**Reduction Types (ReductionType enum):**\nEach presolve operation has a corresponding undo:\n- kLinearTransform: x = scale * x' + constant\n- kFreeColSubstitution: Free variable substituted via equation\n- kDoubletonEquation: Two-variable equation eliminated\n- kSingletonRow/kFixedCol: Single-entry row or fixed column\n- kRedundantRow/kForcingRow: Redundant or forcing constraints\n- kDuplicateRow/kDuplicateColumn: Parallel rows/columns merged\n\n**Index Mapping:**\n- origColIndex[], origRowIndex[]: Map reduced indices to original\n- compressIndexMaps(): Update after columns/rows removed\n\n**Postsolve Operations:**\n- undo(): Reverse all reductions (primal, dual, basis)\n- undoPrimal(): Reverse only primal solution\n- getReducedPrimalSolution(): Map original solution to reduced space\n\n**Data Storage:**\n- reductionValues: HighsDataStack for reduction parameters\n- reductions[]: Vector of (type, stack_position) pairs\n- rowValues[], colValues[]: Temporary storage for sparse vectors", "see": ["presolve/HPresolve.h for presolve engine", "util/HighsDataStack.h for stack implementation"], "has_pass2": false}, "highs/presolve/ICrashX.h": {"path": "layer-4/HiGHS/highs/presolve/ICrashX.h", "filename": "ICrashX.h", "file": "presolve/ICrashX.h", "brief": "Crossover from interior point to basic solution\n\nProvides simplex crossover after IPM solve to obtain vertex solution.\n\n**callCrossover():**\nConverts interior point solution to basic feasible solution:\n- Input: IPM solution (may be interior to polytope)\n- Output: Basis + vertex solution for post-processing\n\n**Why Crossover:**\n- IPM returns interior solutions, not vertices\n- Basic solutions needed for warm-starting, sensitivity analysis\n- Crossover pushes solution to vertex via simplex pivots\n\n**Process:**\n1. Identify near-bound variables\n2. Construct initial basis from binding constraints\n3. Run simplex cleanup to reach vertex", "see": ["ipm/IpxWrapper.h for IPM solver calling crossover", "simplex/HEkk.h for simplex cleanup phase"], "has_pass2": false}, "highs/presolve/PresolveComponent.h": {"path": "layer-4/HiGHS/highs/presolve/PresolveComponent.h", "filename": "PresolveComponent.h", "file": "presolve/PresolveComponent.h", "brief": "Presolve component for HiGHS pipeline integration\n\nEncapsulates presolve as a modular component with data and statistics.\n\n**PresolveComponentData:**\nState managed by presolve component:\n- reduced_lp_: Presolved LP (smaller problem)\n- postSolveStack: Operations to undo presolve\n- recovered_solution_/recovered_basis_: Original-space solution\n- presolve_log_: Reduction history\n\n**PresolveComponentInfo:**\nStatistics exposed for reporting:\n- n_rows_removed, n_cols_removed, n_nnz_removed: Reduction counts\n- Timing information from HighsComponentInfo base\n\n**PresolveComponentOptions:**\nComponent-specific settings (extends HighsComponentOptions):\n- iteration_limit: Maximum presolve passes\n- Timeout and other controls\n\n**PresolveComponent Class:**\nMain component implementing HighsComponent interface:\n- init(): Setup with options\n- run(): Execute presolve\n- has_run_: Track execution state", "see": ["presolve/HPresolve.h for presolve engine", "util/HighsComponent.h for component base class", "presolve/HighsPostsolveStack.h for solution recovery"], "has_pass2": false}, "highs/presolve/HighsSymmetry.h": {"path": "layer-4/HiGHS/highs/presolve/HighsSymmetry.h", "filename": "HighsSymmetry.h", "file": "presolve/HighsSymmetry.h", "brief": "Symmetry detection and orbital fixing for MIP", "author": "Leona Gottwald\n\nDetects variable permutation symmetries and uses them to reduce search.\n\n**HighsMatrixColoring:**\nAssigns distinct colors to distinct coefficient values for graph coloring.\nUsed to build colored graph where symmetries preserve colors.\n\n**HighsSymmetries:**\nStores detected symmetry group:\n- permutationColumns[], permutations[]: Generator permutations\n- orbitPartition[], orbitSize[]: Variable orbits under symmetry\n- orbitopes[]: Detected orbitope structures for faster propagation\n- numPerms, numGenerators: Symmetry group statistics\n\n**StabilizerOrbits:**\nOrbits under stabilizer of fixed variables:\n- orbitCols[], orbitStarts[]: Orbit structure\n- stabilizedCols[]: Already-fixed variables\n- orbitalFixing(): Apply orbital fixing to domain\n\n**HighsOrbitopeMatrix:**\nSpecial symmetry structure (matrix of symmetric binaries):\n- Type::kFull or Type::kPacking (set-packing rows)\n- orbitalFixing(): Specialized propagation for orbitopes\n- getBranchingColumn(): Symmetry-aware branching\n\n**HighsSymmetryDetection:**\nGraph-based symmetry detection algorithm:\n- loadModelAsGraph(): Build vertex-colored graph from LP\n- initializeDetection(): Set up partition refinement\n- run(): Find automorphism generators via search tree\n- Uses nauty-style partition refinement with certificate comparison", "see": ["mip/HighsDomain.h for orbital fixing application", "mip/HighsMipSolverData.h for symmetry integration"], "has_pass2": false}, "highs/pdlp/cupdlp/cupdlp_solver.h": {"path": "layer-4/HiGHS/highs/pdlp/cupdlp/cupdlp_solver.h", "filename": "cupdlp_solver.h", "file": "pdlp/cupdlp/cupdlp_solver.h", "brief": "PDHG (Primal-Dual Hybrid Gradient) Solver for Linear Programming\n\nImplements the PDLP (Primal-Dual Linear Programming) algorithm using\nfirst-order optimization methods instead of traditional simplex or IPM.", "algorithm": "PDHG (Primal-Dual Hybrid Gradient) for LP:\nA first-order method for solving the saddle-point formulation of LP.\n\nLP SADDLE-POINT FORMULATION:\nmin_x max_y  c'x + y'(b - Ax)  s.t. l ≤ x ≤ u\n\nEquivalent to: min c'x  s.t. Ax = b, l ≤ x ≤ u\nThe Lagrangian couples primal (x) and dual (y) variables.\n\nPDHG ITERATION:\nGiven step sizes τ (primal) and σ (dual):\n\n  x̄ = x^k - τ·(c - A'y^k)           // primal gradient step\n  x^{k+1} = proj_{[l,u]}(x̄)         // project onto bounds\n\n  ŷ = y^k + σ·(b - A·(2x^{k+1} - x^k))  // dual gradient with extrapolation\n  y^{k+1} = ŷ                        // no projection for free dual\n\nKEY FEATURE: Extrapolation (2x^{k+1} - x^k) accelerates convergence\ncompared to standard gradient descent.\n\nSTEP SIZE SELECTION:\nFor convergence: τ·σ·‖A‖² < 1\nTypically: τ = σ = 1/(‖A‖_2) or adaptive schemes\n\nRESTART STRATEGIES:\nPeriodically restart to accelerate when making progress:\n- Fixed frequency restart\n- Adaptive restart on normalized duality gap improvement\n- Restart on primal-dual distance decrease\n\nTERMINATION CRITERIA:\nCheck primal/dual feasibility and duality gap:\n- Primal feasibility: ‖Ax - b‖ / (1 + ‖b‖) ≤ ε\n- Dual feasibility: ‖c - A'y - zl + zu‖ / (1 + ‖c‖) ≤ ε\n- Gap: |c'x - b'y| / (1 + |c'x| + |b'y|) ≤ ε", "math": "Convergence rate:\n- O(1/k) ergodic convergence for duality gap\n- O(1/k²) with acceleration (momentum/restart)\n- Not polynomial in problem dimension (unlike IPM)\n- But: iteration cost is O(nnz) vs O(n³) for IPM\n\nComparison with other LP methods:\n- Simplex: exact but exponential worst-case\n- IPM: polynomial O(√n·log(1/ε)) but O(n³) per iteration\n- PDHG: cheap O(nnz) iterations, good for huge sparse LPs", "complexity": "- Per iteration: O(nnz) for matrix-vector products\n- Iterations to ε-accuracy: O(‖A‖/ε) without restart\n- Total for moderate accuracy: competitive with IPM on large sparse problems\n- Memory: O(n + m) vs O(fill) for factorization methods", "ref": ["Chambolle & Pock (2011). \"A First-Order Primal-Dual Algorithm for\n  Convex Problems with Applications to Imaging\". J. Math. Imaging Vis.", "Applegate et al. (2021). \"Practical Large-Scale Linear Programming\n  using Primal-Dual Hybrid Gradient\". NeurIPS."], "see": ["cupdlp_step.h for step computation details", "cupdlp_restart.h for restart strategies"], "has_pass2": true}, "highs/ipm/ipx/iterate.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/iterate.h", "filename": "iterate.h", "file": "ipm/ipx/iterate.h", "brief": "IPM Iterate Management with Variable States\n\nManages the primal-dual iterate for interior point methods, including\nvariable states (fixed/free/barrier) and convergence monitoring.", "algorithm": "Variable State Management in IPM:\nEfficiently handles bound constraints by classifying variables into states.\n\nPRIMAL-DUAL VARIABLES:\n  Primal: x[n+m], xl[n+m] = x - lb, xu[n+m] = ub - x\n  Dual: y[m] (constraints), zl[n+m], zu[n+m] (bound multipliers)\n\nVARIABLE STATES:\n\n1. BARRIER (actively optimized):\n   - Has at least one finite bound\n   - Contributes barrier term: -μ·(ln(xl) + ln(xu))\n   - Scaling factor: 1/√(zl/xl + zu/xu)\n\n2. FREE (no barrier):\n   - Treated as if bounds are ±∞\n   - xl = xu = ∞, zl = zu = 0\n   - Scaling factor: ∞ (special handling needed)\n\n3. FIXED (eliminated):\n   - Not moved by IPM (effective dimension reduction)\n   - Equivalent to reducing b by A[:,j]·x[j]\n   - Scaling factor: 0\n\nIMPLIED BOUNDS (for numerical stability):\nNear-optimal variables can be \"implied\" - treated as free during IPM\nbut fixed to bounds in postprocessing:\n- IMPLIED_LB: x[j] → lb[j], compute zl[j] from dual feasibility\n- IMPLIED_UB: x[j] → ub[j], compute zu[j] from dual feasibility\n- IMPLIED_EQ: lb = ub, choose sign of multiplier\n\nRESIDUALS COMPUTED:\n- rb = b - A·x (primal constraint)\n- rl = lb - x + xl (lower slack)\n- ru = ub - x - xu (upper slack)\n- rc = c - A'y - zl + zu (dual/reduced cost)", "math": "Complementarity measures:\n- μ = Σ(xl·zl + xu·zu) / (2·num_barriers)  (average)\n- Central path: xl·zl = xu·zu = μ for all barrier vars\n- Optimality: μ → 0 as iterate approaches solution\n\nTermination criterion:\n- Primal feasible: ‖rb‖/(1+‖b‖) ≤ ε_feas\n- Dual feasible: ‖rc‖/(1+‖c‖) ≤ ε_feas\n- Optimal: |pobj - dobj|/(1+|pobj|+|dobj|) ≤ ε_opt", "complexity": "- Update(): O(n+m) for step application\n- Evaluate(): O(nnz) for residual computation (lazy evaluation)\n- Memory: O(n+m) for all variable vectors", "see": ["ipm.h for IPM algorithm using this iterate", "crossover.h for converting iterate to basic solution"], "has_pass2": true}, "highs/ipm/ipx/forrest_tomlin.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/forrest_tomlin.h", "filename": "forrest_tomlin.h", "file": "ipm/ipx/forrest_tomlin.h", "brief": "Forrest-Tomlin LU Update for Basis Maintenance\n\nImplements the Forrest-Tomlin update to maintain LU factorization when\na single column of the basis matrix changes (basis exchange/pivot).", "algorithm": "Forrest-Tomlin LU Update:\nEfficiently updates LU factors after column replacement without full\nrefactorization.\n\nSETUP:\nGiven factorization: B = L·U with permutations\nAfter replacing column j: B' differs from B in one column\n\nKEY INSIGHT:\nInstead of refactorizing B' = L'·U', compute update matrices:\n  B' = L · R₁ · R₂ · ... · Rₖ · U'\nwhere Rᵢ are \"row eta\" matrices (identity plus one off-diagonal element).\n\nUPDATE PROCESS:\n1. FTRAN (Forward transformation):\n   Compute spike = L⁻¹·b where b is the new column\n   Append spike to U\n\n2. BTRAN (Backward transformation):\n   Compute eta = eₚ'·U⁻¹ where p is position of leaving column\n   Append eta to R (row eta matrix)\n\n3. Permutation update:\n   Track which columns have been replaced\n\nSOLVE WITH UPDATES:\nTo solve B'·x = rhs after k updates:\n  x = U'⁻¹ · Rₖ⁻¹ · ... · R₁⁻¹ · L⁻¹ · rhs", "math": "Fill-in analysis:\n- Spike column: O(m) nonzeros worst case\n- Row eta: O(m) nonzeros worst case\n- After k updates: O(k²) additional work per solve\n- Refactorize when k > √m or fill exceeds threshold\n\nMemory layout optimization:\nL, U stored in compressed form with permuted indices for contiguous\nmemory access during dense solves (unlike BASICLU which needs\nre-permutation).", "complexity": "- Single update: O(m²) worst case, O(nnz) typical\n- Solve after k updates: O(m·k) additional work\n- Trade-off: update cost vs refactorization cost", "ref": ["Forrest & Tomlin (1972). \"Updated Triangular Factors of the Basis\n  to Maintain Sparsity in the Product Form Simplex Method\".\n  Mathematical Programming 2:263-278."], "see": ["lu_update.h for base class interface", "basis.h for usage in basis maintenance"], "has_pass2": true}, "highs/ipm/ipx/basis.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/basis.h", "filename": "basis.h", "file": "ipm/ipx/basis.h", "brief": "Basis management for IPX interior point solver\n\nManages simplex-style basis for IPM preconditioning and crossover.\n\n**BasicStatus Enum:**\n- NONBASIC_FIXED: Fixed at bound, never enters basis\n- NONBASIC: At bound, may enter basis\n- BASIC: In basis, may leave\n- BASIC_FREE: In basis, never leaves\n\n**Basis Storage:**\n- basis_[p]: Column index at position p (0 <= p < m)\n- map2basis_[j]: Position of variable j, or -1/-2 if nonbasic\n- StatusOf(), PositionOf(), IsBasic(): Query methods\n\n**Linear Algebra Operations:**\n- Factorize(): LU factorization with stability check\n- SolveDense(): FTRAN/BTRAN for dense RHS\n- SolveForUpdate(): Prepare for basis exchange\n- TableauRow(): Compute simplex tableau row\n- ExchangeIfStable(): Pivot with stability check\n\n**Basis Construction:**\n- SetToSlackBasis(): Initialize to identity\n- Load(): Load user-provided basis\n- ConstructBasisFromWeights(): Crash procedure\n- CrashBasis(): Fast weighted crash\n- Repair(): Fix singularities with slack columns\n\n**Statistics:**\n- factorizations(), updates_total(): Counts\n- frac_ftran_sparse(), frac_btran_sparse(): Sparsity metrics\n- mean_fill(), max_fill(): LU fill factors", "see": ["ipm/ipx/lu_update.h for LU factorization", "ipm/ipx/lp_solver.h for usage context"], "has_pass2": false}, "highs/ipm/ipx/crossover.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/crossover.h", "filename": "crossover.h", "file": "ipm/ipx/crossover.h", "brief": "Crossover from Interior Point to Basic Solution\n\nConverts an interior point solution (all variables strictly between bounds)\nto a vertex (basic) solution required for simplex methods and post-processing.", "algorithm": "Crossover (IPM-to-Basis Conversion):\nTransforms interior solution to basic solution via two push phases that\nsystematically eliminate \"superbasic\" variables.\n\nDEFINITIONS:\n- Basic solution: x[nonbasic] at bounds, z[basic] = 0\n- Primal superbasic: nonbasic j with lb[j] < x[j] < ub[j]\n- Dual superbasic: basic j with z[j] ≠ 0\n\nIPM solutions are \"superbasic everywhere\" - all variables are strictly\ninterior. Crossover makes them basic by pushing variables to bounds.\n\nINVARIANTS MAINTAINED:\n1. lb[j] ≤ x[j] ≤ ub[j] for all j\n2. z[j] ≤ 0 if x[j] > lb[j] (complementarity)\n3. z[j] ≥ 0 if x[j] < ub[j] (complementarity)\n4. Ax remains unchanged (primal feasibility)\n5. A'y + z remains unchanged (dual feasibility)\n\nDUAL PUSH PHASE:\nFor each dual superbasic jb (basic with z[jb] ≠ 0):\n1. Compute: Δz = -z[jb] · (B⁻¹·a_jb)_N for nonbasic indices\n2. Find blocking: jn = argmax{α : z + α·Δz satisfies sign constraints}\n3. If jb reaches zero: push complete\n   Else: basis update swaps jb ↔ jn\n\nPRIMAL PUSH PHASE:\nFor each primal superbasic jn (nonbasic with lb < x[jn] < ub):\n1. Choose target bound (nearer, or 0 for free variables)\n2. Compute: Δx_B = -B⁻¹·a_jn · Δx_jn (basic variable update)\n3. Find blocking: jb = argmax{α : x + α·Δx satisfies bounds}\n4. If jn reaches bound: push complete\n   Else: basis update swaps jb ↔ jn", "math": "Crossover preserves optimality:\nStarting from IPM optimal (x,y,z) satisfying KKT conditions with μ → 0,\ncrossover produces vertex optimal solution because:\n- Objective c'x unchanged (same optimal value)\n- Feasibility Ax = b, l ≤ x ≤ u maintained\n- Complementarity x·z = 0 achieved (not approximately)", "complexity": "- Each push: O(m²) for LU update + FTRAN/BTRAN\n- Worst case: O(n+m) pushes needed\n- Total: O((n+m) · m²), but usually much faster\n- Often O(n+m) total pivots (one per superbasic variable)", "ref": ["Megiddo (1991). \"On Finding Primal- and Dual-Optimal Bases\".\n  ORSA Journal on Computing 3(1):63-65.", "Bixby & Saltzman (1994). \"Recovering an Optimal LP Basis from an\n  Interior Point Solution\". Operations Research Letters 15:169-178."], "see": ["ipm.h for interior point solver producing input", "basis.h for basis representation and updates"], "has_pass2": true}, "highs/ipm/ipx/ipm.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/ipm.h", "filename": "ipm.h", "file": "ipm/ipx/ipm.h", "brief": "Mehrotra's Predictor-Corrector Interior Point Method\n\nImplements an interior point method (IPM) for linear programming using\nMehrotra's predictor-corrector technique with two KKT solves per iteration.", "algorithm": "Mehrotra's Predictor-Corrector IPM:\nA two-stage Newton method that achieves superlinear convergence in practice.\n\nPRIMAL-DUAL FORMULATION:\nLP: min c'x  s.t. Ax = b, l ≤ x ≤ u\nIntroducing slacks: xl = x - l, xu = u - x (both ≥ 0)\n\nKKT conditions with barrier:\n  Ax = b                    (primal feasibility)\n  A'y + zl - zu = c         (dual feasibility)\n  Xl·Zl = μe                (complementarity, xl ∘ zl = μ)\n  Xu·Zu = μe                (complementarity, xu ∘ zu = μ)\n  xl, xu, zl, zu ≥ 0        (non-negativity)\n\nPREDICTOR STEP (Affine Scaling):\nSolve Newton system with μ = 0 (pure Newton toward boundary):\n  [G   A'] [Δx]   [rb - A'Δy - Δzl + Δzu]\n  [A   0 ] [Δy] = [rc]\n  Δzl = -zl - (Zl/Xl)·Δxl\n  Δzu = -zu - (Zu/Xu)·Δxu\n\nCORRECTOR STEP (Centering + Higher-order):\nBased on predictor's step length α_aff, choose target:\n  μ_target = σ · μ,  σ = (μ_aff / μ)³\nAdd correction terms for better centrality:\n  sl = σμe - ΔXl_aff · ΔZl_aff\n  su = σμe - ΔXu_aff · ΔZu_aff\n\nSTEP SIZE SELECTION:\nα_primal = max{α : x + α·Δx ≥ 0} · 0.9995\nα_dual   = max{α : z + α·Δz ≥ 0} · 0.9995", "math": "Convergence behavior:\n- Theoretical: O(√n · log(1/ε)) iterations for ε-optimality\n- Practical: 20-60 iterations regardless of problem size\n- Superlinear convergence in terminal phase (σ → 0)\n\nCentering parameter σ ∈ [0,1]:\n- σ = 0: Pure Newton (fast but may leave feasible region)\n- σ = 1: Pure centering (stays centered but slow)\n- Mehrotra: σ = (μ_aff/μ)³ adapts to iterate quality", "complexity": "- Per iteration: O(KKT_solve) = O(m²n) for dense, O(nnz · fill) for sparse\n- Two KKT solves per iteration (predictor + corrector)\n- Total: O(√n · log(1/ε) · KKT_solve) theoretical, much less in practice", "ref": ["Mehrotra (1992). \"On the Implementation of a Primal-Dual Interior\n  Point Method\". SIAM Journal on Optimization 2(4):575-601.", "Wright (1997). \"Primal-Dual Interior-Point Methods\". SIAM."], "see": ["kkt_solver.h for linear system solver interface", "crossover.h for converting IPM solution to basic solution", "iterate.h for iterate data structure"], "has_pass2": true}, "highs/ipm/ipx/kkt_solver.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/kkt_solver.h", "filename": "kkt_solver.h", "file": "ipm/ipx/kkt_solver.h", "brief": "KKT System Solver Interface for Interior Point Methods\n\nDefines the interface for solving the augmented system (KKT system) that\narises in each iteration of primal-dual interior point methods.", "algorithm": "KKT System in IPM:\nThe Newton step in interior point methods requires solving:\n\n  [ G   A' ] [Δx]   [a]\n  [ A   0  ] [Δy] = [b]\n\nwhere:\n- G = diagonal matrix with G[j,j] = zl[j]/xl[j] + zu[j]/xu[j]\n- A = m×(n+m) constraint matrix [A | -I]\n- (a,b) = right-hand side from Newton linearization\n\nSOLUTION APPROACHES:\n\n1. AUGMENTED SYSTEM (Direct):\n   Factor and solve the full (n+m+m) × (n+m+m) symmetric indefinite system.\n   Uses LDL' or similar factorization with pivoting.\n\n2. NORMAL EQUATIONS (Direct):\n   Eliminate Δx: A·G⁻¹·A'·Δy = b - A·G⁻¹·a\n   Smaller system (m×m) but squares condition number.\n\n3. ITERATIVE (Krylov):\n   Use CG/MINRES with preconditioner. The residual tolerance must satisfy:\n   ‖D·res‖_∞ ≤ tol, where D[i,i] = √(1/G[i,i]) or 1 if G[i,i] = 0\n\nREGULARIZATION:\nNear-optimal iterates have G[j,j] → ∞ for variables at bounds, causing\nill-conditioning. Regularization adds small ε to diagonal:\n- Primal regularization: G + εP·I\n- Dual regularization: zero block becomes -εD·I", "math": "Condition number analysis:\n- cond(KKT) ≈ max(G)/min(G) · cond(A)\n- Near optimum: max(G)/min(G) → ∞ (ill-conditioned)\n- Regularization bounds condition: max ≈ 1/ε\n- Normal equations: cond(A·G⁻¹·A') ≈ cond(KKT)²", "complexity": "- Direct (augmented): O((n+m)³) dense, O(nnz · fill²) sparse\n- Direct (normal): O(m³) dense, O(m · nnz + m² · fill) sparse\n- Iterative: O(k · nnz) where k = iterations to convergence", "ref": ["Benzi, Golub & Liesen (2005). \"Numerical Solution of Saddle Point\n  Problems\". Acta Numerica 14:1-137.", "Nocedal & Wright (2006). \"Numerical Optimization\", Chapter 16."], "see": ["ipm.h for interior point method using this interface", "kkt_solver_basis.h for basis-based implementation"], "has_pass2": true}, "highs/ipm/ipx/lp_solver.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/lp_solver.h", "filename": "lp_solver.h", "file": "ipm/ipx/lp_solver.h", "brief": "IPX interior point LP solver main class\n\nImplements primal-dual interior point method with crossover to basic solution.\n\n**Model Loading:**\n- LoadModel(): LP in standard form (c, lb, ub, A, rhs, constr_type)\n- LoadIPMStartingPoint(): User-provided initial point (x, xl, xu, y, zl, zu)\n\n**Solution Methods:**\n- Solve(): Run IPM + optional crossover to basis\n- CrossoverFromStartingPoint(): Convert complementary point to basis\n\n**Solution Access:**\n- GetInfo(): Solver status and statistics\n- GetInteriorSolution(): IPM iterate (primal, slack, dual)\n- GetBasicSolution(): Basic solution with basis status (vbasis, cbasis)\n\n**IPM Pipeline (InteriorPointSolve):**\n1. RunInitialIPM(): Initial centering iterations\n2. BuildStartingBasis(): Construct basis from weighted columns\n3. RunMainIPM(): Main IPM iterations with basis preconditioning\n4. RunCrossover(): Push to basis via dual/primal pushes\n\n**Key Components:**\n- control_: Parameters and logging\n- model_: Preprocessed LP data\n- iterate_: Current IPM point (x, y, z, slacks)\n- basis_: Basis for preconditioning and crossover", "see": ["ipm/ipx/basis.h for basis management", "ipm/ipx/ipm.h for IPM algorithm details", "ipm/IpxWrapper.h for HiGHS integration"], "has_pass2": false}, "highs/ipm/ipx/conjugate_residuals.h": {"path": "layer-4/HiGHS/highs/ipm/ipx/conjugate_residuals.h", "filename": "conjugate_residuals.h", "file": "ipm/ipx/conjugate_residuals.h", "brief": "Conjugate Residuals Method for Symmetric Positive Definite Systems\n\nImplements preconditioned Conjugate Residuals (CR) for iteratively solving\nthe KKT system in interior point methods.", "algorithm": "Conjugate Residuals (CR):\nA Krylov subspace method for solving Cx = b where C is symmetric positive\ndefinite.\n\nKEY DIFFERENCE FROM CG:\n- CG minimizes ‖x - x*‖_C (error in C-norm)\n- CR minimizes ‖Cx - b‖₂ (residual in 2-norm)\nCR is preferred when monitoring residual convergence directly.\n\nUNPRECONDITIONED CR:\n  r₀ = b - C·x₀\n  p₀ = r₀, s₀ = C·r₀\n  for k = 0, 1, 2, ...\n    α = (rₖ, sₖ) / (sₖ, sₖ)\n    xₖ₊₁ = xₖ + α·pₖ\n    rₖ₊₁ = rₖ - α·C·pₖ\n    sₖ₊₁ = C·rₖ₊₁\n    β = (rₖ₊₁, sₖ₊₁) / (rₖ, sₖ)\n    pₖ₊₁ = rₖ₊₁ + β·pₖ\n\nPRECONDITIONED CR (with preconditioner P ≈ C⁻¹):\n  Effectively solves: P^{1/2}·C·P^{1/2}·y = P^{1/2}·b, x = P^{1/2}·y\n  - Apply P to residual: z = P·r\n  - Update formulas modified to use preconditioned vectors\n\nTERMINATION CRITERION:\n- Without scaling: ‖rₖ‖_∞ ≤ tol\n- With scaling: ‖D·rₖ‖_∞ ≤ tol where D = diag(resscale)\n\nThe scaled criterion is important for IPM where different components\nhave vastly different magnitudes.", "math": "Convergence analysis:\nAfter k iterations: ‖rₖ‖ ≤ 2·((√κ-1)/(√κ+1))^k·‖r₀‖\nwhere κ = cond(C) = λ_max/λ_min\n\nPreconditioning reduces effective condition number:\nκ_eff = cond(P·C) << κ\n\nFor IPM: C is the reduced KKT system (normal equations form),\nP is basis-based preconditioner using approximate LU factors.", "complexity": "- Per iteration: 1 mat-vec with C + 1 preconditioner apply (if used)\n- Typical iterations: O(√κ) or O(√κ_eff) with preconditioning\n- Total: O(k · nnz) for k iterations", "ref": ["Saad (2003). \"Iterative Methods for Sparse Linear Systems\", 2nd ed.\n  Algorithm 6.20.", "Schork (2018). \"Basis Preconditioning in Interior Point Methods\".\n  PhD thesis, Section 6.3."], "see": ["kkt_solver.h for iterative KKT solver using this method", "linear_operator.h for matrix/preconditioner interface"], "has_pass2": true}}}, "OS": {"name": "OS", "file_count": 63, "pass2_count": 0, "files": {"OS/src/OSUtils/OSReferenced.hpp": {"path": "layer-4/OS/OS/src/OSUtils/OSReferenced.hpp", "filename": "OSReferenced.hpp", "file": "OSReferenced.hpp", "brief": "OS reference counting base class\n\nReference-counted base class for OS objects.\nEnables automatic memory management via smart pointers.", "has_pass2": false}, "OS/src/OSUtils/OSSmartPtr.hpp": {"path": "layer-4/OS/OS/src/OSUtils/OSSmartPtr.hpp", "filename": "OSSmartPtr.hpp", "file": "OSSmartPtr.hpp", "brief": "OS smart pointer implementation\n\nIntrusive smart pointer for OSReferenced objects.\nSimilar to Ipopt SmartPtr for reference counting.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSgams2osil.hpp": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSgams2osil.hpp", "filename": "OSgams2osil.hpp", "file": "OSgams2osil.hpp", "brief": "GAMS to OSiL model converter\n\nConverts GAMS optimization models to OS Instance Language (OSiL).\nReads GAMS .gms files and produces XML-based OSiL representation.", "return": "whether the instance is created successfully.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSosrl2gams.hpp": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSosrl2gams.hpp", "filename": "OSosrl2gams.hpp", "file": "OSosrl2gams.hpp", "brief": "OSrL to GAMS solution converter\n\nConverts OS Result Language (OSrL) to GAMS solution format.\nWrites solution back to GAMS for post-processing.", "param": ["gmo_ GMO handler.", "osresult Optimization result as object.", "osrl Optimization result as string."], "has_pass2": false}, "OS/src/OSParsers/OSParseosil.tab.hpp": {"path": "layer-4/OS/OS/src/OSParsers/OSParseosil.tab.hpp", "filename": "OSParseosil.tab.hpp", "file": "OSParseosil.tab.hpp", "brief": "OSiL XML parser (bison-generated)\n\nBison-generated parser for OS Instance Language (OSiL).\nParses XML optimization problem instances.", "has_pass2": false}, "OS/src/OSParsers/OSParseosol.tab.hpp": {"path": "layer-4/OS/OS/src/OSParsers/OSParseosol.tab.hpp", "filename": "OSParseosol.tab.hpp", "file": "OSParseosol.tab.hpp", "brief": "OSoL XML parser (bison-generated)\n\nBison-generated parser for OS Option Language (OSoL).\nParses solver options in XML format.", "has_pass2": false}, "OS/src/OSParsers/OSParseosrl.tab.hpp": {"path": "layer-4/OS/OS/src/OSParsers/OSParseosrl.tab.hpp", "filename": "OSParseosrl.tab.hpp", "file": "OSParseosrl.tab.hpp", "brief": "OSrL XML parser (bison-generated)\n\nBison-generated parser for OS Result Language (OSrL).\nParses optimization results in XML format.", "has_pass2": false}, "OS/applications/columnGen/code/OSDecompSolverFactory.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompSolverFactory.h", "filename": "OSDecompSolverFactory.h", "file": "OSDecompSolverFactory.h\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSBearcatSolverXij.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSBearcatSolverXij.h", "filename": "OSBearcatSolverXij.h", "file": "OSBearcatSolverXij.h\n\n\\remarks\nCopyright (C) 2005-2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSNode.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSNode.h", "filename": "OSNode.h", "file": "OSNode.h\n\n\\remarks\nCopyright (C) 2005-2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSColGenApp.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSColGenApp.h", "filename": "OSColGenApp.h", "file": "OSColGenApp.h\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSDecompSolver.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompSolver.h", "filename": "OSDecompSolver.h", "file": "OSDecompSolver.h\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["int numNewRows -- number of new rows generated", "int* numNonz -- number of nonzeros in each row", "int** colIdx -- vectors column indexes of new rows", "double** values -- vectors of matrix coefficient values of new rows", "double* rowLB -- vector of row lower bounds", "double* rowUB -- vector of row upper bounds\n\nINPUT:", "double* thetaVar -- the vector of primal master values", "int numThetaVar -- size of master primal vector", "iny numNewColumns -- number of new columns generated", "int* numNonz -- number of nonzeros in each column", "double* cost -- the objective function coefficient on each new column", "double** rowIdx -- vectors row indexes of new columns", "double** values -- vectors of matrix coefficient values of new columns", "double* yA -- the vector of dual values on the coupling constraints", "int numARows -- size of the yA dual vector", "int numBRows -- size of the yA dual vector", "varIdx -- the variable number x_{ij} for branching", "numNonz -- number of theta indexes in the cut", "indexes -- the indexes of the theta variables", "values -- the number of times the theta indexed\nin indexes appears in the cut\nnote -- set numNonz to zero if the generated cut\nvariable already appears in varConMap\n\nINPUT:\ndouble* thetaVar -- the vector of primal master values\nint numThetaVar -- size of master primal vector\nvarConMap -- the map of variables in x_{ij} space to\na consraint number", "varIdx -- the variable number x_{ij} for branching", "indexes -- the indexes of the theta variables", "values -- the number of times the theta indexed\nin indexes appears in the cut\nnote -- set numNonz to zero if the generated cut\nvariable already appears in varConMap\n\nINPUT:", "double* theta -- the vector of primal master values", "int numThetaVar -- size of master primal vector", "dstd::map<int, int> &inVars -- the mapping of variables, the first\nindex is the variable number before resetting, the second index is the variable\nnumber after the reset", "OsiSolverInterface *si -- the solver interface that corresponds to the master\nthis is what gets rebuilt"], "has_pass2": false}, "OS/applications/columnGen/code/OSDecompParam.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompParam.h", "filename": "OSDecompParam.h", "file": "OSColGenApp.cpp\n\n\\remarks\nCopyright (C) 2005-2008, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSBearcatSolverXkij.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSBearcatSolverXkij.h", "filename": "OSBearcatSolverXkij.h", "file": "OSBearcatSolverXkij.h\n\n\\remarks\nCopyright (C) 2005-2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/applications/columnGen/code/OSDecompFactoryInitializer.h": {"path": "layer-4/OS/OS/applications/columnGen/code/OSDecompFactoryInitializer.h", "filename": "OSDecompFactoryInitializer.h", "file": "OSDecompFactoryInitializer.cpp\n\n\\remarks\nCopyright (C) 2005-2008, Gus Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSAgent/OSWSUtil.h": {"path": "layer-4/OS/OS/src/OSAgent/OSWSUtil.h", "filename": "OSWSUtil.h", "file": "OSWSUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["solverURI is the location of remote solver or scheduler", "theSOAP is a string that SOAP message sent to the Web service", "servIP is a string with IP address or domain name of the server", "solverPortNumber is a string with the port number of Web server (assume 80 by default)", "theXmlString is the string to modify to out in the SOAP envelop", "useCDATA is true if just encase the XML in a CDATA statement", "theXmlString is the string from the SOAP envelop to modify", "useCDATA is true if just encasing the XML in a CDATA statement", "numInputs is the number of OSxL protocols (e.g. osil, osol) in the SOAP message", "solverAddress is the address of the scheduler or solver used", "postURI is the path to the solver that follows the first\n/ in the solverAddress", "smethod is the method invoked, e.g. solve, kill, send, etc.", "msInputs is string pointer to an array of strings are the OSxL protocols\nprotocols that go into the message, e.g. osil, osol", "msInputNames is a string pointer to an array of string names of the OSxL protocols", "sSoapAction is the name of the solver service plus the method, e.g. OSSolverService#solve", "numInputs is the number of OSxL protocols (e.g. osil, osol) in the SOAP message", "solverAddress is the address of the scheduler or solver used", "postURI is the path to the solver that follows the first\n/ in the solverAddress", "smethod is the method invoked, e.g. solve, kill, send, etc.", "msInputs is string pointer to an array of strings are the OSxL protocols\nprotocols that go into the message, e.g. osil, osol", "msInputNames is string pointer to an array of string names of the OSxL protocols", "sSoapAction is the name of the solver service plus the method, e.g. OSSolverService#solve", "soapstring the soap envelop returned from the Web service", "serviceMethod -- extract the string between the <serviceMethodReturn> and </serviceMethodReturn> tags."], "return": "the reply from the Web service in a SOAP message.\n</p>", "has_pass2": false}, "OS/src/OSAgent/OSSolverAgent.h": {"path": "layer-4/OS/OS/src/OSAgent/OSSolverAgent.h", "filename": "OSSolverAgent.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["solverURI is the location of remote solver or scheduler", "osil a string that holds the problem instance", "osol is a string of options for the solver", "osol is the string with the options in OSoL format", "osil is the string with the instance in OSiL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "ospl is the string with the process information in OSpL format", "osol is the string with the options in OSoL format", "osilFileName is the name of the file\nwith the OSiL instance to be written on the server", "osil is a string with the OSiL problem instance\n</p>"], "return": "osrl which is a string with the result.\n</p>", "has_pass2": false}, "OS/src/OSAgent/OShL.h": {"path": "layer-4/OS/OS/src/OSAgent/OShL.h", "filename": "OShL.h", "file": "OShL.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osil is the string with the instance in OSiL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "osil is the string with the instance in OSiL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "osol is the string with the options in OSoL format", "ospl is the string with the process information in OSpL format", "osol is the string with the options in OSoL format"], "return": "a string which is the result in OSrL format.\n</p>", "has_pass2": false}, "OS/src/OSUtils/OSBase64.h": {"path": "layer-4/OS/OS/src/OSUtils/OSBase64.h", "filename": "OSBase64.h", "file": "Base64.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["bytes is the input to be encoded.", "size is the size of the pointer in bytes", "b64bytes is the input to be decoded"], "return": "a string in base 64 format.", "has_pass2": false}, "OS/src/OSUtils/OSFileUtil.h": {"path": "layer-4/OS/OS/src/OSUtils/OSFileUtil.h", "filename": "OSFileUtil.h", "file": "FileUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["fname holds the name of the file.", "fname holds the name of the file.", "fname holds the name of the file to be written.", "thestring holds the string to be written to the file.", "fname holds the name of the file to be written.", "thestring holds the string to be written to the file.", "fname holds the name of the file to be written.", "ch holds a pointer to a char array to be written to the file."], "return": "the file contents as a string.", "has_pass2": false}, "OS/src/OSUtils/OSErrorClass.h": {"path": "layer-4/OS/OS/src/OSUtils/OSErrorClass.h", "filename": "OSErrorClass.h", "file": "ErrorClass.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["errormsg_ holds the error message as a string."], "has_pass2": false}, "OS/src/OSUtils/OSMathUtil.h": {"path": "layer-4/OS/OS/src/OSUtils/OSMathUtil.h", "filename": "OSMathUtil.h", "file": "MathUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\n<p>The <code>MathUtil</code> class contains methods for performing\nmathematics related operations used by many classes in the\nOptimization Services (OS) framework. </p>", "param": ["X holds the number to be rounded.", "precision holds the number of digit after (or before if negative) the decimal point.", "x holds the number before the mod operator.", "x holds the number after the mod operator.", "isColumnMajor holds whether the coefMatrix (AMatrix) holding linear program\ndata is stored by column. If false, the matrix is stored by row.", "startSize holds the size of the start array", "valueSize holds the size of the index and value arrays", "start holds an integer array of start elements in coefMatrix (AMatrix),\nwhich points to the start of a column (row) of nonzero elements in coefMatrix (AMatrix).", "index holds an integer array of rowIdx (or colIdx) elements in coefMatrix (AMatrix).\nIf the matrix is stored by column (row), rowIdx (colIdx) is the array of row (column) indices.", "value holds a double array of value elements in coefMatrix (AMatrix),\nwhich contains nonzero elements.", "dimension holds the column count if the input matrix is row major (row count = start.length-1)\nor the row number if the input matrix is column major (column count = start.length -1)", "x is the double that gets converted into a string\nthis takes the David Gay dtoa and converts to a formatted string", "str is the char* string that gets converted to double\nthis method actually wraps around os_strtod (which is really the\nDavid Gay version of strtod) and will throw an exception\nif the str contains text or is in anyway not a valid number\nstr should be null terminated", "str is the char* string that gets converted to double", "strEnd should point to the end of str\nthis method actually wraps around os_strtod (which is really the\nDavid Gay version of strtod) and will throw an exception\nif the str contains text or is in anyway not a valid number", "i holds a pointer to the array to be processed.", "mult holds the length of the run. This parameter is passed by reference", "incr holds the increment. This parameter is also passed by reference", "size holds the number of elements in the array. This parameter is passed by value", "defaultIncr holds the default value for incr from the schema file. Using just <el mult=\"...\"\nsaves space whenever a run of two or more elements has been encountered, whereas <el mult=\"...\" incr=\"...\"\nsaves space only for runs of three or more elements. Thus the defaultIncr must be treated specially\n(and it might change from one schema element to the next).", "i holds a pointer to the array to be processed.", "mult holds the length of the run. This parameter is passed by reference", "incr holds the increment. This parameter is also passed by reference", "size holds the number of elements in the array. This parameter is passed by value", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array.", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array.", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array.", "i holds a pointer to the array to be processed.", "size holds the number of elements in the array."], "return": "the rounded number.", "has_pass2": false}, "OS/src/OSUtils/OSDataStructures.h": {"path": "layer-4/OS/OS/src/OSUtils/OSDataStructures.h", "filename": "OSDataStructures.h", "file": "OShL.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSUtils/OSParameters.h": {"path": "layer-4/OS/OS/src/OSUtils/OSParameters.h", "filename": "OSParameters.h", "file": "OSParameters.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSUtils/OSOutput.h": {"path": "layer-4/OS/OS/src/OSUtils/OSOutput.h", "filename": "OSOutput.h", "file": "OSOutput.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2012-2013, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["name holds the name of the file or device\n that applies to this output device in all code areas", "area holds the area of the code to which this option is to be applied", "level holds a valid print level", "level holds a valid print level", "level holds an array of valid print levels", "dim holds the number of entries in the array level", "area:  the area in which the output string originated", "level: the print level associated with the string", "str:   the string that is to be printed", "level:  the print level associated with the string", "area:   the area of the code in which the output was generated", "outStr: the string to be output", "name: The name of the channel (\"stdout\" and \"stderr\" are reserved names)", "level: The array of print levels used for the output to this channel", "dim: The number of entries in this array", "name: The name of the channel (\"stdout\" and \"stderr\" are reserved names)", "level: The print level used for the output to this channel\n     if < ENUM_OUTPUT_LEVEL_NUMBER_OF_LEVELS, set the (same) print level in all areas\n     otherwise set the print level only in one particular area", "name: The name of the channel (\"stdout\" and \"stderr\" are reserved names)", "name: The name of the channel", "name: The name of the channel"], "return": "whether the set() was successful", "has_pass2": false}, "OS/src/OSUtils/OSdtoa.h": {"path": "layer-4/OS/OS/src/OSUtils/OSdtoa.h", "filename": "OSdtoa.h", "file": "MathUtil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\n<p>The <code>MathUtil</code> class contains methods for performing\nmathematics related operations used by many classes in the\nOptimization Services (OS) framework. </p>", "has_pass2": false}, "OS/src/OSUtils/OSStringUtil.h": {"path": "layer-4/OS/OS/src/OSUtils/OSStringUtil.h", "filename": "OSStringUtil.h", "file": "OSStringUtil.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2010, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["str holds the string to be output.\nIf the string does not contain double quotes, it is output surrounded by double quotes,\nif the string contains double quotes, it is output surrounded by single quotes,"], "return": "the prepared string, ready to be printed", "has_pass2": false}, "OS/src/OSModelInterfaces/OSmps2OS.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSmps2OS.h", "filename": "OSmps2OS.h", "file": "OSmps2OS.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2013, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "whether the objects are created successfully.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSmps2osil.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSmps2osil.h", "filename": "OSmps2osil.h", "file": "OSmps2osil.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "whether the instance is created successfully.", "has_pass2": false}, "OS/src/OSModelInterfaces/OSnl2OS.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSnl2OS.h", "filename": "OSnl2OS.h", "file": "OSnl2osol.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2011, Horand Gassmann, Jun Ma, Kipp Martin,\nand the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["name carries the name of the ASL object\n(there are three of them: asl, rw, cw)", "stub is the (relevant part of the) file name", "asl carries a pointer to the object named \"asl\"", "rw  carries a pointer to the object named \"rw\"", "cw  carries a pointer to the object named \"cw\"\n(asl should point to the same location as either rw or cw)", "osinstance: a pointer to the OSInstance object", "lower: index of the first variable to be set in this call", "upper: set all variables from lower...upper-1", "vartype: the type of the variable (in AMPL this is 'C', 'B' or 'I')", "osinstance: a pointer to the OSInstance object", "lower: index of the first variable to be set in this call", "upper: set all variables from lower...upper-1"], "return": "the pointer to the object named", "has_pass2": false}, "OS/src/OSModelInterfaces/OSosrl2ampl.h": {"path": "layer-4/OS/OS/src/OSModelInterfaces/OSosrl2ampl.h", "filename": "OSosrl2ampl.h", "file": "OSosrl2ampl.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2012, Horand Gassmann, Jun Ma, Kipp Martin,\nand the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osrl is a string containing the result information", "asl is a pointer to an ASL data structure", "filename is the name of the output file\n(e.g., as returned from the solver)."], "return": "whether the .sol file was created successfully.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSMatlabSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSMatlabSolver.h", "filename": "OSMatlabSolver.h", "file": "OSMatlab.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "a string with the solution in OSrL format", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSDefaultSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSDefaultSolver.h", "filename": "OSDefaultSolver.h", "file": "DefaultSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSLindoSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSLindoSolver.h", "filename": "OSLindoSolver.h", "file": "LindoSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "true if an exception is not thrown.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSKnitroSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSKnitroSolver.h", "filename": "OSKnitroSolver.h", "file": "KnitroSolver.h", "author": "Robert Fourer,  Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSCsdpSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSCsdpSolver.h", "filename": "OSCsdpSolver.h", "file": "OSCsdpSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSBonminSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSBonminSolver.h", "filename": "OSBonminSolver.h", "file": "OSBonmnSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSIpoptSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSIpoptSolver.h", "filename": "OSIpoptSolver.h", "file": "IpoptSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2014, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSCouenneSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSCouenneSolver.h", "filename": "OSCouenneSolver.h", "file": "OSCouenneSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSRunSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSRunSolver.h", "filename": "OSRunSolver.h", "file": "OSRunSolver.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2013, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osol: A string containing the user options in osol format", "osinstance: A pointer to an OSInstance object containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osoption: A pointer to an OSOption object containing the options to be passed to the solver", "osil: A string containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osol: A string containing the user options in osol format", "osil: A string containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected", "osoption: A pointer to an OSOption object containing the options to be passed to the solver", "osinstance: A pointer to an OSInstance object containing the instance to be optimized", "solverName: The name of the solver selected by the user\n     If empty, a default solver is selected based on the characteristics of the problem", "osinstance: A pointer to an OSInstance object containing the instance to be optimized"], "return": "the solution (or error message) in OSrL format", "has_pass2": false}, "OS/src/OSSolverInterfaces/OSCoinSolver.h": {"path": "layer-4/OS/OS/src/OSSolverInterfaces/OSCoinSolver.h", "filename": "OSCoinSolver.h", "file": "CoinSolver.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSnLNode.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSnLNode.h", "filename": "OSnLNode.h", "file": "OSnLNode.h\n\\brief This file defines the OSnLNode class along with its derived classes.", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2015, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\nIn this file we define classes for a subset of the nodes defined in the OSnL schema\nThese nodes fall into three broad classes:\nThose that evaluate to real values (which inherit from OSnLNode),\nthose that evaluate to matrices (and inherit from OSnLMNode), and\nthose that evaluate to complex values (and inherit from OSnLCNode).\nReal and complex-valued nodes derive from a common parent, ScalarNode,\nwhich is derived from ExprNode.\nOSnLNodes can have OSnLMNode children (e.g., matrixDeterminant)\nand vice versa (e.g., matrixScalarTimes); similarly for OSnLCNodes.", "return": "the value of the operator name", "param": ["recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "a pointer prefixVector to a vector of pointers of ExprNodes", "a pointer postfixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to ExprNodes in prefix format", "nlNodeVec holds a vector of pointers to ExprNodes in postfix format", "x holds the values of the variables in a double array.", "x holds the values of the variables in a double array.", "x holds the values of the variables in a double array.", "x holds the values of the variables in a double array.", "a pointer to a map of the variables in the OSnLNode and its children", "x holds the values of the variables in a double array.", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin prefix format", "a pointer prefixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to OSnLNodes\nin postfix format", "a pointer postfixVector to a vector of pointers of ExprNodes", "recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin prefix format", "a pointer prefixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin postfix format", "a pointer postfixVector to a vector of pointers of ExprNodes", "x holds the values of the variables in a double array.", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "recurse controls whether the children of the node are accessed recursively", "indent controls the amount of indentation used to visualize the tree structure", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "mtxLoc     pointer array to all defined matrices to resolve matrix references", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "x holds the values of the variables in a double array.", "nlNodeVec holds a vector of pointers to OSnLNodes and OSnLMNodes\nin prefix format", "a pointer prefixVector to a vector of pointers of ExprNodes", "nlNodeVec holds a vector of pointers to OSnLNodes\nin postfix format", "a pointer postfixVector to a vector of pointers of ExprNodes"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSCommandLine.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSCommandLine.h", "filename": "OSCommandLine.h", "file": "OSCommandLine.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2011-2012, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSrLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSrLWriter.h", "filename": "OSrLWriter.h", "file": "OSrLWriter.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nDalhousie University, Northwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["theosresult is a pointer to an OSResult object", "theosresult is a pointer to an OSResult object"], "return": "a string with the OSResult data that validates against the OSrL schema.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSrLReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSrLReader.h", "filename": "OSrLReader.h", "file": "OSrLReader.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nDalhousie University, Northwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osrl an OSrL string."], "return": "the OSResult object corresponding to the OSrL string.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSMatrix.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSMatrix.h", "filename": "OSMatrix.h", "file": "OSMatrix.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2010-2015, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "the value of nType", "param": ["a pointer postfixVector to a vector of pointers of MatrixNodes", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "isRowMajor_ holds whether the matrix is stored by column.\n       If true, the matrix is stored by row.", "numberOfRows holds the number of rows", "numberOfColumns holds the number of columns", "startSize_ holds the size of the start array.", "valueSize_ holds the size of the value and index arrays.", "type_ describes the type of values held in the matrix (see OSParameters.h).", "nValues gives the size of the value->el array", "sourceElements is the source array from which to copy", "sourceType gives the type of elements in the source array", "sourceIndex gives the location within the source's value array to be copied from", "targetIndex gives the location within the target's value array to be copied to", "scalarMult if present, gives a real scalar by which to multiply the value to be copied", "scalarImag if present, gives the imaginary part of the scalar multiplier", "sourceElements is the source array from which to copy", "sourceType gives the type of elements in the source array", "sourceIndex gives the location within the source's value array to be copied from", "targetIndex gives the location within the target's value array to be copied to", "scalarMult if present, gives a real scalar by which to multiply the value to be copied", "scalarImag if present, gives the imaginary part of the scalar multiplier", "convertTo_ gives the type of elements to be stored into the target matrix", "transpose_ specifies whether the matrix is to be transposed in the process\n        (i.e., the identity of the index array is to be adjusted)", "convertTo_ gives the type of elements to be stored into the target matrix", "scalarMult if present, gives a real scalar by which to multiply the values", "scalarImag if present, gives the imaginary part of the scalar multiplier", "symmetry_ gives the type of symmetry to be used in the target matrix", "copyValues_ determines whether the arrays need to be duplicated", "symmetry_ gives the type of symmetry to be used in the target matrix", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the blocks are stored in row major form or not.", "valueType_ indicates in which form to store the disassembled matrix\n        The default for this optional parameter is ENUM_MATRIX_TYPE_unknown", "symmetry_ determines what kind of symmetry to use in representing the blocks.\n        If this parameter is missing, the default value is NO symmetry", "rowIdx is the row index of the block to be retrieved", "colIdx is the column index of the block to be retrieved", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "symmetry_ contains the symmetry type (see OSParameters.h)", "mtxArray provides pointers to all defined matrices in case expansion is necessary.", "mtxArray   provides pointers to all defined matrices for use within transformations.", "rowMajor_  controls whether the matrix should be expanded into row or column major format", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                   The default value does not change the symmetry", "transpose_ controls whether the expansion is of the matrix or its transpose\n                   The default is to expand the matrix in its natural form", "idx indicates which entry in the vector of expansions is to be printed", "idx indicates which entry in the vector of expansions is to be printed", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor can be used to store the objects in row major form.", "isColumnMajor holds whether the matrix is currently stored by column.\n       If true, the matrix is converted to row major form.\n       If false, the matrix is stored by row and is converted to column major.", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the baseMatrix should be stored in row major (if true)\n        or column major.", "convertTo_ indicates the form of the value array in the expanded matrix\n        The default is \"unknown\", which infers the value type from the content of the matrix", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "currentBlocks is a pointer to the collection of blocks that is to be expanded", "mtxArray  provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the expanded matrix should be stored in row major (if true)\n        or column major.", "convertTo_ indicates the form of the value array in the expanded matrix\n        The default is \"unknown\", which infers the value type from the content of the matrix", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "nConst indicates the position of the constructor in the array m_mChildren, the child\n        elements of the current matrix (i.e., the list of constructors)", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the expanded matrix should be stored in row major (if true)\n        or column major.", "convertTo_ indicates the form of the value array in the expanded matrix\n        The default is \"unknown\", which infers the value type from the content of the matrix", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "nConst gives the number of the constructor within the constructor list\n        of the parent matrix or block", "rowMajor indicates whether the baseMatrix should be stored in row major (if true)\n        or column major.", "convertTo_ can be used to force conversion of matrix elements to a specific form.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "nConst     indicates the position of the constructor in the array m_mChildren, the child\n                   elements of the current matrix (i.e., the list of constructors)", "mtxArray   provides pointers to all defined matrices for use within transformations.", "rowMajor_  indicates whether the baseMatrix should be stored in row major (if true)\n                   or column major.", "convertTo_ can be used to force conversion of matrix elements to a specific form.\n-    *", "symmetry_  can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "mtxArray   provides pointers to all defined matrices for use within transformations.", "rowMajor_  indicates whether the blocks should be stored in row major (if true)\n                   or column major.", "convertTo_ is an optional parameter that can be used to covert the elements\n                   of all blocks to a different type", "symmetry_  can be used to store only the upper or lower triangle, depending\n                   on the parameter value --- see OSParameters.h for definitions.\n                   The default is \"none\".", "mtxArray  provides pointers to all defined matrices for use within transformations.", "rowOffset defines a partition of the matrix rows into the blocks", "rowOffsetSize gives the number of elements in the rowOffset array", "colOffset defines a partition of the matrix columns into the blocks", "colOffsetSize gives the number of elements in the colOffset array", "rowMajor_ controls whether the blocks are stored by row or by column", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "firstrow gives the first row of the block", "firstcol gives the first column of the block", "lastrow gives the last row of the block", "lastcol gives the last column of the block", "rowMajor_ can be used to store the objects in row major form.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "appendToBlockArray determines whether the blocks should be created if not found.", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor indicates whether the blocks are stored in row major form or not.", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "appendToBlockArray determines whether the blocks should be created if not found.", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor indicates whether the blocks are stored in row major form or not.", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "i is the number of the <blocks> constructor in the array of matrix constructors.", "appendToBlockArray determines whether the blocks should be created if not found.", "rowMajor indicates whether the blocks are stored in row major form or not.", "convertTo_ is an optional parameter that can be used to covert the elements\n        of all blocks to a different type.", "symmetry_ can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "rowPartition defines the partition of the set of rows into the blocks", "rowPartitionSize gives the size of the rowPartition array", "colPartition defines the partition of the set of columns into the blocks", "colPartitionSize gives the size of the colPartition array", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor_ indicates whether the blocks are stored in row major form or not.", "valueType_ indicates in which form to store the disassembled matrix\n        The default for this optional parameter is ENUM_MATRIX_TYPE_unknown", "symmetry_ determines what kind of symmetry to use in representing the blocks.\n        If this parameter is missing, the default value is NO symmetry", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n                   (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)\n                   if false, then side constraints are not enforced,\n                   which can be useful for parser stress and compliance tests", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "mtxConstructorVec holds a vector of pointers to matrix constructors,\nmtxConstructorVec and blocks in prefix format", "rowOffset defines a partition of the matrix rows into the blocks", "colOffset defines a partition of the matrix columns into the blocks", "rowMajor controls whether the blocks are stored by row or by column", "symmetry can be used to store only the upper or lower triangle, depending\n        on the parameter value --- see OSParameters.h for definitions", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor can be used to store the objects in row major form.", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "name holds the matrix name; use null or empty std::string (\"\") if no matrix name.", "numberOfRows holds the number of rows. It is required. Use 1 for column vectors.", "numberOfColumns holds the number of columns. It is required. Use 1 for row vectors.", "symmetry holds the type of symmetry used in the definition of the matrix.\n       For more information  see the enumeration ENUM_MATRIX_SYMMETRY in OSGeneral.h.\n       If no symmetry, use ENUM_MATRIX_SYMMETRY_unknown.", "declaredMatrixType tracks the type of elements contained in this matrix.\n       For more information  see the enumeration ENUM_MATRIX_TYPE in OSGeneral.h.\n       If unsure, use ENUM_MATRIX_TYPE_unknown.", "inumberOfChildren is the number of MatrixNode child elements,\n       i.e., the number of matrix constructors in the m_mChildren array.", "m_mChildren is the array of matrix constructors used in the definition of this matrix.", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that an objective reference in this matrix can take", "iMax: greatest index value (inclusive) that an objective reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that an objective reference in this matrix can take", "iMax: greatest index value (inclusive) that an objective reference can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a constraint reference in this matrix can take", "iMax: greatest index value (inclusive) that a constraint reference can take", "that: the instance from which information is to be copied", "firstRow gives the number of the first row in the submatrix (zero-based)", "firstColumn gives the number of the first column in the submatrix (zero-based)", "nRows gives the number of rows in the submatrix", "nColumns gives the number of columns in the submatrix", "mtxArray provides pointers to all defined matrices for use within transformations.", "rowMajor can be used to store the objects in row major form.", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted", "_values is the array  of matrix elements that are to be converted", "nvalues is the number of matrix elements that are to be converted"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSoLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSoLWriter.h", "filename": "OSoLWriter.h", "file": "OSoLWriter.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["theosoption is a pointer to an OSOption object"], "return": "a string with the OSOption data that validates against the OSoL schema.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSoLReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSoLReader.h", "filename": "OSoLReader.h", "file": "OSoLReader.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osol is a string that holds the solver options."], "return": "the instance as an OSOption object.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSExpressionTree.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSExpressionTree.h", "filename": "OSExpressionTree.h", "file": "OSExpressionTree.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2014, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "return": "the expression tree as a vector of ExprNodes in prefix.", "param": ["x holds the values of the variables in a double array.", "new_x is false if any evaluation method was previously called for the current x", "x holds the values of the variables in a double array.", "new_x is false if any evaluation method was previously called for the current x", "x holds the values of the variables in a double array.", "new_x is false if any evaluation method was previously called for the current x"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSResult.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSResult.h", "filename": "OSResult.h", "file": "OSResult.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nDalhousie University, Northwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "name: the name of this file or instance", "source: the source (e.g., in BiBTeX format)", "fileCreator: the creator of this file", "description: further description about this file and/or its contents", "licence: licence information if applicable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "i the number of the substatus (must be between 0 and numberOfSubstatuses)", "i the number of the substatus (must be between 0 and numberOfSubstatuses)", "i holds the number of the result whose name is saught.", "solIdx holds the solution index to get the solution status.", "solIdx holds the solution index to get the solution status type.", "solIdx holds the solution index to get the solution status description.", "solIdx holds the solution index to get the solution status description.", "solIdx holds the solution index to get the solution message.", "solIdx holds the solution index the optimal solution corresponds to.", "solIdx holds the solution index for the current solution", "object describes the kind of indices to be retrieved\n(legal values are described in ENUM_BASIS_STATUS --- see OSGeneral.h)", "status gives the basis status type", "solIdx holds the solution index for the current solution", "object describes the kind of indices to be retrieved\n(legal values are described in ENUM_BASIS_STATUS --- see OSGeneral.h)", "status gives the basis status (basic, atLower, atUpper, etc.)", "j is the (zero-based) position of the desired entry within the index array", "solIdx is the solution index", "object describes the kind of indices to be retrieved\n (legal values are described in ENUM_PROBLEM_COMPONENT --- see OSGeneral.h)", "resultArray is the array that returns the basis information", "dim is the dimension of the resultArray", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n @enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "enumIdx is the index of the current enumeration level", "j is the (zero-based) position of the index within the index array", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "resultArray is the array that returns the content of the <var> or <enumeration> array", "dim is the array dimension", "objIdx holds the objective index the optimal value corresponds to.", "solIdx holds the solution index the optimal value corresponds to.", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "j is the (zero-based) position of the index in the array", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "resultArray is the array that returns the content of the <obj> or <enumeration> array", "dim is the array dimension", "solIdx holds the solution index the optimal solution corresponds to.", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "solIdx is the solution index", "otherIndex is the index of the current <other> result\n@enumIdx is the index of the current enumeration level", "j is the (zeo-based) position of the entry in the array", "solIdx is the solution index", "otherIndex is the index of the current <other> result", "resultArray is the array that returns the content of the <con> or <enumeration> array", "dim is the array dimension", "status holds the general status.", "type holds the general status type", "num holds the number of substatuses (a nonegative integer)", "description holds the general status description.", "name holds the general substatus name", "idx holds the index of the substatus in the array", "description holds the general substatus description.", "idx holds the index of the substatus in the array", "message holds the general message.", "serviceName holds the name of the service.", "serviceURI holds the uri of the service.", "instanceName holds the name of the instance.", "jobID holds the job id.", "solverInvoked holds the solver invoked.", "time holds the time stamp.", "num holds the number of other general results.", "name holds the general otherResult name", "idx holds the index of the otherResult in the array", "value holds the general otherResult value", "idx holds the index of the otherResult in the array", "description holds the general otherResult description", "idx holds the index of the otherResult in the array", "idx holds the index of the otherResult in the array", "name holds the general otherResult description", "value holds the general otherResult value", "description holds the general otherResult description", "systemInformation holds the system information", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte, petabyte)", "description holds further information about available disk space", "value holds the number of disk space units", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte)", "description holds further information about available memory", "value holds the number of memory units", "unit holds unit", "description holds further information about the CPU speed", "value holds the available CPU speed", "description is used to impart further info about the CPUs", "value holds the available number of CPUs", "num holds the number of other system results.", "name holds the system otherResult name", "idx holds the index of the otherResult in the array", "name holds the system otherResult value", "idx holds the index of the otherResult in the array", "name holds the system otherResult description", "idx holds the index of the otherResult in the array", "currentState holds the current state", "jobCount holds the current job count", "number holds the total number of jobs", "startTime holds the starting time", "value holds the service utilitzation", "num holds the number of other service results.", "name holds the service otherResult name", "idx holds the index of the otherResult in the array", "name holds the service otherResult value", "idx holds the index of the otherResult in the array", "name holds the service otherResult description", "idx holds the index of the otherResult in the array", "status holds the job status", "submitTime holds the time when the job was submitted", "scheduledStartTime holds the scheduled start time", "actualStartTime holds the actual start time", "endTime holds the time when the job finished", "time holds the time.", "type holds the timer type (cpuTime/elapsedTime/other).", "category holds the timer category (total/input/preprocessing, etc.)", "unit holds the timer unit (tick/milliscond/second/minute/etc.)", "description holds further information about the timer.", "value holds the time measurement.", "idx holds the index within the time array of the item to be set", "type holds the timer type (cpuTime/elapsedTime/other).", "category holds the timer category (total/input/preprocessing, etc.)", "unit holds the timer unit (tick/milliscond/second/minute/etc.)", "description holds further information about the timer.", "value holds the time measurement.", "numberOfTimes holds the number of measurements", "timeNumber holds the number of measurements", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte, petabyte)", "description holds further information about used disk space", "value holds the number of disk space units", "unit holds unit (byte, kilobyte, megabtye, gigabyte, terabyte)", "description holds further information about used memory", "value holds the number of memory units", "unit holds unit", "description holds further information about the CPU speed", "value holds the used CPU speed", "description is used to impart further info about the CPUs", "value holds the used number of CPUs", "num holds the number of other job results.", "name holds the job otherResult name", "idx holds the index of the otherResult in the array", "name holds the job otherResult value", "idx holds the index of the otherResult in the array", "name holds the job otherResult description", "idx holds the index of the otherResult in the array", "variableNumber holds the number of variables", "objectiveNumber holds the number of objectives", "constraintNumber holds the number of constraints", "number holds the number of solutions to set.", "solIdx holds the solution index to set the solution status.", "status holds the optimization solution status to set.", "solIdx holds the solution index whose status to set.", "type holds the solution status type", "solIdx holds the solution index whose status to set.", "num holds the number of substatuses (a nonegative integer)", "solIdx holds the solution index whose status to set.", "description holds the solution status description.", "solIdx holds the solution index whose status to set.", "substatusIdx holds the index of the substatus in the array", "type holds the general substatus type", "solIdx holds the solution index whose status to set.", "substatusIdx holds the index of the substatus in the array", "description holds the general substatus description.", "solIdx holds the solution index to set the objective index.", "objectiveIdx holds the objective index to set.\nAll the objective indexes are negative starting from -1 downward.", "solIdx holds the solution index to set the objective index.", "objectiveName holds the objective indexname to set.", "solIdx holds the solution index to set the objective index.", "weightedObjectives holds the value \"true\" or \"false\".", "solIdx holds the solution index to set the objective index.", "msg holds the solution message to set.", "solIdx holds the solution index to set the primal variable values.", "n holds the number of elements in the array x", "solIdx holds the solution index to set the primal variable values.", "x holds a vector of type IndexValuePair; the idx component holds the index of the variable;\nthe value component holds its value. The vector could be null if all variables are 0.", "solIdx holds the solution index to set the primal variable values.", "x holds a double dense array of variable values to set; it could be null if all variables are 0.", "solIdx holds the solution index to set the primal variable values.", "numberOfVar holds the number of primal variables that are to be set", "solIdx holds the solution index to set the primal variable values.", "number holds the location within the sparse array var that is to be used", "idx holds the index of the primal variable that is to be set", "name holds the variable name (or an empty string).", "val holds the variable value to set.", "solIdx holds the solution index to set the primal variable values.", "numberOfVar holds the number of primal variables that are to be set", "solIdx holds the solution index to set the primal variable values.", "number holds the location within the sparse array var that is to be used", "idx holds the index of the primal variable that is to be set", "name holds the variable name (or an empty string).", "str holds the variable value to set.", "solIdx holds the index of the solution to which the basis values belong.", "object holds the type of basis object to be used\n    (legal values are taken from the ENUM_PROBLEM_COMPONENT enumeration --- see OSGeneral.h))", "status holds the status which is to be used\n    (legal values are taken from the ENUM_BASIS_STATUS enumeration --- see OSGeneral.h)", "i holds the integer array whose values are to be transferred.\n\n    (NOTE WELL: This method does not handle individual variables --- the entire basis must be processed at once.)", "ni holds the number of elements of i", "solIdx is the solution index", "numberOfOtherVariableResult holds the number of OtherVariableResult objects\nEach other variable result contains the name (required), an optional description (std::string) and an optional\nvalue (std::string). Each other variable result can also optionally contain an array OtherVarResult for each variable.\nThe OtherVarResult contains a variable idx (required) and an optional std::string value.", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "idx holds a pointer to the indexes of the var element", "s holds a pointer to the array of values of the var element", "n holds the number of elements of the array", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "idx holds a pointer to the indexes of the var element", "s holds a pointer to the array of values of the var element", "n holds the number of elements of the array", "type holds the type of the <other> element's value attribute", "varType holds the type of the <other> element's <var> array", "enumType holds the type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "s holds a pointer to the array of values of the var element", "solIdx holds the solution index", "otherIdx holds the index of the new OtherVariableResult object", "name holds the name of the other element", "value holds the value of the other element", "s holds a pointer to the array of values of the var element", "type holds the type of the <other> element's value attribute", "varType holds the type of the <other> element's <var> array", "enumType holds the type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "numberOfVar holds the number of <var> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "numberOfEnumerations holds the number of <enumeration> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "name holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "type holds the type of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varType holds the data type of the <var> array of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "enumType holds the data type of the <enumeration> array of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "value holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "description holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "solver holds the type of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "category holds the type of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varIdx holds the index of the location to which the information is stored", "idx holds the index of the variable to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varIdx holds the index of the location to which the information is stored", "name holds the name of the variable to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "varIdx holds the index of the location to which the information is stored", "value holds the value of the variable to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherVariableResult object", "object holds the object to which this enumeration pertains\n    (legal values are taken from the ENUM_PROBLEM_COMPONENT enumeration --- see OSGeneral.h))", "enumIdx holds the index of the OtherOptionOrResultEnumeration object", "value holds the value of this result", "description holds a description of this result", "i holds the indices of the variables that take on this value", "ni holds the dimension of the index vector i", "solIdx is the solution index", "numberOfOtherObjectiveResult holds the number of OtherObjectiveResult objects\nEach other objective result contains the name (required), an optional description (std::string) and an optional\nvalue (std::string). Each other objective result can also optionally contain an array OtherObjResult for each objective.\nThe OtherObjResult contains an objective idx (required) and an optional std::string value.", "solIdx holds the solution index to set the objective values.", "numberOfObj holds the number of objectives that are to be set", "solIdx holds the solution index to set the constraint values.", "n holds the number of elements in the array x", "solIdx holds the solution index to set the objective values.", "x holds a vector of type IndexValuePair; the idx component holds the index of the objective;\nthe value component holds its value. The vector could be null if all objectives are 0.\nPossibly only the objective that the solution is based on has the value, and the rest of the objective\nvalues all get a Double.NaN value, meaning that they are not calculated.", "solIdx holds the solution index to set the objective values.", "objectiveValues holds the double sparse array of objective values to set.\nPossibly only the objective that the solution is based on has the value, and the rest of the objective\nvalues all get a Double.NaN value, meaning that they are not calculated.", "solIdx holds the solution index to set the objective values.", "number holds the location within the sparse array obj that is to be used", "idx holds the index of the objective that is to be set", "name holds the objective name (or an empty string).", "val holds the objective value to set.", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "numberOfObj holds the number of <obj> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "numberOfObj holds the number of <obj> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the type of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the data type of the <other> element's <var> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "name holds the data type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "value holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "description holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "solver holds the solver of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "category holds the category of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "objIdx holds the index of the location to which the information is stored", "idx holds the index of the objective to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "objIdx holds the index of the location to which the information is stored", "name holds the name of the objective to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherObjectiveResult object", "objIdx holds the index of the location to which the information is stored", "value holds the value of the objective to which the information belongs", "solIdx is the solution index", "numberOfOtherConstraintResults holds the number of OtherConstraintResult objects\nEach other objective result contains the name (required), an optional description (std::string) and an optional\nvalue (std::string). Each other constraint result can also optionally contain an array OtherConResult for each constraint.\nThe OtherConResult contains a constraint idx (required) and an optional std::string value.", "solIdx holds the solution index to set the constraint values.", "numberOfCon holds the number of constraint that are to be set", "solIdx holds the solution index to set the dual variable values.", "n holds the number of elements in the array x", "solIdx holds the solution index to set the dual variable values.", "x holds a vector of type IndexValuePair; the idx component holds the index of the constraint;\nthe value component holds its value. The vector could be null if all dual variables are 0.", "solIdx holds the solution index to set the dual variable values.", "y holds a double dense array of variable dual values; it could be NULL if all values are 0.", "solIdx holds the solution index to set the constraint values.", "constraintValues holds the a double dense array of constraint values to set; it could be null if all constraint values are 0.", "solIdx holds the solution index to set the constraint values.", "number holds the location within the sparse array con that is to be used", "idx holds the index of the constraint that is to be set", "name holds the constraint name (or an empty string).", "val holds the constraint value to set.", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "numberOfCon holds the number of <con> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "numberOfCon holds the number of <con> children", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the type of the <other> element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the type of the <other> element's <con> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "name holds the type of the <other> element's <enumeration> array", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "value holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "description holds the name of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "solver holds the solver of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "category holds the category of the other element", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "conIdx holds the index of the location to which the information is stored", "idx holds the index of the onstraint to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "conIdx holds the index of the location to which the information is stored", "name holds the name of the constraint to which the information belongs", "solIdx holds the solution index", "otherIdx holds the index of the OtherConstraintResult object", "conIdx holds the index of the location to which the information is stored", "value holds the value of the constraint to which the information belongs", "solIdx is the solution index", "numberOfMatrixVar_ holds the number of matrixVar elements\n       for which values are to be provided", "numberOfOtherMatrixVariableResults_ holds the number of <other> elements\n       for which values are to be provided", "solIdx is the solution index", "idx holds the index of the matrixVar (in the MatrixVariableValues array)", "matrixVarIdx holds the index of the matrixVar (as defined in the OS instance)", "numberOfRows holds the number of rows in the matrixVar", "numberOfColumns holds the number of columns in the matrixVar", "symmetry (optional) holds the type of symmetry\n       (if not present, the default value is \"none\"", "type (optional) holds the type of values in the nonzeroes of the matrix\n       (if not present, the default value is \"unknown\")", "name (optional) holds the name of this matrixVar (the default is the empty string)\n       for which values are to be provided", "solIdx is the solution index", "idx holds the index of the matrixVar for which values are to be provided\n       (as derived from the <values> element in matrixProgramming", "colOffset is an array of column offsets that define\n       the partition of the columns within the block structure", "colOffsetSize gives the size of the colOffset array", "rowOffset is an array of row offsets that define\n       the partition of the rows within the block structure", "rowOffsetSize gives the size of the rowOffset array", "numberOfBlocks gives the number of blocks", "blocksConstructorIdx gives the index of the MatrixBlocks node\n       in the array of constructors of the matrixVar. The default is 0.", "solIdx is the solution index", "idx holds the index of the matrixVar for which values are to be provided\n       (as derived from the <values> element in matrixProgramming", "blkno is the number of the block for which elements are provided\n       the partition of the columns within the block structure", "blkRowIdx gives the index of the block row in which the block is located", "blkColIdx gives the index of the block column in which the block is located", "nz gives the number of (nonzero) values", "start gives the start elements (column or row starts, depending on\n       whether rowMajor is false or true)", "index gives the array of row or column (depending on rowMajor) indices", "value gives the data structure for (nonzero) values that need to be stored", "valueType gives the type of values (see OSParameters.h)", "symmetry gives the form of symmetry. (The default is NONE.)", "rowMajor indicates whther the elements are stored column by column (if rowMajor is false)\n       or row by row (if rowMajor is true). The default is rowMajor = false.\n\n@remark each block object can handle only one type of elements,\n        although different blocks may contain different types of values", "solIdx is the solution index", "idx holds the index of the other result (in the array of <other> solutions)", "name holds the name of the <other> result", "description can be used to hold a further description of the result", "value holds a scalar value associated with the <other> result", "type describes the type of value represented by the scalar result", "solver gives the solver with which this result is associated", "category can be used to specify a further category within the solver", "numberOfMatrixVar gives the number of matrixVar elements associated with this result.\n       This argument is optional and defaults to 0.", "matrixType can be used to associate a type with the values of the matrixVar elements.\n       This argument is optional and defaults to the empty string (\"\").", "numberOfEnumerations gives the number of levels associated with an enumeration of matrixVar\n       elements pertaining to this result. The argument is optional and defaults to 0.", "enumType can be used to associate a type with the values of the enumeration.\n       This argument is optional and defaults to the empty string (\"\").", "solIdx is the solution index", "otherIdx holds the index of the other matrix variables result", "matrixVarIdx holds the index of the matrixVar (as defined in the OS instance)", "numberOfRows holds the number of rows in the matrixVar", "numberOfColumns holds the number of columns in the matrixVar", "symmetry (optional) holds the type of symmetry\n       (if not present, the default value is \"none\"", "type (optional) holds the type of values in the nonzeroes of the matrix\n       (if not present, the default value is \"unknown\")", "name (optional) holds the name of this matrixVar (the default is the empty string)\n       for which values are to be provided", "solIdx is the solution index", "otherIdx holds the index of the other solution", "matrixVarIdx holds the index of the matrixVar for which values are to be provided", "colOffset is an array of column offsets that define\n       the partition of the columns within the block structure", "colOffsetSize gives the size of the colOffset array", "rowOffset is an array of row offsets that define\n       the partition of the rows within the block structure", "rowOffsetSize gives the size of the rowOffset array", "numberOfBlocks gives the number of blocks", "blocksConstructorIdx gives the index of the MatrixBlocks node\n       in the array of constructors of the matrixVar. The default is 0.", "solIdx is the solution index", "otherIdx holds the index of the other solution", "matrixVarIdx holds the index of the matrixVar for which values are to be provided", "blkno is the number of the block for which elements are provided\n       the partition of the columns within the block structure", "blkRowIdx gives the index of the block row in which the block is located", "blkColIdx gives the index of the block column in which the block is located", "nz gives the number of (nonzero) values", "start gives the start elements (column or row starts, depending on\n       whether rowMajor is false or true)", "index gives the array of row or column (depending on rowMajor) indices", "value gives the data structure for (nonzero) values that need to be stored", "valueType gives the type of values (see OSParameters.h)", "symmetry gives the form of symmetry. (The default is NONE.)", "rowMajor indicates whther the elements are stored column by column (if rowMajor is false)\n       or row by row (if rowMajor is true). The default is rowMajor = false.\n\n@remark each block object can handle only one type of elements,\n        although different blocks may contain different types of values", "solIdx is the solution index", "numberOfOtherSolutionResults holds the number of OtherSolutionResult objects\nEach other objective result contains the name (required), an optional\ndescription (std::string) and an optional category (std::string).\nEach other solution result can also optionally contain an array Item for each result.\nThe Item content is string-valued.", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "name holds the name of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "value holds the value of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "category holds the category of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "category holds the description of the otherSolutionResult", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "numberOfItems holds the number of items", "solIdx holds the solution index to set the constraint values.", "otherIdx holds the index of the otherSolutionResult", "itemIdx holds the index of the item", "item holds the value of the item", "solIdx holds the solution index i.", "name holds the name of the other solution result", "value holds the value of the other solution result", "category holds the category of the result", "description holds a description of the result", "numberOfItems holds the number of items", "item holds a pointer to the array of items (can be NULL if numberOfItems is 0)", "numberOfOtherSolverOutputs holds the number of SolverOutput objects\nEach solver output can also optionally contain an array Item for each result.\nThe Item content is string-valued.", "otherIdx holds the index of the solverOutput object", "name holds the name of the solver output", "otherIdx holds the index of the solverOutput object", "name holds the category of the solver output", "otherIdx holds the index of the solverOutput object", "name holds the description of the solver output", "otherIdx holds the index of the solverOutput object", "numberOfItems holds the number of items", "otherIdx holds the index of the otherSolutionResult", "itemIdx holds the index of the item", "item holds the value of the item"], "return": "whether the copy was created successfully", "see": ["org.optimizationservices.oscommon.datastructure.osresult.OptimizationSolutionStatus", "#setVariableNumber(int)", "#setObjectiveNumber(int)", "#setConstraintNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OptimizationSolutionStatus", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherVariableResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherVarResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjectiveResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherObjResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherConstraintResult", "org.optimizationservices.oscommon.datastructure.osresult.OtherConResult", "#setSolutionNumber(int)", "org.optimizationservices.oscommon.datastructure.osresult.OtherSolutionResult", "org.optimizationservices.oscommon.datastructure.osresult.Item", "#setSolutionNumber(int)"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSOption.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSOption.h", "filename": "OSOption.h", "file": "OSOption.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "name: the name of the <other> element to be added (required)", "value: the value of the <other> element to be added (optional)", "description: a description of the <other> element (optional)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfJobIDs: number of <jobID> elements to be set", "jobID: the array of <jobID> elements that are to be set", "jobID: the name of the <jobID> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfPaths: number of <path> elements to be set", "path: the array of <path> elements that are to be set", "path: the path to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfPathPairs: number of <pathPair> elements to be set", "path: the array of <pathPair> elements that are to be set", "from: array containing a list of objects to be moved", "to: array containing a list of destinations", "makeCopy: records whether each object is to be moved or copied", "numberOfPathPairs: number of <pathPair> elements to be set", "fromPath: the path from which to copy or move", "toPath: the path to which to copy or move", "makecopy: tracks whether a copy is to be made", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfProcesses: number of <process> elements to be set", "path: the array of <process> elements that are to be set", "process: the ID of the process to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial string value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements to be that are to be set", "idx: the index of the variable to be given an initial basis status", "value: the initial basis status to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements to be that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given a branching weight", "value: the branching weight to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable to be given a branching weight", "value: the branching weight to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfSOS: number of <sos> elements to be set", "sos: the array of <sos> elements that are to be set", "sosIdx: the index of the SOS that is to be added (refer back to OSiL file)", "nvar: the number of variables in this SOS that are to be given weights", "weight: a selection weight for the entire group of variables", "idx: an array of variable indices", "value: the array of corresponding selection weights", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfObj: number of <obj> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of objective names", "idx: the index of the objective to be given an initial value", "value: the initial value to be added", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfObj: number of <obj> elements to be set", "idx: the array of indices", "lbValue: the array of corresponding lower bounds", "ubValue: the array of corresponding upper bounds", "name: the array of objective names", "idx: the index of the objective to be given initial bounds", "lbValue: the initial lower bound for the objective", "ubValue: the initial upper bound for the objective", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "idx: the index of the objective", "value: the value associated with this objective", "lbValue: a lower bound associated with this objective", "ubValue: an upper bound associated with this objective", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "con: the array of <con> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfCon: number of <con> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of constraint names", "idx: the index of the constraint to be given an initial value", "value: the initial value to be added", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "con: the array of <con> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfCon: number of <con> elements to be set", "idx: the array of indices", "lbValue: the array of dual values for the lower bound", "ubValue: the array of dual values for the upper bound", "name: the array of constraint names", "idx: the index of the constraint to be given initial dual variables", "lbDualValue: an initial value for the dual variable associated with the lower bound", "ubDualValue: an initial value for the dual variable associated with the upper bound", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <matrixVar> elements to be set", "var: the array of <matrixVar> elements that are to be set", "numberOfVar: number of <matrixVar> elements to be set", "var: the array of <matrixVar> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <matrixVar> elements to be set", "matrixVar: the array of <matrixVar> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "disp: method of disposition if previous data exist", "numberOfVar: number of <var> elements to be set", "idx: the array of indices", "value: the array of corresponding values", "name: the array of corresponding names", "idx: the index of the variable to be given an initial value", "value: the initial variable value to be added", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of <other> elements to be set", "other: the array of <other> elements that are to be set", "other: the content of the <other> element to be added", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfOptions: number of solver options to be set", "solverOption: the array of solver options that are to be set", "name: the name of the solver option (required)", "value: a value associated with the option (optional)", "solver: the solver to which the option applies (optional)", "category: the category (and subcategories) of the option (optional)", "type: the type of the option (optional)", "description: a description associated with the option (optional)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "name: the name of this file or instance", "source: the source (e.g., in BiBTeX format)", "fileCreator: the creator of this file", "description: further description about this file and/or its contents", "licence: licence information if applicable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the OSOption object from which information is to be copied", "iSOS the number of the SOS", "numberOfVariables holds the dimension of the vector", "numberOfVariables holds the dimension of the vector", "numberOfVariables is the dimension of the array", "type: the type of variable or problem component\n\t(contained in ENUM_PROBLEM_COMPONENT --- see OSGeneral.h)", "status: the basis status\n\t(contained in ENUM_BASIS_STATUS --- see OSGeneral.h)", "type: the type of variable or problem component\n\t(contained in ENUM_PROBLEM_COMPONENT --- see OSGeneral.h)", "status: the basis status\n\t(contained in ENUM_BASIS_STATUS --- see OSGeneral.h)", "elem: pointer to the memory location where the user wants to store the returned values", "numberOfVariables holds the dimension of the vector", "solver_name is the name of the solver whose options we want", "optionNumber is the index of the option in the array", "numberOfObjectives holds the dimension of the vector", "numberOfObjectives holds the dimension of the vector", "numberOfObjectives holds the dimension of the vector", "numberOfObjectives is the dimension of the array", "solver_name is the name of the solver whose options we want", "optionNumber is the index of the option in the array", "numberOfConstraints holds the dimension of the vector", "numberOfConstraints holds the dimension of the vector", "numberOfConstraints holds the dimension of the vector", "numberOfConstraints is the dimension of the array", "solver_name is the name of the solver whose options we want", "optionNumber is the index of the option in the array", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxArray is a pointer to the <matrices> array, which may be needed\n         for the block expansion", "rowPartition is an array of integer values describing the desired row partition", "rowPartitionSize gives the size of the rowPartition array", "colPartition is an array of integer values describing the desired column partition", "colPartitionSize gives the size of the colPartition array", "mtxVarIdx is the index of the matrix variable to be retrieved", "mtxArray is a pointer to the <matrices> array, which may be needed\n         for the block expansion", "rowPartition is an array of integer values describing the desired row partition", "rowPartitionSize gives the size of the rowPartition array", "colPartition is an array of integer values describing the desired column partition", "colPartitionSize gives the size of the colPartition array", "solver_name is the name of the solver whose options we want", "solver_name is the name of the solver whose options we want", "getFreeOptions is a boolean set to true if the free\n       options (not associated with a solver name) should be returned", "numberOfOptions contains the number of other options to be set", "other is a pointer to an array of OtherOption objects", "name - the identifying anme of the option. This string cannot be empty", "value - optional value associated with this option", "description - further information (can be used for documentation)", "unit - select the unit (Kb, Mb, etc.)", "description - further description (can be used for documentation)", "value - number of units of disk space required", "unit - select the unit (Kb, Mb, etc.)", "description - further description (can be used for documentation)", "value - number of units of memory size required", "unit - select the unit (MHz, GHz, TFlops etc.)", "description - further description (can be used for documentation)", "value - number of units of CPU speed required", "number - number of CPU cores required", "description - further description (can be used for documentation)", "object describes the type of pathpairs\n      legal values are\n          ENUM_PATHPAIR_input_dir,\n          ENUM_PATHPAIR_input_file,\n          ENUM_PATHPAIR_output_file,\n          ENUM_PATHPAIR_output_dir", "from is a pointer to an array of strings containing the\n          location of the original object", "to   is a pointer to an array of strings containing the\n          location of the destination object", "makeCopy is a pointer to an array of boolean, describing\n          for each object whether it is to be copied or moved", "numberOfPathPairs is an integer giving the number of PathPairs\n          this must equal the number of entries in the from, to and makeCopy arrays", "type: type of this element (see ENUM_PROBLEM_COMPONENT - OSGeneral.h)", "idx: index of this element (nonnegative for variable or constraint,\n\tnegative for objective)", "status: basis status (see ENUM_BASIS_STATUS - OSGeneral.h)", "iOther: position of this element in the array of <other>", "numberOfVar: number of <var> children contained in this <other> element", "numberOfEnumerations: number of <enumeration> children", "name: name of this <other> element", "value: a value associated with this <other> element", "solver: the solver associated with this <other> element", "category: the category of this <other> element", "type: type of this <other> element", "varType: type of the data in the <var> array", "enumType: type of the data in the <enumeration> array", "description: further description of this <other> element", "object: the object into which the enumeration is to be stored\n\t(legal values see ENUM_PROBLEM_COMPONENT in OSGeneral.h)", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "enumerationNumber: number of the <enumeration> in the list of enumerations (zero-based)", "numberOfEl: number of objects sharing the value of this enumeration", "value: value of the enumeration (as a string)", "description: further information about the enumeration and its value", "idxArray: the array of indices for the objects sharing this enumeration", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "varNumber: number of the <var> in the array (zero-based)", "idx: index of the variable to which this value belongs", "value: value of the option (as a string)", "lbValue: value associated with the lower bound of the variable (as a string)", "ubValue: value associated with the upper bound of the variable (as a string)", "iOther: position of this element in the array of <other>", "numberOfObj: number of <obj> children contained in this <other> element", "numberOfEnumerations: number of <enumeration> children", "name: name of this <other> element", "value: a value associated with this <other> element", "solver: the solver associated with this <other> element", "category: the category of this <other> element", "type: type of this <other> element", "objType: type of the data in the <var> array", "enumType: type of the data in the <enumeration> array", "description: further description of this <other> element", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "objNumber: number of the <obj> in the array (zero-based)", "idx: index of the objective to which this value belongs", "name: name of the objective", "value: value of the option (as a string)", "lbValue: value associated with the lower bound of the objective (as a string)", "ubValue: value associated with the upper bound of the objective (as a string)", "iOther: position of this element in the array of <other>", "numberOfCon: number of <con> children contained in this <other> element", "numberOfEnumerations: number of <enumeration> children", "name: name of this <other> element", "value: a value associated with this <other> element", "solver: the solver associated with this <other> element", "category: the category of this <other> element", "type: type of this <other> element", "conType: type of the data in the <var> array", "enumType: type of the data in the <enumeration> array", "description: further description of this <other> element", "otherOptionNumber: number of the <other> option in the list of <other> options (zero-based)", "conNumber: number of the <obj> in the array (zero-based)", "idx: index of the constraint to which this value belongs", "name: name of the constraint", "value: value of the option (as a string)", "lbValue: value associated with the lower bound of the constraint (as a string)", "ubValue: value associated with the upper bound of the constraint (as a string)", "iOption: position of this element in the array of options", "numberOfMatricess: number of <matrix> children contained in this <other> element", "numberOfItems: number of <item> children contained in this <other> element", "name: name of this solver option", "value: a value associated with this option", "solver: the solver to which this option applies", "category: the category of this option (solver specific)", "type: type of this option (e.g., numeric or string)", "description: further description of this option", "matrix: an array of matrices associated with this option (could be NULL)", "itemList: the list of items associated with this option (could be NULL)"], "return": "whether the copy was created successfully", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSGeneral.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSGeneral.h", "filename": "OSGeneral.h", "file": "OSGeneral.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\nThis file contains the following classes that correspond to ComplexTypes in the OSgL schema:\n     GeneralFileHeader\n     SparseVector\n     SparseIntVector\n     SparseMatrix\n     SparseJacobianMatrix\n     SparseHessianMatrix\n     QuadraticTerms\n     IntVector\n     DoubleVector\n     IndexValuePair\n     BasisStatus\n     StorageCapacity\n     CPUSpeed\n     CPUNumber\n     TimeSpan\n     OtherOptionOrResultElementString\n     OtherObjOptionOrResultElementString\n     OtherOptionOrResultEnumeration : public IntVector\n     OtherVariableOptionOrResult\n     OtherObjectiveOptionOrResult\n     OtherConstraintOptionOrResult\n     OtherSOSOptionOrResult\n     OtherSpecialOrderedSetsOptionOrResult\n     OtherMatrixVariableOptionOrResult\n     OtherMatrixObjectiveOptionOrResult\n     OtherMatrixConstraintOptionOrResult\n     OtherOptionOrResult\n     SolverOptionOrResult", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "item: the type of information sought (name, source, description, fileCreator, licence)", "name: the name of this file or instance", "source: the source (e.g., in BiBTeX format)", "description: further description about this file and/or its contents", "fileCreator: the creator of this file", "licence: licence information if applicable", "number holds the size of the vector.", "number holds the size of the vector.", "isColumnMajor holds whether the coefMatrix (AMatrix) holding linear program\ndata is stored by column. If false, the matrix is stored by row.", "startSize holds the size of the start array.", "valueSize holds the size of the value and index arrays.", "startSize holds the size of the start array.", "valueSize holds the size of the value and index arrays.", "startSize holds the size of the arrays.", "valueSize holds the size of the value and index arrays.", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest value (inclusive) that an entry in this vector can take", "iMax: greatest value (inclusive) that an entry in this vector can take", "that: the instance from which information is to be copied", "ni contains the dimension of the IntVector", "i contains the array of values", "i contains the value to be appended", "j is the index of the entry that is to be retrieved", "i is the location where the user wants to store the array", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that an entry in this basis can take", "iMax: greatest index value (inclusive) that an entry in this basis can take", "that: the instance from which information is to be copied", "status is a string representing the allowed statuses\n       (as defined in enumeration ENUM_BASIS_STATUS - see below)", "i contains the array of indices", "ni contains the number of elements in i", "status is a string representing the allowed statuses\n       (as defined in enumeration ENUM_BASIS_STATUS - see below)", "idx contains the value of the index", "status is a string representing the allowed statuses\n (at present \"basic\", \"atLower\", \"atUpper\", \"isFree\", \"superbasic\", \"unknown\")", "status is an integer representing the allowed statuses\n (as governed by enumeration ENUM_BASIS_STATUS --- see below)", "j is the (zero-based) position of the entry within the array", "status is a string representing the allowed statuses\n (as governed by enumeration ENUM_BASIS_STATUS --- see below)", "i is the location where the user wants to store the array", "resultArray is the location where the user wants to store the array", "dim is the size of the resultArray", "flipIdx indicates whether the index values need to be flipped\n  (used for representations of objective rows)", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest value (inclusive) that an entry in this vector can take", "iMax: greatest value (inclusive) that an entry in this vector can take", "that: the instance from which information is to be copied", "value represents the value of this enumeration member", "description holds additional information about this value", "i contains the array of indices", "ni contains the number of elements in i", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfObj: number of <obj> elements to be set", "obj: the array of <obj> elements that are to be set", "idx: the index of the objective", "value: the value associated with this objective", "lbValue: a lower bound associated with this objective", "ubValue: an upper bound associated with this objective", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfCon: number of <con> elements to be set", "obj: the array of <con> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfSOS: number of <sos> elements to be set", "obj: the array of <sos> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfSOS: number of <sos> elements to be set", "obj: the array of <sos> elements that are to be set", "idx: the index of the constraint", "value: the value associated with this constraint", "lbValue: a lower bound associated with this constraint", "ubValue: an upper bound associated with this constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <var> elements to be set", "var: the array of <var> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <matrixObj> elements to be set", "matrixObj: the array of <matrixObj> elements that are to be set", "idx: the index of the variable", "value: the value associated with this variable", "lbValue: a lower bound associated with this variable", "ubValue: an upper bound associated with this variable", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "numberOfVar: number of <matrixCon> elements to be set", "matrixCon: the array of <matrixCon> elements that are to be set", "idx: the index of the matrix constraint", "value: the value associated with this matrix constraint", "lbValue: a lower bound associated with this matrix constraint", "ubValue: an upper bound associated with this matrix constraint", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "that: the instance from which information is to be copied"], "return": "whether the copy was created successfully", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSInstance.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSInstance.h", "filename": "OSInstance.h", "file": "OSInstance.h\n\\brief This file defines the OSInstance class along with its supporting classes.", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2012, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.\n\n<p> 1. Elements become objects of class type (the ComplexType is the class) </p>\n\n<p> 2. The attributes, children of the element, and text correspond to members of the class.  </p>\n      (Note text does not have a name and becomes .value)\n\n<p> 3. Model groups such as choice and sequence and all correspond to arrays </p>\n\n<p><b>Exceptions:</b> </p>\n<ol>\n<li> anything specific to XML such as base64, multi, incr does not go into classes </li>\n<li> The root OSnLNode of each <nl> element is called ExpressionTree </li>\n<li> Root is not called osil; it is called osinstance </li>\n</ol>", "return": "the type of cone as a string", "param": ["density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "density: corresponds to the probability that a particular child element is created", "conformant: if true enforces side constraints not enforceable in the schema\n    (e.g., agreement of \"numberOfXXX\" attributes and <XXX> children)", "iMin: lowest index value (inclusive) that a variable reference in this matrix can take", "iMax: greatest index value (inclusive) that a variable reference in this matrix can take", "that: the instance from which information is to be copied", "rowIdx is the index of the row we want to express in infix.", "n is the index number associated with the matrix.\n\n@remark only the most general element type is returned.\n(e.g., if matrix contains both constants and general expressions,\nmatrix type is ENUM_MATRIX_TYPE_general\n@remark for possible types see OSParameters.h", "n is the index number associated with the matrix.\n\n@remark for possible symmetry types see OSParameters.h", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "rowMajor determines whether the elements should be\n       in row major form or colum major form. The default is column major form.", "convertTo_ controls whether elements should be converted from one type to another", "symmetry_  controls whether a particular type of symmetry should be enforced\n                  The default value does not change the symmetry", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "n is the index number associated with the matrix.", "columnIdx is the column index of the block's location", "rowIdx is the row index of the block's location", "rowIdx is the index of the row we want to express in infix.", "name holds the instance name.", "source holds the instance source.", "description holds the instance description.", "fileCreator holds the instance creator.", "licence holds the instance licence.", "number holds the number of variables.", "index holds the variable index. It is required.", "name holds the variable name; use null or empty std::string (\"\") if no variable name.", "lowerBound holds the variable lower bound; use -OSDBL_MAX if no lower bound.", "upperBound holds the variable upper bound; use  OSDBL_MAX if no upper bound.", "type holds the variable type character: C for Continuous, B for Binary, I for Integer,\n       S for String, D for semi-continuous, J for semi-integer (i.e., either 0 or integer >=n).", "number holds the number of variables. It is required.", "names holds a std::string array of variable names; use null if no variable names.", "lowerBounds holds a double array of variable lower bounds; use null if all lower bounds are 0;\n use -OSDBL_MAX if no lower bound for a specific variable in the array.", "upperBounds holds a double array of variable upper bounds; use null if no upper bounds;\n use OSDBL_MAX if no upper bound for a specific variable in the array.", "types holds a char array of variable types; use null if all variables are continuous;\n       for a specfic variable in the array use C for Continuous, B for Binary, I for Integer,\n       S for String, D for semi-continuous, J for semi-integer (i.e., either 0 or integer >=n).", "inits holds a double array of variable initial values; use null if no initial values. -- deprecated", "initsString holds a std::string array of varible initial values; use null\n       if no initial std::string values.  -- deprecated", "number holds the number of objectives.", "index holds the objective index. Remember the first objective index is -1, second -2, ...", "name holds the objective name; use null or empty std::string (\"\") if no objective name.", "maxOrMin holds the objective sense or direction; it can only take two values: \"max\" or \"min\".", "constant holds the objective constant; use 0.0 if no objective constant.", "weight holds the objective weight; use 1.0 if no objective weight.", "objectiveCoefficients holds the objective coefficients (null if no objective coefficients)\n       in a sparse representation that holds two arrays: index array and a value array.", "number holds the number of objectives. It is required.", "names holds a std::string array of objective names; use null if no objective names.", "maxOrMins holds a std::string array of objective objective senses or directions: \"max\" or \"min\"; use null if all objectives are \"min\".", "constants holds a double array of objective constants; use null if all objective constants are 0.0.", "weights holds a double array of objective weights; use null if all objective weights are 1.0.", "objectiveCoefficients holds an array of objective coefficients, (null if no objective has any coefficients)\nFor each objective, the coefficients are stored in a sparse representation that holds two arrays: index array and a value array.\nIf for a specific objective, there are no objective coefficients, use null for the corresponding array member.", "number holds the number of constraints.", "index holds the constraint index. It is required.", "name holds the constraint name; use null or empty std::string (\"\") if no constraint name.", "lowerBound holds the constraint lower bound; use -OSDBL_MAX if no lower bound.", "upperBound holds the constraint upper bound; use  OSDBL_MAX if no upper bound.", "number holds the number of constraints. It is required.", "names holds a std::string array of constraint names; use null if no constraint names.", "lowerBounds holds a double array of constraint lower bounds; use null if no lower bounds;\n       use -OSDBL_MAX if no lower bound for a specific constraint in the array.", "upperBounds holds a double array of constraint upper bounds; use null if no upper bounds;\n       use  OSDBL_MAX if no upper bound for a specific constraint in the array.", "numberOfValues holds the number of specified coefficient values (usually nonzero) in the coefficient matrix.", "isColumnMajor holds whether the coefficient matrix is stored in column major (true) or row major (false).", "values holds a double array coefficient values in the matrix.", "valuesBegin holds the begin index of the values array to copy from (usually 0).", "valuesEnd holds the end index of the values array to copy till (usually values.length - 1).", "indexes holds an integer array column/row indexes for each value in the values array.", "indexesBegin holds the begin index of the indexes array to copy from (usually 0).", "indexesEnd holds the end index of the indexes array to copy till (usually indexes.length - 1).", "starts holds an integer array start indexes in the matrix; the first value of starts should always be 0.", "startsBegin holds the begin index of the starts array to copy from (usually 0).", "startsEnd holds the end index of the starts array to copy till (usually starts.length - 1).", "numberOfValues holds the number of specified coefficient values (usually nonzero) in the coefficient matrix.", "isColumnMajor holds whether the coefficient matrix is stored in column major (true) or row major (false).", "values holds a double array coefficient values in the matrix.", "valuesBegin holds the begin index of the values array to copy from (usually 0).", "valuesEnd holds the end index of the values array to copy till (usually values.length - 1).", "indexes holds an integer array column/row indexes for each value in the values array.", "indexesBegin holds the begin index of the indexes array to copy from (usually 0).", "indexesEnd holds the end index of the indexes array to copy till (usually indexes.length - 1).", "starts holds an integer array start indexes in the matrix; the first value of starts should always be 0.", "startsBegin holds the begin index of the starts array to copy from (usually 0).", "startsEnd holds the end index of the starts array to copy till (usually starts.length - 1).", "nq holds the number of quadratic terms.", "number holds the number of quadratic terms.", "rowIndexes holds an integer array of row indexes of all the quadratic terms.\n       A negative integer corresponds to an objective row, e.g. -1 for 1st objective and -2 for 2nd.", "varOneIndexes holds an integer array of the first  variable indexes of all the quadratic terms.", "varTwoIndexes holds an integer array of the second variable indexes of all the quadratic terms.", "coefficients holds an array of double containing all the quadratic term coefficients.", "begin holds the begin index of all the arrays to copy from (usually = 0).", "end holds the end index of all the arrays to copy till (usually = array length -1).", "number holds the number of quadratic terms.", "rowIndexes holds an integer array of row indexes of all the quadratic terms.\n       A negative integer corresponds to an objective row, e.g. -1 for 1st objective and -2 for 2nd.", "varOneIndexes holds an integer array of the first  variable indexes of all the quadratic terms.", "varTwoIndexes holds an integer array of the second variable indexes of all the quadratic terms.", "coefficients holds a double array all the quadratic term coefficients.", "nexpr holds the number of nonlinear expressions.", "root holds a pointer array to the root nodes of all the nonlinear expressions.", "number holds the number of matrices", "index holds the matrix index. It is required.", "name holds the matrix name; use null or empty std::string (\"\") if no matrix name.", "numberOfRows holds the number of rows. It is required. Use 1 for column vectors.", "numberOfColumns holds the number of columns. It is required. Use 1 for row vectors.", "symmetry holds the type of symmetry used in the definition of the matrix.\n       For more information  see the enumeration ENUM_MATRIX_SYMMETRY in OSGeneral.h.\n       If no symmetry, use ENUM_MATRIX_SYMMETRY_none.", "matrixType tracks the type of elements contained in this matrix.\n       For more information  see the enumeration ENUM_MATRIX_TYPE in OSGeneral.h.\n       If unsure, use ENUM_MATRIX_TYPE_unknown.", "inumberOfChildren is the number of MatrixNode child elements,\n       i.e., the number of matrix constructors in the m_mChildren array.", "m_mChildren is the array of matrix constructors used in the definition of this matrix.", "number holds the number of cones", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be one of\n           ENUM_CONE_TYPE_nonnegative,\n           ENUM_CONE_TYPE_nonpositive,\n           ENUM_CONE_TYPE_copositiveMatrices,\n           ENUM_CONE_TYPE_completelyPositiveMatrices.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be one of\n           ENUM_CONE_TYPE_product,\n           ENUM_CONE_TYPE_intersection.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "numberOfComponents holds the number of components of this cone.", "components holds the indexes of the components of this cone.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be one of\n           ENUM_CONE_TYPE_dual,\n           ENUM_CONE_TYPE_polar,\n           ENUM_CONE_TYPE_polyhedral.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "referenceIdx holds the index of a cone or matrix used in the definition of this cone.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_semidefinite.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "semidefiniteness distinguishes positive and negative semidefinite cones.\n       It must be either \"positive\" or \"negative\".", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_quadratic.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "distortionMatrixIdx holds the index of a distortion matrix. Use -1 if there is none.", "normFactor holds a scale factor for the norm. Use 1 if there is none.", "axisDirection holds the index of the axis direction. The most usual value is 0.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_rotatedQuadratic.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "distortionMatrixIdx holds the index of a distortion matrix. Use -1 if there is none.", "normFactor holds a scale factor for the norm. Use 1 if there is none.", "firstAxisDirection holds the index of the  first axis direction. The most usual value is 0.", "secondAxisDirection holds the index of the second axis direction. The most usual value is 1.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_normed.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "distortionMatrixIdx holds the index of a distortion matrix. Use -1 if there is none.", "normFactor holds a scale factor for the norm. Use 1 if there is none.", "pNorm holds the norm descriptor. It must be greater than or equal to 1.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "index holds the cone index. It is required.", "numberOfRows holds the number of rows. It is required.", "numberOfColumns holds the number of columns. It is required.", "coneType holds the cone type. For more information consult the enumeration ENUM_CONE_TYPE\n       further up in this file. This argument is required and must be\n           ENUM_CONE_TYPE_nonnegativePolynomials.\n           ENUM_CONE_TYPE_sumOfSquaresPolynomials.\n           ENUM_CONE_TYPE_moment.", "name holds the cone name; use null or empty std::string (\"\") if no cone name.", "maxDegree holds the maximum degree of the polynomials. Use 1, 2, 3, ..., INF.", "numberOfUB holds the number of (box-type) upper bound constraints. Use 0 if there are none.", "ub holds the upper bound values. Use null if there are no upper bounds.", "numberOfLB holds the number of (box-type) lower bound constraints. Use 0 if there are none.", "lb holds the lower bound values. Use null if there are no lower bounds.", "numberOfOtherIndexes holds the number of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to 0.", "otherIndexes holds the array of other indexes if the cone contains higher-dimensional tensors.\n       This argument is optional and can be omitted. It defaults to null.", "rowIdx is the index of the row we want to express in infix.", "idx is the index on the constraint (0, 1, 2, 3, ...) or objective function (-1, -2, -3, ...).", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called for the current x\nhas been evaluated for the current iterate x\nuse a value of false if not sure", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called for the current x\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called\nfor the current iterate", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called\nfor the current iterate", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins\n@parma idx is the index of the constraint function gradient", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values\n@parma idx is the index of the constraint function gradient", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins\n@parma objIdx is the index of the objective function being optimized", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "objIdx is the index of the objective function being optimized", "new_x is false if any evaluation method was previously called\nfor the current iterate", "x is a pointer (double array) to the current variable values", "objLambda is the Lagrange multiplier on the objective function", "conLambda is pointer (double array) of Lagrange multipliers on\nthe constratins", "new_x is false if any evaluation method was previously called\nfor the current iterate", "highestOrder is the highest order of the derivative being calculated", "x is a pointer (double array) to the current variable values", "new_x is false if any evaluation method was previously called\nfor the current iterate\n@parma idx is the index of the either a constraint or objective\nfunction Hessian", "vdX is a vector of doubles holding the current primal variable values\nthe size of x should equal instanceData->variables->numberOfVariables", "p is the highest order Taylor coefficient", "vdX is a vector of doubles of the current primal variable values\nthe size of vdX m_iNumberOfNonlinearVariables", "p is the order of the sweep", "vdlambda is a vector of doubles of the current dual (lagrange) variable values\nthe size of lambda should equal number of objective functions plus number of constraints", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints", "new_x is false if any evaluation method was previously called", "highestOrder is the highest order derivative to be calculated", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints", "x is a pointer of doubles of primal values  for the current iteration", "objLambda is is a pointer of doubles of the current dual (Lagrange) multipliers\n on the objective functions", "conLambda is a pointer of doubles of the current dual (Lagrange) multipliers\n on the constraints"], "see": ["org.optimizationservices.oscommon.datastructure.SparseVector", "org.optimizationservices.oscommon.datastructure.SparseMatrix", "org.optimizationservices.oscommon.datastructure.SparseMatrix", "org.optimizationservices.oscommon.datastructure.QuadraticTerms"], "has_pass2": false}, "OS/src/OSCommonInterfaces/OSgLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSgLWriter.h", "filename": "OSgLWriter.h", "file": "OSgLWriter.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n@version 1.0, 22/Oct/2010\n@since   OS2.2\n\n\\remarks\nCopyright (C) 2005-2010, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSiLWriter.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSiLWriter.h", "filename": "OSiLWriter.h", "file": "OSiLWriter.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["theosinstance is a pointer to an OSInstance object"], "return": "a string with the OSInstance data that validates against the OSiL schema.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSiLReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSiLReader.h", "filename": "OSiLReader.h", "file": "OSiLReader.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osil a string that holds the problem instance."], "return": "the instance as an OSInstance object.\n</p>", "has_pass2": false}, "OS/src/OSCommonInterfaces/OSCommandLineReader.h": {"path": "layer-4/OS/OS/src/OSCommonInterfaces/OSCommandLineReader.h", "filename": "OSCommandLineReader.h", "file": "OSCommandLineReader.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2011-2013, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "param": ["osss is the command line to be parsed", "osss a command line string.", "osss a command line string."], "return": "the OSCommandLine object corresponding to the command line string.\n@remark Calls method parseString once and if a configFile item is found\n        calls method parseString two more times (with the config file contents\n        and again with the original command line)", "has_pass2": false}, "OS/src/OSParsers/OSrLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSrLParserData.h", "filename": "OSrLParserData.h", "file": "OSrLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSiLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSiLParserData.h", "filename": "OSiLParserData.h", "file": "OSiLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSoLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSoLParserData.h", "filename": "OSoLParserData.h", "file": "OSoLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSgLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSgLParserData.h", "filename": "OSgLParserData.h", "file": "OSgLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2011, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSnLParserData.h": {"path": "layer-4/OS/OS/src/OSParsers/OSnLParserData.h", "filename": "OSnLParserData.h", "file": "OSnLParserData.h", "author": "Horand Gassmann, Jun Ma, Kipp Martin,\n\n\\remarks\nCopyright (C) 2005-2014, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}, "OS/src/OSParsers/OSOptionsStruc.h": {"path": "layer-4/OS/OS/src/OSParsers/OSOptionsStruc.h", "filename": "OSOptionsStruc.h", "file": "OSOptionsStruc.h", "author": "Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin\n\n\\remarks\nCopyright (C) 2005-2012, Robert Fourer, Horand Gassmann, Jun Ma, Kipp Martin,\nNorthwestern University, and the University of Chicago.\nAll Rights Reserved.\nThis software is licensed under the Eclipse Public License.\nPlease see the accompanying LICENSE file in root directory for terms.", "has_pass2": false}}}, "SHOT": {"name": "SHOT", "file_count": 108, "pass2_count": 17, "files": {"ThirdParty/mc++/include/tmodel.hpp": {"path": "layer-4/SHOT/ThirdParty/mc++/include/tmodel.hpp", "filename": "tmodel.hpp", "file": "tmodel.hpp", "brief": "Taylor Model Arithmetic for Rigorous Bound Propagation", "algorithm": "Convergence Properties:", "math": "|R| = O(diam(D)^{q+1}) as domain shrinks\n  Higher order q → faster convergence but more coefficients\n  Used in global optimization branch-and-bound for bound tightening.", "complexity": "Bernstein conversion: O(n^q * 2^n) but provides hull property", "ref": ["Makino & Berz (2003) - Taylor models and validated numerics", "Neumaier (2002) - Taylor forms for global optimization", "Lin & Rokne (1995) - Bernstein form for polynomial bounds", "Sahlodin & Chachuat (2011) - McCormick-Taylor models", "Bompadre et al. (2012) - Convergence of Taylor models"], "has_pass2": true}, "ThirdParty/boost/boost/core/ref.hpp": {"path": "layer-4/SHOT/ThirdParty/boost/boost/core/ref.hpp", "filename": "ref.hpp", "brief": "Contains a reference to an object of type `T`.\n\n `reference_wrapper` is primarily used to \"feed\" references to\n function templates (algorithms) that take their parameter by\n value. It provides an implicit conversion to `T&`, which\n usually allows the function templates to work on references\n unmodified.", "return": "The stored reference.\n     @remark Does not throw.", "has_pass2": false}, "src/EventHandler.h": {"path": "layer-4/SHOT/src/EventHandler.h", "filename": "EventHandler.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "EventHandler.h", "brief": "Observer pattern for algorithm events and callbacks\n\nAllows external code to respond to solver events.\n\n**Event Types (E_EventType):**\n- NewPrimalSolution: New feasible solution found\n- UserTerminationCheck: Allow user to request termination\n\n**Callback Registration:**\n- registerCallback(event, callback): Register handler\n- Callback signature: void()\n- Multiple callbacks per event supported\n\n**Event Notification:**\n- notify(event): Invoke all registered callbacks\n- Called by solver at appropriate points\n\n**Usage Example:**\n```cpp\neventHandler->registerCallback(E_EventType::NewPrimalSolution,\n    [&]() { std::cout << \"New solution found!\" << std::endl; });\n```", "see": ["Solver.h for event handler access", "Enums.h for E_EventType definition"], "has_pass2": false}, "src/Results.h": {"path": "layer-4/SHOT/src/Results.h", "filename": "Results.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Results.h", "brief": "Solution results storage and optimality gap tracking\n\nCentral repository for primal/dual solutions and algorithm progress.\n\n**Primal Solutions:**\n- primalSolutions: All found feasible points\n- addPrimalSolution(): Submit new incumbent\n- getPrimalBound(): Best objective value\n- primalSolutionSourceStatistics: Track solution origins\n\n**Dual Solutions:**\n- dualSolutions: Lower/upper bounds from MIP\n- getCurrentDualBound(), getGlobalDualBound()\n- setDualBound(): Update relaxation bound\n\n**Optimality Gap:**\n- getAbsoluteGlobalObjectiveGap(): |primal - dual|\n- getRelativeGlobalObjectiveGap(): |gap| / |primal|\n- isRelativeObjectiveGapToleranceMet(): Termination check\n\n**Iteration Tracking:**\n- iterations: Per-iteration state snapshots\n- createIteration(), getCurrentIteration()\n\n**Output Formats:**\n- getResultsOSrL(): XML Optimization Services format\n- getResultsTrace(): GAMS trace format\n- getResultsSol(): AMPL .sol format", "see": ["Solver.h for getPrimalSolution(), getResultsOSrL()"], "has_pass2": false}, "src/Settings.h": {"path": "layer-4/SHOT/src/Settings.h", "filename": "Settings.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Settings.h", "brief": "Solver configuration and option management\n\nHierarchical settings system for SHOT algorithm parameters.\n\n**Settings Categories:**\n- Termination: TimeLimit, IterationLimit, ObjectiveGap\n- Subsolver: MIP backend, NLP backend\n- Algorithm: Cut generation, reformulation options\n- Output: Verbosity, logging levels\n\n**Exception Classes:**\n- SettingKeyNotFoundException: Unknown setting key\n- SettingSetWrongTypeException: Type mismatch on set\n- SettingGetWrongTypeException: Type mismatch on get\n\n**Settings Types:**\n- E_SettingType: Integer, Double, String, Boolean, Enum\n\n**File Formats:**\n- OSoL XML format (Options Service Language)\n- Simple key=value format", "see": ["Solver.h for setOptionsFromFile/String methods"], "has_pass2": false}, "src/Environment.h": {"path": "layer-4/SHOT/src/Environment.h", "filename": "Environment.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Environment.h", "brief": "Shared state container for SHOT solver components\n\nCentral hub connecting all solver subsystems.\n\n**Environment Class Members:**\n- problem: Original problem formulation\n- reformulatedProblem: Convexified/linearized version\n- modelingSystem: GAMS/AMPL/OSiL interface\n\n**Solver Components:**\n- dualSolver: MIP solver for dual problem (CPLEX/Gurobi/CBC)\n- primalSolver: NLP solver for primal bounds (Ipopt)\n- rootsearchMethod: Line search for hyperplanes\n\n**Infrastructure:**\n- settings: Solver configuration parameters\n- results: Solution and statistics\n- output: Logging via spdlog\n- timing: Performance profiling\n- events: Callback event handler\n- tasks: Asynchronous task manager", "see": ["Solver.h for the main solver interface"], "has_pass2": false}, "src/Structs.h": {"path": "layer-4/SHOT/src/Structs.h", "filename": "Structs.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Structs.h", "brief": "Core data structures and type definitions for SHOT\n\nFundamental types, forward declarations, and helper structures.\n\n**Constants:**\n- SHOT_DBL_MIN/MAX/INF/EPS: Double precision limits\n- SHOT_INT_MAX, SHOT_LONG_MAX: Integer limits\n\n**Smart Pointer Typedefs:**\n- ProblemPtr, SettingsPtr, ResultsPtr, etc.\n- Shared ownership via std::shared_ptr\n\n**Solution Structures:**\n- PrimalSolution: Variable values + objective + source\n- DualSolution: Dual bound + iteration + source\n- SolutionPoint: Point with constraint deviation\n\n**Hyperplane Structures:**\n- Hyperplane: Supporting hyperplane cut\n- GeneratedHyperplane: Hyperplane with metadata\n- IntegerCut: No-good cut for integer variables\n\n**Utility Types:**\n- VectorDouble, VectorInteger, VectorString\n- PairIndexValue: (index, value) pair", "see": ["Environment.h for component composition"], "has_pass2": false}, "src/Solver.h": {"path": "layer-4/SHOT/src/Solver.h", "filename": "Solver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Solver.h", "brief": "Main solver interface for convex MINLP problems\n\nPrimary entry point for the SHOT optimizer.", "algorithm": "Solution Strategies:\nMULTI-TREE (traditional):\n  - Solve MIP to optimality\n  - Add cuts at solution\n  - Re-solve MIP\n  - Iterate until gap closed\n\nSINGLE-TREE (lazy constraints):\n  - One MIP solve with callback\n  - At integer nodes: add lazy cuts\n  - More efficient for large problems\n\nECP (Extended Cutting Plane):\n  - Add cuts at LP solutions too\n  - Faster convergence but more cuts\n\nESH (Extended Supporting Hyperplane):\n  - Add cuts at interior points\n  - Better approximation quality", "complexity": "- Each iteration: O(MIP) + O(NLP) for primal heuristic\n- Convergence: finite for convex MINLP, exponential worst case\n- Practice: often fast due to warm starts and cut reuse", "ref": ["Kronqvist, Lundell & Westerlund (2016). \"The Extended Supporting\n  Hyperplane Algorithm for Convex Mixed-Integer Nonlinear Programming\".\n  J. Global Optimization 64(2):249-272.\n\n**Solver Class:**\n- setProblem(): Load GAMS/AMPL/OSiL problem file\n- solveProblem(): Execute the selected solution strategy\n- getPrimalSolution(): Retrieve best incumbent solution\n\n**Solution Workflow:**\n1. Initialize settings (setOptionsFromFile/String)\n2. Load problem (setProblem)\n3. Solve (solveProblem)\n4. Retrieve results (getResultsOSrL, getPrimalSolution)\n\n**Callbacks:**\n- registerCallback(): Monitor NewPrimalSolution, UserTerminationCheck\n\n**Results Access:**\n- getCurrentDualBound(): Best lower bound\n- getPrimalBound(): Best upper bound (incumbent)\n- getAbsoluteObjectiveGap(): |primal - dual|\n- getRelativeObjectiveGap(): |primal - dual| / |primal|"], "see": ["Environment.h for shared state container", "SolutionStrategy/ISolutionStrategy.h for algorithm implementations"], "has_pass2": true}, "src/Timer.h": {"path": "layer-4/SHOT/src/Timer.h", "filename": "Timer.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Timer.h", "brief": "High-resolution stopwatch for performance measurement\n\nLightweight timer using std::chrono::high_resolution_clock.\n\n**Timer Class:**\n- start()/stop(): Pause/resume timing\n- restart(): Reset to zero and start\n- elapsed(): Get cumulative time in seconds\n\n**Properties:**\n- name: Timer identifier (for Timing registry)\n- description: Human-readable label\n\n**Usage:**\nTimer t(\"SolveMIP\", \"MIP solver time\");\nt.start();\n// ... computation ...\nt.stop();\ndouble seconds = t.elapsed();", "see": ["Timing.h for timer registry"], "has_pass2": false}, "src/PrimalSolver.h": {"path": "layer-4/SHOT/src/PrimalSolver.h", "filename": "PrimalSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "PrimalSolver.h", "brief": "NLP-based primal bound computation and solution repair\n\nFinds feasible solutions and improves the primal bound.", "algorithm": "Solution Validation:\nVerify candidate satisfies all constraints to tolerance.\n\n  For each constraint g_i(x) <= 0:\n    violation_i = max(0, g_i(x))\n\n  max_violation = max_i(violation_i)\n\n  Point is feasible if max_violation <= tolerance", "math": "Primal bound update:\n  If x is feasible and f(x) < z_P: z_P = f(x)\n  Gap = (z_P - z_D) / |z_P|\n\n**PrimalSolver Class:**\n- primalSolutionCandidates: Points to check for feasibility\n- fixedPrimalNLPCandidates: Integer-fixed NLP subproblems\n\n**Primal Solution Sources:**\n- MIP solution pool points\n- NLP local search from MIP solutions\n- Rounding heuristics\n\n**Solution Validation:**\n- addPrimalSolutionCandidate(): Submit candidate point\n- checkPrimalSolutionCandidates(): Verify feasibility\n- checkPrimalSolutionPoint(): Full constraint check\n\n**Fixed-Integer NLP:**\n- addFixedNLPCandidate(): Queue NLP subproblem\n- hasFixedNLPCandidateBeenTested(): Avoid re-solving", "complexity": "- Solution validation: O(m * nnz_constraint) per candidate\n- Fixed NLP: depends on Ipopt (typically polynomial for convex)", "see": ["DualSolver.h for dual bound computation", "NLPSolver/ for backend NLP solvers (Ipopt)"], "has_pass2": true}, "src/Timing.h": {"path": "layer-4/SHOT/src/Timing.h", "filename": "Timing.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Timing.h", "brief": "Named timer registry for profiling solver phases\n\nCollection of named timers for performance analysis.\n\n**Timing Class:**\n- createTimer(): Register new timer with name/description\n- startTimer()/stopTimer(): Control by name\n- getElapsedTime(): Query accumulated time\n\n**Typical Timers:**\n- \"Total\": Overall solve time\n- \"DualMIP\": MIP solver calls\n- \"PrimalNLP\": NLP subproblem solves\n- \"Reformulation\": Problem transformation\n\n**Thread Safety:**\n- Timers are not thread-safe\n- Use in single-threaded solver context", "see": ["Timer.h for individual timer class", "Environment.h for accessing via env->timing"], "has_pass2": false}, "src/TaskHandler.h": {"path": "layer-4/SHOT/src/TaskHandler.h", "filename": "TaskHandler.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "TaskHandler.h", "brief": "Task scheduling and execution control\n\nOrchestrates the modular task-based algorithm workflow.\n\n**Task Management:**\n- addTask(): Register task with string identifier\n- getTask(): Retrieve task by ID\n- clearTasks(): Reset for new solve\n\n**Execution Control:**\n- getNextTask(): Dequeue next task to run\n- setNextTask(): Jump to specific task (for goto/branching)\n- terminate(): Signal algorithm termination\n- isTerminated(): Check termination flag\n\n**Workflow Pattern:**\n1. SolutionStrategy adds tasks in order\n2. Solver calls getNextTask() in loop\n3. Tasks execute and may setNextTask() for control flow\n4. Termination tasks set terminate()\n\n**Task ID Naming:**\n- \"InitializeIteration\", \"SolveIteration\"\n- \"CheckTimeLimit\", \"CheckAbsoluteGap\"\n- \"SelectHyperplanes\", \"AddHyperplanes\"", "see": ["TaskBase.h for task interface", "SolutionStrategy/ for task configuration"], "has_pass2": false}, "src/Report.h": {"path": "layer-4/SHOT/src/Report.h", "filename": "Report.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Report.h", "brief": "Formatted console output for solver progress\n\nGenerates iteration tables and summary reports.\n\n**Report Class:**\n- outputSolverHeader(): Version and license info\n- outputOptionsReport(): Active settings summary\n- outputProblemInstanceReport(): Problem statistics\n\n**Iteration Reporting:**\n- outputIterationDetailHeader(): Column headers\n- outputIterationDetail(): Per-iteration progress line\n- Shows: iteration, time, cuts, bounds, gap\n\n**Solution Reporting:**\n- outputSolutionReport(): Final summary\n- outputPrimalSolutionDetailedReport(): Full solution\n\n**Output Control:**\n- Tracks last values to avoid redundant output\n- Respects ES_IterationOutputDetail setting", "see": ["Output.h for underlying logging"], "has_pass2": false}, "src/Utilities.h": {"path": "layer-4/SHOT/src/Utilities.h", "filename": "Utilities.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Utilities.h", "brief": "Helper functions for vectors, hashing, and I/O\n\nStandalone utility functions in SHOT::Utilities namespace.\n\n**Vector Operations:**\n- L2Norm(): Euclidean distance between points\n- calculateCenterPoint(): Centroid of point set\n- displayVector(): Debug output for vectors\n\n**Hashing:**\n- calculateHash(): Hash function for duplicate detection\n- Used by hyperplane/integer cut deduplication\n\n**File I/O:**\n- writeStringToFile(), getFileAsString()\n- saveVariablePointVectorToFile(): Solution output\n\n**Numeric:**\n- isAlmostEqual(), isAlmostZero(): Tolerance comparisons\n- isInteger(): Check for integer values\n\n**Sparse Containers:**\n- SparseVariableVector: Variable→coefficient map\n- SparseVariableMatrix: (Variable,Variable)→coefficient\n- combineSparseVariable*(): Merge operations", "see": ["Structs.h for type definitions"], "has_pass2": false}, "src/DualSolver.h": {"path": "layer-4/SHOT/src/DualSolver.h", "filename": "DualSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "DualSolver.h", "brief": "MIP-based dual bound computation via supporting hyperplanes\n\nManages the linearization-based dual problem.", "algorithm": "Integer Cuts (No-Good Cuts):\nExclude previously found integer solutions.\n\n  For integer solution y*:\n    sum_{j: y*_j=1} (1-y_j) + sum_{j: y*_j=0} y_j >= 1\n\n  Forces at least one integer variable to change.\n  Used when NLP subproblem is infeasible.", "math": "Dual bound at iteration k:\n  z_D^k = optimal value of MIP with k hyperplanes\n  z_D^k <= z_D^(k+1) <= z* (monotone non-decreasing)\n\n**DualSolver Class:**\n- MIPSolver: Backend MIP solver (CPLEX/Gurobi/CBC/HiGHS)\n- generatedHyperplanes: All cutting planes added\n- hyperplaneWaitingList: Pending cuts to add\n\n**Hyperplane Management:**\n- addHyperplane(): Add supporting hyperplane cut\n- addGeneratedHyperplane(): Record a hyperplane\n- hasHyperplaneBeenAdded(): Avoid duplicate cuts\n\n**Integer Cuts:**\n- addIntegerCut(): Add no-good cuts for MINLP\n- generatedIntegerCuts: All integer cuts\n\n**Interior Points:**\n- interiorPointCandidates: Candidate interior points\n- interiorPts: Verified interior points for ESH", "complexity": "- Hyperplane addition: O(n) per cut (n = variables)\n- Duplicate check: O(1) via hash\n- MIP solve: depends on backend solver", "see": ["PrimalSolver.h for primal bound computation", "MIPSolver/ for backend implementations"], "has_pass2": true}, "src/Output.h": {"path": "layer-4/SHOT/src/Output.h", "filename": "Output.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Output.h", "brief": "Logging infrastructure using spdlog\n\nCentralized logging with console and file sinks.\n\n**Output Class:**\n- outputCritical/Error/Warning/Info/Debug/Trace(): Log methods\n- setLogLevels(): Configure console and file verbosity\n- setConsoleSink(): Custom console output\n- setFileSink(): Enable file logging\n\n**E_LogLevel (Enums.h):**\n- Off, Critical, Error, Warning, Info, Debug, Trace\n\n**OutputStream Class:**\n- std::ostream wrapper for solver output redirection\n- Routes external solver output through SHOT logging\n- Used by CPLEX, Gurobi, Ipopt adapters\n\n**Implementation:**\n- Uses spdlog library for fast, thread-safe logging\n- consoleSink: stdout sink\n- fileSink: basic_file_sink for persistence", "see": ["Environment.h for Output access"], "has_pass2": false}, "src/Iteration.h": {"path": "layer-4/SHOT/src/Iteration.h", "filename": "Iteration.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Iteration.h", "brief": "Per-iteration state snapshot for algorithm progress tracking\n\nCaptures the state of the solver at each outer approximation iteration.\n\n**Problem State:**\n- dualProblemClass: LP, QP, MIP, MIQP, etc.\n- isDualProblemDiscrete: Whether MIP or LP relaxation\n- solutionStatus: Optimal, Feasible, Infeasible, etc.\n\n**Solution Data:**\n- solutionPoints: All solutions from MIP solution pool\n- objectiveValue: Best objective value\n- currentObjectiveBounds: (dual, primal) bounds\n\n**Constraint Violation:**\n- constraintDeviations: Per-constraint violations\n- maxDeviation, maxDeviationConstraint: Worst violation\n- usedConstraintTolerance: Tolerance for this iteration\n\n**Hyperplane Statistics:**\n- numHyperplanesAdded: Cuts added this iteration\n- totNumHyperplanes: Cumulative cuts\n- hyperplanePoints: Generation points\n\n**Node Statistics:**\n- numberOfExploredNodes, numberOfOpenNodes: B&B tree\n\n**Helper Methods:**\n- getSolutionPointWithSmallestDeviation()\n- isMIP(): Check if discrete problem", "see": ["Results.h for iterations collection"], "has_pass2": false}, "src/Enums.h": {"path": "layer-4/SHOT/src/Enums.h", "filename": "Enums.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Enums.h", "brief": "Enumeration types for SHOT solver states and options\n\nComprehensive enums for algorithm configuration and status tracking.\n\n**Problem Classification:**\n- E_Convexity: Linear, Convex, Concave, Nonconvex, Unknown\n- E_DualProblemClass: LP, QP, QCQP, MIP, MIQP, MIQCQP\n\n**Auxiliary Variable Types:**\n- E_AuxiliaryVariableType: NonlinearObjectiveFunction (epigraph),\n  BilinearTerms, MonomialPartitioning, etc.\n\n**Hyperplane Sources (E_HyperplaneSource):**\n- MIPOptimalRootsearch, MIPSolutionPoolRootsearch\n- LPRelaxedRootsearch, InteriorPointSearch\n- PrimalSolutionSearch, ObjectiveCuttingPlane\n\n**Event Types (E_EventType):**\n- NewPrimalSolution: New incumbent found\n- UserTerminationCheck: Allow user termination\n\n**Termination Reasons (E_TerminationReason):**\n- Optimal, ObjectiveGapTolerance, TimeLimit\n- IterationLimit, InfeasibleProblem, NoDualCutsAdded", "see": ["Solver.h for using these enums"], "has_pass2": false}, "src/ModelingSystem/ModelingSystemOSiL.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemOSiL.h", "filename": "ModelingSystemOSiL.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemOSiL.h", "brief": "OSiL XML format reader for optimization problems\n\nReads Optimization Services instance Language (OSiL) files.\n\n**ModelingSystemOSiL Class:**\n- createProblem(): Parse OSiL XML into Problem\n- augmentSettings(): Add OSiL-specific options\n- finalizeSolution(): Write OSrL result format\n\n**OSiL Format:**\n- XML-based optimization instance representation\n- <instanceData> contains variables, objectives, constraints\n- <nonlinearExpressions> for NLP terms (OSnL format)\n\n**XML Parsing:**\n- Uses tinyxml2 for XML processing\n- convertNonlinearNode(): OSnL to SHOT expression conversion\n\n@note OSiL is part of COIN-OR Optimization Services", "see": ["ModelingSystemOS.h for OS library integration"], "has_pass2": false}, "src/ModelingSystem/ModelingSystemGAMS.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemGAMS.h", "filename": "ModelingSystemGAMS.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemGAMS.h", "brief": "GAMS interface for optimization problem input\n\nReads GAMS models via GMO/GEV API.\n\n**ModelingSystemGAMS Class:**\n- setModelingObject(): Accept gmoHandle_t from GAMS\n- createProblem(): Convert GMO model to SHOT Problem\n- finalizeSolution(): Return solution to GAMS\n\n**GAMS Integration:**\n- gmoHandle_t: GAMS Model Object (GMO)\n- gevHandle_t: GAMS Environment (GEV)\n- palHandle_t: GAMS Audit Licensing (PAL)\n\n**Model Conversion:**\n- copyVariables/Constraints/ObjectiveFunction()\n- parseGamsInstructions(): Bytecode to expression conversion\n\n**GamsOutputSink:**\n- spdlog sink routing to GAMS log\n- gevLogPChar() for normal messages\n- gevLogStatPChar() for warnings/errors", "see": ["GMO API documentation (gams.com)"], "has_pass2": false}, "src/ModelingSystem/IModelingSystem.h": {"path": "layer-4/SHOT/src/ModelingSystem/IModelingSystem.h", "filename": "IModelingSystem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/IModelingSystem.h", "brief": "Abstract interface for problem input formats\n\nBase interface for parsing optimization problem files.\n\n**E_ProblemCreationStatus Enum:**\n- NormalCompletion: Problem loaded successfully\n- FileDoesNotExist, ErrorInFile: I/O errors\n- ErrorInVariables/Constraints/Objective: Parse errors\n- CapabilityProblem: Unsupported problem features\n\n**IModelingSystem Interface:**\n- augmentSettings(): Add format-specific settings\n- updateSettings(): Extract settings from problem file\n- finalizeSolution(): Write solution back to format\n\n**Implementations:**\n- ModelingSystemOSiL: XML OSiL format (Optimization Services)\n- ModelingSystemAMPL: .nl format from AMPL\n- ModelingSystemGAMS: GAMS interface\n- ModelingSystemOS: Optimization Services framework", "see": ["Solver.h for setProblem() to load problems", "Results.h for solution output"], "has_pass2": false}, "src/ModelingSystem/ModelingSystemOS.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemOS.h", "filename": "ModelingSystemOS.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemOS.h", "brief": "Optimization Services library integration\n\nFull OS library integration for OSiL and AMPL files.\n\n**ModelingSystemOS Class:**\n- createProblem(filename, format): Read OSiL or nl files\n- createProblem(OSInstance): Use existing OS instance\n- finalizeSolution(): Generate OSrL output\n\n**File Formats:**\n- E_OSInputFileFormat::OSiL: XML format\n- E_OSInputFileFormat::Ampl: nl binary via OSnl2OS\n\n**OS Library Classes:**\n- OSiLReader: Parse OSiL XML\n- OSInstance: In-memory problem representation\n- OSnl2OS: Convert nl to OSInstance\n- OSnLNode: Nonlinear expression tree nodes\n\n**Expression Conversion:**\n- convertOSNonlinearNode(): OSnLNode to SHOT expression\n- Preserves convexity annotations if available\n\n@note Uses COIN-OR OS library (github.com/coin-or/OS)", "has_pass2": false}, "src/ModelingSystem/ModelingSystemAMPL.h": {"path": "layer-4/SHOT/src/ModelingSystem/ModelingSystemAMPL.h", "filename": "ModelingSystemAMPL.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "ModelingSystem/ModelingSystemAMPL.h", "brief": "AMPL nl-file format reader for optimization problems\n\nReads AMPL .nl (nonlinear) binary problem files.\n\n**ModelingSystemAMPL Class:**\n- createProblem(): Parse .nl file into Problem\n- augmentSettings(): Add AMPL-specific options\n- finalizeSolution(): Write .sol solution file\n\n**AMPL nl Format:**\n- Binary format for optimization instances\n- Contains variables, bounds, constraints, expressions\n- Used by AMPL-compatible solvers\n\n**Usage:**\n- Standalone: SHOT problem.nl\n- Via AMPL: option solver shot; solve;", "see": ["ASL (AMPL Solver Library) for nl format details"], "has_pass2": false}, "src/NLPSolver/NLPSolverIpoptRelaxed.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverIpoptRelaxed.h", "filename": "NLPSolverIpoptRelaxed.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverIpoptRelaxed.h", "brief": "Ipopt solver for continuous relaxation problems\n\nSolves NLP with integer variables relaxed to continuous.\n\n**NLPSolverIpoptRelaxed Class:**\n- Multiple inheritance: NLPSolverBase + NLPSolverIpoptBase\n- setSolverSpecificInitialSettings(): Configure for relaxed solve\n- getSolution(): Return continuous solution values\n\n**Use Case:**\n- Find interior points for ESH algorithm\n- Check feasibility of relaxed problem\n- Generate starting points for primal heuristics", "see": ["NLPSolverIpoptBase.h for fixed-integer variant"], "has_pass2": false}, "src/NLPSolver/NLPSolverCuttingPlaneMinimax.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverCuttingPlaneMinimax.h", "filename": "NLPSolverCuttingPlaneMinimax.h", "author": "Andreas Lundell, Åbo Akademi University\n\n    @section LICENSE\n    This software is licensed under the Eclipse Public License 2.0.\n    Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverCuttingPlaneMinimax.h", "brief": "Cutting-plane solver for minimax LP problems\n\nBuilt-in LP-based solver for simple minimax problems.\n\n**NLPSolverCuttingPlaneMinimax Class:**\n- Uses MIP solver (CPLEX/Gurobi/Cbc) as LP engine\n- Iteratively adds cutting planes\n- No external NLP solver dependency\n\n**Minimax Problem Form:**\n- min t\n- s.t. f_i(x) <= t for all i\n\n**Use Case:**\n- Finding interior points when Ipopt unavailable\n- Solving auxiliary minimax subproblems", "algorithm": "Iterative cutting plane for convex minimax", "has_pass2": true}, "src/NLPSolver/INLPSolver.h": {"path": "layer-4/SHOT/src/NLPSolver/INLPSolver.h", "filename": "INLPSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/INLPSolver.h", "brief": "Abstract interface for NLP solver backends\n\nPure virtual interface for primal bound NLP solvers.\n\n**Starting Point:**\n- setStartingPoint(): Initialize solver from MIP solution\n- clearStartingPoint(): Reset to default\n\n**Variable Fixing:**\n- fixVariables(): Fix integer variables for NLP subproblem\n- unfixVariables(): Restore full problem\n\n**Solution Methods:**\n- solveProblem(): Execute NLP solver\n- getSolution(): Retrieve primal point\n- getObjectiveValue(): Primal objective value\n\n**Bound Management:**\n- getVariableLowerBounds(), getVariableUpperBounds()\n- updateVariableLowerBound(), updateVariableUpperBound()\n\n**Implementations:**\n- NLPSolverIpoptBase: Ipopt interior point solver\n- NLPSolverGAMS: GAMS NLP solvers\n- NLPSolverSHOT: Recursive SHOT for NLP", "see": ["PrimalSolver.h for fixed-integer NLP subproblems", "NLPSolverBase.h for implementation utilities"], "has_pass2": false}, "src/NLPSolver/NLPSolverBase.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverBase.h", "filename": "NLPSolverBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverBase.h", "brief": "Base implementation for NLP solver interface\n\nProvides shared solve logic for all NLP solvers.\n\n**NLPSolverBase Class:**\n- solveProblem(): Template method calling solveProblemInstance()\n- Derived classes implement solveProblemInstance() hook\n\n**Inheritance:**\n- Virtually inherits from INLPSolver\n- Used with multiple inheritance (NLPSolverIpoptRelaxed)", "see": ["INLPSolver.h for interface contract", "NLPSolverIpoptBase.h for Ipopt implementation"], "has_pass2": false}, "src/NLPSolver/NLPSolverIpoptBase.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverIpoptBase.h", "filename": "NLPSolverIpoptBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverIpoptBase.h", "brief": "Ipopt implementation of INLPSolver interface\n\nProvides NLP solving for SHOT's primal problem using Ipopt interior point.\n\n**IpoptProblem Class (Ipopt::TNLP):**\n- Implements Ipopt's TNLP interface\n- Provides callbacks for function/gradient/Hessian evaluation\n- Sparse Jacobian/Hessian via index placement maps\n\n**TNLP Callbacks:**\n- get_nlp_info(): Problem dimensions and sparsity\n- get_bounds_info(): Variable and constraint bounds\n- eval_f(): Objective function value\n- eval_grad_f(): Objective gradient\n- eval_g(): Constraint function values\n- eval_jac_g(): Constraint Jacobian (sparse)\n- eval_h(): Lagrangian Hessian (sparse)\n- finalize_solution(): Store optimal point\n\n**NLPSolverIpoptBase Class:**\n- Wraps IpoptApplication for solve control\n- Variable fixing for integer-fixed NLP subproblems\n- Starting point management\n\n**IpoptJournal:**\n- Routes Ipopt output through SHOT logging system\n\n@note Used for fixed-integer NLP subproblems in primal bound computation", "see": ["PrimalSolver.h for NLP subproblem dispatch"], "algorithm": "Warm Starting (setStartingPoint):\n  Initialize from previous NLP solution or MIP point:\n  - Improves convergence for similar subproblems\n  - Critical for efficiency in repeated NLP solves", "math": "l_i = u_i = x̄_i for integer variables i ∈ I\n  Ipopt solves reduced-space problem over continuous variables.\n  Original bounds restored by unfixVariables().", "ref": ["Bonami et al. (2008) - BONMIN algorithm description", "Wächter & Biegler (2006) - Ipopt implementation", "Nocedal & Wright (2006) - Warm starting in IPM"], "complexity": "O(nnz) vs O(n²) for dense evaluation", "has_pass2": true}, "src/NLPSolver/NLPSolverSHOT.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverSHOT.h", "filename": "NLPSolverSHOT.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverSHOT.h", "brief": "SHOT as NLP solver for convex subproblems\n\nRecursive use of SHOT to solve fixed-integer NLP.\n\n**NLPSolverSHOT Class:**\n- Creates nested SHOT solver instance\n- Solves convex NLP after fixing integer variables\n- Useful when Ipopt is unavailable\n\n**Fixed-Integer NLP:**\n- fixVariables(): Fix discrete variables to integer values\n- solveProblemInstance(): Solve continuous subproblem\n- getSolution(): Return primal solution\n\n**Nested Architecture:**\n- Outer SHOT: Full MINLP (uses this as primal solver)\n- Inner SHOT: Convex NLP (fixed integers)\n\n@note Self-referential design for solver-agnostic NLP solving", "has_pass2": false}, "src/NLPSolver/NLPSolverGAMS.h": {"path": "layer-4/SHOT/src/NLPSolver/NLPSolverGAMS.h", "filename": "NLPSolverGAMS.h", "author": "Stefan Vigerske, GAMS Development Corp.\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "NLPSolver/NLPSolverGAMS.h", "brief": "GAMS-based NLP solver for fixed-integer subproblems\n\nUses GAMS modeling system to call NLP solvers.\n\n**NLPSolverGAMS Class:**\n- Uses gmoHandle_t from GAMS modeling system\n- Calls user-selected NLP solver (CONOPT, IPOPT, etc.)\n- Manages variable bounds and starting points\n\n**GAMS Handles:**\n- modelingObject: GMO for problem data\n- modelingEnvironment: GEV for GAMS environment\n\n**Solver Configuration:**\n- nlpsolver: Solver name (e.g., \"CONOPT\", \"IPOPT\")\n- timelimit, iterlimit: Resource limits\n- solvelink: Execution mode\n\n@note Requires GAMS license for commercial NLP solvers", "see": ["ModelingSystemGAMS.h for GAMS input processing"], "has_pass2": false}, "src/Tasks/TaskSelectHyperplanePointsObjectiveFunction.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectHyperplanePointsObjectiveFunction.h", "filename": "TaskSelectHyperplanePointsObjectiveFunction.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectHyperplanePointsObjectiveFunction.h", "brief": "Hyperplane selection for nonlinear objective\n\nGenerates cuts for epigraph of nonlinear objective.\n\n**TaskSelectHyperplanePointsObjectiveFunction Class:**\n- run(): Process objective at current solutions\n- run(solPoints): Process specific solution points\n\n**Epigraph Linearization:**\n- Nonlinear objective reformulated as: min t, f(x) <= t\n- Generates cuts to approximate f(x) <= t constraint", "see": ["AuxiliaryVariables.h for epigraph variable", "TaskSelectHyperplanePointsESH.h for constraint cuts"], "has_pass2": false}, "src/Tasks/TaskRepairableBase.h": {"path": "layer-4/SHOT/src/Tasks/TaskRepairableBase.h", "filename": "TaskRepairableBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskRepairableBase.h", "brief": "Base class for tasks that can repair failures\n\nExtension point for failure recovery logic.\n\n**TaskRepairableBase Class:**\n- repair(): Attempt to fix failed task\n\n**Usage:**\n- Subclassed by tasks that can recover from errors\n- Called when initial run() fails", "see": ["TaskRepairInfeasibleDualProblem.h for example"], "has_pass2": false}, "src/Tasks/TaskPerformBoundTightening.h": {"path": "layer-4/SHOT/src/Tasks/TaskPerformBoundTightening.h", "filename": "TaskPerformBoundTightening.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskPerformBoundTightening.h", "brief": "Tighten variable bounds via optimization\n\nUses optimization-based bound tightening (OBBT).\n\n**TaskPerformBoundTightening Class:**\n- POASolver: Polyhedral outer approximation solver\n- createPOA(): Build relaxed problem for bound tightening\n\n**OBBT Algorithm:**\n- For each variable: min/max subject to relaxation\n- Tightens bounds beyond constraint propagation\n- Improves relaxation quality", "algorithm": "Optimization-Based Bound Tightening", "see": ["NLPSolverSHOT.h for POA solver", "Problem.h for bound storage"], "has_pass2": true}, "src/Tasks/TaskPrintIterationReport.h": {"path": "layer-4/SHOT/src/Tasks/TaskPrintIterationReport.h", "filename": "TaskPrintIterationReport.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskPrintIterationReport.h", "brief": "Output iteration progress to console\n\nFormatted progress line for each iteration.\n\n**TaskPrintIterationReport Class:**\n- lastNumHyperplane: Track hyperplanes added\n- run(): Format and print iteration status\n\n**Report Contents:**\n- Iteration number, elapsed time\n- Primal bound, dual bound, gap\n- Number of hyperplanes added", "see": ["Report.h for output formatting", "Results.h for bound information"], "has_pass2": false}, "src/Tasks/TaskSimple.h": {"path": "layer-4/SHOT/src/Tasks/TaskSimple.h", "filename": "TaskSimple.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSimple.h", "brief": "Lightweight task wrapper for lambda functions\n\nAdapts arbitrary callable to task interface.\n\n**TaskSimple Class:**\n- setFunction(fn): Set callable to execute\n- run(): Invoke the stored function\n\n**Usage:**\n- Quick inline tasks without subclassing\n- Adapts existing functions to task framework", "see": ["TaskBase.h for task interface"], "has_pass2": false}, "src/Tasks/TaskTerminate.h": {"path": "layer-4/SHOT/src/Tasks/TaskTerminate.h", "filename": "TaskTerminate.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskTerminate.h", "brief": "Signals end of optimization algorithm\n\nFinal task that stops the main loop.\n\n**TaskTerminate Class:**\n- run(): Set termination flag in TaskHandler\n\n**Called When:**\n- Optimal solution found (gap closed)\n- Resource limit exceeded (time, iteration)\n- Infeasibility proven", "see": ["TaskGoto.h for jump to termination", "TaskHandler.h for loop control"], "has_pass2": false}, "src/Tasks/TaskInitializeIteration.h": {"path": "layer-4/SHOT/src/Tasks/TaskInitializeIteration.h", "filename": "TaskInitializeIteration.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskInitializeIteration.h", "brief": "Per-iteration setup and housekeeping\n\nPrepares state for new iteration cycle.\n\n**TaskInitializeIteration Class:**\n- run(): Reset iteration-local state\n\n**Initialization Steps:**\n- Increment iteration counter\n- Clear iteration-specific storage\n- Update timing information", "see": ["TaskPrintIterationReport.h for iteration summary", "Timing.h for time tracking"], "has_pass2": false}, "src/Tasks/TaskCheckIterationError.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckIterationError.h", "filename": "TaskCheckIterationError.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckIterationError.h", "brief": "Detect errors during iteration\n\nHandles solver failures and numerical issues.\n\n**TaskCheckIterationError Class:**\n- taskIDIfTrue: Jump target when error detected\n- run(): Check for MIP solver errors, infeasibility, etc.\n\n**Error Types:**\n- MIP solver failure or timeout\n- Numerical issues in cut generation\n- Unexpected infeasibility", "see": ["TaskTerminate.h for error termination", "DualSolver.h for MIP solve status"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalCandidatesFromSolutionPool.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalCandidatesFromSolutionPool.h", "filename": "TaskSelectPrimalCandidatesFromSolutionPool.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalCandidatesFromSolutionPool.h", "brief": "Check MIP solution pool for feasible MINLP solutions\n\nExtracts primal candidates from MIP solver's solution pool.\n\n**TaskSelectPrimalCandidatesFromSolutionPool Class:**\n- run(): Iterate through MIP solution pool\n- Check each solution for original constraint feasibility\n\n**Primal Candidate Processing:**\n- MIP solution may violate nonlinear constraints\n- Feasible solutions update primal bound\n- Infeasible solutions generate hyperplanes", "see": ["DualSolver.h for solution pool access", "PrimalSolver.h for feasibility checking"], "has_pass2": false}, "src/Tasks/TaskCreateDualProblem.h": {"path": "layer-4/SHOT/src/Tasks/TaskCreateDualProblem.h", "filename": "TaskCreateDualProblem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCreateDualProblem.h", "brief": "Build MIP relaxation from reformulated problem\n\nTranslates SHOT's problem representation to MIP solver format.\n\n**TaskCreateDualProblem Class:**\n- run(): Create initial MIP in DualSolver\n- createProblem(): Transfer variables, constraints, objective\n\n**MIP Construction:**\n- Linear constraints: Added directly\n- Quadratic constraints: Added if solver supports MIQCQP\n- Nonlinear constraints: Omitted (handled via hyperplanes)\n- Integer variables: Marked as such in MIP", "see": ["DualSolver.h for MIP solver interface", "Problem.h for source representation"], "has_pass2": false}, "src/Tasks/TaskCheckAbsoluteGap.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckAbsoluteGap.h", "filename": "TaskCheckAbsoluteGap.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckAbsoluteGap.h", "brief": "Termination check for absolute optimality gap\n\nTerminates when |primal - dual| < tolerance.\n\n**TaskCheckAbsoluteGap Class:**\n- taskIDIfTrue: Jump target when gap closed\n- run(): Check |bestPrimal - bestDual| < AbsoluteGapTol\n\n**Optimality Gap:**\n- Primal bound: Best feasible solution value\n- Dual bound: Relaxation optimal value\n- Gap closed = provably optimal", "see": ["TaskCheckRelativeGap.h for relative gap check", "Results.h for bound storage"], "has_pass2": false}, "src/Tasks/TaskPresolve.h": {"path": "layer-4/SHOT/src/Tasks/TaskPresolve.h", "filename": "TaskPresolve.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskPresolve.h", "brief": "Initial problem simplification\n\nApplies presolve techniques before main algorithm.\n\n**TaskPresolve Class:**\n- isPresolved: Track if presolve completed\n- run(): Apply presolve routines\n\n**Presolve Techniques:**\n- Bound propagation\n- Fixed variable removal\n- Redundant constraint detection\n- Expression simplification", "see": ["TaskReformulateProblem.h for reformulation", "Simplifications.h for expression simplification"], "has_pass2": false}, "src/Tasks/TaskCheckIterationLimit.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckIterationLimit.h", "filename": "TaskCheckIterationLimit.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckIterationLimit.h", "brief": "Termination check for iteration count limit\n\nPart of termination check sequence.\n\n**TaskCheckIterationLimit Class:**\n- taskIDIfTrue: Jump target when limit reached\n- run(): Compare iteration count vs Settings.IterationLimit\n\n**Behavior:**\n- If exceeded: Jump to termination task\n- If not: Continue to next task", "see": ["TaskCheckTimeLimit.h for time-based termination", "TaskTerminate.h for termination handling"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalCandidatesFromNLP.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalCandidatesFromNLP.h", "filename": "TaskSelectPrimalCandidatesFromNLP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalCandidatesFromNLP.h", "brief": "Solve fixed-integer NLP for primal candidates\n\nUses NLP solver (Ipopt) with fixed integer variables.\n\n**TaskSelectPrimalCandidatesFromNLP Class:**\n- solveFixedNLP(): Fix integers, solve continuous NLP\n- createInfeasibilityCut(): Handle infeasible NLP\n- createIntegerCut(): Exclude tested integer assignment\n\n**Fixed-Integer NLP:**\n- Take MIP solution's integer values\n- Solve continuous relaxation with fixed integers\n- Provides locally optimal primal candidates\n\n**Usage:**\n- Triggered by primal stagnation\n- May improve primal bound significantly", "see": ["NLPSolver/ for Ipopt interface", "TaskCheckPrimalStagnation.h for trigger"], "has_pass2": false}, "src/Tasks/TaskClearFixedPrimalCandidates.h": {"path": "layer-4/SHOT/src/Tasks/TaskClearFixedPrimalCandidates.h", "filename": "TaskClearFixedPrimalCandidates.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskClearFixedPrimalCandidates.h", "brief": "Clear pending fixed-integer NLP candidates\n\nResets candidate queue between iterations.\n\n**TaskClearFixedPrimalCandidates Class:**\n- run(): Empty the fixed-NLP candidate list\n\n**Usage:**\n- Called at start of each iteration\n- Ensures fresh candidate selection each cycle", "see": ["TaskSelectPrimalFixedNLPPointsFromSolutionPool.h for selection", "TaskSelectPrimalCandidatesFromNLP.h for NLP solving"], "has_pass2": false}, "src/Tasks/TaskAddPrimalReductionCut.h": {"path": "layer-4/SHOT/src/Tasks/TaskAddPrimalReductionCut.h", "filename": "TaskAddPrimalReductionCut.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskAddPrimalReductionCut.h", "brief": "Add objective cutoff based on incumbent\n\nUses primal bound to tighten MIP relaxation.\n\n**TaskAddPrimalReductionCut Class:**\n- currentLowerBoundForReductionCut: Current cutoff value\n- run(): Update objective bound constraint\n\n**Cutoff Strategy:**\n- Adds constraint: objective <= bestPrimal - epsilon\n- Forces MIP to find improving solutions only\n- Updated when primal bound improves", "see": ["Results.h for primal bound tracking", "TaskCheckMaxNumberOfPrimalReductionCuts.h for limit"], "has_pass2": false}, "src/Tasks/TaskCheckDualStagnation.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckDualStagnation.h", "filename": "TaskCheckDualStagnation.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckDualStagnation.h", "brief": "Detect stalled dual bound improvement\n\nTriggers countermeasures when dual bound stops improving.\n\n**TaskCheckDualStagnation Class:**\n- taskIDIfTrue: Jump target when stagnation detected\n- run(): Check if dual bound unchanged for N iterations\n\n**Stagnation Recovery:**\n- May trigger stronger cuts or reformulation\n- Indicates outer approximation isn't tightening", "see": ["TaskCheckPrimalStagnation.h for primal stagnation", "DualSolver.h for dual bound computation"], "has_pass2": false}, "src/Tasks/TaskCheckConstraintTolerance.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckConstraintTolerance.h", "filename": "TaskCheckConstraintTolerance.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckConstraintTolerance.h", "brief": "Termination check for constraint feasibility\n\nVerifies primal solution satisfies all constraints.\n\n**TaskCheckConstraintTolerance Class:**\n- taskIDIfTrue: Jump target when feasible\n- run(): Check max constraint violation < ConstraintTolerance\n\n**Usage:**\n- Validates feasibility of best primal solution\n- Required for declaring optimality", "see": ["Results.h for constraint violation tracking", "TaskCheckAbsoluteGap.h for gap check"], "has_pass2": false}, "src/Tasks/TaskInitializeRootsearch.h": {"path": "layer-4/SHOT/src/Tasks/TaskInitializeRootsearch.h", "filename": "TaskInitializeRootsearch.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskInitializeRootsearch.h", "brief": "Setup rootsearch method for ESH\n\nOne-time initialization of boundary finding algorithm.\n\n**TaskInitializeRootsearch Class:**\n- run(): Create and configure rootsearch method\n\n**Rootsearch Setup:**\n- Select algorithm (TOMS 748, bisection)\n- Configure tolerances and iteration limits\n- Required for ESH hyperplane generation", "see": ["RootsearchMethod/ for algorithm implementations", "TaskSelectHyperplanePointsESH.h for usage"], "has_pass2": false}, "src/Tasks/TaskSequential.h": {"path": "layer-4/SHOT/src/Tasks/TaskSequential.h", "filename": "TaskSequential.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSequential.h", "brief": "Container task that runs subtasks in sequence\n\nComposite pattern for building task workflows.\n\n**TaskSequential Class:**\n- addTask()/addTasks(): Build task list\n- run(): Execute all subtasks in order\n\n**Usage in SHOT:**\n- Main iteration loop as sequential task chain\n- Groups related operations (all termination checks)", "see": ["TaskBase.h for task interface", "TaskConditional.h for branching logic"], "has_pass2": false}, "src/Tasks/TaskInitializeDualSolver.h": {"path": "layer-4/SHOT/src/Tasks/TaskInitializeDualSolver.h", "filename": "TaskInitializeDualSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskInitializeDualSolver.h", "brief": "Configure and create MIP solver instance\n\nOne-time setup for MIP solver before main loop.\n\n**TaskInitializeDualSolver Class:**\n- useLazyStrategy: Single-tree (true) or multi-tree (false)\n- run(): Create appropriate MIP solver type\n\n**Solver Selection:**\n- CPLEX: MIPSolverCplex or MIPSolverCplexSingleTree\n- Gurobi: MIPSolverGurobi or MIPSolverGurobiSingleTree\n- CBC: MIPSolverCbc (multi-tree only)", "see": ["DualSolver.h for solver management", "MIPSolver/ for solver implementations"], "has_pass2": false}, "src/Tasks/TaskExecuteRelaxationStrategy.h": {"path": "layer-4/SHOT/src/Tasks/TaskExecuteRelaxationStrategy.h", "filename": "TaskExecuteRelaxationStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskExecuteRelaxationStrategy.h", "brief": "Execute LP/MIP relaxation strategy step\n\nControls transition from LP to MIP solving.\n\n**TaskExecuteRelaxationStrategy Class:**\n- run(): Invoke current relaxation strategy\n\n**Strategy Control:**\n- Calls IRelaxationStrategy to decide LP vs MIP\n- May enable/disable integer constraints\n- Affects DualSolver solve behavior", "see": ["IRelaxationStrategy.h for strategy interface", "RelaxationStrategyStandard.h for default strategy"], "has_pass2": false}, "src/Tasks/TaskConditional.h": {"path": "layer-4/SHOT/src/Tasks/TaskConditional.h", "filename": "TaskConditional.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskConditional.h", "brief": "Branching task based on runtime condition\n\nIf-then-else logic for task workflows.\n\n**TaskConditional Class:**\n- setCondition(fn): Boolean predicate to evaluate\n- setTaskIfTrue/False(): Branch targets\n- run(): Evaluate condition and execute appropriate branch\n\n**Usage in SHOT:**\n- Branch on problem type (MINLP vs NLP)\n- Skip tasks based on solver state", "see": ["TaskBase.h for task interface", "TaskSequential.h for sequential execution"], "has_pass2": false}, "src/Tasks/TaskCalculateSolutionChangeNorm.h": {"path": "layer-4/SHOT/src/Tasks/TaskCalculateSolutionChangeNorm.h", "filename": "TaskCalculateSolutionChangeNorm.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCalculateSolutionChangeNorm.h", "brief": "Compute solution movement between iterations\n\nTracks convergence via solution change magnitude.\n\n**TaskCalculateSolutionChangeNorm Class:**\n- run(): Compute ||x_new - x_old|| norm\n\n**Usage:**\n- Detect solution stagnation\n- Used in convergence criteria\n- May trigger algorithm strategy changes", "see": ["Utilities.h for L2Norm computation", "TaskCheckDualStagnation.h for stagnation handling"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalCandidatesFromRootsearch.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalCandidatesFromRootsearch.h", "filename": "TaskSelectPrimalCandidatesFromRootsearch.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalCandidatesFromRootsearch.h", "brief": "Extract primal candidates from ESH rootsearch\n\nByproduct of boundary finding: feasible points near boundary.\n\n**TaskSelectPrimalCandidatesFromRootsearch Class:**\n- run(): Check rootsearch intermediate points\n- run(solPoints): Process specific solution points\n\n**Rootsearch Byproduct:**\n- ESH rootsearch traverses from infeasible to interior\n- Intermediate points may be feasible primal candidates\n- Cheap way to discover feasible solutions", "see": ["RootsearchMethod/ for boundary finding", "TaskSelectHyperplanePointsESH.h for ESH main task"], "has_pass2": false}, "src/Tasks/TaskSelectPrimalFixedNLPPointsFromSolutionPool.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectPrimalFixedNLPPointsFromSolutionPool.h", "filename": "TaskSelectPrimalFixedNLPPointsFromSolutionPool.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectPrimalFixedNLPPointsFromSolutionPool.h", "brief": "Select MIP solutions for fixed-integer NLP solving\n\nChooses promising candidates for NLP refinement.\n\n**TaskSelectPrimalFixedNLPPointsFromSolutionPool Class:**\n- run(): Select best candidates from solution pool\n\n**Selection Criteria:**\n- Prioritize solutions close to feasibility\n- Avoid already-tested integer assignments\n- Respect NLP solve budget", "see": ["TaskSelectPrimalCandidatesFromNLP.h for NLP solving", "DualSolver.h for solution pool access"], "has_pass2": false}, "src/Tasks/TaskAddHyperplanes.h": {"path": "layer-4/SHOT/src/Tasks/TaskAddHyperplanes.h", "filename": "TaskAddHyperplanes.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskAddHyperplanes.h", "brief": "Add generated hyperplanes to MIP relaxation\n\nTransfers cuts from hyperplane pool to MIP solver.\n\n**TaskAddHyperplanes Class:**\n- run(): Add pending hyperplanes to dual problem\n- itersWithoutAddedHPs: Track stagnation\n\n**Cut Management:**\n- Filters duplicate/dominated cuts\n- Respects cut limit per iteration\n- May trigger lazy vs pool constraint handling", "see": ["DualSolver.h for MIP constraint addition", "TaskSelectHyperplanePointsESH.h for cut generation"], "has_pass2": false}, "src/Tasks/TaskUpdateInteriorPoint.h": {"path": "layer-4/SHOT/src/Tasks/TaskUpdateInteriorPoint.h", "filename": "TaskUpdateInteriorPoint.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskUpdateInteriorPoint.h", "brief": "Update interior point during optimization\n\nImproves ESH quality by updating interior point.\n\n**TaskUpdateInteriorPoint Class:**\n- run(): Recompute or update interior point\n\n**Update Strategy:**\n- Use new primal solutions as interior candidates\n- Average with existing interior point\n- Improves cut quality as algorithm progresses", "see": ["TaskFindInteriorPoint.h for initial computation", "TaskSelectHyperplanePointsESH.h for usage"], "has_pass2": false}, "src/Tasks/TaskGoto.h": {"path": "layer-4/SHOT/src/Tasks/TaskGoto.h", "filename": "TaskGoto.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskGoto.h", "brief": "Jump to labeled task in workflow\n\nEnables non-linear control flow in task sequences.\n\n**TaskGoto Class:**\n- gotoTaskID: Target task identifier\n- run(): Signal TaskHandler to jump to target\n\n**Usage in SHOT:**\n- Loop back to iteration start\n- Jump to termination on error", "see": ["TaskHandler.h for jump resolution", "TaskTerminate.h for exit handling"], "has_pass2": false}, "src/Tasks/TaskCheckTimeLimit.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckTimeLimit.h", "filename": "TaskCheckTimeLimit.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckTimeLimit.h", "brief": "Termination check for wall-clock time limit\n\nPart of termination check sequence.\n\n**TaskCheckTimeLimit Class:**\n- taskIDIfTrue: Jump target when limit exceeded\n- run(): Compare elapsed vs Settings.TimeLimit\n\n**Behavior:**\n- If exceeded: Jump to termination task\n- If not: Continue to next task", "see": ["Timer.h for elapsed time tracking", "TaskTerminate.h for termination handling"], "has_pass2": false}, "src/Tasks/TaskException.h": {"path": "layer-4/SHOT/src/Tasks/TaskException.h", "filename": "TaskException.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskException.h", "brief": "Exception types for task framework errors\n\nSpecialized exceptions for task-related failures.\n\n**Exception Classes:**\n- TaskException: General task error\n- TaskExceptionFunctionNotDefined: Missing run() implementation\n- TaskExceptionNotFound: Invalid task ID in goto/jump\n\n**Usage:**\n- Thrown by TaskHandler and individual tasks\n- Caught and handled in main solve loop", "see": ["TaskHandler.h for exception handling", "TaskGoto.h for task lookups"], "has_pass2": false}, "src/Tasks/TaskCheckRelativeGap.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckRelativeGap.h", "filename": "TaskCheckRelativeGap.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckRelativeGap.h", "brief": "Termination check for relative optimality gap\n\nTerminates when |primal - dual| / |primal| < tolerance.\n\n**TaskCheckRelativeGap Class:**\n- taskIDIfTrue: Jump target when gap closed\n- run(): Check (|bestPrimal - bestDual| / max(1, |bestPrimal|)) < RelativeGapTol\n\n**Usage:**\n- More common than absolute gap for practical problems\n- Scales with solution magnitude", "see": ["TaskCheckAbsoluteGap.h for absolute gap check", "Results.h for bound storage"], "has_pass2": false}, "src/Tasks/TaskAddIntegerCuts.h": {"path": "layer-4/SHOT/src/Tasks/TaskAddIntegerCuts.h", "filename": "TaskAddIntegerCuts.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskAddIntegerCuts.h", "brief": "Add integer cuts to exclude visited solutions\n\nPrevents revisiting same integer assignment.\n\n**TaskAddIntegerCuts Class:**\n- run(): Generate cut excluding current integer solution\n\n**Integer Cut Types:**\n- No-good cuts: Exclude exact binary assignment\n- Local branching: Limit Hamming distance\n\n**Usage:**\n- Multi-tree strategy uses these extensively\n- Prevents cycling through same MIP solutions", "see": ["DualSolver.h for cut addition", "SolutionStrategy/ for multi-tree workflow"], "has_pass2": false}, "src/Tasks/TaskCheckUserTermination.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckUserTermination.h", "filename": "TaskCheckUserTermination.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckUserTermination.h", "brief": "Check for external termination request\n\nAllows user to interrupt solver gracefully.\n\n**TaskCheckUserTermination Class:**\n- taskIDIfTrue: Jump target when user requests stop\n- run(): Check for CTRL+C, callback termination, etc.\n\n**Usage:**\n- Called every iteration to check for interrupts\n- Returns best solution found so far", "see": ["EventHandler.h for termination callbacks", "TaskTerminate.h for graceful shutdown"], "has_pass2": false}, "src/Tasks/TaskSelectHyperplanePointsESH.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectHyperplanePointsESH.h", "filename": "TaskSelectHyperplanePointsESH.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectHyperplanePointsESH.h", "brief": "Extended Supporting Hyperplane point selection\n\nCore ESH algorithm: finds boundary points for hyperplane generation.\n\n**TaskSelectHyperplanePointsESH Class:**\n- run(): Process current MIP solutions\n- run(solPoints): Process specific solution points\n\n**ESH Algorithm:**\n1. Take infeasible MIP solution point\n2. Use rootsearch to find boundary with feasible region\n3. Generate supporting hyperplane at boundary\n\n**Extends ECP:**\n- ESH uses interior point + rootsearch\n- ECP generates cuts at infeasible point directly", "algorithm": "Extended Supporting Hyperplane (Kronqvist 2016)", "see": ["RootsearchMethod/ for boundary finding", "TaskAddHyperplanes.h for cut addition"], "has_pass2": true}, "src/Tasks/TaskCheckPrimalStagnation.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckPrimalStagnation.h", "filename": "TaskCheckPrimalStagnation.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckPrimalStagnation.h", "brief": "Detect stalled primal bound improvement\n\nTriggers NLP solves when primal bound stops improving.\n\n**TaskCheckPrimalStagnation Class:**\n- taskIDIfTrue/False: Branch targets for stagnation state\n- run(): Check if primal bound unchanged for N iterations\n\n**Stagnation Recovery:**\n- May trigger fixed-integer NLP to find better solutions\n- Indicates MIP solutions aren't improving primal", "see": ["TaskCheckDualStagnation.h for dual stagnation", "PrimalSolver.h for primal bound computation"], "has_pass2": false}, "src/Tasks/TaskBase.h": {"path": "layer-4/SHOT/src/Tasks/TaskBase.h", "filename": "TaskBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskBase.h", "brief": "Base class for modular algorithm tasks\n\nFoundation for task-based algorithm composition.\n\n**TaskBase Class:**\n- isActive(), activate(), deactivate(): Enable/disable tasks\n- initialize(): Setup before first run\n- run(): Execute the task logic\n- getType(): Task identifier for debugging\n\n**Task Categories:**\n- Termination: TimeLimit, IterationLimit, Gap checks\n- Hyperplane: SelectHyperplanePointsESH/ECP, AddHyperplanes\n- Primal: SelectPrimalCandidates, NLP solving\n- Dual: SolveIteration, CreateDualProblem\n- Control: Sequential, Conditional, Goto\n\n**Task Composition:**\n- TaskSequential: Run tasks in order\n- TaskConditional: Branch on conditions\n- TaskGoto: Jump to labeled task", "see": ["TaskHandler.h for task scheduling", "SolutionStrategy/ for task workflow definition"], "has_pass2": false}, "src/Tasks/TaskExecuteSolutionLimitStrategy.h": {"path": "layer-4/SHOT/src/Tasks/TaskExecuteSolutionLimitStrategy.h", "filename": "TaskExecuteSolutionLimitStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskExecuteSolutionLimitStrategy.h", "brief": "Control MIP solution pool size dynamically\n\nAdjusts how many MIP solutions are collected per iteration.\n\n**TaskExecuteSolutionLimitStrategy Class:**\n- solutionLimitStrategy: Strategy implementation\n- previousSolLimit: Track limit changes\n- run(): Apply current solution limit\n\n**Strategy Control:**\n- Start with small limit for fast iterations\n- Increase when MIP optimal to explore pool\n- Balance speed vs solution diversity", "see": ["IMIPSolutionLimitStrategy.h for strategy interface", "MIPSolutionLimitStrategyIncrease.h for default strategy"], "has_pass2": false}, "src/Tasks/TaskSolveIteration.h": {"path": "layer-4/SHOT/src/Tasks/TaskSolveIteration.h", "filename": "TaskSolveIteration.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSolveIteration.h", "brief": "Solve MIP relaxation for current iteration\n\nCore MIP solve step in outer approximation loop.\n\n**TaskSolveIteration Class:**\n- run(): Call DualSolver to solve MIP with current cuts\n\n**Iteration Flow:**\n1. MIP has linear approximation of nonlinear constraints\n2. Solve gives candidate solution + dual bound\n3. Candidate checked for feasibility\n4. New cuts generated at infeasible points", "see": ["DualSolver.h for MIP solve call", "TaskSelectHyperplanePointsESH.h for cut generation"], "has_pass2": false}, "src/Tasks/TaskSelectHyperplanePointsECP.h": {"path": "layer-4/SHOT/src/Tasks/TaskSelectHyperplanePointsECP.h", "filename": "TaskSelectHyperplanePointsECP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskSelectHyperplanePointsECP.h", "brief": "Extended Cutting Plane point selection\n\nClassic outer approximation: linearize at infeasible points.\n\n**TaskSelectHyperplanePointsECP Class:**\n- run(): Process current MIP solutions\n- run(solPoints): Process specific solution points\n\n**ECP Algorithm:**\n- Generate gradient cut at infeasible solution\n- Simpler than ESH but may converge slower\n- Used as fallback when ESH rootsearch fails", "algorithm": "Extended Cutting Plane (Westerlund-Pettersson)", "see": ["TaskSelectHyperplanePointsESH.h for ESH variant", "TaskAddHyperplanes.h for cut addition"], "has_pass2": true}, "src/Tasks/TaskFindInteriorPoint.h": {"path": "layer-4/SHOT/src/Tasks/TaskFindInteriorPoint.h", "filename": "TaskFindInteriorPoint.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskFindInteriorPoint.h", "brief": "Find strictly feasible interior point for ESH\n\nCritical for ESH: rootsearch needs interior point endpoint.\n\n**TaskFindInteriorPoint Class:**\n- NLPSolvers: Solvers for finding interior\n- run(): Attempt to find strictly feasible point\n\n**Interior Point Methods:**\n- Solve feasibility problem with slack maximization\n- Use NLP solver (Ipopt) with modified objective\n- Required for ESH rootsearch to work", "see": ["TaskUpdateInteriorPoint.h for runtime updates", "RootsearchMethod/ for interior point usage"], "has_pass2": false}, "src/Tasks/TaskReformulateProblem.h": {"path": "layer-4/SHOT/src/Tasks/TaskReformulateProblem.h", "filename": "TaskReformulateProblem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskReformulateProblem.h", "brief": "Transform problem for efficient outer approximation\n\nKey preprocessing: converts problem to reformulated form.\n\n**TaskReformulateProblem Class:**\n- reformulateObjectiveFunction(): Handle nonlinear objective\n- reformulateConstraint(): Process each constraint type\n- createEpigraphConstraint(): Epigraph reformulation\n\n**Reformulation Techniques:**\n- Bilinear terms: McCormick envelopes (addBilinearMcCormickEnvelope)\n- Integer products: Binary expansion\n- Eigenvalue decomposition: Convexify quadratics\n- Term extraction: Separate linear/quadratic/monomial/signomial\n\n**Auxiliary Variables:**\n- Created for bilinear products, squares, absolute values\n- Tracked in squareAuxVariables, bilinearAuxVariables maps", "see": ["AuxiliaryVariables.h for auxiliary variable types", "Problem.h for reformulated problem storage"], "has_pass2": false}, "src/Tasks/TaskCheckMaxNumberOfPrimalReductionCuts.h": {"path": "layer-4/SHOT/src/Tasks/TaskCheckMaxNumberOfPrimalReductionCuts.h", "filename": "TaskCheckMaxNumberOfPrimalReductionCuts.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskCheckMaxNumberOfPrimalReductionCuts.h", "brief": "Limit number of objective cutoff updates\n\nPrevents excessive cutoff constraint modifications.\n\n**TaskCheckMaxNumberOfPrimalReductionCuts Class:**\n- taskIDIfTrue: Jump target when limit reached\n- run(): Check cutoff update count\n\n**Usage:**\n- Limit objective bound updates per iteration\n- Avoid MIP solver overhead from repeated bound changes", "see": ["TaskAddPrimalReductionCut.h for cutoff addition", "Settings.h for limit configuration"], "has_pass2": false}, "src/Tasks/TaskRepairInfeasibleDualProblem.h": {"path": "layer-4/SHOT/src/Tasks/TaskRepairInfeasibleDualProblem.h", "filename": "TaskRepairInfeasibleDualProblem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Tasks/TaskRepairInfeasibleDualProblem.h", "brief": "Recover from infeasible MIP relaxation\n\nAttempts to restore feasibility when cuts cause infeasibility.\n\n**TaskRepairInfeasibleDualProblem Class:**\n- mainRepairTries, totRepairTries: Repair attempt counters\n- run(): Attempt repair strategies\n\n**Repair Strategies:**\n- Remove recently added cuts\n- Relax cut coefficients\n- Remove dominated cuts\n\n**When Called:**\n- MIP solver returns infeasible\n- May indicate numerical issues with cuts", "see": ["DualSolver.h for MIP status", "TaskAddHyperplanes.h for cut management"], "has_pass2": false}, "src/Model/NonlinearExpressions.h": {"path": "layer-4/SHOT/src/Model/NonlinearExpressions.h", "filename": "NonlinearExpressions.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @note The monotonicity and convexity identification is strongly influenced by that of Suspect\n   (https://github.com/cog-imperial/suspect) by Francesco Ceccon, Imperial College London\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/NonlinearExpressions.h", "brief": "Expression tree nodes for nonlinear functions\n\nDAG-based representation of nonlinear expressions with convexity analysis.\n\n**E_NonlinearExpressionTypes:**\n- Constant, Variable: Leaf nodes\n- Unary: Negate, Invert, SquareRoot, Log, Exp, Square, Sin, Cos, etc.\n- Binary: Divide, Power\n- N-ary: Sum, Product\n\n**NonlinearExpression Base Class:**\n- calculate(point): Evaluate at numeric point\n- calculate(intervals): Interval arithmetic evaluation\n- getFactorableFunction(): CppAD automatic differentiation\n- getConvexity(): Analyze convexity via composition rules\n- getMonotonicity(): Analyze monotonicity\n- tightenBounds(): FBBT bound propagation\n\n**Expression Subclasses:**\n- ExpressionConstant, ExpressionVariable\n- ExpressionNegate, ExpressionSquare, ExpressionSquareRoot\n- ExpressionLog, ExpressionExp\n- ExpressionSin, ExpressionCos\n- ExpressionSum, ExpressionProduct\n- ExpressionDivide, ExpressionPower\n\n**Convexity Rules:**\n- Based on composition theorems (e.g., convex ∘ nondecreasing convex = convex)\n- Interval bounds inform sign-dependent rules\n\n@note Convexity analysis influenced by SUSPECT library", "see": ["Constraints.h for expressions in constraints"], "has_pass2": false}, "src/Model/AuxiliaryVariables.h": {"path": "layer-4/SHOT/src/Model/AuxiliaryVariables.h", "filename": "AuxiliaryVariables.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/AuxiliaryVariables.h", "brief": "Variables introduced during problem reformulation\n\nVariables created to linearize or convexify expressions.\n\n**AuxiliaryVariable Class:**\n- Extends Variable with expression components\n- calculate(): Evaluate auxiliary from original variables\n- Properties: constant, linear/quadratic/monomial/signomial terms\n\n**Auxiliary Variable Types (E_AuxiliaryVariableType):**\n- NonlinearObjectiveFunction: Epigraph reformulation\n- BilinearTerms: McCormick relaxation variables\n- MonomialPartitioning: Product term linearization\n- EigenvalueDecomposition: Convexification variables\n\n**AuxiliaryVariables Collection:**\n- Vector of AuxiliaryVariablePtr with Problem ownership\n- sortByIndex(): Order by variable index", "see": ["Variables.h for base Variable class", "Problem.h for reformulated problem usage"], "has_pass2": false}, "src/Model/Constraints.h": {"path": "layer-4/SHOT/src/Model/Constraints.h", "filename": "Constraints.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Constraints.h", "brief": "Constraint hierarchy: linear, quadratic, and nonlinear\n\nConstraint classes forming an inheritance hierarchy.\n\n**Constraint Properties:**\n- classification: Linear, Quadratic, Signomial, Nonlinear\n- convexity: Linear, Convex, Concave, Nonconvex, Unknown\n- type: Equality, LessThan, GreaterThan\n\n**NumericConstraintValue Struct:**\n- functionValue: f(x) at evaluation point\n- normalizedLHSValue: L - f(x), normalizedRHSValue: f(x) - U\n- error: max(0, max(L - f(x), f(x) - U))\n\n**Constraint Class Hierarchy:**\n- Constraint (abstract): Base with index, name, properties\n- NumericConstraint: LHS/RHS bounds, gradient/Hessian computation\n- LinearConstraint: linearTerms, sparse gradient\n- QuadraticConstraint: + quadraticTerms, Hessian\n- NonlinearConstraint: + monomials, signomials, nonlinearExpression\n\n**Derivative Computation:**\n- calculateGradient(): Sparse first derivatives\n- calculateHessian(): Upper triangular second derivatives\n- Sparsity patterns for efficient NLP solver interfaces", "see": ["Terms.h for LinearTerm, QuadraticTerm definitions", "NonlinearExpressions.h for expression tree evaluation"], "has_pass2": false}, "src/Model/Problem.h": {"path": "layer-4/SHOT/src/Model/Problem.h", "filename": "Problem.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Problem.h", "brief": "Core problem representation with variables, constraints, and objective\n\nCentral data structure holding the optimization problem definition.\n\n**ProblemProperties Struct:**\n- Convexity classification (Convex, Nonconvex, NotSet)\n- Problem type flags (MINLP, MIQP, MILP, NLP, etc.)\n- Variable counts by type (real, binary, integer, auxiliary)\n- Constraint counts by type (linear, quadratic, nonlinear)\n\n**SpecialOrderedSet Struct:**\n- SOS1 (at most one variable nonzero) or SOS2 (contiguous nonzeros)\n- Variables and optional weights\n\n**Problem Class:**\n- allVariables, realVariables, binaryVariables, etc.\n- linearConstraints, quadraticConstraints, nonlinearConstraints\n- objectiveFunction (linear, quadratic, or nonlinear)\n- Sparsity patterns for Jacobian and Hessian\n- Feasibility bound propagation (FBBT) for tightening bounds\n\n**Key Methods:**\n- add(): Add variables, constraints, objective\n- finalize(): Compute properties and sparsity patterns\n- getMostDeviatingNumericConstraint(): Find worst violation\n- createCopy(): Clone for reformulation", "see": ["Solver.h for problem loading via setProblem()", "ReformulatedProblem for auxiliary variable introduction"], "algorithm": "Sparsity Pattern Extraction (getConstraintsJacobianSparsityPattern):\n  Identifies nonzero structure for efficient derivative computation:", "math": "Jacobian ∂g_i/∂x_j: sparse pattern for AD evaluation\n  Used by NLP solvers (Ipopt) and for hyperplane construction.", "complexity": "O(nnz) where nnz = number of variable appearances in constraints", "ref": ["Belotti et al. (2009) - Branching and bounds tightening techniques", "ESH (Extended Supporting Hyperplane) uses this for cut generation", "McCormick (1976) - Convex relaxations via factorable functions"], "has_pass2": true}, "src/Model/Variables.h": {"path": "layer-4/SHOT/src/Model/Variables.h", "filename": "Variables.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Variables.h", "brief": "Decision variable representation with bounds and types\n\nVariable classes for MINLP problem modeling.\n\n**VariableProperties Struct:**\n- type: Real, Binary, Integer, Semicontinuous, Semiinteger\n- auxiliaryType: Tracks origin of reformulation variables\n- Context flags: inObjectiveFunction, inLinearConstraints, etc.\n- Bound tightening status\n\n**Variable Class:**\n- index, name: Problem identifier\n- lowerBound, upperBound: Variable domain\n- semiBound: For semicontinuous/semiinteger types\n- calculate(): Evaluate at a point\n- getBound(): Interval arithmetic bounds\n- tightenBounds(): FBBT bound update\n\n**Variables Collection:**\n- Vector wrapper with ownership tracking\n- sortByIndex(): Reorder for solver compatibility\n\n**Interval Arithmetic (CppAD):**\n- Uses mc::Interval for bound propagation\n- FactorableFunction for automatic differentiation", "see": ["Constraints.h for variable usage in constraints", "AuxiliaryVariables.h for reformulation-generated variables"], "has_pass2": false}, "src/Model/ObjectiveFunction.h": {"path": "layer-4/SHOT/src/Model/ObjectiveFunction.h", "filename": "ObjectiveFunction.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/ObjectiveFunction.h", "brief": "Objective function hierarchy with derivative computation\n\nObjective function classes mirroring the constraint hierarchy.\n\n**ObjectiveFunctionProperties:**\n- direction: Minimize or Maximize\n- classification: Linear, Quadratic, Signomial, Nonlinear\n- convexity: Convex, Concave, Linear, Nonconvex\n- Term presence flags for structure detection\n\n**ObjectiveFunction Base:**\n- calculateValue(): Evaluate at point or interval\n- calculateGradient(), calculateHessian(): Derivatives\n- getGradientSparsityPattern(): Nonzero structure\n\n**LinearObjectiveFunction:**\n- linearTerms + constant\n- isDualUnbounded(): Unboundedness detection\n\n**QuadraticObjectiveFunction:**\n- + quadraticTerms\n- Inherits from LinearObjectiveFunction\n\n**NonlinearObjectiveFunction:**\n- + monomialTerms, signomialTerms, nonlinearExpression\n- factorableFunction: CppAD automatic differentiation\n- Inherits from QuadraticObjectiveFunction\n\n@note For minimization with nonlinear objective, SHOT uses\n      epigraph reformulation: min t s.t. f(x) <= t", "see": ["DualSolver.h for objective cut generation"], "has_pass2": false}, "src/Model/Simplifications.h": {"path": "layer-4/SHOT/src/Model/Simplifications.h", "filename": "Simplifications.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Simplifications.h", "brief": "Expression tree simplification and term extraction\n\nAlgebraic simplification and structured term extraction.\n\n**Simplification Functions:**\n- simplify(): Recursive expression simplification\n- simplifyExpression(ExprType): Type-specific handlers\n- Constant folding, negation cancellation, identity removal\n\n**Term Extraction:**\n- extractTermsAndConstant(): Decompose expression into:\n  LinearTerms, QuadraticTerms, MonomialTerms, SignomialTerms,\n  NonlinearExpression remainder, and constant\n\n**Conversion Functions:**\n- convertProductToLinearTerm/QuadraticTerm/MonomialTerm()\n- convertPowerToLinearTerm/QuadraticTerm()\n- convertToSignomialTerm(): Extract signomial structure\n\n**Use Case:**\n- Problem reformulation for better convexity analysis\n- Extract quadratics for QCQP solvers\n- Identify linear/quadratic substructure in NLP", "see": ["NonlinearExpressions.h for expression tree nodes"], "has_pass2": false}, "src/Model/Terms.h": {"path": "layer-4/SHOT/src/Model/Terms.h", "filename": "Terms.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "Model/Terms.h", "brief": "Term hierarchy for linear, quadratic, monomial, and signomial expressions\n\nBuilding blocks for constraint and objective function representation.\n\n**Term Base Class:**\n- coefficient: Scaling factor\n- calculate(): Evaluate at point or interval\n- getConvexity(): Linear, Convex, Concave, Nonconvex\n- getMonotonicity(): Nondecreasing, Nonincreasing, Constant\n\n**LinearTerm:** coeff * x\n- Always linear convexity\n- Monotonicity depends on coefficient sign\n\n**QuadraticTerm:** coeff * x1 * x2\n- isSquare: x1 == x2 (convex if coeff > 0)\n- isBilinear: x1 != x2 (nonconvex)\n- isBinary, isInteger: Type detection for reformulation\n\n**QuadraticTerms Collection:**\n- Eigenvalue analysis via Eigen for convexity\n- allSquares, allPositive flags for structure detection\n\n**MonomialTerm:** coeff * x1 * x2 * ... * xn\n- isBinary: All variables binary (linearizable)\n- Generally nonconvex\n\n**SignomialTerm:** coeff * x1^p1 * x2^p2 * ... * xn^pn\n- SignomialElement: (variable, power) pair\n- Convexity rules based on power signs and sums\n\n**Gradient/Hessian Methods:**\n- calculateGradient(): Sparse first derivatives\n- calculateHessian(): Upper triangular second derivatives", "see": ["Constraints.h, ObjectiveFunction.h for term usage"], "has_pass2": false}, "src/SolutionStrategy/ISolutionStrategy.h": {"path": "layer-4/SHOT/src/SolutionStrategy/ISolutionStrategy.h", "filename": "ISolutionStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/ISolutionStrategy.h", "brief": "Abstract interface for solution strategies\n\nBase interface for algorithm selection.\n\n**Strategy Interface:**\n- initializeStrategy(): Configure tasks and solvers\n- solveProblem(): Execute the solution algorithm\n\n**Strategy Implementations:**\n- SolutionStrategyMultiTree: Iterative outer approximation\n  - Solve MIP, add cuts, repeat\n- SolutionStrategySingleTree: Lazy constraint callback\n  - Single MIP with callback cut injection\n- SolutionStrategyNLP: Pure NLP (no integer variables)\n- SolutionStrategyMIQP/MIQCQP: Direct quadratic solving\n\n**Selection Logic:**\n- Problem classification determines best strategy\n- Convex MINLP → ESH (multi-tree or single-tree)\n- QP/QCQP → Direct solver if supported\n- NLP → Interior point only", "see": ["TaskHandler.h for task execution flow", "Solver.h for strategy selection"], "has_pass2": false}, "src/SolutionStrategy/SolutionStrategyMIQCQP.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategyMIQCQP.h", "filename": "SolutionStrategyMIQCQP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategyMIQCQP.h", "brief": "Direct MIQCQP solver for convex quadratic problems\n\nBypasses ESH for problems solvable by CPLEX/Gurobi MIQCQP.\n\n**SolutionStrategyMIQCQP Class:**\n- initializeStrategy(): Configure for direct MIQCQP solve\n- solveProblem(): Single solver call, no outer approximation\n\n**Use Case:**\n- Convex MIQCQP (quadratic constraints, convex)\n- CPLEX and Gurobi support convex QCQP natively\n- Faster than iterative linearization for small problems\n\n**Problem Classification:**\n- All constraints must be convex quadratic\n- Solver must support QCQP (supportsQuadraticConstraints)", "algorithm": "Direct MIQCQP branch-and-bound", "see": ["SolutionStrategyMultiTree.h for general MINLP"], "has_pass2": true}, "src/SolutionStrategy/SolutionStrategySingleTree.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategySingleTree.h", "filename": "SolutionStrategySingleTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategySingleTree.h", "brief": "Lazy constraint callback strategy (single-tree)\n\nCallback-based outer approximation within single B&B tree.\n\n**Algorithm Pattern:**\n1. Solve MIP with lazy constraint callback\n2. Callback checks integer solutions for feasibility\n3. Generate hyperplanes for violated constraints\n4. Add as lazy constraints, solver continues\n\n**Task Flow (initializeStrategy):**\n- CreateDualProblem → ConfigureCallback → SolveProblem\n- (cuts added via callback during solve)\n\n**Advantages:**\n- Single B&B tree exploration\n- Potentially fewer node evaluations\n- Better integration with MIP solver\n\n**Disadvantages:**\n- Requires solver callback support\n- Limited to CPLEX/Gurobi\n- More complex debugging", "algorithm": "Branch-and-cut with lazy ESH constraints", "see": ["MIPSolverCplexSingleTree, MIPSolverGurobiSingleTree"], "has_pass2": true}, "src/SolutionStrategy/SolutionStrategyMultiTree.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategyMultiTree.h", "filename": "SolutionStrategyMultiTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategyMultiTree.h", "brief": "Iterative outer approximation strategy (multi-tree)\n\nClassic outer approximation loop for convex MINLP.\n\n**Algorithm Pattern:**\n1. Solve MIP relaxation to get candidate point\n2. Generate supporting hyperplanes at violated points\n3. Add cuts to MIP and resolve\n4. Repeat until convergence or termination\n\n**Task Flow (initializeStrategy):**\n- CreateDualProblem → SolveIteration → SelectHyperplanes\n- AddHyperplanes → CheckTermination → loop\n\n**Advantages:**\n- No callback complexity\n- Can use any MIP solver\n- Easier debugging/logging\n\n**Disadvantages:**\n- Multiple MIP solves\n- May regenerate same B&B tree work", "algorithm": "Standard ESH outer approximation", "see": ["SolutionStrategySingleTree for callback-based variant"], "has_pass2": true}, "src/SolutionStrategy/SolutionStrategyNLP.h": {"path": "layer-4/SHOT/src/SolutionStrategy/SolutionStrategyNLP.h", "filename": "SolutionStrategyNLP.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "SolutionStrategy/SolutionStrategyNLP.h", "brief": "Direct NLP solver for continuous problems\n\nBypasses ESH for purely continuous NLP problems.\n\n**SolutionStrategyNLP Class:**\n- initializeStrategy(): Configure for direct NLP solve\n- solveProblem(): Single NLP solver call (Ipopt)\n\n**Use Case:**\n- Problems with no integer variables\n- Convex NLP where outer approximation is unnecessary\n- Falls back to standard NLP solvers (Ipopt)\n\n**When Selected:**\n- Problem type is NLP (no discrete variables)\n- Simpler than ESH for continuous problems", "algorithm": "Direct interior point NLP solve", "see": ["NLPSolver/NLPSolverIpoptBase.h for Ipopt wrapper"], "has_pass2": true}, "src/RootsearchMethod/IRootsearchMethod.h": {"path": "layer-4/SHOT/src/RootsearchMethod/IRootsearchMethod.h", "filename": "IRootsearchMethod.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "RootsearchMethod/IRootsearchMethod.h", "brief": "Interface for constraint boundary rootsearch algorithms\n\nFinds points on constraint boundaries for ESH hyperplane generation.\n\n**Purpose:**\nGiven interior point A and exterior point B, find point on boundary\nwhere constraint function equals zero (feasibility boundary).\n\n**Constraint Rootsearch:**\n- findZero(ptA, ptB, ...): Search along line segment A→B\n- Returns (boundary point, exterior point) pair\n- lambdaTol: Tolerance on line parameter\n- constrTol: Tolerance on constraint violation\n\n**Objective Rootsearch:**\n- findZero(pt, objLB, objUB, ...): Find objective level set\n- Used for objective function cuts\n\n**Algorithm:**\n- Uses bisection or hybrid methods (TOMS 748)\n- Requires f(A) < 0 (feasible) and f(B) > 0 (infeasible)\n\n**Implementations:**\n- RootsearchMethodBoost: Boost.Math TOMS 748 or bisection", "algorithm": "Essential for Extended Supporting Hyperplane (ESH) method", "see": ["DualSolver.h for hyperplane generation workflow"], "has_pass2": true}, "src/RootsearchMethod/RootsearchMethodBoost.h": {"path": "layer-4/SHOT/src/RootsearchMethod/RootsearchMethodBoost.h", "filename": "RootsearchMethodBoost.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "RootsearchMethod/RootsearchMethodBoost.h", "brief": "Boost.Math rootfinding for constraint boundaries\n\nFinds constraint boundary points using TOMS 748 or bisection.\n\n**RootsearchMethodBoost Class:**\n- findZero(ptA, ptB, constraints): Find boundary between points\n- findZero(pt, objLB, objUB, objective): Objective rootsearch\n\n**Helper Classes:**\n- Test: Functor for constraint max-violation evaluation\n- TestObjective: Functor for objective function evaluation\n- TerminationCondition: Convergence criterion\n\n**Algorithm:**\n- TOMS 748: Optimal bracketing algorithm (4th order)\n- Bisection: Fallback for difficult cases\n- Returns boundary point for hyperplane generation", "see": ["IRootsearchMethod.h for interface contract", "Boost.Math TOMS 748 documentation"], "has_pass2": false}, "src/MIPSolver/MIPSolverCplexSingleTreeLegacy.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCplexSingleTreeLegacy.h", "filename": "MIPSolverCplexSingleTreeLegacy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCplexSingleTreeLegacy.h", "brief": "Legacy CPLEX callback API for single-tree ESH\n\nUses older callback API (pre-CPLEX 12.10 generic callbacks).\n\n**Legacy Callback Classes:**\n- HCallbackI: Heuristic callback for primal solutions\n- InfoCallbackI: Progress information callback\n- CtCallbackI: Lazy constraint callback for hyperplanes\n\n**MIPSolverCplexSingleTreeLegacy Class:**\n- Inherits MIPSolverCplex for base functionality\n- Uses IloCplex::LazyConstraintCallbackI/HeuristicCallbackI\n\n@deprecated Prefer MIPSolverCplexSingleTree with generic callbacks", "see": ["MIPSolverCplexSingleTree.h for modern callback API"], "has_pass2": false}, "src/MIPSolver/IRelaxationStrategy.h": {"path": "layer-4/SHOT/src/MIPSolver/IRelaxationStrategy.h", "filename": "IRelaxationStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/IRelaxationStrategy.h", "brief": "Interface for LP relaxation control strategies\n\nManages when to solve LP relaxation vs full MIP.\n\n**IRelaxationStrategy Interface:**\n- executeStrategy(): Apply relaxation decision\n- setActive()/setInactive(): Enable/disable strategy\n- setInitial(): Reset to initial state\n\n**Implementations:**\n- RelaxationStrategyStandard: Periodic LP solves\n- RelaxationStrategyNone: Always solve full MIP\n\n**Use Case:**\n- LP relaxations generate more hyperplanes quickly\n- MIP solves provide better candidate points\n- Strategy balances cut generation vs exploration", "see": ["RelaxationStrategyBase.h for base implementation"], "has_pass2": false}, "src/MIPSolver/MIPSolverGurobi.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverGurobi.h", "filename": "MIPSolverGurobi.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverGurobi.h", "brief": "Gurobi implementation of IMIPSolver interface\n\nProvides MIP solving for SHOT's dual problem using Gurobi C++ API.\n\n**MIPSolverGurobi Class:**\n- Implements full IMIPSolver interface\n- Uses GRBModel for problem representation\n- Supports LP, MIP, QP, MIQP, QCQP problem types\n\n**Key Data Structures:**\n- gurobiModel: Shared GRBModel pointer\n- objectiveLinearExpression, objectiveQuadraticExpression\n- constraintLinearExpression, constraintQuadraticExpression\n\n**GurobiCallbackMultiTree:**\n- GRBCallback for multi-tree algorithm\n- Handles termination checks during solve\n\n**Quadratic Support:**\n- supportsQuadraticObjective(): Yes\n- supportsQuadraticConstraints(): Yes (QCQP)\n\n@note Requires Gurobi C++ headers", "see": ["MIPSolverGurobSingleTree for lazy constraint callback"], "has_pass2": false}, "src/MIPSolver/MIPSolverCallbackBase.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCallbackBase.h", "filename": "MIPSolverCallbackBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCallbackBase.h", "brief": "Shared logic for single-tree callback handlers\n\nBase class for CPLEX and Gurobi callback implementations.\n\n**MIPSolverCallbackBase Class:**\n- addLazyConstraint(): Generate and add ESH cuts\n- checkFixedNLPStrategy(): Decide if NLP should be called\n- checkIterationLimit()/checkUserTermination(): Termination checks\n- printIterationReport(): Console output in callback\n\n**Task Objects:**\n- taskSelectPrimNLPOriginal/Reformulated: Fixed-integer NLP\n- taskSelectHPPts: ESH/ECP hyperplane selection\n- taskSelectPrimalSolutionFromRootsearch: Rootsearch primal\n- tUpdateInteriorPoint: Interior point maintenance", "see": ["MIPSolverCplexSingleTree.h, MIPSolverGurobiSingleTree.h"], "has_pass2": false}, "src/MIPSolver/IMIPSolutionLimitStrategy.h": {"path": "layer-4/SHOT/src/MIPSolver/IMIPSolutionLimitStrategy.h", "filename": "IMIPSolutionLimitStrategy.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/IMIPSolutionLimitStrategy.h", "brief": "Interface for MIP solution pool limit strategies\n\nControls how many solutions MIP solver collects per iteration.\n\n**IMIPSolutionLimitStrategy Interface:**\n- updateLimit(): Adjust limit based on progress\n- getNewLimit(): Current solution limit value\n- getInitialLimit(): Starting limit\n\n**Implementations:**\n- MIPSolutionLimitStrategyIncrease: Grow limit over time\n- MIPSolutionLimitStrategyUnlimited: No limit\n- MIPSolutionLimitStrategyAdaptive: Adjust based on gap\n\n**Purpose:**\n- Balance cut generation vs solver time per iteration\n- More solutions = more hyperplane generation points\n- Fewer solutions = faster MIP solves", "see": ["TaskExecuteSolutionLimitStrategy for strategy execution"], "has_pass2": false}, "src/MIPSolver/MIPSolverGurobiSingleTree.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverGurobiSingleTree.h", "filename": "MIPSolverGurobiSingleTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverGurobiSingleTree.h", "brief": "Gurobi with lazy constraint callback for single-tree ESH\n\nExtends MIPSolverGurobi with callback-based cut generation.\n\n**GurobiCallbackSingleTree Class:**\n- Inherits GRBCallback + MIPSolverCallbackBase\n- callback(): Called at candidate solution points\n- createHyperplane()/createIntegerCut(): Add lazy constraints\n- addLazyConstraint(): Generate ESH cuts from solution\n\n**MIPSolverGurobiSingleTree Class:**\n- solveProblem(): Single MIP solve with callbacks enabled\n- isCallbackInitialized: Track callback registration\n\n**Node Information:**\n- lastExploredNodes/lastOpenNodes: B&B tree progress\n- Used for iteration reporting within callback", "see": ["SolutionStrategySingleTree.h for algorithm context"], "has_pass2": false}, "src/MIPSolver/RelaxationStrategyStandard.h": {"path": "layer-4/SHOT/src/MIPSolver/RelaxationStrategyStandard.h", "filename": "RelaxationStrategyStandard.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/RelaxationStrategyStandard.h", "brief": "Standard LP-to-MIP relaxation strategy\n\nImplements the default relaxation phase behavior.\n\n**RelaxationStrategyStandard Class:**\n- executeStrategy(): Decide LP vs MIP solve\n- setActive()/setInactive(): Control strategy state\n- setInitial(): Reset for new problem\n\n**LP Phase Termination:**\n- isIterationLimitReached(): Max LP iterations\n- isTimeLimitReached(): LP time budget\n- isLPStepFinished(): Convergence detection\n- isObjectiveStagnant(): No bound improvement\n\n**Algorithm:**\n- Initial iterations solve LP for fast hyperplanes\n- Switch to MIP after LP phase completion\n- LPFinished flag tracks phase transition", "see": ["IRelaxationStrategy.h for interface"], "has_pass2": false}, "src/MIPSolver/MIPSolverCplexSingleTree.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCplexSingleTree.h", "filename": "MIPSolverCplexSingleTree.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCplexSingleTree.h", "brief": "CPLEX with lazy constraint callback for single-tree ESH\n\nExtends MIPSolverCplex with callback-based cut generation.\n\n**MIPSolverCplexSingleTree Class:**\n- solveProblem(): Single MIP solve with callbacks enabled\n- Uses generic callback API (Context-based)\n\n**CplexCallback Class:**\n- Inherits IloCplex::Callback::Function + MIPSolverCallbackBase\n- invoke(): Called at candidate solution points\n- createHyperplane()/createIntegerCut(): Add lazy constraints\n- addLazyConstraint(): Generate ESH cuts from solution\n\n**Threading:**\n- callbackMutex: Protect shared state in multi-threaded solve\n- CPLEX invokes callbacks from multiple threads", "see": ["SolutionStrategySingleTree.h for algorithm context"], "has_pass2": false}, "src/MIPSolver/MIPSolutionLimitStrategyIncrease.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolutionLimitStrategyIncrease.h", "filename": "MIPSolutionLimitStrategyIncrease.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolutionLimitStrategyIncrease.h", "brief": "Solution limit strategy with gradual increase\n\nIncreases MIP solution pool limit over iterations.\n\n**MIPSolutionLimitStrategyIncrease Class:**\n- updateLimit(): Increase limit when MIP finds optimal\n- getNewLimit()/getInitialLimit(): Current and start values\n\n**State Tracking:**\n- lastIterSolLimIncreased: When limit was last raised\n- numSolLimIncremented: Total increases performed\n- lastIterOptimal: Last iteration with optimal MIP\n\n**Algorithm:**\n- Start with low limit for fast initial iterations\n- Increase when solver proves optimality\n- More solutions = more hyperplane generation points", "see": ["IMIPSolutionLimitStrategy.h for interface"], "has_pass2": false}, "src/MIPSolver/MIPSolverCplex.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCplex.h", "filename": "MIPSolverCplex.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCplex.h", "brief": "IBM CPLEX implementation of IMIPSolver interface\n\nProvides MIP solving for SHOT's dual problem using CPLEX Concert API.\n\n**MIPSolverCplex Class:**\n- Implements full IMIPSolver interface\n- Uses IloModel, IloCplex for problem representation\n- Supports LP, MIP, QP, MIQP, QCQP problem types\n\n**Key Data Structures:**\n- cplexModel: IloModel for optimization model\n- cplexInstance: IloCplex solver instance\n- cplexVars: Variable array\n- cplexConstrs: Constraint array\n\n**Quadratic Support:**\n- supportsQuadraticObjective(): Yes\n- supportsQuadraticConstraints(): Yes (QCQP)\n\n**UserTerminationCallbackI:**\n- MIP info callback for user termination\n- Allows early termination on events\n\n@note Requires CPLEX Concert Technology headers", "see": ["MIPSolverCplexSingleTree for lazy constraint callback"], "has_pass2": false}, "src/MIPSolver/MIPSolutionLimitStrategyUnlimited.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolutionLimitStrategyUnlimited.h", "filename": "MIPSolutionLimitStrategyUnlimited.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolutionLimitStrategyUnlimited.h", "brief": "No limit on MIP solution pool size\n\nAllows solver to collect all feasible solutions.\n\n**MIPSolutionLimitStrategyUnlimited Class:**\n- getInitialLimit(): Returns maximum integer\n- updateLimit(): No-op (always unlimited)\n\n**Use Case:**\n- Thorough exploration of solution space\n- May slow down iterations", "see": ["IMIPSolutionLimitStrategy.h for interface", "MIPSolutionLimitStrategyIncrease.h for gradual increase"], "has_pass2": false}, "src/MIPSolver/MIPSolutionLimitStrategyAdaptive.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolutionLimitStrategyAdaptive.h", "filename": "MIPSolutionLimitStrategyAdaptive.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolutionLimitStrategyAdaptive.h", "brief": "Adaptive solution limit based on solver progress\n\nAdjusts limit based on optimization state.\n\n**MIPSolutionLimitStrategyAdaptive Class:**\n- lastIterSolLimIncreased: Track when limit changed\n- numSolLimIncremented: Count of increases\n\n**Adaptive Logic:**\n- Increase when finding good solutions\n- Decrease when solver is struggling\n- Balances exploration vs speed", "see": ["IMIPSolutionLimitStrategy.h for interface", "MIPSolutionLimitStrategyIncrease.h for simpler strategy"], "has_pass2": false}, "src/MIPSolver/RelaxationStrategyNone.h": {"path": "layer-4/SHOT/src/MIPSolver/RelaxationStrategyNone.h", "filename": "RelaxationStrategyNone.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/RelaxationStrategyNone.h", "brief": "No-op relaxation strategy (always MIP)\n\nDisables LP relaxation phase, always solves full MIP.\n\n**RelaxationStrategyNone Class:**\n- executeStrategy(): No-op (MIP always active)\n- setActive()/setInactive()/setInitial(): State management\n\n**Use Case:**\n- Problems where LP phase doesn't help\n- When integer solutions are needed immediately\n- Benchmarking MIP-only performance\n\n**Behavior:**\n- Every iteration solves full MIP problem\n- No LP warm-start phase for hyperplane generation", "see": ["RelaxationStrategyStandard.h for LP-then-MIP"], "has_pass2": false}, "src/MIPSolver/MIPSolverCbc.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverCbc.h", "filename": "MIPSolverCbc.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverCbc.h", "brief": "COIN-OR Cbc implementation of IMIPSolver interface\n\nProvides open-source MIP solving using Cbc branch-and-cut solver.\n\n**MIPSolverCbc Class:**\n- Implements IMIPSolver interface\n- Uses OsiClpSolverInterface for LP subproblems\n- CbcModel for branch-and-cut\n- CoinModel for problem construction\n\n**Key Data Structures:**\n- osiInterface: OSI LP solver (Clp)\n- cbcModel: MIP solver model\n- coinModel: Problem builder\n- objectiveLinearExpression: CoinPackedVector\n\n**CbcMessageHandler:**\n- Custom message handler for SHOT logging\n- Routes Cbc output through SHOT's Output system\n\n**Limitations:**\n- supportsQuadraticObjective(): No\n- supportsQuadraticConstraints(): No\n- LP/MIP only (no MIQP/QCQP)\n\n@note Default open-source MIP solver, no license required", "see": ["Cbc branch-and-cut library"], "algorithm": "MIP Solution Pool (getAllVariableSolutions):\n  CBC can return multiple feasible integer solutions.\n  SHOT uses these as candidate points for NLP evaluation:\n  - More solutions = more chances to find primal feasible point\n  - Controlled by setSolutionLimit()", "math": "For binary x with solution x̂ ∈ {0,1}ⁿ:\n        ∑(x_j : x̂_j=1) - ∑(x_j : x̂_j=0) <= |{j: x̂_j=1}| - 1\n  Prevents cycling when NLP subproblem fails or solution is infeasible.", "complexity": "Each MIP solve is NP-hard; CBC uses B&C with CGL cuts", "ref": ["Duran & Grossmann (1986) - Outer Approximation for MINLP", "Kronqvist et al. (2019) - SHOT solver description", "Kelley's cutting-plane method (1960)", "Solution pool heuristics in MIP solvers"], "has_pass2": true}, "src/MIPSolver/IMIPSolver.h": {"path": "layer-4/SHOT/src/MIPSolver/IMIPSolver.h", "filename": "IMIPSolver.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/IMIPSolver.h", "brief": "Abstract interface for MIP solver backends\n\nPure virtual interface for dual problem MIP solvers.\n\n**Problem Construction:**\n- initializeProblem(), finalizeProblem(): Setup/teardown\n- addVariable(): With type, bounds, and semi-bounds\n- addLinearTermToObjective/Constraint(): Build incrementally\n- addQuadraticTermToObjective/Constraint(): For MIQP/MIQCQP\n\n**Solution Methods:**\n- solveProblem(): Execute MIP solver\n- repairInfeasibility(): Attempt feasibility repair\n- getObjectiveValue(), getDualObjectiveValue(): Bounds\n- getVariableSolution(), getAllVariableSolutions(): Points\n\n**Hyperplane/Cut Management:**\n- createHyperplane(): Add supporting hyperplane linearization\n- createInteriorHyperplane(): Interior point cuts\n- createIntegerCut(): No-good cuts for integer variables\n\n**Bound Management:**\n- setCutOff(): Objective cutoff for pruning\n- fixVariable(), unfixVariables(): For integer fixing\n- presolveAndUpdateBounds(): Bound tightening\n\n**Implementations:**\n- MIPSolverCplex, MIPSolverGurobi, MIPSolverCbc\n- SingleTree variants for callback-based cut addition", "see": ["DualSolver.h for MIP solver orchestration", "IRelaxationStrategy.h for LP relaxation handling"], "algorithm": "MIP Start (Warm Starting) (addMIPStart):\n  Provide known feasible solution to MIP solver:\n  - From NLP subproblem solutions in outer approximation\n  - Improves primal bound immediately\n  - Guides B&B exploration toward good solutions", "math": "Prune node n if: LP(n) >= z* (current best solution)\n  Can be set as hard cutoff or as objective constraint.", "ref": ["Kronqvist et al. (2016) - Extended Supporting Hyperplane Algorithm", "Kelley (1960) - Cutting-plane method for convex programming", "Achterberg et al. (2007) - Conflict analysis in MIP solving", "Achterberg (2007) - SCIP: Solving Constraint Integer Programs"], "has_pass2": true}, "src/MIPSolver/MIPSolverBase.h": {"path": "layer-4/SHOT/src/MIPSolver/MIPSolverBase.h", "filename": "MIPSolverBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/MIPSolverBase.h", "brief": "Common base class for MIP solver implementations\n\nShared functionality for CPLEX, Gurobi, and Cbc backends.\n\n**Problem State:**\n- numberOfVariables, numberOfConstraints: Problem size\n- variableTypes, variableLowerBounds, variableUpperBounds\n- isMinimizationProblem, isProblemDiscrete\n\n**Hyperplane Creation:**\n- createHyperplane(): Add supporting hyperplane cut\n- createInteriorHyperplane(): Interior point cut\n- createHyperplaneTerms(): Compute cut coefficients\n- getConstraintIdentifier(): Named constraint IDs\n\n**Variable Fixing:**\n- fixVariable(), fixVariables(): Fix for NLP subproblem\n- unfixVariables(): Restore original bounds\n- updateVariableBound(): Dynamic bound update\n\n**Relaxation Strategy:**\n- relaxationStrategy: LP relaxation handling\n- executeRelaxationStrategy(): Apply relaxation\n\n**Dual Auxiliary Variable:**\n- Tracks epigraph variable for nonlinear objective\n- hasDualAuxiliaryObjectiveVariable(), getDualAuxiliaryObjectiveVariableIndex()", "see": ["MIPSolverCplex, MIPSolverGurobi, MIPSolverCbc"], "has_pass2": false}, "src/MIPSolver/RelaxationStrategyBase.h": {"path": "layer-4/SHOT/src/MIPSolver/RelaxationStrategyBase.h", "filename": "RelaxationStrategyBase.h", "author": "Andreas Lundell, Åbo Akademi University\n\n   @section LICENSE\n   This software is licensed under the Eclipse Public License 2.0.\n   Please see the README and LICENSE files for more information.", "file": "MIPSolver/RelaxationStrategyBase.h", "brief": "Shared helper methods for relaxation strategies\n\nCommon termination checks for LP/MIP relaxation control.\n\n**RelaxationStrategyBase Class:**\n- isRelaxedSolutionInterior(): Check if LP solution is interior\n- isConstraintToleranceReached(): Feasibility achieved\n- isGapReached(): Optimality gap tolerance met\n\n**Use Case:**\n- Determines when to stop LP phase and switch to MIP\n- Checks convergence criteria during relaxation", "see": ["RelaxationStrategyStandard.h for concrete implementation"], "has_pass2": false}, "src/GAMS/GamsNLinstr.h": {"path": "layer-4/SHOT/src/GAMS/GamsNLinstr.h", "filename": "GamsNLinstr.h", "file": "GAMS/GamsNLinstr.h", "brief": "GAMS nonlinear instruction opcodes and function codes\n\nDefines GAMS bytecode for nonlinear expressions.\n\n**GamsOpCode Enum:**\n- Stack operations: nlPushV, nlPushI, nlStore\n- Arithmetic: nlAdd, nlSub, nlMul, nlDiv, nlUMin\n- Function calls: nlCallArg1, nlCallArg2, nlCallArgN\n\n**GamsFuncCode Enum:**\n- Math functions: fnexp, fnlog, fnsqrt, fnabs, fnsin, fncos\n- Comparison: fnmin, fnmax, fnifthen\n- Special: fnpower, fnsignpower, fnentropy\n\n@warning Bytecode format may change without notice", "see": ["ModelingSystemGAMS.h for bytecode interpretation"], "has_pass2": false}, "ThirdParty/eigen/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h": {"path": "layer-4/SHOT/ThirdParty/eigen/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h", "filename": "SyclMemoryModel.h", "brief": "Returns an accessor to the buffer of the given virtual pointer", "param": ["accessMode", "accessTarget", "ptr The virtual pointer", "accessMode", "accessTarget", "ptr The virtual pointer", "cgh Reference to the command group scope", "node A reference to the free node to be fused", "node A reference to the free node to be fused"], "has_pass2": false}}}, "Sonnet": {"name": "Sonnet", "file_count": 12, "pass2_count": 0, "files": {"src/SonnetWrapper/CoinMessageHandler.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CoinMessageHandler.h", "filename": "CoinMessageHandler.h", "file": "SonnetWrapper/CoinMessageHandler.h", "brief": ".NET wrapper for CoinMessageHandler (logging)\n\nC++/CLI wrapper for COIN-OR logging infrastructure.\nCoinMessageHandlerProxy bridges .NET virtual methods to native callbacks.\nCoinOneMessage and CoinMessages for custom message definitions.", "see": ["CoinMpsIO.h for file I/O using message handler"], "has_pass2": false}, "src/SonnetWrapper/CbcEventHandler.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CbcEventHandler.h", "filename": "CbcEventHandler.h", "file": "SonnetWrapper/CbcEventHandler.h", "brief": ".NET wrapper for CBC event handling\n\nCbcEvent enum (node, solution, treeStatus, endSearch, etc.) and\nCbcAction enum (noAction, stop, restart, killSolution).\nCbcDelegateEventHandlerProxy bridges .NET delegates to native callbacks.", "see": ["CbcModel.h for model wrapper", "CbcSolver.h for solver entry points"], "has_pass2": false}, "src/SonnetWrapper/ClpModel.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/ClpModel.h", "filename": "ClpModel.h", "file": "SonnetWrapper/ClpModel.h", "brief": ".NET wrapper for ClpModel (COIN-OR LP solver)\n\nC++/CLI wrapper exposing CLP simplex solver to .NET languages.\nSupports quadratic objectives via loadQuadraticObjective().", "see": ["ClpObjective.h, OsiClpSolverInterface.h"], "has_pass2": false}, "src/SonnetWrapper/CbcModel.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CbcModel.h", "filename": "CbcModel.h", "file": "SonnetWrapper/CbcModel.h", "brief": ".NET wrapper for CbcModel (COIN-OR Branch and Cut)\n\nC++/CLI wrapper exposing CBC MIP solver to .NET languages.\nWraps branch-and-bound with cut generation capabilities.", "see": ["CbcEventHandler.h, CbcCutGenerator.h, CbcStrategy.h"], "has_pass2": false}, "src/SonnetWrapper/CbcSolver.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CbcSolver.h", "filename": "CbcSolver.h", "file": "SonnetWrapper/CbcSolver.h", "brief": ".NET wrapper for CBC solver entry points (CbcMain0/CbcMain1)\n\nC++/CLI wrapper for CBC solver with callback support.\nCbcSolverCallBack delegate for monitoring solve phases.", "see": ["CbcModel.h for model wrapper", "CbcEventHandler.h for event handling"], "has_pass2": false}, "src/SonnetWrapper/CglCutGenerator.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CglCutGenerator.h", "filename": "CglCutGenerator.h", "file": "SonnetWrapper/CglCutGenerator.h", "brief": ".NET wrapper for CglCutGenerator (cut generator base class)\n\nC++/CLI wrapper exposing CGL cut generators to .NET languages.\nGeneric template CglCutGeneratorGeneric<T> for derived generators.", "see": ["CglProbing.h for probing cut generator", "CbcCutGenerator.h for CBC integration"], "has_pass2": false}, "src/SonnetWrapper/OsiSolverInterface.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/OsiSolverInterface.h", "filename": "OsiSolverInterface.h", "file": "SonnetWrapper/OsiSolverInterface.h", "brief": ".NET wrapper for OsiSolverInterface (Open Solver Interface)\n\nC++/CLI wrapper exposing OsiSolverInterface to .NET languages.", "see": ["ClpModel.h, CbcModel.h for solver wrappers"], "has_pass2": false}, "src/SonnetWrapper/OsiClpSolverInterface.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/OsiClpSolverInterface.h", "filename": "OsiClpSolverInterface.h", "file": "SonnetWrapper/OsiClpSolverInterface.h", "brief": ".NET wrapper for OsiClpSolverInterface (CLP via OSI)\n\nC++/CLI wrapper exposing CLP solver through OSI interface to .NET.\nIncludes matrix access methods and optional CPLEX/Gurobi support.", "see": ["ClpModel.h for direct CLP wrapper", "OsiSolverInterface.h for base OSI wrapper"], "has_pass2": false}, "src/SonnetWrapper/CoinPackedMatrix.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CoinPackedMatrix.h", "filename": "CoinPackedMatrix.h", "file": "SonnetWrapper/CoinPackedMatrix.h", "brief": ".NET wrapper for CoinPackedMatrix (sparse matrix)\n\nC++/CLI wrapper exposing CoinUtils sparse matrix to .NET languages.", "see": ["CoinShallowPackedVector.h for vector wrapper"], "has_pass2": false}, "src/SonnetWrapper/Helpers.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/Helpers.h", "filename": "Helpers.h", "file": "SonnetWrapper/Helpers.h", "brief": "Core wrapper infrastructure for Sonnet .NET bindings\n\nTemplate base classes for C++/CLI wrappers around native COIN-OR objects.\nWrapperAbstractBase<T> and WrapperBase<T> handle ownership and lifetime.", "see": ["OsiSolverInterface.h, ClpModel.h, CbcModel.h for concrete wrappers"], "has_pass2": false}, "src/SonnetWrapper/CoinMpsIO.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/CoinMpsIO.h", "filename": "CoinMpsIO.h", "file": "SonnetWrapper/CoinMpsIO.h", "brief": ".NET wrapper for CoinMpsIO (MPS file I/O)\n\nC++/CLI wrapper for reading LP problems from MPS format files.\nExposes row/column bounds, objective coefficients, and matrix data.", "see": ["CoinPackedMatrix.h for sparse matrix wrapper", "CoinMessageHandler.h for custom logging"], "has_pass2": false}, "src/SonnetWrapper/OsiCbcSolverInterface.h": {"path": "layer-4/Sonnet/src/SonnetWrapper/OsiCbcSolverInterface.h", "filename": "OsiCbcSolverInterface.h", "file": "SonnetWrapper/OsiCbcSolverInterface.h", "brief": ".NET wrapper for OsiCbcSolverInterface (CBC via OSI)\n\nC++/CLI wrapper exposing CBC MIP solver through OSI interface.\nProvides access to CbcModel for node count, strategy, and solution.", "see": ["CbcModel.h for CBC model wrapper", "OsiClpSolverInterface.h for CLP solver wrapper"], "has_pass2": false}}}, "cuopt": {"name": "cuopt", "file_count": 135, "pass2_count": 0, "files": {"benchmarks/linear_programming/cuopt/benchmark_helper.hpp": {"path": "layer-4/cuopt/benchmarks/linear_programming/cuopt/benchmark_helper.hpp", "filename": "benchmark_helper.hpp", "file": "benchmark_helper.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "benchmarks/linear_programming/cuopt/initial_solution_reader.hpp": {"path": "layer-4/cuopt/benchmarks/linear_programming/cuopt/initial_solution_reader.hpp", "filename": "initial_solution_reader.hpp", "file": "initial_solution_reader.hpp", "brief": "cuOpt LP solution storage\n\nPrimal/dual solution and basis information.", "has_pass2": false}, "benchmarks/linear_programming/cuopt/initial_problem_check.hpp": {"path": "layer-4/cuopt/benchmarks/linear_programming/cuopt/initial_problem_check.hpp", "filename": "initial_problem_check.hpp", "file": "initial_problem_check.hpp", "brief": "cuOpt LP problem representation\n\nSparse matrix storage for LP constraints.", "has_pass2": false}, "cpp/src/math_optimization/solution_writer.hpp": {"path": "layer-4/cuopt/cpp/src/math_optimization/solution_writer.hpp", "filename": "solution_writer.hpp", "file": "solution_writer.hpp", "brief": "cuOpt optimization utilities\n\nCommon utilities for mathematical optimization.", "param": ["sol_file_path Path to the .sol file to write", "status Status of the solution", "objective_value Objective value of the solution", "variable_names Vector of variable names", "variable_values Vector of variable values"], "has_pass2": false}, "cpp/src/math_optimization/solution_reader.hpp": {"path": "layer-4/cuopt/cpp/src/math_optimization/solution_reader.hpp", "filename": "solution_reader.hpp", "file": "solution_reader.hpp", "brief": "cuOpt optimization utilities\n\nCommon utilities for mathematical optimization.", "param": ["sol_file_path Path to the .sol file to read", "variable_names Vector of variable names to extract values for"], "return": "std::vector<double> Vector of values corresponding to the variable names", "has_pass2": false}, "cpp/src/distance/distance.hpp": {"path": "layer-4/cuopt/cpp/src/distance/distance.hpp", "filename": "distance.hpp", "file": "distance.hpp", "brief": "cuOpt distance computation\n\nDistance/time matrix computation for routing.\nHaversine, Euclidean, and graph-based distances.", "has_pass2": false}, "cpp/src/routing/routing_details.hpp": {"path": "layer-4/cuopt/cpp/src/routing/routing_details.hpp", "filename": "routing_details.hpp", "file": "routing_details.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/vehicle_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/vehicle_info.hpp", "filename": "vehicle_info.hpp", "file": "vehicle_info.hpp", "brief": "cuOpt VRP vehicle/fleet model\n\nVehicle capacity, time windows, and fleet constraints.", "has_pass2": false}, "cpp/src/routing/solver.hpp": {"path": "layer-4/cuopt/cpp/src/routing/solver.hpp", "filename": "solver.hpp", "file": "solver.hpp", "brief": "cuOpt VRP solver\n\nMain VRP solver orchestrating metaheuristics.", "tparam": ["i_t Integer type. int (32bit) is expected at the moment. Please\nopen an issue if other type are needed.", "f_t Floating point type. float (32bit) is expected at the moment.\nPlease open an issue if other type are needed."], "return": "assignment_t owning container for the solver output.", "has_pass2": false}, "cpp/src/routing/structures.hpp": {"path": "layer-4/cuopt/cpp/src/routing/structures.hpp", "filename": "structures.hpp", "file": "structures.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "tparam": ["i_t"], "has_pass2": false}, "cpp/src/routing/fleet_order_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/fleet_order_info.hpp", "filename": "fleet_order_info.hpp", "file": "fleet_order_info.hpp", "brief": "cuOpt VRP vehicle/fleet model\n\nVehicle capacity, time windows, and fleet constraints.", "has_pass2": false}, "cpp/src/routing/fleet_order_constraints.hpp": {"path": "layer-4/cuopt/cpp/src/routing/fleet_order_constraints.hpp", "filename": "fleet_order_constraints.hpp", "file": "fleet_order_constraints.hpp", "brief": "cuOpt VRP vehicle/fleet model\n\nVehicle capacity, time windows, and fleet constraints.", "tparam": ["i_t", "f_t"], "param": ["data_model", "fleet_order_constraints_"], "has_pass2": false}, "cpp/src/routing/hyper_params.hpp": {"path": "layer-4/cuopt/cpp/src/routing/hyper_params.hpp", "filename": "hyper_params.hpp", "file": "hyper_params.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/order_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/order_info.hpp", "filename": "order_info.hpp", "file": "order_info.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "tparam": ["i_t", "f_t"], "param": ["data_model", "order_info"], "has_pass2": false}, "cpp/src/routing/arc_value.hpp": {"path": "layer-4/cuopt/cpp/src/routing/arc_value.hpp", "filename": "arc_value.hpp", "file": "arc_value.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/fleet_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/fleet_info.hpp", "filename": "fleet_info.hpp", "file": "fleet_info.hpp", "brief": "cuOpt VRP vehicle/fleet model\n\nVehicle capacity, time windows, and fleet constraints.", "tparam": ["i_t", "f_t", "i_t", "f_t"], "param": ["data_model", "fleet_info", "data_model", "fleet_info"], "has_pass2": false}, "cpp/src/dual_simplex/pseudo_costs.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/pseudo_costs.hpp", "filename": "pseudo_costs.hpp", "file": "pseudo_costs.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/initial_basis.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/initial_basis.hpp", "filename": "initial_basis.hpp", "file": "initial_basis.hpp", "brief": "cuOpt dual simplex basis management\n\nLU factorization and basis updates for GPU simplex.", "has_pass2": false}, "cpp/src/dual_simplex/folding.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/folding.hpp", "filename": "folding.hpp", "file": "folding.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/random.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/random.hpp", "filename": "random.hpp", "file": "random.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/logger.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/logger.hpp", "filename": "logger.hpp", "file": "logger.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/presolve.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/presolve.hpp", "filename": "presolve.hpp", "file": "presolve.hpp", "brief": "cuOpt LP presolve\n\nBound tightening and constraint reduction for LP.", "has_pass2": false}, "cpp/src/dual_simplex/user_problem.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/user_problem.hpp", "filename": "user_problem.hpp", "file": "user_problem.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/phase2.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/phase2.hpp", "filename": "phase2.hpp", "file": "phase2.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/types.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/types.hpp", "filename": "types.hpp", "file": "types.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/triangle_solve.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/triangle_solve.hpp", "filename": "triangle_solve.hpp", "file": "triangle_solve.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/branch_and_bound.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/branch_and_bound.hpp", "filename": "branch_and_bound.hpp", "file": "branch_and_bound.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/conjugate_gradient.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/conjugate_gradient.hpp", "filename": "conjugate_gradient.hpp", "file": "conjugate_gradient.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/phase1.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/phase1.hpp", "filename": "phase1.hpp", "file": "phase1.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/crossover.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/crossover.hpp", "filename": "crossover.hpp", "file": "crossover.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/dense_vector.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/dense_vector.hpp", "filename": "dense_vector.hpp", "file": "dense_vector.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/iterative_refinement.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/iterative_refinement.hpp", "filename": "iterative_refinement.hpp", "file": "iterative_refinement.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/barrier.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/barrier.hpp", "filename": "barrier.hpp", "file": "barrier.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/basis_updates.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/basis_updates.hpp", "filename": "basis_updates.hpp", "file": "basis_updates.hpp", "brief": "cuOpt dual simplex basis management\n\nLU factorization and basis updates for GPU simplex.", "has_pass2": false}, "cpp/src/dual_simplex/dense_matrix.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/dense_matrix.hpp", "filename": "dense_matrix.hpp", "file": "dense_matrix.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/sparse_vector.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/sparse_vector.hpp", "filename": "sparse_vector.hpp", "file": "sparse_vector.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/solve.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/solve.hpp", "filename": "solve.hpp", "file": "solve.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/primal.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/primal.hpp", "filename": "primal.hpp", "file": "primal.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/tic_toc.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/tic_toc.hpp", "filename": "tic_toc.hpp", "file": "tic_toc.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/singletons.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/singletons.hpp", "filename": "singletons.hpp", "file": "singletons.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/mip_node.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/mip_node.hpp", "filename": "mip_node.hpp", "file": "mip_node.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/right_looking_lu.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/right_looking_lu.hpp", "filename": "right_looking_lu.hpp", "file": "right_looking_lu.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/vector_math.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/vector_math.hpp", "filename": "vector_math.hpp", "file": "vector_math.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/sparse_matrix.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/sparse_matrix.hpp", "filename": "sparse_matrix.hpp", "file": "sparse_matrix.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/solution.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/solution.hpp", "filename": "solution.hpp", "file": "solution.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/pinned_host_allocator.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/pinned_host_allocator.hpp", "filename": "pinned_host_allocator.hpp", "file": "pinned_host_allocator.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/cusparse_info.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/cusparse_info.hpp", "filename": "cusparse_info.hpp", "file": "cusparse_info.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/scaling.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/scaling.hpp", "filename": "scaling.hpp", "file": "scaling.hpp", "brief": "cuOpt matrix preprocessing\n\nScaling and permutation for numerical stability.", "has_pass2": false}, "cpp/src/dual_simplex/simplex_solver_settings.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/simplex_solver_settings.hpp", "filename": "simplex_solver_settings.hpp", "file": "simplex_solver_settings.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/cusparse_view.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/cusparse_view.hpp", "filename": "cusparse_view.hpp", "file": "cusparse_view.hpp", "brief": "cuOpt GPU dual simplex component\n\nPart of CUDA-accelerated dual simplex LP solver.", "has_pass2": false}, "cpp/src/dual_simplex/basis_solves.hpp": {"path": "layer-4/cuopt/cpp/src/dual_simplex/basis_solves.hpp", "filename": "basis_solves.hpp", "file": "basis_solves.hpp", "brief": "cuOpt dual simplex basis management\n\nLU factorization and basis updates for GPU simplex.", "has_pass2": false}, "cpp/src/linear_programming/translate.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/translate.hpp", "filename": "translate.hpp", "file": "translate.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/pdlp_constants.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/pdlp_constants.hpp", "filename": "pdlp_constants.hpp", "file": "pdlp_constants.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/saddle_point.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/saddle_point.hpp", "filename": "saddle_point.hpp", "file": "saddle_point.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "tparam": ["f_t  Data type of the variables and their weights in the equations", "i_t  Data type of indexes", "i_t", "f_t"], "param": ["handle_ptr Pointer to library handle (RAFT) containing hardware resources\ninformation. A default handle is valid.", "primal_size The size of the primal problem", "dual_size The size of the dual problem\n\n@throws cuopt::logic_error if the problem sizes are not larger than 0.", "other saddle_point_state_t object from which the solution should be copied", "stream cuda stream used for the copying\n\n@pre Primal and dual solutions must be of the same size respectively\n\n@throws cuopt::logic_error if the solutions are not of the same size"], "has_pass2": false}, "cpp/src/linear_programming/pdhg.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/pdhg.hpp", "filename": "pdhg.hpp", "file": "pdhg.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/cusparse_view.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/cusparse_view.hpp", "filename": "cusparse_view.hpp", "file": "cusparse_view.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/mip/logger.hpp": {"path": "layer-4/cuopt/cpp/src/mip/logger.hpp", "filename": "logger.hpp", "file": "logger.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/mip/mip_constants.hpp": {"path": "layer-4/cuopt/cpp/src/mip/mip_constants.hpp", "filename": "mip_constants.hpp", "file": "mip_constants.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/utilities/omp_helpers.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/omp_helpers.hpp", "filename": "omp_helpers.hpp", "file": "omp_helpers.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "has_pass2": false}, "cpp/src/utilities/unique_pinned_ptr.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/unique_pinned_ptr.hpp", "filename": "unique_pinned_ptr.hpp", "file": "unique_pinned_ptr.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "has_pass2": false}, "cpp/src/utilities/double_buffer.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/double_buffer.hpp", "filename": "double_buffer.hpp", "file": "double_buffer.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "has_pass2": false}, "cpp/src/utilities/strided_span.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/strided_span.hpp", "filename": "strided_span.hpp", "file": "strided_span.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "has_pass2": false}, "cpp/src/utilities/timer.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/timer.hpp", "filename": "timer.hpp", "file": "timer.hpp", "brief": "cuOpt timing utilities\n\nPerformance timing and profiling.", "has_pass2": false}, "cpp/src/utilities/version_info.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/version_info.hpp", "filename": "version_info.hpp", "file": "version_info.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "has_pass2": false}, "cpp/src/utilities/scope_guard.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/scope_guard.hpp", "filename": "scope_guard.hpp", "file": "scope_guard.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "has_pass2": false}, "cpp/src/utilities/copy_helpers.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/copy_helpers.hpp", "filename": "copy_helpers.hpp", "file": "copy_helpers.hpp", "brief": "cuOpt utility functions\n\nCommon utilities for cuOpt library.", "tparam": ["T", "T", "T", "T", "T", "T", "T", "T", "T", "T"], "param": ["device_ptr", "size", "stream_view", "device_vec", "stream_view", "device_vec", "device_vec", "stream_view", "device_vec", "stream_view", "device_vec", "stream_view"], "return": "auto", "has_pass2": false}, "cpp/src/utilities/high_res_timer.hpp": {"path": "layer-4/cuopt/cpp/src/utilities/high_res_timer.hpp", "filename": "high_res_timer.hpp", "file": "high_res_timer.hpp", "brief": "cuOpt timing utilities\n\nPerformance timing and profiling.", "has_pass2": false}, "cpp/src/routing/diversity/helpers.hpp": {"path": "layer-4/cuopt/cpp/src/routing/diversity/helpers.hpp", "filename": "helpers.hpp", "file": "helpers.hpp", "brief": "cuOpt VRP diversity management\n\nPopulation diversity for evolutionary VRP.\nPrevents premature convergence.", "has_pass2": false}, "cpp/src/routing/diversity/diversity_config.hpp": {"path": "layer-4/cuopt/cpp/src/routing/diversity/diversity_config.hpp", "filename": "diversity_config.hpp", "file": "diversity_config.hpp", "brief": "cuOpt VRP diversity management\n\nPopulation diversity for evolutionary VRP.\nPrevents premature convergence.", "has_pass2": false}, "cpp/src/routing/diversity/macros.hpp": {"path": "layer-4/cuopt/cpp/src/routing/diversity/macros.hpp", "filename": "macros.hpp", "file": "macros.hpp", "brief": "cuOpt VRP diversity management\n\nPopulation diversity for evolutionary VRP.\nPrevents premature convergence.", "has_pass2": false}, "cpp/src/routing/diversity/population.hpp": {"path": "layer-4/cuopt/cpp/src/routing/diversity/population.hpp", "filename": "population.hpp", "file": "population.hpp", "brief": "cuOpt VRP diversity management\n\nPopulation diversity for evolutionary VRP.\nPrevents premature convergence.", "has_pass2": false}, "cpp/src/routing/diversity/injection_info.hpp": {"path": "layer-4/cuopt/cpp/src/routing/diversity/injection_info.hpp", "filename": "injection_info.hpp", "file": "injection_info.hpp", "brief": "cuOpt VRP diversity management\n\nPopulation diversity for evolutionary VRP.\nPrevents premature convergence.", "has_pass2": false}, "cpp/src/routing/diversity/diverse_solver.hpp": {"path": "layer-4/cuopt/cpp/src/routing/diversity/diverse_solver.hpp", "filename": "diverse_solver.hpp", "file": "diverse_solver.hpp", "brief": "cuOpt VRP diversity management\n\nPopulation diversity for evolutionary VRP.\nPrevents premature convergence.", "has_pass2": false}, "cpp/src/routing/generator/generator.hpp": {"path": "layer-4/cuopt/cpp/src/routing/generator/generator.hpp", "filename": "generator.hpp", "file": "generator.hpp", "brief": "cuOpt VRP solution generator\n\nInitial solution construction for VRP.", "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed.", "f_t Floating point type. Needs to be float (32bit) at the moment."], "return": "dataset_t holding matrices, orders and vehicle info.", "has_pass2": false}, "cpp/src/routing/crossovers/ab_cycle.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/ab_cycle.hpp", "filename": "ab_cycle.hpp", "file": "ab_cycle.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/crossovers/eax_recombiner.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/eax_recombiner.hpp", "filename": "eax_recombiner.hpp", "file": "eax_recombiner.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/crossovers/inversion_recombiner.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/inversion_recombiner.hpp", "filename": "inversion_recombiner.hpp", "file": "inversion_recombiner.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/crossovers/set_covering.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/set_covering.hpp", "filename": "set_covering.hpp", "file": "set_covering.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/crossovers/ox_graph.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/ox_graph.hpp", "filename": "ox_graph.hpp", "file": "ox_graph.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/crossovers/dispose.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/dispose.hpp", "filename": "dispose.hpp", "file": "dispose.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/crossovers/srex_recombiner.hpp": {"path": "layer-4/cuopt/cpp/src/routing/crossovers/srex_recombiner.hpp", "filename": "srex_recombiner.hpp", "file": "srex_recombiner.hpp", "brief": "cuOpt VRP genetic crossover\n\nCrossover operators for evolutionary VRP algorithm.\nEAX (edge assembly), OX (order crossover), etc.", "has_pass2": false}, "cpp/src/routing/utilities/constants.hpp": {"path": "layer-4/cuopt/cpp/src/routing/utilities/constants.hpp", "filename": "constants.hpp", "file": "constants.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/utilities/check_input.hpp": {"path": "layer-4/cuopt/cpp/src/routing/utilities/check_input.hpp", "filename": "check_input.hpp", "file": "check_input.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/utilities/env_utils.hpp": {"path": "layer-4/cuopt/cpp/src/routing/utilities/env_utils.hpp", "filename": "env_utils.hpp", "file": "env_utils.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/utilities/md_utils.hpp": {"path": "layer-4/cuopt/cpp/src/routing/utilities/md_utils.hpp", "filename": "md_utils.hpp", "file": "md_utils.hpp", "brief": "cuOpt vehicle routing component\n\nPart of GPU-accelerated VRP solver.", "has_pass2": false}, "cpp/src/routing/local_search/cycle_finder/device_bitset.hpp": {"path": "layer-4/cuopt/cpp/src/routing/local_search/cycle_finder/device_bitset.hpp", "filename": "device_bitset.hpp", "file": "device_bitset.hpp", "brief": "cuOpt VRP local search\n\nLocal search moves for route improvement.\n2-opt, Or-opt, swap, relocate operators.", "has_pass2": false}, "cpp/src/routing/local_search/cycle_finder/cycle_graph.hpp": {"path": "layer-4/cuopt/cpp/src/routing/local_search/cycle_finder/cycle_graph.hpp", "filename": "cycle_graph.hpp", "file": "cycle_graph.hpp", "brief": "cuOpt VRP local search\n\nLocal search moves for route improvement.\n2-opt, Or-opt, swap, relocate operators.", "has_pass2": false}, "cpp/src/routing/local_search/cycle_finder/cycle.hpp": {"path": "layer-4/cuopt/cpp/src/routing/local_search/cycle_finder/cycle.hpp", "filename": "cycle.hpp", "file": "cycle.hpp", "brief": "cuOpt VRP local search\n\nLocal search moves for route improvement.\n2-opt, Or-opt, swap, relocate operators.", "has_pass2": false}, "cpp/src/routing/local_search/cycle_finder/cycle_finder.hpp": {"path": "layer-4/cuopt/cpp/src/routing/local_search/cycle_finder/cycle_finder.hpp", "filename": "cycle_finder.hpp", "file": "cycle_finder.hpp", "brief": "cuOpt VRP local search\n\nLocal search moves for route improvement.\n2-opt, Or-opt, swap, relocate operators.", "has_pass2": false}, "cpp/src/linear_programming/restart_strategy/weighted_average_solution.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/restart_strategy/weighted_average_solution.hpp", "filename": "weighted_average_solution.hpp", "file": "weighted_average_solution.hpp", "brief": "cuOpt LP solution storage\n\nPrimal/dual solution and basis information.", "has_pass2": false}, "cpp/src/linear_programming/restart_strategy/localized_duality_gap_container.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/restart_strategy/localized_duality_gap_container.hpp", "filename": "localized_duality_gap_container.hpp", "file": "localized_duality_gap_container.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/termination_strategy/termination_strategy.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/termination_strategy/termination_strategy.hpp", "filename": "termination_strategy.hpp", "file": "termination_strategy.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/termination_strategy/infeasibility_information.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/termination_strategy/infeasibility_information.hpp", "filename": "infeasibility_information.hpp", "file": "infeasibility_information.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/termination_strategy/convergence_information.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/termination_strategy/convergence_information.hpp", "filename": "convergence_information.hpp", "file": "convergence_information.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/step_size_strategy/adaptive_step_size_strategy.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/step_size_strategy/adaptive_step_size_strategy.hpp", "filename": "adaptive_step_size_strategy.hpp", "file": "adaptive_step_size_strategy.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/linear_programming/utilities/logger_init.hpp": {"path": "layer-4/cuopt/cpp/src/linear_programming/utilities/logger_init.hpp", "filename": "logger_init.hpp", "file": "logger_init.hpp", "brief": "cuOpt linear programming component\n\nPart of GPU-accelerated LP solver.", "has_pass2": false}, "cpp/src/mip/diversity/diversity_config.hpp": {"path": "layer-4/cuopt/cpp/src/mip/diversity/diversity_config.hpp", "filename": "diversity_config.hpp", "file": "diversity_config.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/mip/local_search/local_search_config.hpp": {"path": "layer-4/cuopt/cpp/src/mip/local_search/local_search_config.hpp", "filename": "local_search_config.hpp", "file": "local_search_config.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/mip/presolve/third_party_presolve.hpp": {"path": "layer-4/cuopt/cpp/src/mip/presolve/third_party_presolve.hpp", "filename": "third_party_presolve.hpp", "file": "third_party_presolve.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/mip/presolve/gf2_presolve.hpp": {"path": "layer-4/cuopt/cpp/src/mip/presolve/gf2_presolve.hpp", "filename": "gf2_presolve.hpp", "file": "gf2_presolve.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/mip/diversity/recombiners/recombiner_configs.hpp": {"path": "layer-4/cuopt/cpp/src/mip/diversity/recombiners/recombiner_configs.hpp", "filename": "recombiner_configs.hpp", "file": "recombiner_configs.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/src/mip/diversity/recombiners/recombiner_stats.hpp": {"path": "layer-4/cuopt/cpp/src/mip/diversity/recombiners/recombiner_stats.hpp", "filename": "recombiner_stats.hpp", "file": "recombiner_stats.hpp", "brief": "cuOpt MIP solver component\n\nPart of GPU-accelerated mixed-integer solver.", "has_pass2": false}, "cpp/include/cuopt/logger.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/logger.hpp", "filename": "logger.hpp", "file": "cuopt/logger.hpp", "brief": "Logging infrastructure for cuOpt based on rapids_logger\n\nProvides configurable logging for cuOpt operations.\n\n**Configuration:**\n- CUOPT_DEBUG_LOG_FILE env var: Log to file instead of stderr\n- Log levels: trace, debug, info, warn, error, critical\n\n**Functions:**\n- default_logger(): Get/create the global cuOpt logger\n- reset_default_logger(): Reset to default configuration\n- default_sink(): Get file or stderr sink\n- default_pattern(): Timestamp format string", "see": ["cuopt/logger_macros.hpp for logging macros"], "return": "sink_ptr The sink to use", "has_pass2": false}, "cpp/include/cuopt/error.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/error.hpp", "filename": "error.hpp", "file": "cuopt/error.hpp", "brief": "Error handling and exception types for cuOpt\n\nDefines error types and macros for cuOpt error handling.\n\n**Error Types (error_type_t):**\n- Success: Operation completed successfully\n- ValidationError: Input validation failed\n- OutOfMemoryError: GPU/CPU memory allocation failed\n- RuntimeError: General runtime error\n\n**Exception Class:**\n- logic_error: Exception with error_type_t classification\n\n**Macros:**\n- EXE_CUOPT_EXPECTS(cond, fmt, ...): Assert with formatted message\n- EXE_CUOPT_FAIL(fmt, ...): Unconditional failure", "see": ["cuopt/linear_programming/constants.h for error code constants"], "return": "Pointer to a null-terminated string with explanatory information.", "param": ["error error_type_t type enum value", "variable set of arguments used for fmt\n@throw cuopt::logic_error if the condition evaluates to false.", "args"], "tparam": ["Args"], "has_pass2": false}, "cpp/include/cuopt/routing/data_model_view.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/data_model_view.hpp", "filename": "data_model_view.hpp", "file": "cuopt/routing/data_model_view.hpp", "brief": "VRP problem definition via non-owning GPU memory views\n\nContainer for Vehicle Routing Problem input data.\n\n**data_model_view_t Class:**\n- Non-owning views to GPU memory for routing problem data\n- Supports heterogeneous fleet with multiple vehicle types\n- Configurable constraints and objectives\n\n**Cost/Time Matrices:**\n- add_cost_matrix(): Travel cost between locations\n- add_transit_time_matrix(): Travel time for constraint checking\n- Multiple matrices for heterogeneous fleet\n\n**Constraints:**\n- add_capacity_dimension(): Vehicle capacity limits (CVRP)\n- set_order_time_windows(): Delivery time windows (VRPTW)\n- set_pickup_delivery_pairs(): PDP pickup-before-delivery\n- add_order_precedence(): Order sequencing constraints\n- set_vehicle_time_windows(): Vehicle availability windows\n- add_vehicle_break(): Required driver breaks\n\n**Fleet Configuration:**\n- set_vehicle_locations(): Start/return depot per vehicle\n- set_vehicle_types(): Vehicle type identifiers\n- set_drop_return_trips(): Open-ended routes\n- add_vehicle_order_match(): Vehicle-order compatibility", "see": ["cuopt/routing/solve.hpp for solving", "cuopt/routing/assignment.hpp for solution output"], "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed.", "f_t Floating point type. Needs to be float (32bit) at the moment.\nPlease open an issue if other type are needed."], "param": ["num_locations number of locations to visit, including the depot.", "fleet_size number of vehicles in the fleet. This is primarily used\nto model vehicle properties in a mixed fleet context. cuOpt solution\ncontains the smallest possible number of vehicles which may be smaller or\nequal to the fleet_size.", "vehicle_id", "break_earliest", "break_latest", "break_duration", "break_locations", "num_break_locations", "validate_input", "vehicle_id  vehicle id that has constraints", "orders      device memory pointer to integer values corresponding to\nlist of orders", "norders     number of customer orders that are served by this\nvehicle", "order_id    order id that has constraints", "vehicles    device memory pointer to integer values corresponding to\nlist of vehicles", "nvehicles   number of vehicles that can serve this order", "vehicle_id  vehicle id that has constraints", "service_times      device memory pointer to integer values\ncorresponding to list of service_times", "order_id Order id that has a precedence constraint.", "preceding_orders The orders that need to be scheduled prior to\nnode_id", "n_preceding_orders Number of prior orders."], "return": "Matrix pointer", "has_pass2": false}, "cpp/include/cuopt/routing/routing_structures.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/routing_structures.hpp", "filename": "routing_structures.hpp", "file": "cuopt/routing/routing_structures.hpp", "brief": "Core data structures for VRP problem specification\n\nEnums and helper classes for VRP constraint modeling.\n\n**Objective Types (objective_t):**\n- COST: Total route cost from cost matrix\n- TRAVEL_TIME: Total driving time (excluding waits)\n- VARIANCE_ROUTE_SIZE: Balance route lengths\n- VARIANCE_ROUTE_SERVICE_TIME: Balance service times\n- PRIZE: Sum of collected order prizes\n- VEHICLE_FIXED_COST: Per-vehicle usage cost\n\n**Node Types (node_type_t):**\n- DEPOT: Start/end location for vehicles\n- PICKUP: Pickup location for PDP\n- DELIVERY: Delivery location for PDP\n- BREAK: Driver break location\n\n**Internal Detail Classes:**\n- break_dimension_t: Uniform break constraints\n- vehicle_break_t: Per-vehicle break constraints\n- capacity_t: Capacity dimension (demand/capacity pair)\n- order_time_window_t: Time window bounds\n- vehicle_time_window_t: Vehicle availability bounds", "see": ["cuopt/routing/data_model_view.hpp for using these structures"], "has_pass2": false}, "cpp/include/cuopt/routing/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "cuopt/routing/solver_settings.hpp", "brief": "Configuration options for VRP solver behavior\n\nControls solver runtime behavior and output options.\n\n**solver_settings_t Class:**\n- set_time_limit(): Maximum solving time in seconds\n- set_verbose_mode(): Enable internal progress output\n- set_error_logging_mode(): Log constraint violations\n- dump_best_results(): Write progress to CSV file\n\n**Time Limit Guidelines:**\n- Small problems (<100 locations): ~1 second\n- Large problems: num_locations/5 seconds default\n- More time → better solution quality", "see": ["cuopt/routing/solve.hpp for solver entry point"], "return": "Solving time set in seconds", "has_pass2": false}, "cpp/include/cuopt/routing/solve.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/solve.hpp", "filename": "solve.hpp", "file": "cuopt/routing/solve.hpp", "brief": "GPU-accelerated Vehicle Routing Problem (VRP) solver entry point\n\nMain solve function for vehicle routing optimization on GPU.\n\n**solve() Function:**\n- Takes data_model_view_t (problem) and solver_settings_t (config)\n- Returns assignment_t with optimized routes\n\n**Supported VRP Variants:**\n- CVRP: Capacitated VRP with vehicle capacity constraints\n- VRPTW: VRP with time windows at customer locations\n- VRPPD: VRP with pickup and delivery pairs\n- Heterogeneous fleet: Different vehicle types/costs\n- Multi-depot: Multiple start/return locations\n\n**Algorithm:**\n- Genetic algorithm with GPU-parallel population\n- EAX/SREX crossover operators\n- Local search refinement", "see": ["cuopt/routing/data_model_view.hpp for problem input", "cuopt/routing/solver_settings.hpp for configuration", "cuopt/routing/assignment.hpp for solution output"], "tparam": ["i_t", "f_t"], "return": "assignment_t<i_t> owning container for the solver output", "has_pass2": false}, "cpp/include/cuopt/routing/assignment.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/assignment.hpp", "filename": "assignment.hpp", "file": "cuopt/routing/assignment.hpp", "brief": "VRP solution output container with routes and timing\n\nContainer for Vehicle Routing Problem solver output.\n\n**assignment_t Class:**\n- Optimized routes stored in GPU device memory\n- Arrival times, truck assignments, node types\n- Objective values and solution status\n\n**Solution Data:**\n- get_route(): Ordered sequence of node IDs\n- get_truck_id(): Vehicle assignment per stop\n- get_arrival_stamp(): Arrival time at each stop\n- get_unserviced_nodes(): Orders not serviced (if any)\n\n**Solution Status (solution_status_t):**\n- SUCCESS: Feasible solution found\n- INFEASIBLE: No feasible solution exists\n- TIMEOUT: Time limit reached\n- EMPTY: Solver did not run\n- ERROR: Runtime error occurred\n\n**host_assignment_t:**\n- Host-side copy for CPU access\n- std::vector storage for routes and stamps", "see": ["cuopt/routing/solve.hpp for generating solutions", "cuopt/routing/data_model_view.hpp for problem input"], "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed."], "param": ["status Solution status.", "stream_view Non-owning stream_view object.", "error_status Error status.", "stream_view Non-owning stream_view object.", "vehicle_count Number of vehicles in the solution.", "total_objective_value Total objective value of the solution.", "objective_values Objective value of each objective", "route Device vector containing the ordered node ids.", "arrival_stamp Device vector containing arrival time of each node.", "truck_id Device vector containing truck id of each node.", "route_locations Device vector containing the location of orders", "node_type Device vector containing the type of the order", "unserviced_nodes Device vector containing unserviced orders", "accepted Device vector containing accepted solutions", "status Solution status.", "solution_string Solution string explaining the status.", "filename Name of the output file", "stream_view Non-owning stream view object", "vehicle_count Vehicle count", "status Solution status", "os Output stream to print"], "return": "Best objective value", "has_pass2": false}, "cpp/include/cuopt/linear_programming/optimization_problem.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/optimization_problem.hpp", "filename": "optimization_problem.hpp", "file": "cuopt/linear_programming/optimization_problem.hpp", "brief": "GPU-resident linear/integer programming problem representation\n\nContainer for LP/MIP optimization problems stored in GPU memory.\n\n**optimization_problem_t Class:**\n- Represents: min/max c'x s.t. Ax ≤/=/≥ b, l ≤ x ≤ u\n- Constraint matrix A in CSR (Compressed Sparse Row) format\n- GPU device memory via rmm::device_uvector\n\n**Problem Components:**\n- set_csr_constraint_matrix(): Sparse constraint matrix\n- set_constraint_bounds(): RHS values (b)\n- set_objective_coefficients(): Cost vector (c)\n- set_variable_lower/upper_bounds(): Variable bounds\n- set_variable_types(): CONTINUOUS or INTEGER\n\n**Variable Types:**\n- var_t::CONTINUOUS: Real-valued variable\n- var_t::INTEGER: Integer-valued variable\n\n**Problem Categories:**\n- LP: All continuous variables\n- MIP: Mixed integer and continuous\n- IP: All integer variables", "see": ["cuopt/linear_programming/solve.hpp for solve functions"], "tparam": ["f_t  Data type of the variables and their weights in the equations\n\nThis structure stores all the information necessary to represent the\nfollowing LP:\n\n<pre>\nMinimize:\n  dot(c, x)\nSubject to:\n  matmul(A, x) (= or >= or)<= b\nWhere:\n  x = n-dim vector\n  A = mxn-dim sparse matrix\n  n = number of variables\n  m = number of constraints\n\n</pre>\n\n@note: By default this assumes objective minimization.\n\nObjective value can be scaled and offset accordingly:\nobjective_scaling_factor * (dot(c, x) + objective_offset)\nplease refer to the `set_objective_scaling_factor()` and\n`set_objective_offset()` methods."], "param": ["size_values Size of the A_values array.", "size_indices Size of the A_indices array.", "size_offsets Size of the A_offsets array.", "size Size of the b array.", "size Size of the c array.", "objective_scaling_factor Objective scaling factor value.", "objective_offset Objective offset value.", "size Size of the variable_lower_bounds array", "size Size of the variable_upper_bounds array.", "size Size of the variable_types array.", "size Size of the constraint_lower_bounds array", "size Size of the constraint_upper_bounds array", "size Size of the row_types array"], "has_pass2": false}, "cpp/include/cuopt/linear_programming/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "solver_settings.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/solve.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/solve.hpp", "filename": "solve.hpp", "file": "cuopt/linear_programming/solve.hpp", "brief": "GPU-accelerated LP and MIP solving functions\n\nMain entry points for linear programming optimization on GPU.\n\n**LP Solving (PDLP):**\n- solve_lp(): GPU-accelerated first-order LP solver\n- Uses Primal-Dual Hybrid Gradient (PDHG) algorithm\n- Supports warm starting from previous solutions\n\n**MIP Solving:**\n- solve_mip(): Mixed-integer programming with GPU B&B\n- Branch-and-bound with LP relaxations\n\n**Input Formats:**\n- optimization_problem_t: Native cuOpt problem representation\n- mps_data_model_t: MPS file format parser output", "see": ["cuopt/linear_programming/optimization_problem.hpp for problem definition", "cuopt/linear_programming/pdlp/solver_settings.hpp for LP settings", "cuopt/linear_programming/mip/solver_settings.hpp for MIP settings"], "tparam": ["i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations", "i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations", "i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations", "i_t Data type of indexes", "f_t Data type of the variables and their weights in the equations"], "return": "optimization_problem_solution_t<i_t, f_t> owning container for the solver solution", "has_pass2": false}, "cpp/include/cuopt/utilities/timestamp_utils.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/utilities/timestamp_utils.hpp", "filename": "timestamp_utils.hpp", "file": "timestamp_utils.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "return": "true if extra timestamps are enabled, false otherwise", "param": ["label The label to print with the timestamp"], "has_pass2": false}, "cpp/include/cuopt/routing/distance_engine/waypoint_matrix.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/distance_engine/waypoint_matrix.hpp", "filename": "waypoint_matrix.hpp", "file": "waypoint_matrix.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "tparam": ["i_t Integer type. int (32bit) is expected at the moment. Please\nopen an issue if other type are needed.", "f_t Floating point type. float (32bit) is expected at the moment.\nPlease open an issue if other type are needed."], "param": ["n_vertices Number of vertices", "n_target_locations Number of target locations.", "n_target_locations Number of target locations", "n_locations Number of locations", "n_target_locations Number of target locations."], "return": "std::pair<rmm::device_uvector<int>, rmm::device_uvector<int>> First is a device buffer\nof size L (L : n_locations) containing an array of offsets. Second is a device buffer\ncontaining the full path for all the route. For each element in the route, the corresponding\nfull route can accessed in the full path through the offsets array.", "has_pass2": false}, "cpp/include/cuopt/routing/cython/generator.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/cython/generator.hpp", "filename": "generator.hpp", "file": "generator.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "tparam": ["i_t Integer type. Needs to be int (32bit) at the moment. Please open\nan issue if other type are needed.", "f_t Floating point type. Needs to be float (32bit) at the moment."], "has_pass2": false}, "cpp/include/cuopt/routing/cython/cython.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/routing/cython/cython.hpp", "filename": "cython.hpp", "file": "cython.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/pdlp/pdlp_warm_start_data.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/pdlp/pdlp_warm_start_data.hpp", "filename": "pdlp_warm_start_data.hpp", "file": "pdlp_warm_start_data.hpp", "brief": "cuOpt GPU linear programming API\n\nPublic API for GPU-accelerated linear programming.\nCUDA-based simplex and interior point methods.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/pdlp/solver_solution.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/pdlp/solver_solution.hpp", "filename": "solver_solution.hpp", "file": "solver_solution.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "tparam": ["i_t Integer type. Currently only int is supported.", "f_t Floating point type. Currently only float (32bit) and double (64bit) are supported."], "param": ["ms Time in ms", "termination_status termination reason", "filename Name of the output file", "stream_view Non-owning stream view object", "filename Name of the output file", "stream_view Non-owning stream view object", "handle_ptr The handle pointer", "other The other solution object"], "return": "Time in seconds", "has_pass2": false}, "cpp/include/cuopt/linear_programming/pdlp/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/pdlp/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "cuopt/linear_programming/pdlp/solver_settings.hpp", "brief": "PDLP first-order LP solver configuration\n\nSettings for GPU-accelerated Primal-Dual Hybrid Gradient LP solver.\n\n**pdlp_solver_mode_t Modes:**\n- Stable3: Best overall balance (default)\n- Stable2: Legacy stable mode\n- Methodical1: Slower steps but fewer iterations, 1.3-1.7x memory\n- Fast1: Highest speed, lower convergence success\n\n**pdlp_solver_settings_t Class:**\n- set_solver_mode(): Select algorithm mode\n- set_eps_optimal(): Primal/dual convergence tolerance\n- set_time_limit(): Maximum solve time\n- set_warm_start(): Initialize from previous solution\n\n**Algorithm (PDHG):**\n- First-order method, highly parallelizable on GPU\n- O(1/k) convergence for LP\n- Efficient for large sparse problems", "see": ["cuopt/linear_programming/solve.hpp for solve_lp()"], "param": ["eps_optimal Tolerance to optimality", "size Size of the initial_primal_solution array.", "size Size of the initial_dual_solution array.", "pdlp_warm_start_data_view Pdlp warm start data from your solution\nobject to warm start from", "var_mapping Variables indices to scatter to in case the new problem\nhas less variables", "constraint_mapping Constraints indices to scatter to in case the new\nproblem has less constraints"], "return": "pdlp warm start data", "has_pass2": false}, "cpp/include/cuopt/linear_programming/mip/solver_solution.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/mip/solver_solution.hpp", "filename": "solver_solution.hpp", "file": "solver_solution.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/mip/solver_settings.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/mip/solver_settings.hpp", "filename": "solver_settings.hpp", "file": "cuopt/linear_programming/mip/solver_settings.hpp", "brief": "Mixed-integer programming solver configuration\n\nSettings for GPU-accelerated MIP solver.\n\n**mip_solver_settings_t Class:**\n- set_mip_callback(): User callback for incumbent solutions\n- set_time_limit(): Maximum solve time\n- set_mip_gap(): Optimality gap tolerance\n- set_verbosity(): Output level control\n\n**MIP Algorithm:**\n- Branch-and-bound with GPU LP relaxations\n- Population-based heuristics\n- Recombination operators for primal improvement", "see": ["cuopt/linear_programming/solve.hpp for solve_mip()", "cuopt/linear_programming/mip/solver_solution.hpp for output"], "param": ["size Size of the initial_solution array."], "return": "callback pointer", "has_pass2": false}, "cpp/include/cuopt/linear_programming/mip/solver_stats.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/mip/solver_stats.hpp", "filename": "solver_stats.hpp", "file": "solver_stats.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/utilities/callbacks_implems.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/utilities/callbacks_implems.hpp", "filename": "callbacks_implems.hpp", "file": "callbacks_implems.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/utilities/internals.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/utilities/internals.hpp", "filename": "internals.hpp", "file": "internals.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/utilities/cython_solve.hpp": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/utilities/cython_solve.hpp", "filename": "cython_solve.hpp", "file": "cython_solve.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/libmps_parser/src/mps_parser.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/src/mps_parser.hpp", "filename": "mps_parser.hpp", "file": "mps_parser.hpp", "brief": "cuOpt MPS file parser\n\nReads LP/MIP problems in MPS format.\nHandles fixed and free format MPS files.", "tparam": ["f_t  data type of the weights and variables\n\n@note this parser assumes that the sections occur in the following order:\n      `NAME -> ROWS -> COLUMNS -> RHS`"], "has_pass2": false}, "cpp/libmps_parser/src/utilities/error.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/src/utilities/error.hpp", "filename": "error.hpp", "file": "error.hpp", "brief": "cuOpt MPS file parser\n\nReads LP/MIP problems in MPS format.\nHandles fixed and free format MPS files.", "param": ["error error_type_t type enum value", "variable set of arguments used for fmt\n@throw std::logic_error if the condition evaluates to false.", "variable set of arguments used for fmt\n@throw std::logic_error if the condition evaluates to false."], "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/parser.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/parser.hpp", "filename": "parser.hpp", "file": "parser.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "return": "mps_data_model_t A fully formed LP/QP problem which represents the given file", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/data_model_view.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/data_model_view.hpp", "filename": "data_model_view.hpp", "file": "data_model_view.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "tparam": ["f_t  Data type of the variables and their weights in the equations\n\nA linear programming optimization problem is defined as follows:\n<pre>\nMinimize:\n  dot(c, x)\nSubject to:\n  matmul(A, x) (= or >= or)<= b\nWhere:\n  x = n-dim vector\n  A = mxn-dim sparse matrix\n  n = number of variables\n  m = number of constraints\n\n</pre>\n\n@note: By default this assumes objective minimization.\n\nObjective value can be scaled and offset accordingly:\nobjective_scaling_factor * (dot(c, x) + objective_offset)\nplease refeto to the `set_objective_scaling_factor()` and `set_objective_offset()` method."], "param": ["size_values Size of the A_values array.", "size_indices Size of the A_indices array.", "size_offsets Size of the A_offsets array.", "size Size of the b array.", "size Size of the c array.", "objective_scaling_factor Objective scaling factor value.", "objective_offset Objective offset value.", "size Size of the variable_lower_bounds array", "size Size of the variable_upper_bounds array.", "size Size of the variable_types array.", "size Size of the row_types array", "size Size of the row_types array", "size Size of the row_types array", "size Size of the initial_primal_solution array.", "size Size of the initial_dual_solution array.", "size_values Size of the Q_values array", "size_indices Size of the Q_indices array", "size_offsets Size of the Q_offsets array"], "return": "Sense value", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/writer.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/writer.hpp", "filename": "writer.hpp", "file": "writer.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/mps_writer.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/mps_writer.hpp", "filename": "mps_writer.hpp", "file": "mps_writer.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "tparam": ["f_t  data type of the weights and variables", "i_t  data type of the indices"], "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/mps_data_model.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/mps_data_model.hpp", "filename": "mps_data_model.hpp", "file": "mps_data_model.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "tparam": ["f_t  Data type of the variables and their weights in the equations\n\nA linear programming optimization problem is defined as follows:\n<pre>\nMinimize:\n  dot(c, x)\nSubject to:\n  matmul(A, x) (= or >= or)<= b\nWhere:\n  x = n-dim vector\n  A = mxn-dim sparse matrix\n  n = number of variables\n  m = number of constraints\n\n</pre>\n\n@note: By default this assumes objective minimization.\n\nObjective value can be scaled and offset accordingly:\nobjective_scaling_factor * (dot(c, x) + objective_offset)\nplease refer to the `set_objective_scaling_factor()` and `set_objective_offset()` methods."], "param": ["size_values Size of the A_values array.", "size_indices Size of the A_indices array.", "size_offsets Size of the A_offsets array.", "size Size of the b array.", "size Size of the c array.", "objective_scaling_factor Objective scaling factor value.", "objective_offset Objective offset value.", "size Size of the variable_lower_bounds array", "size Size of the variable_upper_bounds array.", "size Size of the constraint_lower_bounds array", "size Size of the constraint_upper_bounds array", "size Size of the row_types array", "size Size of the initial_primal_solution array.", "size Size of the initial_dual_solution array.", "size_values Size of the Q_values array", "size_indices Size of the Q_indices array", "size_offsets Size of the Q_offsets array"], "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/utilities/cython_mps_parser.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/utilities/cython_mps_parser.hpp", "filename": "cython_mps_parser.hpp", "file": "cython_mps_parser.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/libmps_parser/include/mps_parser/utilities/span.hpp": {"path": "layer-4/cuopt/cpp/libmps_parser/include/mps_parser/utilities/span.hpp", "filename": "span.hpp", "file": "span.hpp", "brief": "cuOpt public API header\n\nPublic interface for NVIDIA cuOpt optimization library.", "has_pass2": false}, "cpp/include/cuopt/linear_programming/cuopt_c.h": {"path": "layer-4/cuopt/cpp/include/cuopt/linear_programming/cuopt_c.h", "filename": "cuopt_c.h", "brief": "A ``cuOptOptimizationProblem`` object contains a representation of\nan LP or MIP. It is created by ``cuOptCreateProblem`` or ``cuOptCreateRangedProblem``.\nIt is passed to ``cuOptSolve``. It should be destroyed using ``cuOptDestroyProblem``.", "return": "The size in bytes of the float type.", "has_pass2": false}}}}}}, "stats": {"total_layers": 5, "total_libraries": 28, "total_files": 1902, "total_pass2_files": 692}}